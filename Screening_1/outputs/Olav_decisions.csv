paper_id,title,abstract,authors,year,doi,link,database,llm_summary,i1_hint,i2_hint,i3_hint,model,reviewer,decision,notes,timestamp
7887726f5a97f696,A Bayesian approach to term structure modeling using heavy-tailed distributions,"In this paper, we introduce a robust extension of the three-factor model of Diebold and Li (J. Econometrics, 130: 337-364, 2006) using the class of symmetric scale mixtures of normal distributions. Specific distributions examined include the multivariate normal, Student-t, slash, and variance gamma distributions. In the presence of non-normality in the data, these distributions provide an appealing robust alternative to the routine use of the normal distribution. Using a Bayesian paradigm, we developed an efficient MCMC algorithm for parameter estimation. Moreover, the mixing parameters obtained as a by-product of the scale mixture representation can be used to identify outliers. Our results reveal that the Diebold-Li models based on the Student-t and slash distributions provide significant improvement in in-sample fit and out-of-sample forecast to the US yield data than the usual normal-based model. Copyright © 2011 John Wiley & Sons, Ltd. © 2012 Elsevier B.V., All rights reserved.","Abanto-Valle, C.A.; Lachos, V.H.; Ghosh, P.",2012,10.1002/asmb.920,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867581627&doi=10.1002%2Fasmb.920&partnerID=40&md5=d0718ed07907681fd60bc5e4fb51dca0,scopus,"This paper proposes a robust extension of the Diebold and Li three-factor model for term structure modeling by incorporating heavy-tailed distributions (multivariate normal, Student-t, slash, and variance gamma). A Bayesian approach with an MCMC algorithm is used for parameter estimation, and mixing parameters help identify outliers. The study demonstrates that models using Student-t and slash distributions offer improved in-sample fit and out-of-sample forecasts for US yield data compared to the standard normal-based model.",True,True,True,gemini-2.5-flash-lite,Olav,M,,2025-10-13T15:16:20.479045
22865f3093f9375d,"A Comparison of Neural Network, Statistical Methods, and Variable Choice for Life Insurers' Financial Distress Prediction","This study examines the effect of the statistical-mathematical model selected and the variable set considered on the ability to identify financially troubled life insurers. Models considered are two artificial neural network methods (back-propagation and learning vector quantization (LVQ)) and two more standard statistical methods (multiple discriminant analysis and logistic regression analysis). The variable sets considered are the insurance regulatory information system (IRIS) variables, the financial analysis solvency tracking (FAST) variables, and Texas early warning information system (EWIS) variables, and a data set consisting of twenty-two variables selected by us in conjunction with the research staff at TDI and a review of the insolvency prediction literature. The results show that the back-propagation (BP) and LVQ outperform the traditional statistical approaches for all four variable sets with a consistent superiority across the two different evaluation criteria (total misclassification cost and resubstitution risk criteria), and that the twenty-two variables and the Texas EWIS variable sets are more efficient than the IRIS and the FAST variable sets for identification of financially troubled life insurers in most comparisons.",,2006,10.1111/j.1539-6975.2006.00181.x,,proquest,"This study compares neural network methods (back-propagation and LVQ) with statistical methods (discriminant analysis and logistic regression) for predicting financial distress in life insurers. It evaluates the impact of different variable sets (IRIS, FAST, EWIS, and a custom set) on prediction accuracy. The findings indicate that neural networks, particularly back-propagation, generally outperform statistical methods, and a custom variable set of twenty-two variables along with the Texas EWIS variables are more effective for identifying distressed insurers.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:16:38.007344
aaa155be6197dbf8,A Cox model for gradually disappearing events,"Innovations in medicine provide us longer and healthier life, leading lower mortality. Sooner rather than later, much greater longevity would be possible for us due to artificial intelligence advances in health care. Similarly, Advanced Driver Assistance Systems (ADAS) in highly automated vehicles may reduce or even eventually eliminate accidents by perceiving dangerous situations, which would minimize the number of accidents and lead to fewer loss claims for insurance companies. To model the survivor function capturing greater longevity as well as the number of claims reflecting less accidents in the long run, in this paper, we study a Cox process whose intensity process is piecewise-constant and decreasing. We derive its ultimate distributional properties, such as the Laplace transform of intensity integral process, the probability generating function of point process, their associated moments and cumulants, and the probability of no more claims for a given time point. In general, this simple model may be applicable in many other areas for modeling the evolution of gradually disappearing events, such as corporate defaults, dividend payments, trade arrivals, employment of a certain job type (e.g., typists) in the labor market, and release of particles. In particular, we discuss some potential applications to insurance.","Jang, Jiwook; Qu, Yan; Zhao, Hongbiao; Dassios, Angelos",2023,10.1017/s0269964821000553,,wos,"This paper proposes a Cox process model with a piecewise-constant and decreasing intensity process to capture gradually disappearing events, such as reduced accidents due to advanced driver assistance systems or increased longevity due to medical advances. The model's distributional properties, moments, and the probability of no further events are derived. Potential applications in insurance and other fields are discussed.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:16:54.306889
3cf2316bbcf2ce76,A Hybrid Vector Autoregressive Model for Accurate Macroeconomic Forecasting: An Application to the U.S. Economy,"Forecasting macroeconomic variables is essential to macroeconomics, financial economics, and monetary policy analysis. Due to the high dimensionality of the macroeconomic dataset, it is challenging to forecast efficiently and accurately. Thus, this study provides a comprehensive analysis of predicting macroeconomic variables by comparing various vector autoregressive models followed by different estimation techniques. To address this, this paper proposes a novel hybrid model based on a smoothly clipped absolute deviation estimation method and a vector autoregression model that combats the curse of dimensionality and simultaneously produces reliable forecasts. The proposed hybrid model is applied to the U.S. quarterly macroeconomic data from the first quarter of 1959 to the fourth quarter of 2023, yielding multi-step-ahead forecasts (one-, three-, and six-step ahead). The multi-step-ahead out-of-sample forecast results (root mean square error and mean absolute error) for the considered data suggest that the proposed hybrid model yields a highly accurate and efficient gain. Additionally, it is demonstrated that the proposed models outperform the baseline models. Finally, the authors believe the proposed hybrid model may be expanded to other countries to assess its efficacy and accuracy.",,2025,10.3390/math13111706,,proquest,"This study proposes a novel hybrid vector autoregressive (VAR) model using a smoothly clipped absolute deviation estimation method for accurate macroeconomic forecasting. Applied to U.S. quarterly data, the model effectively addresses the curse of dimensionality and outperforms baseline models in multi-step-ahead out-of-sample forecasts.",True,False,True,gemini-2.5-flash-lite,Olav,M,,2025-10-13T15:17:29.812929
f1779cf7208a0e34,A MODEL-SELECTION APPROACH TO ASSESSING THE INFORMATION IN THE TERM STRUCTURE USING LINEAR-MODELS AND ARTIFICIAL NEURAL NETWORKS,"We take a model-selection approach to the question of whether forward-interest rates are useful in predicting future spot rates, using a variety of out-of-sample forecast-based model-selection criteria-forecast mean squared error, forecast direction accuracy, and forecast-based trading-system profitability. We also examine the usefulness of a class of novel prediction models called artificial neural networks and investigate the issue of appropriate window sizes for rolling-window-based prediction methods. Results indicate that the premium of the forward rate over the spot rate helps to predict the sign of future changes in the interest rate. Furthermore, model selection based on an in-sample Schwarz information criterion (SIC) does not appear to be a reliable guide to out-of-sample performance in the case of short-term interest rates. Thus, the in-sample SIC apparently fails to offer a convenient shortcut to true out-of-sample performance measures.","SWANSON, NR; WHITE, H",1995,10.2307/1392186,,wos,"This study uses a model-selection approach to evaluate the predictive power of forward interest rates for future spot rates, employing criteria such as forecast mean squared error, direction accuracy, and trading system profitability. It also explores the utility of artificial neural networks and optimal window sizes for prediction models. The findings suggest that the forward rate premium can predict the direction of future interest rate changes, but in-sample model selection criteria like the Schwarz Information Criterion (SIC) do not reliably predict out-of-sample performance for short-term rates.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T15:18:15.236661
5d4eeb7e05f11ede,A Nonlinear Factor Analysis of S&P 500 Index Option Returns,"Growing evidence suggests that extraordinary average returns may be obtained by trading equity index options, and that at least part of this abnormal performance is attributable to volatility and jump risk premia. This paper asks whether such priced risk factors are alone sufficient to explain these average returns. To provide an answer in as general as possible a setting, I estimate a flexible class of nonlinear models using all S&P 500 Index futures options traded between 1986 and 2000. The results show that priced factors contribute to these expected returns but are insufficient to explain their magnitudes, particularly for short-term out-of-the-money puts.",,2006,10.1111/j.1540-6261.2006.01059.x,,proquest,"This paper investigates whether priced risk factors are sufficient to explain the extraordinary average returns observed in S&P 500 Index options. Using a nonlinear factor analysis on options traded between 1986 and 2000, the study finds that while priced factors contribute to expected returns, they do not fully account for their magnitudes, especially for short-term out-of-the-money puts.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:18:39.634312
77af4344c3d8f866,A Stock Market Decision-Making Framework Based on CMR-DQN,"In the dynamic and uncertain stock market, precise forecasting and decision-making are crucial for profitability. Traditional deep neural networks (DNN) often struggle with capturing long-term dependencies and multi-scale features in complex financial time series data. To address these challenges, we introduce CMR-DQN, an innovative framework that integrates discrete wavelet transform (DWT) for multi-scale data analysis, temporal convolutional network (TCN) for extracting deep temporal features, and a GRU–LSTM–Attention mechanism to enhance the model’s focus and memory. Additionally, CMR-DQN employs the Rainbow DQN reinforcement learning strategy to learn optimal trading strategies in a simulated environment. CMR-DQN significantly improved the total return rate on six selected stocks, with increases ranging from 20.37% to 55.32%. It also demonstrated substantial improvements over the baseline model in terms of Sharpe ratio and maximum drawdown, indicating increased excess returns per unit of total risk and reduced investment risk. These results underscore the efficiency and effectiveness of CMR-DQN in handling multi-scale time series data and optimizing stock market decisions.",,2024,10.3390/app14166881,,proquest,"This paper proposes CMR-DQN, a novel framework for stock market decision-making that combines discrete wavelet transform (DWT) for multi-scale analysis, temporal convolutional network (TCN) for feature extraction, and a GRU-LSTM-Attention mechanism for enhanced memory and focus. The framework uses Rainbow DQN for reinforcement learning to optimize trading strategies. Empirical results show significant improvements in total return rate, Sharpe ratio, and maximum drawdown compared to a baseline model on six selected stocks.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:18:50.426219
2e9eec71c0351724,A Study on Project Portfolio Models with Skewness Risk and Staffing,"When it comes to business operation, the institutions have to choose some appropriate projects from numerous projects to invest. To this end, they should consider to establish a project portfolio to make decisions. When building such a portfolio, the project selection and the staff assignment are the most essential parts, which greatly affect the profit of project portfolios. As for the project selection, market returns tend to be asymmetric and investors are often concerned about the skewness risk which is ignored by the traditional project portfolio. Meanwhile, as for the staff assignment, the institutional investors aim at achieving the highest returns by adopting a proper assignment of project managers. In addition, since the exact possibility distributions of uncertain parameters in practical project portfolio problems are often unavailable, we adopt variable parametric credibility measure to characterize uncertain model parameters. In view of these problems, this article proposes a project portfolio model with skewness risk constraints and a project portfolio model with staffing based on credibility measure theory and fuzzy theory in uncertain circumstances. Our two models are associated with risk-free assets so that the remaining funds can be utilized effectively. Finally, we use genetic algorithms to solve our proposed models and present some numerical examples to demonstrate the effectiveness of the proposed models. © 2023 Elsevier B.V., All rights reserved.","Xu, W.; Liu, G.; Li, H.; Luo, W.",2017,10.1007/s40815-017-0295-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037379412&doi=10.1007%2Fs40815-017-0295-0&partnerID=40&md5=5d175108a94ab2d9fd4c70c6ba35eaf2,scopus,"This article proposes two project portfolio models addressing skewness risk and staffing issues under uncertainty, utilizing credibility measure and fuzzy theories. The models incorporate risk-free assets and are solved using genetic algorithms, with numerical examples demonstrating their effectiveness.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:18:55.956249
644a6f98cf40c70c,A comparison of linear and nonlinear statistical techniques in performance attribution,"Performance attribution is usually conducted under the linear framework of multifactor models. Although commonly used by practitioners in finance, linear multifactor models are known to be less than satisfactory in many situations. After a brief survey of nonlinear methods, nonlinear statistical techniques are applied to performance attribution of a portfolio constructed from a fixed universe of stocks using factors derived from some commonly used cross sectional linear multifactor models. By rebalancing this portfolio monthly, the cumulative returns for procedures based on standard linear multifactor model and three nonlinear techniques-model selection, additive models, and neural networks-are calculated and compared. It is found that the first two nonlinear techniques, especially in combination, outperform the standard linear model. The results in the neural-network case are inconclusive because of the great variety of possible models. Although these methods are more complicated and may require some tuning, toolboxes are developed and suggestions on calibration are proposed. This paper demonstrates the usefulness of modern nonlinear statistical techniques in performance attribution.",Ngai Hang Chan; C. R. Genovese,2001,10.1109/72.935100,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=935100,ieeexplore,"This paper compares linear and nonlinear statistical techniques for portfolio performance attribution. It applies nonlinear methods like model selection, additive models, and neural networks to a stock portfolio and finds that nonlinear techniques, particularly model selection and additive models, outperform standard linear models. The study suggests that modern nonlinear techniques are useful for performance attribution, despite their complexity.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:19:02.059418
a4cfbac9053df77e,A comparison of machine learning and econometric models for pricing perpetual Bitcoin futures and their application to algorithmic trading,"Bitcoin (BTC) perpetual futures contracts are highly leveraged speculative trading instruments with daily market trading of $45 Billion. BTC perpetual futures are derivative contracts, which depend upon the underlying BTC SPOT (current) price. Pricing perpetual futures fairly is hard, using traditional arbitrage arguments, because of the volatile nature of the so called funding rate, which is used as the replacement of risk free rate in the Cryptocurrency market. This work presents a novel technique for pricing BTC futures contracts using conditional volatility and mean models. Intra‐day high‐frequency futures' return volatility and mean are modelled using different ML and econometric techniques. A comparison is made using statistical measures to find the model that best captures the intra‐day conditional mean and volatility. Exponential generalized autoregressive conditional heteroskedasticity is shown to be an almost unbiased predictor of intra‐day volatility, while a constant autoregressive moving average (0, 0) model best captures the conditional mean of the returns. A market directional high frequency trading algorithm is developed using the volatility and mean models. The algorithm first prices the futures contract at some future point of time using the volatility and mean regression models. Next, the slope between the current futures price and the expected price are used to predict the market direction. A long or short position is taken depending upon the expected market direction movement. Extensive back‐testing results show absolute returns of 1500%–8000% depending upon the transaction fees and leverage used. On average, the market direction is predicted correctly 85% of the time by the best model. Finally, the trading technique is market neutral, in that it gives large positive returns, with low SD, in both bull and bear markets.",,2023,10.1111/exsy.13414,,proquest,"This study compares machine learning and econometric models for pricing Bitcoin perpetual futures, considering conditional volatility and mean. An Exponential Generalized Autoregressive Conditional Heteroskedasticity (EGARCH) model performed well for volatility, and a Constant Autoregressive Moving Average (0,0) model for mean. A trading algorithm based on these models achieved significant returns (1500%-8000%) by predicting market direction with 85% accuracy, demonstrating market neutrality.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:19:08.469019
7736932495ba615c,A comprehensive investigation on the predictive power of economic policy uncertainty from non-US countries for US stock market returns,"This study uses economic policy uncertainty (EPU) indices for ten developed countries, three diffusion models, and five combination methods to forecast excess returns in the U.S. stock market. It shows empirically that, over the period January 1997 to January 2022, non-U.S. EPU indices have better predictive power for U.S. equity market excess returns than the U.S. EPU index itself. This illustrates how economic information from interna-tional markets can affect the U.S. stock market. This finding challenges the extensively recognized view that the U.S. is where important market signals are initially transmitted to other markets, suggesting that this belief is incomplete. Our outcomes are robust to a battery of tests covering model selection, model specification, forecast horizons, and the pandemic period, and their economic values are assessed. The findings are essential for the financial field to confront future fierce situations and crises.","Huang, Yisu; Ma, Feng; Bouri, Elie; Huang, Dengshi",2023,10.1016/j.irfa.2023.102656,,wos,"This study investigates the predictive power of economic policy uncertainty (EPU) from ten developed non-US countries for US stock market returns. Using three diffusion models and five combination methods from January 1997 to January 2022, the research finds that non-US EPU indices are better predictors of US equity market excess returns than the US EPU index. This suggests that international economic information impacts the US stock market, challenging the traditional view of US market primacy. The findings are robust across various tests and have implications for the financial field.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:19:30.873566
6b84e9b3a4dbcbf6,A differential evolution algorithm for yield curve estimation,"Modeling the term structure of government bond yields is of great interest to macroeconomists and financial market practitioners. It is crucial for bonds and derivatives pricing, risk management, and reveals market expectations, which is essential for monetary policy decisions. This paper suggests the use of a differential evolutionary algorithm to estimate yield curves for US Treasury bonds. It considers parsimonious modeling to avoid non-convergence and high instability of traditional optimization algorithms when estimating model parameters caused by the choice of their initial values during curve fitting. In this approach, the whole yield curve for different maturities is obtained by models parameters estimates. Computational experiments show that the differential evolutionary algorithm provides more accurate yield curves than the ones derived by nonlinear least squares and genetic algorithm approaches. © 2016 Elsevier B.V., All rights reserved.","Maciel, L.; Gomide, F.; Ballini, R.",2016,10.1016/j.matcom.2016.04.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971623035&doi=10.1016%2Fj.matcom.2016.04.004&partnerID=40&md5=b9d939ee042f35134b929a881b51fb90,scopus,"This paper proposes a differential evolutionary algorithm for estimating yield curves of US Treasury bonds. It aims to overcome limitations of traditional optimization methods by using a parsimonious modeling approach. The algorithm's accuracy is compared to nonlinear least squares and genetic algorithms, showing superior performance.",True,True,False,gemini-2.5-flash-lite,Olav,M,,2025-10-13T15:21:00.385216
24e4d383bd202c0a,A dynamic factor model of the yield curve components as a predictor of the economy,"In this paper, we propose an econometric model of the joint dynamic relationship between the Treasury yield curve components and the economy, for predicting business cycle turning points. The nonlinear multivariate dynamic factor model takes into account not only the popular slope, but also information extracted from the level and curvature of the yield curve, and from macroeconomic variables. We investigate the interrelationship between the phases of cyclical fluctuations in yield curve components and the phases of the business cycle. The results indicate a strong interrelationship between the yield curve and the economy. The proposed model has substantial incremental predictive value relative to alternative specifications. This result holds both in-sample and out-of-sample, using revised and real time unrevised data. © 2016 Elsevier B.V., All rights reserved.","Chauvet, M.; Senyuz, Z.",2016,10.1016/j.ijforecast.2015.05.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955313253&doi=10.1016%2Fj.ijforecast.2015.05.007&partnerID=40&md5=a41c2d9e062771db7347649b91b21ab4,scopus,"This paper proposes a nonlinear multivariate dynamic factor model to predict business cycle turning points using Treasury yield curve components (slope, level, curvature) and macroeconomic variables. The model demonstrates significant predictive value, both in-sample and out-of-sample, highlighting a strong interrelationship between the yield curve and the economy.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T15:23:18.176997
c25e22f6b2dd0dd1,A further analysis of robust regression modeling and data mining corrections testing in global stocks,"In this analysis of the risk and return of stocks in global markets, we build a reasonably large number of stock selection models and create optimized portfolios to outperform a global benchmark. We apply robust regression techniques, LAR regression, and LASSO regression modeling to estimate stock selection models. Markowitz-based optimization techniques is used in portfolio construction within a global stock universe. We apply the Markowitz–Xu data mining corrections test to a global stock universe. We find that (1) robust regression applications are appropriate for modeling stock returns in global markets; (2) weighted latent root regression robust regression techniques work as well as LAR and LASSO-Regressions in building effective stock selection models; (3) mean–variance techniques continue to produce portfolios capable of generating excess returns above transactions costs; and (4) our models pass several data mining tests such that regression models produce statistically significant asset selection for global stocks. Recent Sturdy-Regression modeling technique may offer the greatest potential for further research for statistically based stock selection modeling. © 2021 Elsevier B.V., All rights reserved.","Guerard, J.B.; Xu, G.; Markowitz, H.",2021,10.1007/s10479-020-03521-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079228702&doi=10.1007%2Fs10479-020-03521-y&partnerID=40&md5=e087ffa22478c084792e3b0edf880973,scopus,"This study analyzes stock returns in global markets using robust regression, LAR, and LASSO regression for stock selection models and Markowitz-based optimization for portfolio construction. The findings suggest that robust regression is suitable for modeling stock returns, weighted latent root regression performs comparably to LAR and LASSO, mean-variance techniques can generate excess returns, and the models pass data mining tests, indicating statistically significant asset selection. The authors suggest further research into Sturdy-Regression modeling.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:23:33.109870
d009405b1fa5fcaa,A hybrid novel framework for flood disaster risk control in developing countries based on smart prediction systems and prioritized scenarios,"A Decision Support System (DSS) is a highly efficient concept for managing complex objects in nature or human-made phenomena. The main purpose of the present study is related to designing and implementation of real-time monitoring, prediction, and control system for flood disaster management as a DSS. Likewise, the problem of statement in the research is correlated to implementation of a system for different climates of Iran as a unique flood control system. For the first time, this study coupled hydrological data mining, Machine Learning (ML), and Multi-Criteria Decision Making (MCDM) as smart alarm and prevention systems. Likewise, it created the platform for conditional management of floods in Iran's different clusters of climates. According to the KMeans clustering system, which determines homogeneity of the hydrology of a specific region, Iran's rainfall is heterogeneous with 0.61 score, which is approved high efficiency of clustering in a vast country such as Iran with four seasons and different climates. In contrast, the relation of rainfall and flood disaster is evaluated by Nearest Neighbors Classification (NNC), Stochastic Gradient Descent (SGD), Gaussian Process Classifier (GPC), and Neural Network (NN) algorithms which have an acceptable correlation coefficient with a mean of 0.7. The machine learning outputs demonstrated that based on valid data existence problems in developing countries, just with verified precipitation records, the flood disaster can be estimated with high efficiency. In the following, Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) method as a Game Theory (GT) technique ranked the preventive flood damages strategies through three social (Se 1), environmental (Se 2), and economic (Se 3) crises scenarios. The solutions of flood disaster management are collected from literature review, and the opinion approves them of 9 senior experts who are retired from a high level of water resource management positions of Iran. The outcomes of the TOPSIS method proved that National announcement for public-institutional participation for rapid response and funding (G1-2), Establishment of delay structures to increase flood focus time to give the animals in the ecosystem the opportunity to escape to the upstream points and to preserve the habitat (G 2–8), and Granting free national financial resources by government agencies in order to rebuild sensitive infrastructure such as railways, hospitals, schools, etc. to the provincial treasury (G3-10) are selected as the best solution of flood management in Social, Environmental, and Economic crises, respectively. Finally, the collected data are categorized in Social, Environmental, and Economic aspects as three dimensions of Sustainable Development Goals (SDGs) and ranked based on the opinion of 32 experts in the five provinces of present case studies.",,2022,10.1016/j.jenvman.2022.114939,,proquest,"This study proposes a hybrid framework for flood disaster risk control in developing countries, integrating real-time monitoring, prediction, and control as a Decision Support System (DSS). It combines hydrological data mining, Machine Learning (ML), and Multi-Criteria Decision Making (MCDM) for smart alarm and prevention systems. The framework was applied to Iran's diverse climates, using KMeans clustering for hydrological homogeneity and ML algorithms (Nearest Neighbors, SGD, GPC, NN) to correlate rainfall with flood disasters, achieving a mean correlation coefficient of 0.7. The TOPSIS method, a Game Theory technique, ranked preventive strategies based on social, environmental, and economic crises, with expert opinions guiding the selection of top solutions for each crisis type. The findings were categorized according to Sustainable Development Goals (SDGs).",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:23:41.590847
ac5e1389100fd935,A machine learning based asset pricing factor model comparison on anomaly portfolios,"We frame asset pricing linear factor models in a machine learning context and consider related comparisons of their predictive performance against ordinary least squares linear regression over a dataset of anomaly portfolios. Specific regression models involved in the comparison include regularized linear, support vector machines, neural networks, and tree based models among others. Performance metrics are presented on a model, portfolio group, and sequential basis, and the strongest predictors are recommended as alternative techniques for the problem of excess return forecasting.",,2021,10.1016/j.econlet.2021.109919,,proquest,"This study compares the predictive performance of various machine learning-based asset pricing factor models against ordinary least squares linear regression using anomaly portfolios. The models evaluated include regularized linear models, support vector machines, neural networks, and tree-based models. Performance is assessed across different levels, and the most effective predictors for excess return forecasting are identified.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:23:49.828562
0c90039e02d794b5,A machine learning portfolio allocation system for IPOs in Korean markets using GA-rough set theory,"An initial public offering (IPO) is a type of public offering in which a company's shares are sold to institutional and individual investors. While the majority of studies on IPOs have focused on the efficiency of raising capital and price adequacy in IPOs, studies on portfolio allocation strategies for IPO stocks are relatively scarce. This paper develops a machine learning investment strategy for IPO stocks based on rough set theory and a genetic algorithm (GA-rough set theory). To reduce issues of information asymmetry, we use nonfinancial data that are publicly available to individual and institutional investors in the IPO process. Based on the rule sets generated from the training sets, we conduct 120 tests with various conditions involving the target days and the partition of the training and testing sets, and we find excess returns of the constructed portfolios compared to the benchmark portfolios. Investors in IPO stocks can formulate more efficient investment strategies using our system. In this sense, the system developed in this paper contributes to the efficiency of financial markets and helps achieve sustained economic growth. © 2021 Elsevier B.V., All rights reserved.","Kim, J.; Shin, S.; Lee, H.S.; Oh, K.J.",2019,10.3390/su11236803,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076596987&doi=10.3390%2Fsu11236803&partnerID=40&md5=13ff053dd22d5b0ae2fb194bf4f0dde3,scopus,"This paper proposes a machine learning system for IPO stock portfolio allocation in Korean markets, utilizing GA-rough set theory and nonfinancial data to address information asymmetry. The system demonstrated excess returns compared to benchmarks across 120 tests, suggesting potential for more efficient investment strategies and contributing to financial market efficiency.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:23:55.201754
d87bd95a61553bb9,A micro data approach to the identification of credit crunches,"This article presents a micro data approach to the identification of credit crunches. Using a survey among German firms which regularly queries the firms' assessment of the current willingness of banks to extend credit, we estimate the probability of a restrictive loan supply policy by time taking into account the creditworthiness of borrowers. Creditworthiness is approximated by firm-specific factors, e.g. the firms' assessment of their current business situation and their business expectations. After controlling for the return on the banks' risk-free investment alternative, which is also likely to affect the supply of loans, we derive a credit crunch indicator, which measures that part of the shift in the loan supply that is neither explained by firm-specific factors nor by the opportunity costs of providing risky loans. © 2012 Taylor and Francis Group, LLC. © 2012 Elsevier B.V., All rights reserved.","Rottmann, H.; Wollmershäuser, T.",2013,10.1080/00036846.2012.665604,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859759058&doi=10.1080%2F00036846.2012.665604&partnerID=40&md5=f41992b5331f82089cdad15d091c36c9,scopus,"This article proposes a microdata-based method to identify credit crunches by analyzing German firms' perceptions of bank credit availability. It estimates the probability of restrictive loan supply policies, accounting for borrower creditworthiness (approximated by firm-specific factors like business situation and expectations) and the opportunity cost of lending. A credit crunch indicator is derived, isolating unexplained shifts in loan supply.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:24:00.126101
8fa57119b90ec594,A model-selection approach to assessing the information in the term structure using linear models and artificial neural networks,"We take a model-selection approach to the question of whether forward-interest rates are useful in predicting future spot rates, using a variety of out-of-sample forecast-based model-selection criteria—forecast mean squared error, forecast direction accuracy, and forecast-based trading- system profitability. We also examine the usefulness of a class of novel prediction models called artificial neural networks and investigate the issue of appropriate window sizes for rolling-window- based prediction methods. Results indicate that the premium of the forward rate over the spot rate helps to predict the sign of future changes in the interest rate. Furthermore, model selection based on an in-sample Schwarz information criterion (SIC) does not appear to be a reliable guide to out-of-sample performance in the case of short-term interest rates. Thus, the in-sample SIC apparently fails to offer a convenient shortcut to true out-of-sample performance measures. © 1995 Taylor & Francis Group, LLC. © 2016 Elsevier B.V., All rights reserved.","Swanson, N.R.; White, H.",1995,10.1080/07350015.1995.10524600,https://www.scopus.com/inward/record.uri?eid=2-s2.0-21844518145&doi=10.1080%2F07350015.1995.10524600&partnerID=40&md5=3fb7e2b2653ab9f836c914042957ea00,scopus,"This study uses a model-selection approach to evaluate the predictive power of forward interest rates for future spot rates, employing forecast-based criteria like mean squared error, direction accuracy, and trading system profitability. It also explores artificial neural networks (ANNs) as prediction models and investigates optimal window sizes for rolling-window methods. The findings suggest that the forward rate premium can predict the direction of future interest rate changes, but in-sample model selection criteria like the Schwarz Information Criterion (SIC) do not reliably predict out-of-sample performance for short-term rates.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T15:24:14.885545
6094397f5a71118e,A new portmanteau test for predictive regression models with possible embedded endogeneity,"In the widely used predictive regression model, any possible serial correlation in innovations leads to estimation bias and statistical inference distortions. Hence, it is important to pretest the existence of such serial correlation. Nevertheless, in the presence of embedded endogeneity, which is a common problem in the predictive regression setting, traditional serial correlation tests such as Box-Pierce (BP) and Ljung-Box (LB) tests are found to perform poorly. Motivated by this, we develop a new portmanteau test in this article as a pretest for serial correlation in predictive regression under possible embedded endogeneity. This test is based on the sample splitting idea and the jackknife empirical likelihood method. The asymptotic distribution of the proposed test has been derived, and the Monte Carlo simulations confirm good finite sample performances. As an illustration, we apply our proposed test in pretesting the serial correlation in predictive regression, where financial variables are used to predict the excess return of S&P 500.","Rao, Yao; Fan, Yawen; Ao, Huimin; Liu, Xiaohui",2024,10.1111/jtsa.12745,,wos,"This paper proposes a new portmanteau test to detect serial correlation in predictive regression models, particularly when endogeneity is present. The test utilizes sample splitting and jackknife empirical likelihood methods. Simulation results show good performance, and the test is illustrated with an application predicting S&P 500 excess returns using financial variables.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:24:22.324484
f1226e3a19defec0,A nonlinear Bayesian filtering approach to estimating adaptive market efficiency,"The adaptive market hypothesis (AMII) supplies a convincing motivation for why market efficiency should not be regarded as a stable property in time. This paper explores a Bayesian methodology for estimating weak-form market efficiency under the AMII using a test of evolving efficiency (TEE). More precisely, a generalized TEE (GTEE) approach is proposed in which the conditional first moment of a time series is assumed to be a nonlinear function of its conditional second moment, i.e., a nonlinear feedback term is present in the conditional mean equation. We then discuss a maximum likelihood estimation procedure for the resulting nonlinear model using the state-space approach and extended Kalman filtering. This methodology is used to estimate time-varying, weak-form market efficiency in four, specifically chosen, markets over a time period that includes the global financial crisis of 2007/2008.","Kulikov, Gennady Yu.; Taylor, David R.; Kulikova, Maria V.",2019,10.1515/rnam-2019-0003,,wos,"This paper proposes a Bayesian methodology using a generalized test of evolving efficiency (GTEE) to estimate weak-form market efficiency under the adaptive market hypothesis. The GTEE assumes a nonlinear relationship between the conditional first and second moments of a time series. The authors use a state-space approach and extended Kalman filtering for estimation and apply it to four markets, including the period of the 2007/2008 global financial crisis.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:24:30.096281
cf9f62572b2d6956,A nonlinear general equilibrium model of the term structure of interest rates,"We derive and test an alternative closed-form general equilibrium model of the term structure within the Cox, Ingersoll, and Ross theoretical framework in which yields are nonlinear functions of the risk-free rate. We show that equilibrium bond prices and the risk-free rate are not always inversely related and that bond risk need not be strictly increasing in maturity. Using Hansen's generalized method of moments to obtain parameter estimates, this nonlinear model outperforms the Cox, Ingersoll, and Ross square root model in describing actual Treasury bill yields for the 1964-1986 period. © 1989. © 2014 Elsevier B.V., All rights reserved.","Longstaff, F.A.",1989,10.1016/0304-405x(89)90056-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0344971168&doi=10.1016%2F0304-405X%2889%2990056-1&partnerID=40&md5=da37d31c6e2c300ffdead5c18b5cebfc,scopus,"This paper presents a nonlinear general equilibrium model of the term structure of interest rates, building upon the Cox, Ingersoll, and Ross framework. The model suggests that yields are nonlinear functions of the risk-free rate, and that bond prices and risk-free rates are not always inversely related, with bond risk not necessarily increasing with maturity. Empirical testing using the generalized method of moments shows this nonlinear model outperforms the original Cox, Ingersoll, and Ross model in describing Treasury bill yields from 1964-1986.",True,True,False,gemini-2.5-flash-lite,Olav,M,,2025-10-13T15:24:48.137090
0f09673e5b50a64e,A nonlinear model of the term structure of interest rates,"We present an economically motivated two-factor term structure model that generalizes existing stochastic mean term structure models. By allowing a certain parameter to acquire dynamical behavior we extend the two-factor model to obtain a nonlinear three-factor model that is shown, in a deterministic version, to be equivalent to the Lorenz system of differential equations. With reasonable parameter values the model exhibits chaotic behavior. It successfully emulates certain properties of interest rates including cyclical behavior on a business cycle time scale. Estimation and pricing issues are discussed. Standard PCA techniques used to estimate HJM type models are observed to be equivalent to dimensional estimates commonly applied to 'spatial data' in nonlinear systems analysis. It is concluded that techniques commonly used in the analysis of nonlinear systems may be directly applicable to interest rate models, offering new insights in the development of these models. Tests of nonlinearity in interest rate behavior may need to focus on long cycle times. © 2018 Elsevier B.V., All rights reserved.","Tice, J.; Webber, N.",1997,10.1111/1467-9965.00030,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040657144&doi=10.1111%2F1467-9965.00030&partnerID=40&md5=c5199d870e62e9702b95e8b3acf50c5b,scopus,"This paper introduces a nonlinear, two-factor term structure model for interest rates that generalizes existing stochastic mean models. The model can exhibit chaotic behavior and successfully emulates properties of interest rates, including cyclical behavior on a business cycle timescale. It suggests that techniques from nonlinear systems analysis can be applied to interest rate models, and that tests for nonlinearity in interest rate behavior should consider long cycle times.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T15:24:58.601561
27cdb45de7c97a28,A parametric nonlinear model of term structure dynamics,"Recent nonparametric estimation studies pioneered by Aït-Sahalia document that the diffusion of the short rate is similar to the parametric function, r1.5, estimated by Chan et al., whereas the drift is substantially nonlinear in the short rate. These empirical properties call into question the efficacy of the existing affine term structure models and beg for alternative models which admit the observed behavior. This article presents such a model. Our model delivers closed-form solutions for bond prices and a concave relationship between the interest rate and the yields. We show that in empirical analyses, our model outperforms the one-factor affine models in both time-series as well as cross-sectional tests. © 2016 Elsevier B.V., All rights reserved.","Ahn, D.-H.; Gao, B.",1999,10.1093/rfs/12.4.721,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033410115&doi=10.1093%2Frfs%2F12.4.721&partnerID=40&md5=1194eb86b455e11c6bbe2174bdea396f,scopus,"This article proposes a nonlinear parametric model for term structure dynamics, addressing limitations of existing affine models by incorporating nonlinear drift and a diffusion similar to r1.5. The model provides closed-form solutions for bond prices and a concave interest rate-yield relationship, outperforming one-factor affine models in empirical tests.",True,True,False,gemini-2.5-flash-lite,Olav,M,,2025-10-13T15:25:30.366093
78921cc3f5612ddb,A re-evaluation of the term spread as a leading indicator,"Forecasting the evolution path of macroeconomic variables has always been of keen interest to policy makers and market participants. A common tool used in the relevant forecasting literature is the term spread of Treasury bond yields. In this paper, we decompose the term spread into an expectation and a term premium component and evaluate the informational content of each component in forecasting the GDP growth rate and inflation in various forecasting horizons. In doing so, we employ alternative decomposition procedures and introduce the Support Vector Regression (SVR) methodology from the field of Machine Learning, coupled with linear and non-linear kernels as a novel forecasting method in the field. Using rolling windows in producing point and conditional probability distribution forecasts we find that neither the term spread, nor its decomposition components possess the ability to accurately forecast output growth or inflation. Our findings extend the existing literature, since they are focused on an explicit out-of-sample evaluation in contrast to most existing empirical studies that produce only in-sample forecasts. To strengthen our findings, we also consider several control variables suggested in the relevant literature without significant qualitative differences from the initial results. The main innovation of our approach stems from the use of the non-linear Support Vectors Machine methodology, that is introduced for the first time in this line of research for forecasting out-of-sample. © 2019 Elsevier B.V., All rights reserved.","Plakandaras, V.; Gogas, P.; Papadimitriou, T.; Gupta, R.",2019,10.1016/j.iref.2019.07.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071871619&doi=10.1016%2Fj.iref.2019.07.002&partnerID=40&md5=b1cce98b67b0a1da2aeeb00c223c6d90,scopus,"This paper re-evaluates the term spread of Treasury bond yields as a leading indicator for GDP growth and inflation. It decomposes the term spread into expectation and term premium components and uses Support Vector Regression (SVR) with linear and non-linear kernels for out-of-sample forecasting. The study finds that neither the term spread nor its components accurately forecast output growth or inflation, even with control variables. The novelty lies in the application of non-linear SVR for out-of-sample forecasting in this research area.",True,True,True,gemini-2.5-flash-lite,Olav,M,,2025-10-13T15:26:20.625398
fade9effb1bee170,A risk index model for uncertain portfolio selection with background risk,"This study proposes a new uncertain risk index model with background risk and presents its deterministic equivalents. The security returns and background asset returns are assumed as uncertain variables and estimated by experts. To discuss the influence of background risk on investment decisions, we compare the proposed model with a variant without background risk and find that the portfolio with background risk produces an equal or lower return than the one without background risk. The effects of changes in the standard deviation of background asset and the risk-free interest rate on optimal expected value are discussed. Two different risk measures for portfolio optimization model with background risk are compared, viz., the risk index model with background risk is further compared with the mean chance model with background risk. The nonlinear risk index model is solved by using a genetic algorithm. The efficiency of the genetic algorithm and the applications of the proposed models are illustrated through numerical experiments.",,2021,10.1016/j.cor.2021.105331,,proquest,"This study introduces an uncertain risk index model incorporating background risk, with its deterministic equivalents presented. It assumes security and background asset returns as uncertain variables, estimated by experts. The research compares the model with background risk to one without, finding that background risk can lead to equal or lower returns. It also analyzes the impact of background asset standard deviation and risk-free interest rates on optimal expected value. The study compares two risk measures for portfolio optimization with background risk and uses a genetic algorithm to solve the nonlinear risk index model, demonstrating its efficiency and applications through numerical experiments.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:26:24.815689
4c3e70c64d535aeb,A semiparametric model for the systematic factors of portfolio credit risk premia,"The aim of this paper is to investigate the empirical relationship between daily fluctuations in the risk premium for holding a large diversified credit portfolio, which we approximate by a benchmark credit index, and some tradeable market factors which capture systematic risk. The analysis is based on an adaptive nonparametric modelling approach which allows for the data-driven estimation of the nonlinear dynamic relationship between portfolio credit risk premia and their hypothetical components. Our main finding is that the empirical weights of the systematic factors display sudden jumps during market crises and a less intense time-dependent behaviour during normal market conditions. In addition, we find that during market crises the directions of the empirical relationships are often inconsistent with ordinary economic intuition, as they are influenced by the specific circumstances of financial markets distress. All rights reserved, Elsevier",,2009,10.1016/j.jempfin.2009.05.001,,proquest,"This paper proposes a semiparametric model to analyze the relationship between daily fluctuations in credit portfolio risk premium and systematic market factors. It uses an adaptive nonparametric approach to estimate nonlinear dynamics and finds that factor weights change abruptly during market crises, with relationships sometimes deviating from economic intuition during distress.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:26:34.940891
1cef435702aea303,A simple test of momentum in foreign exchange markets,"This study proposes a new method for testing for the presence of momentum in nominal exchange rates, using a probabilistic approach. We illustrate our methodology estimating a binary response model using information on local currency / US dollar exchange rates of eight emerging economies. After controlling for important variables affecting the behavior of exchange rates in the short-run, we show evidence of exchange rate inertia; in other words, we find that exchange rate momentum is a common feature in this group of emerging economies, and thus foreign exchange traders participating in these markets are able to make excess returns by following technical analysis strategies. We find that the presence of momentum is asymmetric, being stronger in moments of currency depreciation than of appreciation. This behavior may be associated with central bank intervention. © 2014 Elsevier B.V., All rights reserved.","García-Suaza, A.F.; Gomez-Gonzalez, J.E.",2012,10.2753/ree1540-496x480504,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905081033&doi=10.2753%2FREE1540-496X480504&partnerID=40&md5=8cc1c4643bfbf2bfda3ee8a5e71341b6,scopus,"This study introduces a probabilistic method to detect momentum in nominal exchange rates, specifically focusing on eight emerging economies' local currency/USD exchange rates. The findings indicate exchange rate inertia and the potential for excess returns through technical analysis, with momentum being more pronounced during currency depreciation, possibly due to central bank intervention.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:26:37.952786
2aa4181eccce522e,A three-level nested portfolio optimization model with position allocation,"Existing portfolio optimization models cannot well capture the real position allocation requirement, leading to limited impact in practice. To overcome this challenge, we propose a three-level nested portfolio optimization model with position allocation. Within this model, the inner and middle levels collaboratively determine the optimal portfolio, while the outer level focuses on optimizing the holding proportion of each stock in the optimal portfolio. Compared with existing models with portfolio weights, the proposed model imposes the position allocation constraint that precisely characterizes the limitations on holding each stock. This constraint is crucial for investors to obey securities trading regulations involving position limitations and to mitigate the potential impact of market risks. To address the nonlinear and nonconvex nature of the novel model, we develop an intelligent optimization algorithm by effectively hybridizing the support vector regression and the enhanced grey wolf optimizer. We comprehensively evaluate its performance using eight metrics, including accumulative return, annual return, Sharpe ratio, maximum drawdown, absolute and relative win ratios, predictive precision and accuracy. The experimental results indicate that (i) the proposed model can achieve more excess returns than those stock selection models not considering position allocation, especially for the large-cap stocks; (ii) compared with other state-of-the-art meta-heuristics, the enhanced grey wolf optimizer can yield better portfolio in conjunction with the support vector regression; (iii) in the context of the Chinese A-share stock market, specific financial indicators such as return on equity, inventory turnover rate, net income growth rate, and debt-to-equity ratio should be given greater consideration compared to other financial metrics. © 2024 Elsevier B.V., All rights reserved.","Ma, J.; Yang, K.; Luo, K.; Li, P.; He, A.",2024,10.1016/j.asoc.2024.112054,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200798925&doi=10.1016%2Fj.asoc.2024.112054&partnerID=40&md5=7b415008c2dba69527715c4c4b28a8f5,scopus,"This paper introduces a three-level nested portfolio optimization model that incorporates position allocation, addressing limitations in existing models. It uses a hybrid support vector regression and enhanced grey wolf optimizer algorithm to handle the model's complexity. The model is evaluated using eight metrics and shows improved returns, particularly for large-cap stocks, and demonstrates the effectiveness of the proposed algorithm and highlights key financial indicators for the Chinese A-share market.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:26:44.175152
5b8c2d356e2c63ed,A threshold model for the spread,"Using annual data from two panels, one of 11 Eurozone countries and another of 31 OECD countries, we estimate a two-regime log-linear as well as a nonlinear model for the spread as a function of macroeconomic and quality-of-institutions variables. The two regimes, a high-spread and a low-spread regime, are distinguished by using a threshold, in accordance with the perceived ""fair""value of the spread as a reference point. Our results suggest that government-bond spreads are regime-dependent, as most of the regression coefficients of the determinants of the spread are larger (in absolute value) in the high-spread regime than in the low-spread regime. That is, an improvement in the macroeconomic environment (e.g., lower unemployment, lower inflation, lower growth of the debt-to-GDP ratio, less macroeconomic uncertainty, higher growth of real GDP), and/or an improvement in the quality of institutions (e.g., less corruption) reduce the spread facing a country (by enhancing its creditworthiness) to a greater extent in high-spread situations than in low-spread situations. A possible explanation is that the demand for and the supply of loans are inelastic at higher than ""fair""interest rates and elastic at lower rates. © 2023 Elsevier B.V., All rights reserved.","Hatzinikolaou, D.; Sarigiannidis, G.",2023,10.1515/snde-2020-0007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131677707&doi=10.1515%2Fsnde-2020-0007&partnerID=40&md5=13323e4318936c45cc6c14e3e03f58f0,scopus,This study uses annual data from Eurozone and OECD countries to estimate models for government bond spreads. It identifies two regimes (high and low spread) based on a threshold and finds that macroeconomic and institutional factors have a greater impact on reducing spreads in the high-spread regime. The findings suggest that improvements in economic conditions and institutional quality enhance creditworthiness more significantly when spreads are already high.,True,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:27:10.348990
412fc4122a3c36ac,AN ARBITRAGE-FREE ESTIMATE OF PREPAYMENT OPTION PRICES IN FIXED-RATE GNMA MORTGAGE-BACKED SECURITIES,"In an efficient market, the no-arbitrage condition implies that the price difference between any two assets must be the market value of ail differences in their cash flows. We use this logic to deduce the price of the prepayment option embedded in fixed-rate Government National Mortgage Association (GNMA) mortgage-backed securities. The option price equals the difference between an observed GNMA price and the cost of a synthetic, nonprepayable GNMA constructed from the least expensive portfolio of Treasury securities that exactly replicates the promised GNMA cash flow stream, assuming prepayment is precluded. We regress the option prices on variables found significant in previous prepayment studies, finding that five key regressors explain more than 90% of the prepayment option value in pooled time-series cross-sectional analysis. We also show that the time value of the prepayment option calculated by our method displays a pattern similar to that produced by the Black-Scholes (1973) option pricing model. An additional empirical result is the existence of negative option prices and negative time value of the option prices. We attribute these to the fact that homeowners sometimes exercise their prepayment options when they are out-of-the-money, and to refinancing transaction costs. Our method is independent of assumptions regarding interest rate processes and the homeowner's prepayment behavior, and it provides a benchmark for testing theoretical prepayment models.","RONN, EI; RUBINSTEIN, PD; PAN, FS",1995,10.1111/1540-6229.00655,,wos,"This paper develops an arbitrage-free method to estimate the price of the prepayment option in fixed-rate GNMA mortgage-backed securities. It constructs a synthetic non-prepayable GNMA from Treasury securities and calculates the option price as the difference between the observed GNMA price and the synthetic price. The study finds that five key variables explain over 90% of the prepayment option value and observes negative option prices and time values, attributing them to homeowners exercising options out-of-the-money and refinancing costs. The method is independent of interest rate process assumptions and homeowner behavior, serving as a benchmark for prepayment models.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:27:23.414994
54aa7f1532e8bf7d,"Accounting-based probabilistic prediction of ROE, the residual income valuation model and the assessment of mispricing in the Swedish stock market","Using Swedish stock market data, this study investigates whether an investment strategy based on publicly available accounting information can generate abnormal investment returns. The strategy involves two steps. First, an accounting-based probabilistic prediction model of changes in the medium-term book return on owners' equity (ROE) is estimated. Second, market expectations of changes in medium-term ROE are assessed through observed stock prices and the residual income valuation model. Stock market positions over thirty-six-month holding periods are taken when the accounting-based predictions of ROE and the market expectations differ. Over the period 1983-2003, the investment strategy generated values of Jensen's alpha corresponding to an average monthly excess return for a hedge position of up to 0.8% for a sample of manufacturing companies. In the main this hedge return was caused by strong positive returns to the long positions, and additional analyses show that the returns appear to have been affected by a positive market sentiment bias (i.e. positive ROE surprises being associated with stronger price reactions than negative ROE surprises) making out-of-sample inferences somewhat dubious. Furthermore, most of the investment returns accrued over holding periods up to around 1995, with no indications of market mispricing over the last third (1995-2003) of the investment period. The empirical results are consistent with market investors having become more sophisticated in their use of publicly available accounting information over time. Reprinted by permission of Blackwell Publishers",,2010,10.1111/j.1467-6281.2010.00325.x,,proquest,"This study investigates an investment strategy based on accounting information in the Swedish stock market. It uses a probabilistic prediction model for Return on Equity (ROE) and the residual income valuation model to assess market expectations. The strategy generated abnormal returns, particularly before 1995, but the authors suggest potential biases and increasing market sophistication over time, making out-of-sample inferences questionable.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:27:32.418863
1b82b421c3fc63d2,Adaptive Algorithm for Selecting the Optimal Trading Strategy Based on Reinforcement Learning for Managing a Hedge Fund,"In hedge fund management, the ability to dynamically select optimal trading strategies is paramount for maximizing returns and mitigating risk. This paper presents a pioneering approach that integrates Reinforcement Learning (RL), specifically the Proximal Policy Optimization (PPO) algorithm, into the strategy selection process for hedge fund management. Our model considers a diverse array of strategies, including Mean Reversion and Momentum, and employs advanced mathematical frameworks to evaluate and select the strategies. By leveraging RL, our algorithm learns to adaptively adjusts strategy allocations to maximize cumulative returns while adhering to the risk constraints. We demonstrate the effectiveness of our approach through extensive backtesting and validation of historical market data, demonstrating superior performance compared to traditional methods. Nevertheless, it is important to understand that training trading agents requires a considerable amount of time, computing power, and other resources. Our research offers a novel perspective on leveraging RL to optimize strategy selection in hedge fund management and underscores the potential of AI-driven approaches in finance.",B. Belyakov; D. Sizykh,2024,10.1109/access.2024.3515039,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10792442,ieeexplore,"This paper introduces a novel approach using Proximal Policy Optimization (PPO), a Reinforcement Learning algorithm, for adaptive trading strategy selection in hedge fund management. The model evaluates and selects from strategies like Mean Reversion and Momentum, aiming to maximize returns and manage risk. The approach is validated through backtesting, showing improved performance over traditional methods, though it requires significant computational resources for training.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:27:37.834457
5b1ba23957effe90,Affine arbitrage-free yield net models with application to the euro debt crisis,"We develop a parsimonious class of affine arbitrage-free yield net models for consistent bond pricing across maturities and issuers of different risk levels. Containing a core curve and multiple peripheral curves, the yield net is spanned by three layers of factors: base factors spanning all curves, and common and individual spread factors. Under the arbitrage-free assumption, we prove a parsimonious solution to the risk-neutral process that guarantees joint identification of parameters and latent states. By using a Bayesian estimation method with a marginal Metropolis–Hastings algorithm and specification tests based on MCMC output, we apply the model to weekly treasury yields of Germany, Italy, Spain, and Greece from 2009 to 2016. The results show that the extracted common credit risk is a level factor in spread, and market liquidity risk is a slope factor. Further, the net structure helps reconstruct the Greek yield curve even with only its 10-year yield available throughout the sample. © 2022 Elsevier B.V., All rights reserved.","Hong, Z.; Niu, L.; Zhang, C.",2022,10.1016/j.jeconom.2021.11.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122961603&doi=10.1016%2Fj.jeconom.2021.11.002&partnerID=40&md5=01905cc014d6ef66741afd239638d650,scopus,"This paper introduces a class of affine arbitrage-free yield net models for bond pricing, incorporating multiple curves and factor layers. It uses a Bayesian estimation method with a Metropolis-Hastings algorithm and applies it to European treasury yields from 2009-2016. The findings highlight the roles of common credit risk and market liquidity risk, and demonstrate the model's ability to reconstruct yield curves.",True,True,False,gemini-2.5-flash-lite,Olav,M,,2025-10-13T15:28:29.466867
f8e2d8cdac56b7a1,"Air pollution, weather factors, and realized volatility forecasts of agricultural commodity futures","This study investigates the potential effects of environmental factors on fluctuations in agricultural commodity futures markets, by constructing a new category of daily exogenous predictors related to air pollution, weather, climate change, and investor attention. The empirical results from out‐of‐sample analyses suggest that the heterogeneous autoregressive (HAR) model incorporating all these exogenous predictors is more likely to outperform other HAR‐type models. Additionally, economic evaluations demonstrate the superior performance of models incorporating investors' attention to climate change or extreme weather as predictors. While not all exogenous predictors are equally important for volatility forecasts, adopting appropriate variable selection methods to handle different sets of exogenous predictors can lead to better performance than the HAR benchmark. With the inclusion of air pollution or weather factors in the HAR model, a portfolio with an annualized average excess return of 16.2068% or a Sharpe ratio of 10.0431 can be achieved for the wheat futures, respectively.",,2024,10.1002/fut.22467,,proquest,"This study explores how air pollution, weather, climate change, and investor attention impact agricultural commodity futures volatility. It proposes an enhanced Heterogeneous Autoregressive (HAR) model incorporating these factors, showing superior out-of-sample performance compared to standard HAR models. The research highlights the importance of variable selection and demonstrates that including air pollution or weather factors can significantly improve portfolio returns and Sharpe ratios for wheat futures.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:28:34.108884
b812228bd22517dc,Algorithmic estimation of risk factors in financial markets with stochastic drift,"We assume a financial market governed by a diffusion process reverting to a stochastic mean which is itself governed by an unobservable ergodic diffusion, similar to those observed in electricity and other energy markets. We develop a moment method algorithm for the estimation of the parameters of both the observable process and the unobservable stochastic mean. Our approach is contrasted with other methods for parameter estimation of partially observed diffusions, and applications to the modelling of interest rates and commodity prices are discussed. (C) 2010 Elsevier Ltd. All rights reserved.","Hernandez, Janko; Saunders, David; Seco, Luis",2012,10.1016/j.cor.2010.09.007,,wos,"This paper proposes a moment method algorithm to estimate parameters in a financial market model with a stochastic drift, applicable to interest rates and commodity prices. The model assumes a diffusion process reverting to a stochastic mean, which is itself governed by an unobservable ergodic diffusion, drawing parallels to energy markets.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:28:46.560206
bcfa69807c6a868e,Alternative models for stock price dynamics,"This paper evaluates the role of various volatility specifications, such as multiple stochastic volatility (SV) factors and jump components, in appropriate modeling of equity return distributions. We use estimation technology that facilitates nonnested model comparisons and use a long data set which provides rich information about the conditional and unconditional distribution of returns. We consider two broad families of models: (I) the multifactor loglinear family, and (2) the affine-jump family. Both classes of models have attracted much attention in the derivatives and econometrics literatures. There are various tradeoffs in considering such diverse specifications. If pure diffusion SV models are chosen over jump diffusions, it has important implications for hedging strategies. If logarithmic models are chosen over affine ones, it may seriously complicate option pricing. Comparing many different specifications of pure diffusion multifactor models and jump diffusion models, we find that (I) log linear models have to be extended to two factors with feedback in the mean reverting factor, (2) affine models have to have a jump in returns, stochastic volatility or probably both. Models (I) and (2) are observationally equivalent on the data set in hand. In either (I) or (2) the key is that the volatility can move violently. As we obtain models with comparable empirical fit, one must make a choice based on arguments other than statistical goodness-of-fit criteria. The considerations include facility to price options, to hedge and parsimony. The affine specification with jumps in volatility might therefore be preferred because of the closed-form derivatives prices. (C) 2003 Elsevier B.V. All rights reserved.","Chernov, M; Gallant, AR; Ghysels, E; Tauchen, G",2003,10.1016/s0304-4076(03)00108-8,,wos,"This paper compares different models for stock price dynamics, including multifactor stochastic volatility and affine-jump models. It finds that models need to account for significant volatility movements, potentially with jumps. The choice between models depends on factors like option pricing and hedging, with affine models with jumps in volatility being potentially preferred for closed-form derivative prices.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:28:52.610986
a68df8aeccec0afd,An Analysis of Some Effects of Rating Information using an Artificial Market,"Standard & Poor' s (S & P) downgraded American government bonds from AAA to AA+ last year. The effects of the downgrade on financial markets have been studied in financial engineering, economics and computational finance, but not in agent-based simulation studies. In this paper, we investigate the effect of the rating system (e.g.: S & P) on asset price fluctuations in the artificial market, which is the agent-based simulation model of the financial market. The rating information is defined as a discrete version of the fundamental value. Four strategies : the noise trader, the fundamentalist, the trend predictor, and the contrarian trader, were assumed in previous studies of the artificial market, plus we assume a new agent called “rating user” which uses the rating value, defined as the discrete value of the fundamental value of an asset. We investigate if the rating user makes the artificial market unstable. First, the simulation results show that kurtosis of an asset price return in the market, without fundamentalists, is higher than without rating users. This suggests the usage of rating information makes the artificial market unstable. The simulation outcomes also suggest volatility continuity of asset price return is stronger in the market without fundamentalists than without rating users. Second, we investigate how two parameters, the update interval and rating length, which control the rating value, makes the market stable. The simulation outcomes show that both standard deviation of asset price return and kurtosis of asset price return becomes smaller as the update interval increases. The standard deviation gets larger and kurtosis of that gets larger with the increasing length of rating. These results imply that the rating information should be updated at short intervals and the length of rating should be moderate to make the artificial market stable. keywords: raing-information, financial market, artificial market, agent-based simulation. © 2012, The Japanese Society for Artificial Intelligence. All rights reserved. © 2017 Elsevier B.V., All rights reserved.","Katsumi, S.; Shimao, H.; Nishiyama, N.",2012,10.1527/tjsai.27.384,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024750711&doi=10.1527%2Ftjsai.27.384&partnerID=40&md5=14a23a707402b1332239499f679a1415,scopus,"This paper investigates the impact of rating information on asset price fluctuations within an artificial market simulation. It introduces a ""rating user"" agent that incorporates rating values into trading strategies. The study finds that the use of rating information can increase market instability, as indicated by higher kurtosis in asset price returns. It also suggests that shorter update intervals and moderate rating lengths contribute to market stability.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:28:59.664374
7ac4d584ce43caa8,An Automated Framework for Incorporating News into Stock Trading Strategies,"In this paper we present a framework for automatic exploitation of news in stock trading strategies. Events are extracted from news messages presented in free text without annotations. We test the introduced framework by deriving trading strategies based on technical indicators and impacts of the extracted events. The strategies take the form of rules that combine technical trading indicators with a news variable, and are revealed through the use of genetic programming. We find that the news variable is often included in the optimal trading rules, indicating the added value of news for predictive purposes and validating our proposed framework for automatically incorporating news in stock trading strategies.",W. Nuij; V. Milea; F. Hogenboom; F. Frasincar; U. Kaymak,2014,10.1109/tkde.2013.133,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6574843,ieeexplore,"This paper introduces a framework for automatically integrating news into stock trading strategies. It extracts events from free-text news and combines them with technical indicators using genetic programming to create trading rules. The results show that news information improves the trading rules, validating the framework's utility.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:29:06.498396
dde2fd0b63571dc0,An Efficient Deep Learning Based Model to Predict Interest Rate Using Twitter Sentiment,"In macroeconomics, decision making is highly sensitive and significantly influences the financial and business world, where the interest rate is a crucial factor. In addition, the interest rate is used by the governments to manage the monetary policy. There is a need to design an efficient algorithm for interest rate prediction. The analysis of the social media sentiment impact on financial decision making is also an open research area. In this study, we deploy a deep learning model for the accurate forecasting of the interest rate for the UK, Turkey, China, Hong Kong, and Mexico. For this purpose, daily data of the interest rate and exchange rate covering the period from Jan 2010 to Oct 2019 is used for all the mentioned countries. We also incorporate the input of the twitter sentiments of six mega-events, namely the US election 2012, Mexican election 2012, Gaza under attack 2014, Hong Kong protest 2014, Refugee Welcome 2015, and Brexit 2016. Our results provide evidence that the error of the deep learning model significantly decreases when event sentiment is incorporated. A notable improvement has been observed in the case of the Hong Kong interest rate, i.e., a 266% decline in the error after incorporating event sentiments as an input in the deep learning model.","Yasir, Muhammad; Afzal, Sitara; Latif, Khalid; Chaudhary, Ghulam Mujtaba; Malik, Nazish Yameen; Shahzad, Farhan; Song, Oh-young",2020,10.3390/su12041660,,wos,"This study proposes a deep learning model to predict interest rates for the UK, Turkey, China, Hong Kong, and Mexico, incorporating daily interest rate, exchange rate, and Twitter sentiment data from six major global events between 2010 and 2019. The model demonstrated improved accuracy, particularly for Hong Kong, with a significant reduction in prediction error when event sentiment was included.",True,False,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T15:29:23.025579
f3ace5f4c4fc6584,An Empirical Investigation of the Level Effect in Australian Interest Rates,"An extensive literature examines the dynamics of interest rates, with particular attention given to the positive relationship between interest-rate volatility and the level of interest rates—the so-called level effect. This paper examines the interaction between the estimated level effect and competing parameterisations of interest-rate volatility for the Australian yield curve. We adopt a new methodology that estimates elasticity in a multivariate setting that explicitly accommodates the correlations that exist between various yield factors. Results show that significant correlations exist between the residuals of yield factors and that such correlations do indeed impact on model estimates. Within the multivariate setting, the level of the short rate is shown to be a crucial determinant of the conditional volatility of all three yield factors. Measures of model fit suggest that, in addition to the usual level effect, the incorporation of GARCH effects and possible regime shifts is important. © 2008, SAGE Publications. All rights reserved. © 2019 Elsevier B.V., All rights reserved.","Gray, P.; Smith, D.R.",2008,10.1177/031289620803300103,https://www.scopus.com/inward/record.uri?eid=2-s2.0-54849404302&doi=10.1177%2F031289620803300103&partnerID=40&md5=dd13438b705d6c7e42bc5b6e7c4e98a3,scopus,"This paper empirically investigates the level effect in Australian interest rates, examining the relationship between interest rate volatility and the level of interest rates. Using a new multivariate methodology that accounts for correlations between yield factors, the study finds that the level of the short rate significantly influences the conditional volatility of all yield factors. The results suggest that incorporating GARCH effects and regime shifts is important for model fit, in addition to the standard level effect.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:29:36.586494
b51ac46514e0b0ea,An automated financial indices-processing scheme for classifying market liquidity regimes,"A multivariate hidden Markov model (HMM)-based approach is developed to capture simultaneously the regime-switching dynamics of four financial market indicators: Treasury-Euro Dollar rate spread, US dollar index, volatility index and S&P 500 bid-ask spread. These indicators exhibit stochasticity, mean reversion, spikes and state memory, and they are deemed to drive the main characteristics of liquidity risk and regarded to mirror financial markets' liquidity levels. In this paper, an online system is proposed in which observed indicators are processed and the results are then interfaced with an advanced alert mechanism that gives out appropriate measures. In particular, two stochastic models, with HMM-modulated parameters switching between liquidity regimes, are integrated to capture the evolutions of the four time series or their transformations. Parameter estimation is accomplished by deriving adaptive multivariate filters. Indicators' joint empirical characteristics are captured well and useful early warnings are obtained for occurrence prediction of illiquidity episodes.",,2021,10.1080/00207179.2019.1616225,,proquest,"This paper proposes an automated system using a multivariate hidden Markov model (HMM) to classify market liquidity regimes based on four financial indicators: Treasury-Euro Dollar rate spread, US dollar index, volatility index, and S&P 500 bid-ask spread. The system processes these indicators and interfaces with an alert mechanism to provide early warnings for illiquidity episodes.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:29:42.172556
4b946252a5b5a44e,An indicator of future inflation extracted from the steepness of the interest rate yield curve along its entire length,"The term-structure slope contains information about expected future inflation. Mishkin shows that the spread between the twelve-month and three-month interest rates helps predict the difference between twelve-month and three-month inflation. We apply a simple existing framework, which lets the real interest rate vary in the short run but converge to a constant in the long run, to this problem. The appropriate indicator of expected inflation uses the entire length of the yield curve, estimating the steepness of a specific nonlinear transformation, rather than being restricted to a spread between two points. The resulting indicator better predicts inflation, over 1960–1991. © 1994 by the President and Fellows of Harvard College and the Massachusetts Institute of Technology. © 2016 Elsevier B.V., All rights reserved.","Frankel, J.A.; Lown, C.S.",1994,10.2307/2118472,https://www.scopus.com/inward/record.uri?eid=2-s2.0-21344483312&doi=10.2307%2F2118472&partnerID=40&md5=f5ed43ec582dfc6b2cde18634449f72f,scopus,"This paper proposes an indicator for future inflation derived from the entire length of the interest rate yield curve, rather than just a two-point spread. The method involves estimating the steepness of a nonlinear transformation of the yield curve. The resulting indicator demonstrates improved inflation prediction capabilities over the period 1960-1991.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:29:55.656834
e254ebb1201c5b13,An interval constraint-based trading strategy with social sentiment for the stock market,"Developing effective strategies to earn excess returns in the stock market is a cutting-edge topic in the field of economics. At the same time, stock price forecasting that supports trading strategies is considered one of the most challenging tasks. Therefore, this study analyzes and extracts news media data, expert comments, social opinion data, and pandemic text data using natural language processing, and then combines the data with a deep learning model to forecast future stock price patterns based on historical stock prices. An interval constraint-based trading strategy is constructed. Using data from several typical stocks in the Chinese stock market during the COVID-19 period, the empirical studies and trading simulations show, first, that the sentiment composite index and the deep learning model can improve the accuracy of stock price forecasting. Second, the interval constraint-based trading strategy based on the proposed approach can effectively enhance returns and thus, can assist investors in decision-making.",,2024,10.1186/s40854-023-00567-2,,proquest,"This study proposes an interval constraint-based trading strategy for the stock market by combining social sentiment analysis (extracted using NLP from news, expert comments, social opinions, and pandemic data) with a deep learning model for stock price forecasting. Empirical results from the Chinese stock market during COVID-19 indicate that the sentiment index and deep learning model improve forecasting accuracy, and the proposed strategy enhances returns, aiding investor decision-making.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:30:00.787741
0f172ab615288205,An overview of maintenance management strategies for corroded steel structures in extreme marine environments,"Maintenance is playing an important role in integrity management of marine assets such as ship structures, offshore renewable energy platforms and subsea oil and gas facilities. The service life of marine assets is heavily influenced by the involvement of numerous material degradation processes (such as fatigue cracking, corrosion and pitting) as well as environmental stresses that vary with geographic locations and climatic factors. The composition of seawater constituents (e.g. dissolved oxygen, salinity, temperature content, etc.) is one of the major influencing factors in degradation of marine assets. Improving the efficiency and effectiveness of maintenance management strategies can have a significant impact on operational availability and reliability of marine assets. Many research studies have been conducted over the past few decades to predict the degradation behaviour of marine structures operating under different environmental conditions. The utilisation of structural degradation data – particularly on marine corrosion – can be very useful in developing a reliable, risk-free and cost-effective maintenance strategy. This paper presents an overview of the state-of-the-art and future trends in asset maintenance management strategies applied to corroded steel structures in extreme marine environments. The corrosion prediction models as well as industry best practices on maintenance of marine steel structures are extensively reviewed and analysed. Furthermore, some applications of advanced technologies such as computerized maintenance management system (CMMS), artificial intelligence (AI) and Bayesian network (BN) are discussed. Our review reveals that there are significant variations in corrosion behaviour of marine steel structures and their industrial maintenance practices from one climatic condition to another. This has been found to be largely attributed to variation in seawater composition/characteristics and their complex mutual relationships. © 2020 Elsevier B.V., All rights reserved.","Abbas, M.; Shafiee, M.",2020,10.1016/j.marstruc.2020.102718,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078962228&doi=10.1016%2Fj.marstruc.2020.102718&partnerID=40&md5=06e8029f84c8b4e4d649d53d85207f63,scopus,"This paper provides an overview of maintenance management strategies for corroded steel structures in extreme marine environments. It reviews corrosion prediction models, industry best practices, and discusses applications of advanced technologies like CMMS, AI, and Bayesian networks. The study highlights variations in corrosion behavior and maintenance practices due to differing seawater compositions and climatic conditions.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:30:07.507856
90fdb546691bd626,Analysis of herding behavior in individual investor portfolios using machine learning algorithms,"This paper examines the determinants of herding at both stock and individual investor levels and studies the portfolio performance of herd vs. non-herd portfolios using machine learning algorithms. The disposition effect and the attention effect seem to explain herding behavior at the stock level. At the individual investor level, the cumulative number of buys and portfolio values reduce the prediction of herding behavior, while high values of portfolio return lead to a small increase in herding. Individuals who herd do not outperform either market or non-herd portfolios, suggesting that herding is a behavioral bias. Thus, such behavior seems to destabilize stock markets, creating temporary discrepancies in stock prices followed by reversals back to fundamentals. The most predictive factor in the performance tests of individual portfolios is the market risk premium and using equally-weighted factors rather than value-weighted factors seem to provide more consistent results in the portfolio performance analyses. © 2022 Elsevier B.V., All rights reserved.","Mavruk, T.",2022,10.1016/j.ribaf.2022.101740,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136526119&doi=10.1016%2Fj.ribaf.2022.101740&partnerID=40&md5=9a05e65026a399027b17b7c424c6c9fc,scopus,"This study investigates herding behavior in individual investor portfolios using machine learning. It identifies factors influencing herding at stock and individual levels, finding that herding investors do not outperform non-herding ones, suggesting it's a behavioral bias that can destabilize markets. The analysis highlights market risk premium and factor weighting as key to portfolio performance.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:30:14.401507
1b54463d1a0cc54d,Analysis of multifactor affine yield curve models,"In finance and economics much work has been done on the theoretical modeling and statistical estimation of the yield curve, defined as the relationship between -1/τ logp <inf>t</inf>.(τ) and τ,where p <inf>t</inf>(τ) is the time t price of a zero - coupon bond with payoff 1 at maturity date t + τ. Of considerable current interest are models of the yield curve in which a collection of observed and latent factors determine the market price of factor risks, the stochastic discount factor, and the arbitrage - free bond prices. The model is particularly interesting from a statistical perspective, because the yields are complicated nonlinear functions of the underlying parameters (e.g., those appearing in the evolution dynamics of the factors and those appearing in the model of the factor risks). This nonlinearity tends to produce a likelihood function that is multimodal. In this article we revisit the question of how such models should be fit from the Bayesian viewpoint. Key aspects of the inferential framework include (a) a prior on the parameters of the model that is motivated by economic considerations, in particular, those involving the slope of the implied yield curve; (b) posterior simulation of the parameters in ways to improve the efficiency of the MCMC output, for example, through sampling of the parameters marginalized over the factors and tailoring of the proposal densities in the Metropolis - Hastings steps using information about the mode and curvature of the current target based on the output of a simulating annealing algorithm; and (c) measures to mitigate numerical instabilities in the fitting through reparameterizations and square root filtering recursions. We apply the techniques to explain the monthly yields on nine U.S. Treasury Bills (with maturities ranging from 1 month to 120 months) over the period January 1986 - December 2005. The model contains three factors, one latent and two observed. We also consider the problem of predicting the nine yields for each month of 2006. We show that the (multi-step-ahead) prediction regions properly bracket the actual yields in those months, thus highlighting the practical value of the fitted model. © 2009 American Statistical Association. © 2012 Elsevier B.V., All rights reserved.","Chib, S.; Ergashev, B.",2009,10.1198/jasa.2009.ap08029,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049132711&doi=10.1198%2Fjasa.2009.ap08029&partnerID=40&md5=e3022f035f007aa8736ec76106515f58,scopus,"This article revisits the Bayesian approach to fitting multifactor affine yield curve models, focusing on statistical inference and practical application. It proposes an inferential framework incorporating economically motivated priors, efficient posterior simulation techniques (MCMC, Metropolis-Hastings, simulated annealing), and methods to address numerical instabilities. The techniques are applied to U.S. Treasury Bill yields from 1986-2005, using a three-factor model (one latent, two observed). The study also addresses yield prediction for 2006, demonstrating the practical utility of the fitted model through accurate prediction regions.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T15:30:24.638310
f9ea0fbfd77a9f06,Analysis of the relevance of sentiment data for the prediction of excess returns in a multiasset framework,"In this study, we look at the relevance of sentiment data for the prediction of excess returns in a multiasset analysis. We start by initial exploratory data analysis in order to assess the pertinence of the sentiment data. We then compare the performance of rule-based algorithms with and without the sentiment data. The data considered are provided by RavenPack. Finally, we explore the economic relevance of the forecast model in a long-only and long-short context. Inclusion of sentiment data leads to encouraging results.","Desforges, Perceval; Geissler, Christophe; Liu, Fei",2023,10.1002/for.2967,,wos,"This study investigates the predictive power of sentiment data for excess returns in a multi-asset framework. It compares rule-based algorithms with and without sentiment data, using data from RavenPack, and explores the economic relevance of the forecasting model in different investment contexts. The inclusion of sentiment data yielded positive results.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:30:49.490318
ef29280b50c0dfb2,Anchoring Bias in Consensus Forecasts and Its Effect on Market Prices,"Previous empirical studies on the rationality of economic. and financial forecasts generally test for generic properties such as bias or autocorrelated errors but provide only limited insight into the behavior behind inefficient forecasts. This paper tests for a specific form of forecast bias. In particular, we examine whether expert consensus forecasts of monthly economic releases are systematically biased toward the value of previous months' releases. Such a bias would be consistent with the anchoring and adjustment heuristic described by Tversky and Kahneman (1974) or could arise from professional forecasters' strategic incentives. We find broad-based and significant evidence for this form of bias, which in some cases results in sizable predictable forecast errors. To investigate whether market participants' expectations are influenced by this bias, we examine interest rate reactions to economic news. We find that bond yields react only to the residual, or unpredictable, component of the forecast error and not to the component induced by anchoring, suggesting that expectations of market participants anticipate this bias embedded in expert forecasts.","Campbell, Sean D.; Sharpe, Steven A.",2009,10.1017/s0022109009090127,,wos,"This study investigates anchoring bias in consensus forecasts of economic releases, finding significant evidence that forecasts are influenced by previous months' values. The research also examines the impact of this bias on market prices by analyzing interest rate reactions to economic news. Results indicate that bond yields react only to the unpredictable component of forecast errors, suggesting market participants anticipate the anchoring bias in expert forecasts.",False,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:30:57.721935
30d5228daba342c6,Arbitrage bounds in markets with noisy prices and the puzzle of negative option prices implicit in bonds,"The term structure of interest rates has occupied economists for many years. This is testimony to the importance of the term structure (or, equivalently, valuation operator) and to the difficulty of obtaining reliable estimates of it. Until recently, it was not possible to reconcile the theoretical existence of a multiplicity of valuation operators in an incomplete bond market and the empirical nonexistence of even a single one. MacKay and Prisman [Estimating valuation operators in incomplete market with noises: Can noise complete the market. Working paper, 1996] tackle this problem. In this paper, an amendment to and a generalization of their methodology is presented. The amendment preserves the linearity of valuation operators and allows use of efficient linear programming techniques. Further, it facilitates an admissible consideration of the puzzle of negative option prices embedded in bonds. The empirical results presented in this paper were obtained using data on extendable Government of Canada bonds. The results indicate that although the gap between the lowest and the highest prices assigned to a cash flow by different valuation operators is significant, it is not sufficient to resolve the above-mentioned puzzle. (C) 2002 Elsevier Science B.V. All rights reserved.","Ioffe, LD",2002,10.1016/s0378-4266(01)00172-8,,wos,"This paper presents an amendment and generalization to MacKay and Prisman's methodology for estimating valuation operators in incomplete bond markets with noisy prices. It preserves linearity, allows for efficient linear programming, and addresses the puzzle of negative option prices implicit in bonds. Empirical results using Canadian government bond data show that while valuation operator price gaps are significant, they do not resolve the puzzle.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:31:02.910310
91e92775d7b84bf0,Are quantitative easing effects transitory? Evidence from out-of-sample forecasts,"Purpose>Advocates of quantitative easing (QE) policies have emphasized some evidence that structural models do not predict long-term asset yields as well as naive forecasts, implying that predictions of price reversals cannot be profitable and that QE effects are not transitory. The purpose of this study is to reconsider the out-of-sample forecasting performance of structural time series processes relative to that of a random walk with or without drift.Design/methodology/approach>This study uses bivariate vector autoregression and Markov switching representations to generate out-of-sample forecasts of ten-year sovereign bond yields, when the information set is augmented by including the growth rate of the monetary base, and the estimation relies on monthly data from countries that have pursued unconventional policies over the last decade.Findings>The results show that naive forecasts are not better than those of structural time series models, based on root mean squared errors, while the Markov model provides additional information on price reversals, through probabilistic inferences regarding policy regime switches, which can induce agents to counteract QE interventions and reduce their effectiveness.Originality/value>The novel features of this work are the use of a large information set including the instrument of unconventional monetary policy, the use of a structural model (Markov process) that can really inform about potential asset price reversals and the use of a large sample over which QE policies have been pursued.",,2022,10.1108/jfep-04-2022-0099,,proquest,"This study investigates whether quantitative easing (QE) effects are transitory by comparing the out-of-sample forecasting performance of structural time series models against naive forecasts. Using bivariate vector autoregression and Markov switching models with monthly data from countries that have implemented QE, the research finds that naive forecasts are not superior to structural models. The Markov model, in particular, offers insights into price reversals and policy regime switches that can mitigate QE effectiveness.",True,True,True,gemini-2.5-flash-lite,Olav,M,,2025-10-13T15:31:24.629128
bd15d7787199c49d,Artificial Intelligence and Firm Performance: Does Machine Intelligence Shield Firms from Risks?,"We estimate and compare the impact of the coronavirus pandemic (COVID-19) on the performance of Artificial Intelligence (AI) and conventional listed firms using stock market indices. The single-group and multiple-group Interrupted Time-Series Analyses (ITSA) with panel data were used with four interventions: when the news of COVID-19 spread and the pandemic entered the first, second, third, and fourth months (24 February 2020, 23 March 2020, 20 April 2020, and 18 May 2020, respectively). The results show that the negative impact of COVID-19 on the AI stock market was less severe than on the conventional stock market in the first month of the pandemic. The performance of the AI stock market recovered quicker than the conventional stock market when the pandemic went into its third month. The results suggest that the AI stocks were more resilient than conventional stocks when the financial market was exposed to uncertainty caused by the COVID-19 pandemic. The deployment of AI in firms serves as a resilient, crucial driver for sustainable performance in challenging environments. Observing the performance of AI-adopted firms is an interesting direction for technical and fundamental analysts. Investors and portfolio managers should consider an AI market index to minimize risk or invest in stocks of AI-adopted listed firms to maximize excess returns.",,2022,10.3390/jrfm15070302,,proquest,"This study investigates the impact of the COVID-19 pandemic on the performance of Artificial Intelligence (AI) firms compared to conventional firms using stock market data. The analysis, employing Interrupted Time-Series Analyses (ITSA), found that AI stocks were less affected and recovered faster than conventional stocks during the pandemic. The findings suggest that AI adoption can enhance firm resilience and performance in uncertain environments, making AI-adopted firms an attractive investment.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:31:30.281225
c7505f1ac2274c1a,Artificial Intelligence in Economics Research: What Have We Learned? What Do We Need to Learn?,"Motivated by the recent boom in artificial intelligence (AI) playing a significant role in the economics of individuals, firms, and government bodies, we investigate the role of AI in economics by reviewing the literature (2231 articles) during the last 34 years (1990 to November 2024). We identify five research streams: (1) AI and economic modeling, (2) AI and macroeconomics (eight sub-streams), (3) AI and equity and debt market, (4) AI and prediction modeling (three sub-streams), and (5) AI and economics of innovation. Further, we offer suggestions for future research (20 questions). Additionally, we outline a framework to consider changes in economics before and after AI adoption. Further, the critical AI-based methods are identified and discussed.","Bahoo, Salman; Goodell, John W.; Rhattat, Rachid; Shahid, Subhan",2025,10.1111/joes.12694,,wos,"This review examines the role of AI in economics over the past 34 years, identifying five research streams: AI and economic modeling, macroeconomics, equity and debt markets, prediction modeling, and innovation. It suggests future research directions and outlines a framework for understanding economic changes due to AI adoption, while also discussing critical AI methods.",False,False,True,gemini-2.5-flash-lite,Olav,Review,,2025-10-13T15:31:36.415685
cecdcfa68c964887,Assessing the relevance of sell-side analyst recommendations,"This paper evaluates the informational value and alpha-generating potential of sell-side analyst recommendations. We explore this by employing a monthly portfolio-sorted long-short strategy based on consensus analyst recommendations. Our findings indicate that the long-short equal-weighted and value-weighted portfolios yield significant excess returns. However, the value-weighted excess returns are primarily driven by the predictive power of the lowest decile (sell recommendations). The long-short strategy for the value-weighted portfolios yields a monthly excess return ranging from 1.36% to 1.57%, above the risk-free rate. Our analysis further examines variations in recommendation effectiveness across economic cycles, industries, investment banks/brokers, and firm sizes, providing further insights into the value of analyst recommendations.","Aguegboh, Ekene S.; Onuoha, Uchenna C.; Patel, Poojan",2025,10.1002/rfe.70015,,wos,"This study assesses the informational value and alpha-generating potential of sell-side analyst recommendations using a monthly portfolio-sorted long-short strategy. Significant excess returns were found, particularly for value-weighted portfolios driven by sell recommendations. The effectiveness of recommendations was further analyzed across various factors like economic cycles and firm sizes.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:31:41.447917
2cc3114ff3eae9ca,"Asset Returns Under Model Uncertainty: Evidence from the Euro Area, the US and the UK","We analyze predictability of risk premium in the context of model uncertainty. Using data for the euro area, the US and the UK, we show that there is a large amount of model uncertainty and one can improve the forecasts of stock returns with a Bayesian Model Averaging (BMA) approach. The empirical evidence for the euro area suggests that several macroeconomic, financial and macro-financial variables are consistently among the most prominent determinants of risk premium. As for the US, only a few number of predictors play an important role. In the case of the UK, future stock returns are better forecasted by financial variables. These results are corroborated for both the M-open and the M-closed perspectives, different model priors and in the context of “in-sample” and “out-of-sample” forecasting. Finally, we highlight that the predictive ability of the BMA framework is stronger at longer periods, and clearly outperforms the constant expected returns and the autoregressive benchmark models.",,2019,10.1007/s10614-017-9696-2,,proquest,"This study investigates risk premium predictability under model uncertainty using Bayesian Model Averaging (BMA) for the euro area, US, and UK. It demonstrates that BMA improves stock return forecasts compared to simpler models and highlights different key predictors across regions (macroeconomic/financial for the euro area, financial for the UK, and few for the US). The BMA framework's predictive ability is stronger for longer periods and outperforms benchmark models.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:31:52.921846
db174cb1df2093f1,Asset prices in a production network,"The relative importance of sectoral and aggregate productivity shocks in asset pricing is examined using a nonlinear dynamic equilibrium model where heterogeneous sectors interact in a production network. The model accounts for the heterogeneity in sectoral stock returns and endogenously generates conditional heteroskedasticity and fat tails. The equity risk premium is shown to be driven by sectoral shocks – specially to investment good producers and mining – with a limited contribution from the aggregate shock. SMM estimates of the elasticities of substitution between material inputs and between investment goods support the assumption of gross complementarity employed by previous network literature. © 2024 Elsevier B.V., All rights reserved.","Ruge-Murciá, F.",2024,10.1016/j.euroecorev.2024.104751,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193980037&doi=10.1016%2Fj.euroecorev.2024.104751&partnerID=40&md5=09a671fb1dc467fc6c86b4a434bc87f1,scopus,"This paper uses a nonlinear dynamic equilibrium model with a production network to analyze asset pricing. It finds that sectoral productivity shocks, particularly those affecting investment good producers and mining, are the primary drivers of the equity risk premium, with aggregate shocks playing a limited role. The model also accounts for heterogeneous sectoral returns, conditional heteroskedasticity, and fat tails. The study's findings on input substitution elasticities align with previous network literature.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:31:55.533194
7be43e8757080b63,Asset pricing and foreign exchange risk,"According to the International Capital Asset Pricing Model (ICAPM), the covariance of assets with foreign exchange currency returns should be a risk factor that must be priced when the purchasing power parity is violated. The goal of this study is to re-examine the relationship between stock returns and foreign exchange risk. The novelties of this work are: (a) a data set that makes use of daily observations for the measurement of the foreign exchange exposure and volatility of the sample firms and (b) data from a Eurozone country. The methodology we make use in reference to the estimation of the sensitivity of each stock to exchange rate movements is that it allows regressing stock returns against factors controlling for market risk, size, value, momentum, foreign exchange exposure and foreign exchange volatility. Stocks are then classified according to their foreign exchange sensitivity portfolios and the return of a hedge (zero-investment) portfolio is calculated. Next, the abnormal returns of the hedge portfolio are regressed against the return of the factors. Finally, we construct a foreign exchange risk factor in such manner as to obtain a monotonic relation between foreign exchange risk and expected returns. The empirical findings show that the foreign exchange risk is priced in the cross section of the German stock returns over the period 2000-2008. Furthermore, they show that the relationship between returns and foreign exchange sensitivity is nonlinear, but it takes an inverse U-shape and that foreign exchange sensitivity is larger for small size firms and value stocks. © 2011 Elsevier B.V. © 2011 Elsevier B.V., All rights reserved.","Apergis, N.; Artikis, P.; Sorros, J.",2011,10.1016/j.ribaf.2011.02.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953279666&doi=10.1016%2Fj.ribaf.2011.02.005&partnerID=40&md5=711719c805262e6682f85c3b8ccd3aaf,scopus,"This study re-examines the relationship between stock returns and foreign exchange risk using daily observations from a Eurozone country (Germany) between 2000-2008. It employs a methodology that estimates stock sensitivity to exchange rate movements, controlling for various risk factors. The findings indicate that foreign exchange risk is priced in German stock returns, with a nonlinear, inverse U-shaped relationship between returns and foreign exchange sensitivity. This sensitivity is found to be higher for small-cap and value stocks.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:31:58.368942
34b352d3f2e22998,Asset pricing with financial bubble risk,"This paper characterizes systematic risk stemming from the possible occurrence of price bubbles and measures the impact of this additional risk factor on asset prices. Historical stock market behavior and recent empirical experience have led economists and policy makers to acknowledge that price bubbles in financial markets do occur and need to be accounted for in risk analysis. New econometric tools for analyzing mildly explosive behavior (Phillips and Magdalinos, 2007; Phillips et al., 2011) have made it possible to detect the presence of bubbles in data and to date stamp their origination and collapse, providing empirical confirmation of such episodes in recent data. The potential for price bubbles and market collapse provides another source of stock market risk and adds to the risk premium. We provide an analytic and empirical investigation of this additional risk factor. The standard present value model is extended to allow for possible price bubbles and the effects of integrating bubble behavior into a consumption-based asset pricing model are analyzed. The theory involves attention to the investor time horizon and a study of the validity of conventional log linear approximations in the presence of nonstationary and mildly explosive data. Finite decision horizons accommodate myopic investors and are a component of speculative behavior that focuses on short run market gains rather than long run effects of fundamentals. An econometric approach to estimate bubble risk effects is developed and the methods are applied to composite stock market index data, giving new model-based equity premium and market volatility estimates that more closely match the data than traditional consumption based asset pricing models. © 2016 Elsevier B.V., All rights reserved.","Lee, J.H.; Phillips, P.C.B.",2016,10.1016/j.jempfin.2015.11.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954285389&doi=10.1016%2Fj.jempfin.2015.11.004&partnerID=40&md5=ee8c418940be84168dc9bdae30a188ed,scopus,"This paper investigates the impact of financial bubble risk on asset pricing. It extends the standard present value model to incorporate potential price bubbles and analyzes their effects within a consumption-based asset pricing framework. The study considers investor time horizons and the validity of log-linear approximations with nonstationary data. An econometric approach is developed and applied to stock market data, yielding new estimates for the equity premium and market volatility that better align with observed data compared to traditional models.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:32:50.646669
0c787f997c47e0be,Asset-pricing implications of biologically based non-expected utility,"Results in population ecology suggest that evolutionary successful species should have an adaptive (reference-based) S-shaped utility function that is intrinsically more sensitive to aggregate than uninsured idiosyncratic shocks the former cannot be diversified demographically. To test the asset-pricing relevance of these ideas, I embed the non-expected utility specification implied by evolutionary theory into an economy with partial risk sharing due to limited commitment. For the benchmark specification (CARA = 6 over gains), Monte Carlo simulations of a Markov growth economy produce the following results: (i) matching the degree of consumption-smoothing in the cross section, the Sharpe ratio for a Lucas tree is 0.33, an increase of 44 percent relative to expected utility; (ii) the risk-free rate is low, stable and counter-cyclical, hence equity returns, unlike in the expected utility case, have the correct pattern of predictability; (iii) in the cross section, excess returns across equity classes exhibit both a value premium and a size discount with risk adjusted returns that are at least two times higher than their expected utility counterparts. (C) 2012 Elsevier Inc. All rights reserved.","Iantchev, Emil P.",2013,10.1016/j.red.2012.08.002,,wos,"This paper explores the asset-pricing implications of a biologically inspired non-expected utility function, derived from evolutionary theory. The study embeds this utility specification into an economy with partial risk sharing due to limited commitment. Monte Carlo simulations in a Markov growth economy show that this non-expected utility model can better match cross-sectional consumption smoothing, leading to a higher Sharpe ratio for a Lucas tree compared to the expected utility model. It also generates a low, stable, and counter-cyclical risk-free rate, and provides a more accurate pattern of equity return predictability, including value premiums and size discounts.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:32:54.794687
9cf59f608ef4415d,Autoencoder-Based Three-Factor Model for the Yield Curve of Japanese Government Bonds and a Trading Strategy,"Interest rates are representative indicators that reflect the degree of economic activity. The yield curve, which combines government bond interest rates by maturity, fluctuates to reflect various macroeconomic factors. Central bank monetary policy is one of the significant factors influencing interest rate markets. Generally, when the economy slows down, the central bank tries to stimulate the economy by lowering the policy rate to establish an environment in which companies and individuals can easily raise funds. In Japan, the shape of the yield curve has changed significantly in recent years following major changes in monetary policy. Therefore, an increasing need exists for a model that can flexibly respond to the various shapes of yield curves. In this research, we construct a three-factor model to represent the Japanese yield curve using the machine learning approach of an autoencoder. In addition, we focus on the model parameters of the intermediate layer of the neural network that constitute the autoencoder and confirm that the three automatically generated factors represent the “Level,” “Curvature,” and “Slope” of the yield curve. Furthermore, we develop a long–short strategy for Japanese government bonds by setting their valuation with the autoencoder, and we confirm good performance compared with the trend-follow investment strategy.",,2020,10.3390/jrfm13040082,,proquest,"This study develops an autoencoder-based three-factor model to represent the Japanese government bond yield curve, identifying factors corresponding to Level, Curvature, and Slope. The model's parameters are used to create a long-short trading strategy for Japanese government bonds, which demonstrates superior performance compared to a trend-following strategy.",True,True,False,gemini-2.5-flash-lite,Olav,M,,2025-10-13T15:33:10.496596
28706c3e7bae92e6,BEKKs: An R Package for Estimation of Conditional Volatility of Multivariate Time Series,"We describe the R package BEKKs, which implements the estimation and diagnostic analysis of a prominent family of multivariate generalized autoregressive conditionally heteroskedastic (MGARCH) processes, the so-called BEKK models. Unlike existing software packages, we make use of analytical derivatives implemented in efficient C++ code for nonlinear log-likelihood optimization. This allows fast parameter estimation even in higher model dimensions N > 3. The baseline BEKK model is complemented with an asymmetric parameterization that allows for a flexible modeling of conditional (co)variances. Furthermore, we provide the user with the simplified scalar and diagonal BEKK models to deal with high dimensionality of heteroskedastic time series. The package is designed in an object-oriented way featuring a comprehensive toolbox of methods to investigate and interpret, for instance, volatility impulse response functions, risk estimation and forecasting (VaR) and a backtesting algorithm to compare the forecasting performance of alternative BEKK models. For illustrative purposes, we analyze a bivariate ETF return series (S&P, US treasury bonds) and a four-dimensional system comprising, in addition, a gold ETF and changes of a log oil price by means of the suggested package. We find that the BEKKs package is more than 100 times faster for time series systems of dimension N > 3 than other existing packages. © 2024 Elsevier B.V., All rights reserved.","Fülle, M.J.; Lange, A.; Hafner, C.M.; Herwartz, H.",2024,10.18637/jss.v111.i04,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211314976&doi=10.18637%2Fjss.v111.i04&partnerID=40&md5=64e7416bf0ca62d1bfe6d56bc4055e20,scopus,"This paper introduces the R package BEKKs for estimating and analyzing multivariate GARCH (MGARCH) models, specifically BEKK models. It highlights the package's efficiency through C++ implementation of analytical derivatives, enabling faster parameter estimation, especially for higher dimensions. The package supports asymmetric parameterizations, simplified scalar and diagonal BEKK models, and includes tools for volatility impulse response functions, risk estimation (VaR), and backtesting. The authors demonstrate the package's utility by analyzing bivariate and four-dimensional ETF return series, showing significant speed improvements over existing software.",False,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:33:17.919960
9eb0be674d53d7ff,"Bank risks, capital and loan supply: evidence from Sierra Leone","Purpose -- The study aims to investigate the factors that influence banks' loan supply in Sierra Leone. More specifically, it seeks to look into the effects of risk premium, leverage ratio and credit risk on banks' loan supply in Sierra Leone. Design/methodology/approach -- Using annual bank level data on an unbalanced panel of 13 commercial banks data observed over a period of ten years (2002 to 2011), the study employs time and bank-specific fixed effects model for estimation. Findings -- The findings indicate that risk premium, the share of non-performing loans in the banks' loan portfolio, tier 1 capital ratio (leverage ratio) and local currency deposit levels positively and significantly affect the share of loan supply to the private sector in banks' earning assets. On the other hand, advances to local currency deposit ratio and bank size have significant negative effects on the share of loans in banks assets. The study also finds bank type and the growth rate of real GDP (a proxy for economic activity) to be important determinants of the share of loans in banks' earning assets. Practical implications -- The study recommends that the monetary authorities, banking practitioners and the government should pay keen attention to the key risk factors such as non-performing loans and risk premium in the operation of the banking sector to boost commercial banks' loan supply. Originality/value -- Sierra Leone's banking sector presents a unique opportunity to study bank loan supply in relation to bank-specific features in the context of post-war financial reconstruction. Adapted from the source document.",,2013,10.1108/jfep-09-2012-0041,,proquest,"This study examines factors influencing banks' loan supply in Sierra Leone, specifically the impact of risk premium, leverage ratio, and credit risk. Using a panel data fixed-effects model for 13 commercial banks from 2002-2011, the research found that risk premium, non-performing loans, and tier 1 capital ratio positively affect loan supply. Conversely, advances to local currency deposit ratio and bank size negatively impact loan supply. Bank type and GDP growth rate were also identified as significant determinants. The study suggests monetary authorities and practitioners focus on risk factors like non-performing loans to enhance loan supply.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:33:21.082383
3b84aee29f05f089,Bayesian estimation of stable CARMA spot models for electricity prices,"We develop a Bayesian estimation procedure for the electricity spot price model in Benth et al. (2014). This model incorporates a trend and seasonality component, a stable CARMA process for the price spikes, and an additional Lévy process for mid-range price level changes. Our MCMC algorithm has two advantages over the existing stepwise estimation procedure presented in Benth et al. (2014): First, since our algorithm produces samples from the full posterior distribution over all parameters, we can estimate the parameters much more accurately, which is shown in simulation studies. Second, we can provide accuracy measures as credibility intervals in addition to the point estimates. The approach is quite general, so that it can be adapted also to other similar pricing models. For illustration, we analyse spot and future prices from the EEX using the new Bayesian method and provide estimates for the risk premium together with credibility regions. © 2019 Elsevier B.V., All rights reserved.","Müller, G.; Seibert, A.",2019,10.1016/j.eneco.2018.10.016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057586272&doi=10.1016%2Fj.eneco.2018.10.016&partnerID=40&md5=c80f6183b56a7b047b29e1f7324535da,scopus,"This paper presents a Bayesian estimation procedure for a stable CARMA spot model for electricity prices, which includes trend, seasonality, price spikes, and mid-range price level changes. The proposed MCMC algorithm offers more accurate parameter estimation and provides credibility intervals compared to existing methods. The approach is demonstrated by analyzing EEX spot and future prices, estimating risk premiums and their credibility regions.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:33:27.801899
aaff782e22358a7e,Bayesian inference and state number determination for hidden Markov models: an application to the information content of the yield curve about inflation,"This paper is concerned with Bayesian inference in hidden Markov models. Focusing oil switching regression models, we propose a new methodology that delivers a joint estimation of the parameters and the number of regimes that have actually appeared in the studied sample. The only prior information that is required on the latter quantity is an upper bound. We implement a particle filter algorithm to compute the corresponding estimates. Applying this methodology to the information content of the yield curve regarding future inflation in four OECD countries, we show that the predictive content for given country and combination of maturities is subject to regime switching. (C) 2003 Elsevier B.V. All rights reserved.","Chopin, N; Pelgrin, F",2004,10.1016/j.jeconom.2003.12.010,,wos,"This paper proposes a Bayesian inference methodology for hidden Markov models, specifically for switching regression models. It jointly estimates model parameters and the number of regimes, requiring only an upper bound for the number of regimes. A particle filter algorithm is used for computation. The methodology is applied to analyze the information content of the yield curve regarding future inflation in four OECD countries, revealing regime switching in predictive content.",True,True,True,gemini-2.5-flash-lite,Olav,M,,2025-10-13T15:33:52.994884
b0bcf13b6f1f1508,Bayesian inference for long memory term structure models,"In this study, we propose a novel adaptation of the Dynamic Nelson–Siegel term structure model, incorporating long memory properties to enhance its forecasting accuracy. Our approach involves modelling the evolution of latent factors using fractional Gaussian noise processes, approximated by a weighted sum of independent first-order autoregressive components. The resulting formulation allows for a Gaussian Markov Random Field representation, facilitating the application of computationally efficient Bayesian techniques through Integrated Nested Laplace Approximations. Extensive simulation and empirical analysis demonstrate that integrating long memory significantly improves the model's forecasting performance, particularly for longer time horizons. By shedding light on the potential benefits of incorporating long memory concepts into traditional term structure models, our research highlights its utility in capturing intricate temporal dependencies and enhancing prediction precision.",,2024,10.1080/00949655.2023.2299938,,proquest,"This study adapts the Dynamic Nelson–Siegel term structure model by incorporating long memory properties using fractional Gaussian noise processes. The model is approximated by a weighted sum of first-order autoregressive components, allowing for a Gaussian Markov Random Field representation and efficient Bayesian inference via Integrated Nested Laplace Approximations. Both simulation and empirical analyses show that the long memory integration improves forecasting accuracy, especially for longer horizons, by capturing complex temporal dependencies.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T15:34:15.416499
fd600085dd1ac45f,Bayesian inference via filtering equations for ultrahigh frequency data (I): Model and estimation,"We propose a general partially observed framework of Markov processes with marked point process observations for ultrahigh frequency (UHF) data. The model fits well the stylized facts of UHF data in both macro- and micro-movements, subsumes important existing models, and incorporates the influence of other observable economic or market factors. We develop the corresponding Bayes estimation via a filtering equation to quantify parameter uncertainty. Namely, we derive the normalized filtering equation to characterize the evolution of the posterior distribution, present a weak convergence theorem, and construct consistent, easily parallelizable, recursive algorithms to calculate the joint posteriors and the Bayes estimates for streaming UHF data. Moreover, a sufficient condition for the consistency of the Bayes estimators is provided. The general estimation theory is illustrated by four specific models built for U.S. Treasury notes transactions data from GovPX. We show that in this market, both information-based and inventory management-based motives are significant factors in the trade-to-trade price volatility. © 2021 Elsevier B.V., All rights reserved.","Hu, G.X.; Kuipers, D.R.; Zeng, Y.",2018,10.1137/16m1094762,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046007287&doi=10.1137%2F16M1094762&partnerID=40&md5=09a7b04e5b748673463cf6778c6055f0,scopus,"This paper introduces a Bayesian inference framework for ultrahigh frequency (UHF) data using partially observed Markov processes and marked point process observations. It develops a filtering equation for parameter estimation and provides algorithms for recursive calculation of posterior distributions and Bayes estimates for streaming UHF data. The model is illustrated with U.S. Treasury notes transactions data, revealing significant factors influencing price volatility.",True,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:34:29.394987
4353d80b6dc1ec37,Bias in estimating multivariate and univariate diffusions,"Multivariate continuous time models are now widely used in economics and finance. Empirical applications typically rely on some process of discretization so that the system may be estimated with discrete data. This paper introduces a framework for discretizing linear multivariate continuous time systems that includes the commonly used Euler and trapezoidal approximations as special cases and leads to a general class of estimators for the mean reversion matrix. Asymptotic distributions and bias formulae are obtained for estimates of the mean reversion parameter. Explicit expressions are given for the discretization bias and its relationship to estimation bias in both multivariate and in univariate settings. in the univariate context, we compare the performance of the two approximation methods relative to exact maximum likelihood (ML) in terms of bias and variance for the Vasicek process. The bias and the variance of the Euler method are found to be smaller than the trapezoidal method, which are in turn smaller than those of exact ML. Simulations suggest that when the mean reversion is slow, the approximation methods work better than ML, the bias formulae are accurate, and for scalar models the estimates obtained from the two approximate methods have smaller bias and variance than exact ML For the square root process, the Euler method outperforms the Nowman method in terms of both bias and variance. Simulation evidence indicates that the Euler method has smaller bias and variance than exact ML, Nowman's method and the Milstein method. (C) 2010 Elsevier B.V. All rights reserved.","Wang, Xiaohu; Phillips, Peter C. B.; Yu, Jun",2011,10.1016/j.jeconom.2010.12.006,,wos,"This paper presents a framework for discretizing linear multivariate continuous time systems, including Euler and trapezoidal approximations, to estimate the mean reversion matrix. It derives asymptotic distributions and bias formulas for the mean reversion parameter, detailing discretization bias and its relation to estimation bias in both multivariate and univariate settings. The study compares the Euler and trapezoidal methods against exact maximum likelihood (ML) for the Vasicek process, finding that approximation methods generally have smaller bias and variance than ML, especially for scalar models with slow mean reversion. For the square root process, the Euler method is shown to outperform other methods in terms of bias and variance.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:34:32.871588
d7cc9979d5792ab1,Biased Bayesian learning with an application to the risk-free rate puzzle,"Based on the axiomatic framework of Choquet decision theory, we develop a closed-form model of Bayesian learning with ambiguous beliefs about the mean of a normal distribution. In contrast to rational models of Bayesian learning the resulting Choquet Bayesian estimator results in a long-run bias that reflects the agent's ambiguity attitudes. By calibrating the standard equilibrium conditions of the consumption based asset pricing model we illustrate that our approach contributes towards a resolution of the risk-free rate puzzle. For a plausible parameterization we obtain a risk-free rate in the range of 3.5-5%. This is 1-2.5% closer to the empirical risk-free rate than according calibrations of the rational expectations model. (C) 2013 Elsevier B.V. All rights reserved.","Ludwig, Alexander; Zimper, Alexander",2014,10.1016/j.jedc.2013.11.007,,wos,"This paper presents a model of Bayesian learning with ambiguous beliefs about the mean of a normal distribution, based on Choquet decision theory. The model results in a long-run bias that reflects the agent's ambiguity attitudes. When applied to the consumption-based asset pricing model, this approach helps resolve the risk-free rate puzzle, yielding a risk-free rate closer to empirical observations than rational expectations models.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:34:45.359339
732f5350febe431a,"Big data analytics in economics: What have we learned so far, and where should we go from here?","Research into predictive accuracy testing remains at the forefront of the forecasting field. One reason for this is that rankings of predictive accuracy across alternative models, which under misspecification are loss function dependent, are universally utilized to assess the usefulness of econometric models. A second reason, which corresponds to the objective of this paper, is that researchers are currently focusing considerable attention on so‐called big data and on new (and old) tools that are available for the analysis of this data. One of the objectives in this field is the assessment of whether big data leads to improvement in forecast accuracy. In this survey paper, we discuss some of the latest (and most interesting) methods currently available for analyzing and utilizing big data when the objective is improved prediction. Our discussion includes a summary of various so‐called dimension reduction, shrinkage and machine learning methods as well as a summary of recent tools that are useful for ranking prediction models associated with the implementation of these methods. We also provide a brief empirical illustration of big data in action, in which we show that big data are indeed useful when predicting the term structure of interest rates.",,2018,10.1111/caje.12336,,proquest,"This survey paper discusses methods for analyzing big data to improve prediction accuracy, focusing on dimension reduction, shrinkage, and machine learning techniques. It includes an empirical illustration showing the usefulness of big data in predicting the term structure of interest rates.",True,True,True,gemini-2.5-flash-lite,Olav,Review,,2025-10-13T15:34:53.766389
60a879db8106ce03,Bilateral multiple gamma returns: Their risks and rewards,"The bilateral gamma model for returns is naturally derived from the lognormal model. Maximizing entropy in a random time change delivers the symmetric variance gamma model. The asymmetric variance gamma follows on incorporating skewness. Differential speeds for the upward and downward motions lead to the bilateral gamma. A further generalizations results in the bilateral double gamma model when the speed parameter of the bilateral gamma model is itself taken to be gamma distributed on entropy maximization. A rich five to seven parameter specification of preferences renders possible the extraction of physical densities from option prices. The quality of such extraction is measured by examining the uniformity of the estimated distribution functions evaluated at realized forward returns. The economic value of risky returns is seen to embed three/five risk premia for the bilateral gamma/bilateral double gamma. For the bilateral gamma they are up and down side volatilities compensated in up and down side drifts, and the down side drift compensated in the up side drift. For the bilateral double gamma one adds in addition compensations for skewness. Results reveal a drop in the down side risk premiumsince the crisis with an increase in the recent period.","Madan, Dilip B.; Schoutens, Wim; Wang, King",2020,10.1142/s2424786320500085,,wos,"This paper introduces the bilateral gamma model for returns, derived from the lognormal model and incorporating skewness and differential speeds for upward and downward movements. It generalizes to the bilateral double gamma model. The model allows for the extraction of physical densities from option prices using a five to seven parameter specification of preferences. The economic value of risky returns is analyzed through risk premia, identifying three/five such premia for the bilateral gamma/bilateral double gamma models, respectively. The study observes a decrease in downside risk premiums since the crisis and an increase in the recent period.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:35:08.831477
72beeece03449e4d,Bitcoin transaction strategy construction based on deep reinforcement learning,"The emerging cryptocurrency market has lately received great attention for asset allocation due to its decentralization uniqueness. However, its volatility and brand new trading mode has made it challenging to devising an acceptable automatically-generating strategy. This study proposes a framework for automatic high-frequency bitcoin transactions based on a deep reinforcement learning algorithm — proximal policy optimization (PPO). The framework creatively regards the transaction process as actions, returns as awards and prices as states to align with the idea of reinforcement learning. It compares advanced machine learning-based models for static price predictions including support vector machine (SVM), multi-layer perceptron (MLP), long short-term memory (LSTM), temporal convolutional network (TCN), and Transformer by applying them to the real-time bitcoin price and the experimental results demonstrate that LSTM outperforms. Then an automatically-generating transaction strategy is constructed building on PPO with LSTM as the basis to construct the policy. Extensive empirical studies validate that the proposed method perform superiorly to various common trading strategy benchmarks for a single financial product. The approach is able to trade bitcoins in a simulated environment with synchronous data and obtains a 31.67% more return than that of the best benchmark, improving the benchmark by 12.75%. The proposed framework can earn excess returns through both the period of volatility and surge, which opens the door to research on building a single cryptocurrency trading strategy based on deep learning. Visualizations of trading the process show how the model handles high-frequency transactions to provide inspiration and demonstrate that it can be expanded to other financial products. © 2021 Elsevier B.V., All rights reserved.","Liu, F.; Li, Y.; Li, B.; Li, J.; Xie, H.",2021,10.1016/j.asoc.2021.107952,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117191473&doi=10.1016%2Fj.asoc.2021.107952&partnerID=40&md5=3f33a1f33bc09779c5d4f95e042f6975,scopus,"This study proposes a deep reinforcement learning framework using Proximal Policy Optimization (PPO) with Long Short-Term Memory (LSTM) for automated high-frequency Bitcoin trading. The framework treats transactions as actions, returns as rewards, and prices as states. It compares various machine learning models for price prediction, finding LSTM to be superior. The PPO-LSTM strategy outperforms common trading benchmarks, achieving higher returns and demonstrating effectiveness during volatile periods. The approach is validated in a simulated environment and can potentially be expanded to other financial products.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:35:16.783220
b347734b53091f80,Black-Scholes Fuzzy Numbers as Indexes of Performance,"We use the set of propositions of some previous papers to define a fuzzy version of the Black-Scholes value where the risk free instantaneous interest intensity, the volatility and the initial stock price are fuzzy numbers whose parameters are built with statistical financial data. With our Black-Scholes fuzzy numbers we define indexes of performance varing in time. As an example, with data of the Italian Stock Exchange on MIB30, we see that in 2004 and 2006 our indexes are negative, that is, they are indexes of the refuse to invest and this refuse increased. So, on November 11, 2006 we could forecast that the market will become with more risk: the risk of loss will increase. Now, on January 25, 2010, we know that this forecast has happened. Obviously, the parameters of our Black-Scholes fuzzy numbers can be valued also with incomplete, possibilistic data. With respect to the probabilistic one, our fuzzy method is more simple and immediate to have a forecast on the financial market.",,2010,10.1155/2010/607214,,proquest,"This paper introduces a fuzzy version of the Black-Scholes model, treating key parameters like interest rates and stock prices as fuzzy numbers derived from financial data. These fuzzy numbers are used to create time-varying performance indexes. An example using Italian Stock Exchange data shows these indexes can indicate a 'refusal to invest' and forecast increased market risk, a prediction that reportedly came true. The authors suggest their fuzzy approach is simpler and more immediate than probabilistic methods for market forecasting, and can handle incomplete or possibilistic data.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:35:23.245905
244baf9a591723da,Bond Market Prediction using an Ensemble of Neural Networks,"The characteristics of a successful financial forecasting system are the exploitation of inefficiencies of a given market and the precise application to that market. Overwhelming evidence indicates that opportunities exist for consistent positive returns over a given period of time. This project aims to provide means for the yield curve projection of government bonds. An ensemble of networks such as back propagation, radial basis function, linear regression, is used to predict the yield. The yield is forecasted using technical analysis using historical data and the output is tested for accuracy and accordingly assigned weights. Using the ensemble of neural networks, accuracy has been tried to be maximized and offer near to actual prediction. Using the yield curve, the investor can assess not only the yield of that bond, but can also the interest rates, and hence, has a very useful tool in his hand for investment purpose, thus making decisions about whether to invest or not , and if invest then when to invest. The yield curve prediction not only provides the investor a tool to make investment decisions in bond market, but it also serves as a tool to gauge the macroeconomic conditions of the country and hence predict the movement in various other markets as well, and hence make investment decisions accordingly.",,2013,10.5120/14105-2144,,proquest,"This project proposes an ensemble of neural networks (back propagation, radial basis function, linear regression) to predict government bond yields and project the yield curve. The model uses historical data and technical analysis, with weights assigned based on accuracy to maximize prediction performance. The yield curve serves as a tool for investors to make decisions and gauge macroeconomic conditions.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T15:35:28.587044
067ccda7c63409f5,Brand Alliances and Stock Reactions,"Purpose: This paper examines the performance and risk of brand alliances by investigating the market value of brand alliances through the analysis of investors’ response, and look into the different reactions of the stock market to brand alliance-type in terms of co-branding and joint-promotion, as well as into the potential different effects in the contexts of B to B versus B to C. Brand alliances, whereby two or more brands are jointly presented to the consumer, have been investigated extensively. The importance of brand alliances is emphasized by two factors: (1) brands are considered critical elements in business-to-business marketing settings; and (2) firms use brand alliances due to the trading costs and investment necessary to buy brands, the increasingly higher costs of launching a new brand onto the market, the high failure rates in new brand launches and brand extensions, the competitive pressures around product launches and diffusion, and the limitations imposed on the extension of a brand by its own identity. Consequently, brand alliances have exploded over recent years. As indicated later, by accomplishing the purpose of this research we fill a gap in the literature as most of the research on brand alliances revolves around consumers’ perspective. Methodology/Approach: The methodology followed is based on the event study method. First, the event study estimates the excess returns of share prices generated by events that were unanticipated by the market. To this end, we estimate the market model and the subsequent abnormal returns. To examine the impact of the publication of a brand alliance announcement on the share prices of the company, we use the cumulative abnormal returns calculated over k days of the event window for 55 announcements. In the second step, we analyze the returns of the different brand alliances. In particular, the abnormal returns are used as dependent variable in a regression analysis, wherein the central explanatory variable is brand alliance type (co-branding vs joint promotion). Finally, the third stage of the methodology analyzes the change in the variance of returns between the periods before and after the brand alliance announcements. Findings: The results show that brand alliance announcements generate positive abnormal returns, which support the hypothesis that brand alliance announcements are positively related to company stock returns. In particular, we observe that the reactions to brand alliances are spread over the event window. In fact, the window (−5,+5) produces returns that stand at 1.6%, which is the greatest abnormal return over the five days around the publication date. The economic impact of a cumulative return of 1.6% in eleven days is tantamount to annual returns of 69.33%. Considering that the average market value of the sample is €17,494 million, it represents an increase of €279 million for the sample stocks on the period (−5,+5). The regression analysis shows that the coefficients of the variable “co-branding” are positive and significant, which supports the hypothesis that co-branding presents higher abnormal returns than joint promotion. However, no differential effect are found between B to B versus B to C paradigms. The results obtained present an increase in the variance of the share prices after the alliance announcement date, which supports the hypothesis that the variance of the company stock returns is positively associated to announcements of brand alliances. Research Implications: The key implication of the measurement of the market value of brand alliances is that research should be reoriented toward a better understanding of the role of marketing in the value creation of a company. Instead of just concentrating on marketing research into consumer behavior, more emphasis should be given to the core company processes that create shareholder value. Practical Implications: The managerial implications of the specific results obtained are the following: the result that companies increase their market value when they implement brand alliance strategies, leads to a better ken of the way alliance activities can be managed when dealing with other organizations. In this way, finding a partner to form a brand alliance with could be a useful objective in terms of firm performance. Moreover, the results show that co-branding presents higher abnormal returns than joint promotion, which suggests that co-branding is the most valuable strategic decision (or long-term decision) for companies, as it implies the simultaneous participation of two or more brands in a single product. In this way, deciding on whether a short- or long-term branding strategy is pursued turns to be fundamental. Originality/Value/Contribution of the paper: The literature has analyzed the consequences of brand alliance, which looks at each partner’s brand attitude after the alliance, the brand equity of the constituent brands after the alliance, and the impact of the allied brand on the evaluation of the host brand. These studies have focused on the area of consumer behavior; that is, by measuring consumers´ attitudes and evaluation. Still, the measurement of dimensions reflecting the other side of the relationship, i.e. the firm, via brand image and equity is critical. Nevertheless, the examination of the impact of brand alliances on the partner company performance and risk has received little attention, despite the fact that “brand perceptions of companies’ products spill over to investment decisions in the market for companies’ stock”. © 2021 Elsevier B.V., All rights reserved.","Mas-Ruiz, F.J.; Nicolau, J.L.; Calderón-Martínez, A.",2021,10.1080/1051712x.2021.1893029,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106229558&doi=10.1080%2F1051712X.2021.1893029&partnerID=40&md5=b86e75377e394e1b65e18c69efb1907f,scopus,"This paper investigates the stock market's reaction to brand alliances, specifically co-branding and joint promotion, using an event study methodology. The findings indicate that brand alliance announcements positively impact company stock returns, with co-branding yielding higher abnormal returns than joint promotion. The study also observed an increase in stock price variance post-announcement but found no differential effect between B2B and B2C contexts. The research emphasizes the importance of marketing's role in value creation and suggests co-branding as a valuable strategic decision.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:35:51.522474
2aa1f42bb57aced5,Building portfolios based on machine learning predictions,"This paper demonstrates that portfolio optimization techniques represented by Markowitz mean-variance and Hierarchical Risk Parity (HRP) optimizers increase the risk-adjusted return of portfolios built with stocks preselected with a machine learning tool. We apply the random forest method to predict the cross-section of expected excess returns and choose n stocks with the highest monthly predictions. We compare three different techniques—mean-variance, HRP, and 1/N— for portfolio weight creation using returns of stocks from the S&P500 and STOXX600 for robustness. The out-of-sample results show that both mean-variance and HRP optimizers outperform the 1/N rule. This conclusion is in the opposition to a common criticism of optimizers’ efficiency and presents a new light on their potential practical usage. © 2022 Elsevier B.V., All rights reserved.","Kaczmarek, T.; Perez, K.",2022,10.1080/1331677x.2021.1875865,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100727648&doi=10.1080%2F1331677X.2021.1875865&partnerID=40&md5=e01a1a40410b7343778ed004d9e97d84,scopus,"This paper explores the use of machine learning (ML) to enhance portfolio optimization. It applies the random forest method to predict stock returns and then uses Markowitz mean-variance and Hierarchical Risk Parity (HRP) optimizers to construct portfolios. The study compares these methods against a simple 1/N rule using S&P500 and STOXX600 stock data, finding that mean-variance and HRP outperform the 1/N rule, challenging common criticisms of optimizer efficiency.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:36:25.637805
aa51b3156f3ce072,"Business cycles, stock market wealth, and gambling at the racetracks","Purpose: As digital technologies expand access to new forms of legalized gambling, including sports betting and online gaming, it is important to assess the impact of macroeconomic and equity market outcomes on fund flows into gambling. The authors’ findings will be of interest to policymakers and the gambling industry, as various forms of gambling, including day trading, gain broad public acceptance. Design/methodology/approach: The authors examine the impact of macroeconomic forces, business cycles, and financial market wealth on gambling. The authors propose a nonlinear model linking aggregate gambling expenditures to macroeconomic, stock market, and gambling industry variables. The authors estimate the proposed model using nonlinear estimation procedures. Findings: The authors find that price of wagering, incomes, and supply of gambling opportunities are the primary determinants of wagering demand. Aggregate wagering is negatively impacted by realized stock returns and market volatility, but rises during recessions. Originality/value: To the best of the authors’ knowledge, the questions posed and addressed in this manuscript have not been addressed in prior literature. © 2024 Elsevier B.V., All rights reserved.","Ramezani, C.A.; Ahern, J.J.",2024,10.1108/jes-03-2023-0120,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164109195&doi=10.1108%2FJES-03-2023-0120&partnerID=40&md5=f3427f470bd95119b06fc28721e86dd2,scopus,"This study investigates how macroeconomic factors, business cycles, and stock market performance influence gambling expenditures. Using a nonlinear model, the research finds that income, the price of wagering, and the availability of gambling opportunities are key drivers of demand. Notably, aggregate wagering decreases with positive stock returns and market volatility but increases during economic recessions. The authors highlight the novelty of their approach in addressing these questions.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:36:46.796748
acc0253aba3ad46f,CAPM model for the valuation of shares of companies in the construction market during the period 2015 - 2020; Modelo CAPM para la valoración de acciones de las empresas en el mercado de la construcción durante el periodo 2015 - 2020,"This article consisted of the application of the CAPM model on companies listed on the stock exchange of and related to the construction sector in Colombia during the period from January 1, 2015 to December 31, 2020. This research has a quantitative approach with a type of descriptive and longitudinal research. Its methodology consisted in the application of ordinary least squares to the daily volatilities of the asset based on the estimation of the betas, which mostly complied with the probabilistic values and the intercepts were not different from 0, which is consistent with the model hypothesis. As for the variables that accompany the model, the risk-free rate TFIT16240724 was chosen and the ICOLCAP index was chosen as the variable that measures the market risk. Finally obtained the results, these were evaluated, concluding that the beta coefficient is an acceptable indicator in the risk-return assessment of the asset during the period in question, however, as an estimator it is not effective, which reflects the ineffectiveness of the model. © 2023 Elsevier B.V., All rights reserved.","González, G.A.O.; Domínguez, M.R.",2023,10.46661/revmetodoscuanteconempresa.7350,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162148528&doi=10.46661%2Frevmetodoscuanteconempresa.7350&partnerID=40&md5=f21ca467e8fee4b68a7af008bd550b76,scopus,"This study applies the Capital Asset Pricing Model (CAPM) to construction companies listed on the Colombian stock exchange between 2015 and 2020. Using ordinary least squares to estimate betas, the research found that while the beta coefficient is an acceptable indicator for risk-return assessment, it is not an effective estimator, suggesting the CAPM model's ineffectiveness for this market during the period. The study used the TFIT16240724 as the risk-free rate and the ICOLCAP index for market risk.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:36:55.655982
374c0b6ef5ceccd7,CDS risk premia forecasting with multi-featured deep RNNs: An application on BR[I]CS countries,"Using state-of-the-art recurrent neural network architectures, this study attempts to predict credit default swap risk premia for BR[I]CS countries as accurately as possible. In the time series setting, these recurrent neural networks are ELMAN, NARX, GRU, and LSTM RNNs, considering local and global features. The predictive power of each architecture is compared, and the results differ depending on the country. NARX RNN was the best predictor for Brazil and South Africa in various settings. Meanwhile, ELMAN RNN produces more accurate results in China, whereas Russia's long short-term memory RNN achieves the best predictors among other countries’ RNNs. © 2023 Elsevier B.V., All rights reserved.","Kutuk, Y.",2023,10.1016/j.bir.2023.10.013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175864895&doi=10.1016%2Fj.bir.2023.10.013&partnerID=40&md5=a144e84f118885624d72b08a1f1e2dfa,scopus,"This study forecasts credit default swap (CDS) risk premia for BRICS countries using deep recurrent neural networks (RNNs), including ELMAN, NARX, GRU, and LSTM. The performance of these models varies by country, with NARX excelling in Brazil and South Africa, ELMAN in China, and LSTM in Russia.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:37:19.739309
21724fc90a15e265,CONDITIONAL HETEROSKEDASTICITY IN ASSET RETURNS - A NEW APPROACH,"GARCH models have been applied in modelling the relation between conditional variance and asset risk premia. These models, however, have at least three major drawbacks in asset pricing applications: (i) Researchers beginning with Black (1976) have found a negative correlation between current returns and future returns volatility. GARCH models rule this out by assumption. (ii) GARCH models impose parameter restrictions that are often violated by estimated coefficients and that may unduly restrict the dynamics of the conditional variance process. (iii) Interpreting whether shocks to conditional variance persist or not is difficult in GARCH models, because the usual norms measuring persistence often do not agree. A new form of ARCH is proposed that meets these objections. The method is used to estimate a model of the risk premium on the CRSP Value-Weighted Market Index from 1962 to 1987.","NELSON, DB",1991,10.2307/2938260,,wos,"This paper proposes a new approach to modeling conditional heteroskedasticity in asset returns, addressing limitations of existing GARCH models such as the inability to capture the negative correlation between current returns and future volatility, parameter restriction issues, and difficulties in interpreting shock persistence. The new method is applied to estimate the risk premium on the CRSP Value-Weighted Market Index.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:37:23.779171
0c6ee05d8b799ba1,Can Brazilian Central Bank communication help to predict the yield curve?,"This paper investigates whether Brazilian Central Bank communication helps to forecast the yield curve. Our forecast strategy involves two steps: First, we analyze textual Central Bank documents to extract sentiment variables that describe its communication, and then, we include those sentiment variables as additional factors into the dynamic Nelson–Siegel term structure model. We found that sentiment variables contain predictive information for yield curve forecasting. Specifically, when combined with macroeconomic variables, the sentiment variables improve the accuracy of the forecast for short maturities and forecast horizons. In addition, sentiment variables are useful in forecasting for medium and long forecast horizons for all maturities. Besides finding a new source of information to forecast the yield curve, the results indicate that the information provided by Central Bank affects market participants, proving to be a useful tool for monetary policy. © 2023 Elsevier B.V., All rights reserved.","de Andrade Alves, C.R.; Joseph Abraham, K.; Laurini, M.",2023,10.1002/for.2964,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148706166&doi=10.1002%2Ffor.2964&partnerID=40&md5=b789cdf18b904f4f1c913cf6b72bf13b,scopus,"This study explores the predictive power of Brazilian Central Bank communication for the yield curve. It extracts sentiment from Central Bank documents and incorporates these sentiment variables into a dynamic Nelson–Siegel term structure model. The findings indicate that sentiment variables enhance yield curve forecasting accuracy, particularly for short maturities and horizons when combined with macroeconomic variables, and are also useful for medium and long horizons across all maturities. This suggests that Central Bank communication is a valuable tool for monetary policy and a new source of information for yield curve prediction.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T15:37:30.126148
64bed7c0c4bb24cd,Can google search volume index predict the returns and trading volumes of stocks in a retail investor dominant market,"This research examines whether Google search volume index (GSVI), a proxy of investor attention, can predict the excess returns and abnormal trading volumes of TPEx 50 index constituents. It also explores the motive underlying GSVI based on positive or negative shocks to stock prices. The empirical data include 48 companies from TPEx 50 index constituents and cover a period from 1 September 2016 to 31 August 2019. The empirical results present that (1) lagged GSVI negatively affects current excess returns, perhaps due to the characteristics of TPEx, in which there are a higher proportion of retail investors, smaller listed companies, and a higher information asymmetry problem. (2) Lagged GSVI can positively affect abnormal current trading volumes. (3) If GSVI is driven by positive shocks, then it can predict excess returns and abnormal trading volumes positively.",,2022,10.1080/23322039.2021.2014640,,proquest,"This study investigates if Google Search Volume Index (GSVI), as a measure of investor attention, can predict stock returns and trading volumes for companies in the TPEx 50 index. The findings indicate that GSVI negatively impacts current excess returns but positively affects abnormal trading volumes. Furthermore, positive shocks to GSVI are associated with positive predictions for both excess returns and abnormal trading volumes. The study suggests that the characteristics of the TPEx market, such as a high proportion of retail investors and information asymmetry, may influence these relationships.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:37:34.756171
7a8f143dc92d2032,"Capital accumulation, external indebtedness, and macroeconomic performance of emerging countries","This paper aims at presenting a nonlinear post Keynesian growth model to evaluate at the theoretical and empirical levels the relationship between external indebtedness and economic growth in emerging countries. To this end, a post Keynesian endogenous growth model is presented, in which: (1) the desired rate of capital accumulation is assumed to be a nonlinear function of external indebtedness as a share of capital stock; (2) an endogenous country risk premium is assumed to be an increasing (linear) function of external indebtedness (as a share of capital stock); (3) there is a fixed exchange rate regime and perfect capital mobility in the sense of Mundell and Fleming. The main theoretical result of the model is the existence of two long-run equilibrium positions, one of which has a high level of external indebtedness (as a ratio of capital stock) and a low profit rate and the other has a low level of external indebtedness and a high profit rate. This means that ""excessive"" external indebtedness can result in stagnant growth due to its negative effect on the rate of profit. To test the effects of external indebtedness on the rate of economic growth in emerging economies, a dynamic panel is estimated to evaluate whether external debt has an effective negative influence on economic growth in emerging countries. An empirical test of demand-led growth equations with a dynamic panel for fifty-five emerging countries confirms the potential negative effects of external debt on long-term growth rates in the sample countries. © 2013 M.E. Sharpe, Inc. All rights reserved. © 2013 Elsevier B.V., All rights reserved.","Rocha, M.; Oreiro, J.L.",2013,10.2753/pke0160-3477350405,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880320477&doi=10.2753%2FPKE0160-3477350405&partnerID=40&md5=fb91bb6c33303ec63cde6a40b6bd6773,scopus,This paper develops and empirically tests a nonlinear Post Keynesian growth model to examine the relationship between external indebtedness and economic growth in emerging countries. The model suggests that excessive external debt can lead to stagnant growth by negatively impacting the profit rate. An empirical analysis using a dynamic panel of 55 emerging countries confirms this potential negative effect of external debt on long-term growth.,False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:37:39.264094
5eb44d63138a04b7,Capitalisation rates for commercial real estate investments: evidence from Australia,"Purpose: The paper is motivated by the paucity of empirical research on the determinants of capitalisation rates/yield in the commercial property market. Compared to property price determinants, the capitalisation rate has received significantly less attention. This is somewhat surprising given that the capitalisation rate is a more insightful indicator for investors on commercial property market performance than merely price changes or trends. The capitalisation rate, measured as the ratio of net operating income to the property’s capital value, captures the asset’s overall ability to generate income which is crucial for investors who typically invest in property for their income-generating capacity. The purpose of this paper is to address these issues. Design/methodology/approach: To evaluate the determinants of capitalisation rates, time series analysis was used. The data capture performance in the Australian commercial property market between 2005 and 2018. All macroeconomic and financial data are freely available from official sources such as the Australian Bureau of Statistics and the nation’s central bank. Methodology wise, given the problematic nature of the data such as a mixed order of integration and the possibility of cointegration amongst some of the I (1) variables, the autoregressive distributed lag model was selected given its flexibility and relative lack of assumptions. Findings: Bond rates, market risk premiums, stock market excess returns and other macroeconomic variables were found to drive capitalisation rates of Australian commercial properties. A 1% increase in the bond rate results in approximately 0.3–2.4% increase in capitalisation rates depending on the sub-market. Further, a 1% increase in excess market returns results in a 0.01–0.02% increase in capitalisation rates. Regarding risk premiums, a 100 basis point increase in the BBB spread results in approximately 0.92–1.27% reduction in cap rates in certain markets. Practical implications: Asset managers will find these results useful in asset allocation strategies. Commercial properties offer attractive investment qualities such as yield stability in periods of economic uncertainty while allowing for the possibility of capital growth through appreciation of the underlying asset. By understanding the factors that affect the capitalisation rate, practitioners may predict emerging trends and identify threats to portfolio return and stability. This allows better integration of commercial property in the construction of portfolios that remain robust in a variety of market conditions. Originality/value: The contribution to literature is significant given the lack of similar studies in the Australian market. The performance of real estate assets using cap rates as a comparative measure to equities and bonds influences decisions in asset allocation strategies. It provides crucial information for investors to estimate the performance of commercial property. This research supports the notion that both space and capital market indicators jointly affect capitalisation rates. The findings expand the knowledge base relating to commercial properties and validate the assessments of investors, developers and valuers who utilise yield as a performance benchmark for asset allocation strategies. © 2023 Elsevier B.V., All rights reserved.","Wong, W.W.; Mintah, K.; Baako, K.; Wong, P.Y.",2023,10.1108/jpif-09-2022-0063,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146597624&doi=10.1108%2FJPIF-09-2022-0063&partnerID=40&md5=a7597a55a32cf9357caad38cd9cdc398,scopus,"This study investigates the determinants of capitalisation rates in the Australian commercial property market from 2005 to 2018 using time series analysis and an autoregressive distributed lag model. Key findings indicate that bond rates, market risk premiums, and stock market excess returns significantly influence capitalisation rates. The research provides practical implications for asset managers in their allocation strategies and contributes to the literature by offering insights into commercial property performance relative to other asset classes.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:37:44.837987
46aaeaa8f110a015,Capturing the Regime-Switching and Memory Properties of Interest Rates,"We propose a mean-reverting interest rate model whose mean-reverting level, speed of mean-reversion and volatility are all modulated by a weak Markov chain (WMC). This model features a simple way to capture the regime-switching evolution of the parameters as well as the memory property of the data. Concentrating on the second-order WMC framework, we derive the filters of the WMC and other auxiliary processes through a change of reference probability measure. Optimal estimates of model parameters are provided by employing the EM algorithm. The h-step ahead forecasts under our proposed set-up are examined and compared with those under the usual Markovian regime-switching framework. We obtain better goodness-of-fit performance based on our numerical results generated from the implementation of WMC-based filters to a 10-year dataset of weekly short-term-maturity Canadian yield rates. Some statistical inference issues of the proposed modelling approach are also discussed.","Xi, Xiaojing; Mamon, Rogemar",2014,10.1007/s10614-013-9396-5,,wos,"This paper introduces a new interest rate model that uses a weak Markov chain (WMC) to capture regime-switching and memory properties. The model allows for changes in mean-reversion level, speed, and volatility based on the WMC. The authors derive filters for the WMC and other processes, use the EM algorithm for parameter estimation, and provide forecasts. They demonstrate better performance compared to standard Markovian regime-switching models using Canadian yield rate data and discuss statistical inference.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T15:38:03.194992
220322bfc22a7fa8,Cash Flow Forecasting in SAP ERP Enhanced by UiPath Automation: A Predictive Analytics Approach,"Maintaining liquidity, mitigating financial risks, and making strategic business decisions in today’s enterprises require accurate cash flow forecasting. Unfortunately, the native forecasting features of the SAP ERP are often constrained by outdated input streams, static data assumptions, and rigid model structures, severely impeding responsiveness and accuracy. This study proposes and evaluates the results-focused integration of UiPath robotic process automation (RPA) with predictive analytics to improve short and medium-term cash flow forecasting in SAP environments. We automated real-time data extraction from SAP FI, FI-CA, and bank interface modules, then employed machine learning and deep learning models (regression trees, LSTM networks, and ensemble methods) to demonstrate substantial gains in forecasting accuracy, cycle time, and exception handling. The framework was tested on large data sets from multi-currency, multi-business unit enterprises, achieving forecast accuracy improvement estimates of 15% to 28% compared to SAP’s baseline predictions. Aside from significantly reducing manual effort associated with forecast preparation, automation also expedited scenario-based liquidity analysis while enhancing governance through exception-based audit logging. These results provide a proven scaling architecture for intelligent real-time cash forecasting that is reliable and compliant, placing RPA and AI at the core of cash management operations of the future, and integrating deeply within ERP systems. © 2025 Elsevier B.V., All rights reserved.","Jamithireddy, N.S.",2025,10.51983/ijiss-2025.ijiss.15.2.45,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010042509&doi=10.51983%2Fijiss-2025.IJISS.15.2.45&partnerID=40&md5=b869ac625a26cb67b95a63d8895c5ea9,scopus,"This study integrates UiPath RPA with predictive analytics to enhance cash flow forecasting in SAP ERP systems. It automates data extraction from SAP modules and uses machine learning (regression trees, LSTM, ensemble methods) to improve accuracy, reduce cycle time, and enhance exception handling. The approach demonstrated significant forecast accuracy improvements (15-28%) compared to SAP's baseline, offering a scalable architecture for intelligent, real-time cash forecasting.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:38:06.704725
b127fcd8bbc8c65b,Changing beliefs and the term structure of interest rates: Cross-equation restrictions with drifting parameters,"This paper shows how to estimate a Bayesian VAR with drifting parameters and nonlinear cross-equation restrictions. The restrictions promote parsimony by reducing the dimension of the drifting component in conditional mean parameters. As an application, the paper investigates an anticipated-utility version of the expectations model of the term structure. The estimates suggest that changing beliefs matter for understanding the yield curve and point to an intriguing clue about risk premia. Local approximations to the mean yield spread are highly correlated with the variance of the trend short rate, suggesting a connection between uncertainty about the long-run target of monetary policy and risk premia on long-term bonds. © 2005 Elsevier Inc. All rights reserved. © 2018 Elsevier B.V., All rights reserved.","Cogley, T.",2005,10.1016/j.red.2005.01.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-16244390866&doi=10.1016%2Fj.red.2005.01.004&partnerID=40&md5=0033d48127db50efe621f016c846fa59,scopus,"This paper presents a method for estimating a Bayesian VAR with drifting parameters and nonlinear cross-equation restrictions, applied to the expectations model of the term structure. The findings suggest that evolving beliefs influence the yield curve and are linked to risk premia, with uncertainty about monetary policy's long-run target correlating with risk premia on long-term bonds.",True,True,False,gemini-2.5-flash-lite,Olav,M,,2025-10-13T15:38:29.655069
93d50033bed897f3,ChatGPT and Commodity Return,"This paper investigates the ability of a ChatGPT‐based indicator to forecast excess returns of the commodity futures index. Using ChatGPT to extract information from over 2.5 million articles from nine international newspapers, we demonstrate that our constructed commodity news ratio index significantly predicts future commodity returns, both in‐sample and out‐of‐sample. Furthermore, it outperforms traditional textual analysis methods, including Bidirectional Encoder Representations from Transformers (BERT) and Bag‐of‐Words (BoW), while indicating economic significance within an asset allocation framework. The results highlight the critical role of ChatGPT in forecasting commodity market dynamics and provide valuable insights for both financial market participants and researchers.",,2025,10.1002/fut.22568,,proquest,"This study explores the predictive power of a ChatGPT-derived index for commodity futures returns. By analyzing over 2.5 million news articles, the research shows that the ""commodity news ratio index"" significantly forecasts future commodity returns, outperforming traditional methods like BERT and BoW, and demonstrating economic relevance in asset allocation.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:38:32.370489
d649020ccc11a84c,Classification Ratemaking Using Decision Tree in the Insurance Market of Bosnia and Herzegovina,"This paper investigates the impact of risk classification on life insurance ratemaking with particular reference to Bosnia and Herzegovina (BiH). The research is based on a sample of over eighteen thousand insurance policies for passenger vehicles collected over the period 2015-2020. In our empirical investigation we develop a standard risk model based on the application of Poisson Generalized linear models (GLM) for claims frequency estimate and Gamma GLM for claim severity estimate. The analysis reveals that GLM does not provide a reliable parameter estimates for Multi-level factor (MLF) categorical predictors. Although GLM is widely used method to deter insurance premiums, improvements of GLM by using the data mining methods identified in this paper may solve practical challenges for the risk models. The popularity of applying data mining methods in the actuarial community has been growing in recent years due to its efficiency and precision. These models are recommended to be considered in BiH and South East European region in general. © 2021 Elsevier B.V., All rights reserved.","Omerašević, A.; Selimović, J.",2020,10.2478/jeb-2020-0020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099800105&doi=10.2478%2Fjeb-2020-0020&partnerID=40&md5=c11ce006692f8cbd8140a430aeca56e2,scopus,"This paper explores the use of data mining methods, specifically decision trees, to improve risk classification and ratemaking in the insurance market of Bosnia and Herzegovina. It compares these methods to traditional Generalized Linear Models (GLMs) using a dataset of over 18,000 passenger vehicle insurance policies from 2015-2020. The study suggests that data mining techniques offer more reliable parameter estimates for categorical predictors than GLMs and recommends their adoption in the region.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:38:39.304027
8a2720754be4ab08,Comparative analysis of responses of risky and safe haven assets to stock market risk before and after the yield curve inversions in the US,"This study examines the response of safe -haven assets (gold, US dollar) and Bitcoin to market risk before and after yield curve inversions in the U.S. market. Using the VIX volatility index and static and dynamic cross-quantilogram approach, the analysis reveals that gold and the U.S. dollar act as safe havens, showing positive responses to increased VIX values, while Bitcoin behaves as a risky asset, negatively responding to higher VIX values during market turbulence. Changes in the VIX index have an immediate impact on asset price returns, but the effect diminishes over time, suggesting the need for timely updates to investment strategies. Yield curve inversions have altered the VIX-US dollar relationship: pre -inversion changes were influential in calmer markets, but post -inversion, they played a bigger role during turbulent phases, suggesting potential changes in investor behavior and market dynamics. The findings offer practical insights for investors seeking stability and protection during uncertain market conditions.","Sokhanvar, Amin; Hammoudeh, Shawkat",2024,10.1016/j.iref.2024.103376,,wos,"This study analyzes how gold, the US dollar, and Bitcoin respond to stock market risk (measured by VIX) before and after US yield curve inversions. Gold and the US dollar act as safe havens, while Bitcoin acts as a risky asset. The yield curve inversion appears to have altered the relationship between VIX and the US dollar, particularly during turbulent market conditions.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:38:42.502265
280680c0ee200d86,Comparison of non-linear optimization algorithms for yield curve estimation,"The yield curve is a very important financial tool used in investment and policy decisions. Its estimation from market data is essentially a non-linear optimization problem. In this paper, we compare a diversity of non-linear optimization algorithms for estimating yield curves based on actual bond market data and conclude that certain classes of algorithms are more effective due to the nature of the problem. (C) 2007 Elsevier B.V. All rights reserved.","Manousopoulos, Polychronis; Michalopoulos, Michalis",2009,10.1016/j.ejor.2007.09.017,,wos,"This paper compares various non-linear optimization algorithms for estimating yield curves using real bond market data, finding certain algorithms to be more effective for this financial application.",True,True,False,gemini-2.5-flash-lite,Olav,M,,2025-10-13T15:38:56.132952
d7114845f2509422,Comparisons of mean-variance analysis and entropy-based approaches to portfolio selection under asymmetric returns in bear and bull markets,"This paper considers dynamic portfolio selection models where uncertainty in the financial market is characterized by business cycles. We consider that the financial market is defined by factors and present a regime switching autoregressive model for macro-economic factors to reflect financial cycles. It is assumed that the regime dynamics are Markovian and the parameters in the autoregressive model depend on regime dynamics. We then define a factor model for asset returns, with returns depending on regimes through the factors. The joint distribution of regimes and asset returns is the input to optimal portfolio selection models. Contrasting approaches to risk measurement of returns on investment are variance and exponential Rényi entropy (Rényi, 1960). We compare portfolio models with minimum variance and minimum entropy objectives. In the empirical analysis, we use the select sector ETFs to test the asset pricing model and examine the portfolio performance. Weekly financial data from 04-March-2016 to 26-June-2020 is employed for the estimation of the hidden Markov model including the asset return parameters, while the out-of-sample period from 26-June-2020 and 14-July-2023 is used for portfolio performance testing. It is found that, under both the empirical Sharpe and excess return to entropy ratios, the dynamic portfolio strategy with the entropy objective is an improvement on mean-variance models. © 2025 Elsevier B.V., All rights reserved.","MacLean, L.; Zhao, Y.; Miao, H.",2025,10.1007/s10479-025-06746-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013467192&doi=10.1007%2Fs10479-025-06746-x&partnerID=40&md5=9f90626e433c0bcb688c46baef7ae42a,scopus,"This paper compares mean-variance analysis and entropy-based approaches for dynamic portfolio selection, considering market business cycles modeled by a regime-switching autoregressive model. It uses sector ETFs and weekly financial data from 2016-2023 to test the models, finding that the entropy objective improves portfolio performance over mean-variance models.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:39:01.431103
c2146119cca81457,"Complex systems and ‘‘Spatio ‐Temporal Anti‐Compliance Coordination’’ In cyber‐physical networks: A critique of the Hipster Effect, bankruptcy prediction and alternative risk premia","The Hipster Effect is a group of evolutionary ‘‘Diffusive Learning’’ processes of networks of individuals and groups (and their communication devices) that form Cyber‐Physical Systems; and the Hipster Effect theory has potential applications in many fields of research. This study addresses decision‐making parameters in machine‐learning algorithms, and more specifically, critiques the explanations for the Hipster Effect, and discusses the implications for portfolio management and corporate bankruptcy prediction (two areas where AI has been used extensively). The methodological approach in this study is entirely theoretical analysis. The main findings are as follows: (i) the Hipster Effect theory and associated mathematical models are flawed; (ii) some decision‐making and learning models in machine‐learning algorithms are flawed; (iii) but regardless of whether or not the Hipster Effect theory is correct, it can be used to develop portfolio management models, some of which are summarised herein; (iv) the [1] corporate bankruptcy prediction model can also be used for portfolio‐selection (stocks and bonds).",,2021,10.1049/ccs2.12029,,proquest,"This theoretical analysis critiques the ""Hipster Effect"" in cyber-physical networks and its applications in machine learning, portfolio management, and bankruptcy prediction. The study argues that the Hipster Effect theory and some machine learning models are flawed, but suggests potential applications for portfolio management and corporate bankruptcy prediction models.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:39:04.994482
8c2778fc6d16b718,"Complexity, nonlinearity and high frequency financial data modeling: lessons from computational approaches","This editorial introduces the special issue Complexity, Nonlinearity and High Frequency Financial Data Modeling: Lessons from Computational Approaches in Annals of Operations Research, which brings together 19 contributions exploring advanced methods and applications in the analysis of financial markets. The collected works reflect the growing importance of complexity and nonlinear dynamics in understanding modern financial systems, marked by high volatility, interdependence, and structural shifts. The papers are organized thematically into five main areas: (i) complexity and nonlinearity in financial markets, (ii) advanced forecasting and econometric modeling, (iii) network theory, causality, and information flows, (iv) banking, credit risk, and economic growth, and (v) continuous-time and structural model reviews. There is an additional section on methodological innovations, which include time–frequency and multi-scale analysis, recent developments of nonlinear and regime-switching models, machine learning, and complex network approaches. A heartfelt tribute is dedicated to the late Marco Tucci, co-editor of this special issue, whose vision and scholarly contributions significantly shaped its content. Sadly, Marco passed away while we were in the process of compiling this special issue. The editorial concludes by highlighting common methodological threads, synthesizing key insights, and outlining promising avenues for future research in complexity-informed financial modeling.",,2025,10.1007/s10479-025-06809-z,,proquest,"This editorial introduces a special issue focused on complexity, nonlinearity, and high-frequency financial data modeling using computational approaches. It highlights 19 contributions covering various themes such as market complexity, advanced forecasting, network theory, banking risk, and model reviews, with a focus on methodological innovations like machine learning and time-frequency analysis. The editorial also pays tribute to co-editor Marco Tucci and outlines future research directions.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:39:13.094038
d03ccef6cf62c8a5,Comprehensive evidence implies a higher social cost of CO2,"The social cost of carbon dioxide (SC-CO2) measures the monetized value of the damages to society caused by an incremental metric tonne of CO2 emissions and is a key metric informing climate policy. Used by governments and other decision-makers in benefit-cost analysis for over a decade, SC-CO2 estimates draw on climate science, economics, demography and other disciplines. However, a 2017 report by the US National Academies of Sciences, Engineering, and Medicine1 (NASEM) highlighted that current SC-CO2 estimates no longer reflect the latest research. The report provided a series ofrecommendations for improving the scientific basis, transparency and uncertainty characterization of SC-CO2 estimates. Here we show that improved probabilistic socioeconomic projections, climate models, damage functions, and discounting methods that collectively reflect theoretically consistent valuation of risk, substantially increase estimates of the SC-CO2. Our preferred mean SC-CO2 estimate is $185 per tonne ofCO2 ($44-$413 per tCO2: 5%-95% range, 2020 US dollars) at a near-term risk-free discount rate of 2%, a value 3.6 times higher than the US government's current value of $51per tCO2. Our estimates incorporate updated scientific understanding throughout all components of SC-CO2 estimation in the new open-source Greenhouse Gas Impact Value Estimator (GIVE) model, in a manner fully responsive to the near-term NASEM recommendations. Our higher SC-CO2 values, compared with estimates currently used in policy evaluation, substantially increase the estimated benefits of greenhouse gas mitigation and thereby increase the expected net benefits of more stringent climate policies.",,2022,10.1038/s41586-022-05224-9,,proquest,"This study presents a significantly higher estimate for the social cost of carbon dioxide (SC-CO2), proposing a mean value of $185 per tonne of CO2. This updated estimate, derived from improved socioeconomic projections, climate models, damage functions, and discounting methods within the new GIVE model, is substantially higher than the current US government estimate. The authors argue that this higher SC-CO2 value strengthens the case for more stringent climate policies by increasing the estimated benefits of greenhouse gas mitigation.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:39:16.683787
807b76db734ed6fd,Conditional Skewness in Asset Pricing: 25 Years of Out-of-Sample Evidence,"Much attention is paid to portfolio variance, but skewness is also important for both portfolio design and asset pricing. We revisit the empirical research on systematic skewness that we initiated 25 years ago. We analyze the out-of-sample evidence for the skewness risk premium presented in the literature including the recent work of Anghel et al. (2023). We also conduct an out-of-sample test and focus on the sensitivity of the risk premium estimate to different research choices. Overall, we find that the risk premium associated with systematic skewness is similar to the one reported in our original paper. © 2024 Elsevier B.V., All rights reserved.","Harvey, C.R.; Siddique, A.",2023,10.1561/104.00000134,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192779205&doi=10.1561%2F104.00000134&partnerID=40&md5=bf020e243fcc8a2f732bf1780d4d5319,scopus,"This paper revisits the empirical evidence for the skewness risk premium in asset pricing, building on 25 years of research. It analyzes out-of-sample data, including recent studies, and finds that the risk premium associated with systematic skewness remains consistent with earlier findings. The study also examines the sensitivity of the risk premium estimate to various research choices.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:39:20.413226
465418f6b1f7d557,Conditional risk-return relationship in a time-varying beta model,"We investigate the asymmetric risk-return relationship in a time-varying beta CAPM. A state space model is established and estimated by the Adaptive Least Squares with Kalman foundations proposed by McCulloch. Using SP 500 daily data from 1987:11-2003:12, we find a positive risk-return relationship in the up market (positive market excess returns) and a negative relationship in the down market (negative market excess returns). This supports the argument of Pettengill et al., who use a constant beta model. However, our model outperforms theirs by eliminating the unexplained returns and improving the accuracy of the estimated risk price.","Huang, Peng; Hueng, C. James",2008,10.1080/14697680701191361,,wos,"This study examines the asymmetric risk-return relationship within a time-varying beta Capital Asset Pricing Model (CAPM). Employing a state space model estimated via Adaptive Least Squares with Kalman filters, and using S&P 500 daily data from 1987 to 2003, the research reveals a positive risk-return link during market upturns and a negative link during downturns. This aligns with prior findings using constant beta models but offers improved accuracy and explanation of returns.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:39:23.561945
f4767033c2a4eee5,Constrained Factor Models for High-Dimensional Matrix-Variate Time Series,"High-dimensional matrix-variate time series data are becoming widely available in many scientific fields, such as economics, biology, and meteorology. To achieve significant dimension reduction while preserving the intrinsic matrix structure and temporal dynamics in such data, Wang, Liu, and Chen proposed a matrix factor model, that is, shown to be able to provide effective analysis. In this article, we establish a general framework for incorporating domain and prior knowledge in the matrix factor model through linear constraints. The proposed framework is shown to be useful in achieving parsimonious parameterization, facilitating interpretation of the latent matrix factor, and identifying specific factors of interest. Fully utilizing the prior-knowledge-induced constraints results in more efficient and accurate modeling, inference, dimension reduction as well as a clear and better interpretation of the results. Constrained, multi-term, and partially constrained factor models for matrix-variate time series are developed, with efficient estimation procedures and their asymptotic properties. We show that the convergence rates of the constrained factor loading matrices are much faster than those of the conventional matrix factor analysis under many situations. Simulation studies are carried out to demonstrate finite-sample performance of the proposed method and its associated asymptotic properties. We illustrate the proposed model with three applications, where the constrained matrix-factor models outperform their unconstrained counterparts in the power of variance explanation under the out-of-sample 10-fold cross-validation setting. for this article are available online.","Chen, Elynn Y.; Tsay, Ruey S.; Chen, Rong",2020,10.1080/01621459.2019.1584899,,wos,"This article proposes a constrained matrix factor model for high-dimensional matrix-variate time series data, incorporating domain knowledge through linear constraints to improve parameterization, factor interpretation, and overall modeling accuracy. The constrained models demonstrate faster convergence rates and better out-of-sample performance compared to unconstrained models in empirical applications.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:39:34.419082
9b6490fe957a02c8,Constructing Risk Analysis for Changes in China’s Local Government Bond System Based on SSP,"The local government bond system of China has experienced a series of changes from its initial creation to its abolition and then to a recovery again. During the period, the central government always dominated the changing direction of the local government bond system. However, as fiscal decentralization reform has progressed, the institutional needs of local governments and investors have gradually gained attention. As a result, the size and variety of local government bonds are expanding. Through the introduction of analysis of system change based on situation structure performance (SSP), this paper uses Machine Learning (ML) approaches to predict the risk of government debt of China in the context of changing the local government bond system. Besides, this research work includes the comprehensive weight assignment for government debt hazard, fiscal revenue forecasting, default risk calculation, and finally an analysis of the validity of government debt hazard. The system may provide financial signal advice and strategy reference for dealing with hazards in early payment, organizing debt repayment significance order, optimizing fiscal revenue and cost structure, and so on.",,2022,10.1155/2022/4606905,,proquest,"This paper analyzes risks associated with changes in China's local government bond system using a Situation Structure Performance (SSP) framework and Machine Learning (ML) approaches. It aims to predict government debt risk, considering factors like fiscal revenue forecasting and default risk, to provide financial signal advice and strategy references.",True,True,True,gemini-2.5-flash-lite,Olav,M,,2025-10-13T15:39:54.943776
52cf1b9349ff6fe8,Consumption and equilibrium asset pricing: An empirical assessment,"In the various attempts to solve the equity premium puzzle, the characterization of the utility function has received a lot of attention, along with the postulated nature of the economy. In this paper, we specify and estimate by maximum likelihood over the period 1871 - 1985 a heteroskedastic joint consumption and dividend Markov endowment process in an exchange asset pricing model. To assess the model, we try to replicate both the first and second unconditional moments of the return series, the negative serial correlation present in real and excess returns and the forecasting power of the dividend-price ratio for multiperiod returns. For the real returns, the model captures to some extent the main features of the data for values of the coefficient of risk aversion below 10. The main failure of the model comes from the excess returns. We also assess the model by inferring the consumption growth that rationalizes the observed stock and safe asset returns, but it is too variable to be plausible. © 2022 Elsevier B.V., All rights reserved.","Bonomo, M.; Garcia, R.",1996,10.1016/0927-5398(96)00002-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030243487&doi=10.1016%2F0927-5398%2896%2900002-3&partnerID=40&md5=e7b71931b2764b48a8f6f6888b7ef14b,scopus,"This paper empirically assesses an asset pricing model by estimating a heteroskedastic joint consumption and dividend Markov endowment process. It attempts to replicate key features of return series, including moments, serial correlation, and dividend-price ratio forecasting power. The model shows some success for real returns but struggles with excess returns and implies implausibly variable consumption growth.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:39:58.371390
438204aa4a86c9c1,Corporate Probability of Default: A Single-Index Hazard Model Approach,"Corporate probability of default (PD) prediction is vitally important for risk management and asset pricing. In search of accurate PD prediction, we propose a flexible yet easy-to-interpret default-prediction single-index hazard model (DSI). By applying it to a comprehensive U.S. corporate bankruptcy database we constructed, we discover an interesting V-shaped relationship, indicating a violation of the common linear hazard specification. Most importantly, the single-index hazard model passes the Hosmer-Lemeshow goodness-of-fit calibration test while neither does a state-of-the-art linear hazard model in finance nor a parametric class of Box-Cox transformation survival models. In an economic value analysis, we find that this may translate to as much as three times of profit compared to the linear hazard model. In model estimation, we adopt a penalized-spline approximation for the unknown function and propose an efficient algorithm. With a diverging number of spline knots, we establish consistency and asymptotic theories for the penalized-spline likelihood estimators. Furthermore, we reexamine the distress risk anomaly, that is, higher financially distressed stocks deliver anomalously lower excess returns. Based on the PDs from the proposed single-index hazard model, we find that the distress risk anomaly has weakened or even disappeared during the extended period.","Li, Shaobo; Tian, Shaonan; Yu, Yan; Zhu, Xiaorui; Lian, Heng",2023,10.1080/07350015.2022.2120484,,wos,"This paper introduces a single-index hazard model (DSI) for predicting corporate probability of default (PD), which is crucial for risk management and asset pricing. The model demonstrates a V-shaped relationship, outperforming traditional linear hazard models and Box-Cox transformation models in calibration tests. The study also finds that the DSI model weakens or eliminates the distress risk anomaly. The methodology involves penalized-spline approximation and establishes theoretical consistency for estimators.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:40:02.109057
c924d5a2b4699539,Creating investment scheme with state space modeling,"This paper proposes a unified approach to creating investment strategies with various desirable properties for investors. Particularly, we provide a new interpretation and the resulting formulations for state space models to attain our investment objectives, which are possibly specified as generating additional returns over benchmark stock indexes or achieving target risk-adjusted returns.Our state space models with particle filtering algorithm are employed to develop expert systems for investment strategies in highly complex financial markets. More concretely, in our state space framework, we apply a system model to representing portfolio weight processes with various constraints, as well as the standard underlying state variables such as volatility processes, Further, we formulate an observation model to stand for target value processes with non-linear functions of observed and latent variables.Numerical experiments demonstrate the effectiveness of our methodology through creating excess returns over S&P 500 and generating investment portfolios with fine risk-return profiles. (C) 2017 Elsevier Ltd. All rights reserved.","Nakano, Masafumi; Takahashi, Akihiko; Takahashi, Soichiro",2017,10.1016/j.eswa.2017.03.045,,wos,This paper presents a unified approach using state space models and particle filtering to create investment strategies. The models aim to generate excess returns over benchmarks or achieve target risk-adjusted returns by representing portfolio weights and underlying variables like volatility. Numerical experiments show the methodology's effectiveness in outperforming the S&P 500 and creating portfolios with good risk-return profiles.,True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:40:12.089774
893197e230ff363c,Cross-market volatility forecasting with attention-based spatial-temporal graph convolutional networks,"We propose a cross-market volatility forecasting framework by applying attention-based spatial-temporal graph convolutional network model (ASTGCN) to forecast future volatility of stock indices in 18 financial markets. In our work, we construct cross-market volatility networks to integrate interrelations among financial markets and the corresponding features of each market. ASTGCN combines the spatial-temporal attention mechanisms with the spatial-temporal convolutions to simultaneously capture the dynamic spatial-temporal characteristics of global volatility data. Compared with competitive models, ASTGCN exhibits superiority in multivariate predictive accuracies under multiple forecasting horizons. Our proposed framework demonstrates outstanding stability through several robustness checks. We also inspect the training process of ASTGCN by extracting spatial attention matrices and find that interrelations among global financial markets perform differently in tranquil and turmoil periods. Our study levitates empirical findings in financial networks to practical application with a novel forecasting method in the deep learning community.","Gong, Jue; Wang, Gang-Jin; Zhou, Yang; Xie, Chi",2025,10.1016/j.jempfin.2025.101639,,wos,This paper introduces an attention-based spatial-temporal graph convolutional network (ASTGCN) for forecasting stock index volatility across 18 financial markets. The model integrates inter-market relationships and individual market features to capture dynamic spatial-temporal characteristics of global volatility data. ASTGCN outperforms competitive models in predictive accuracy and demonstrates stability. Analysis of attention matrices reveals differing inter-market relationships during tranquil and turmoil periods.,True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:40:23.028945
c54779c722cfa540,Cross-sectional regression of returns on betas and portfolio grouping procedures,"This paper shows that the deviation of the estimated coefficient of beta from the market risk premium in cross-sectional regression of returns on betas is a direct consequence of the cross-sectional relation between the estimated alphas and betas. Therefore, the portfolio grouping procedure results in systematic cross-sectional relationship between the alphas and betas, causing a deviation in the estimated coefficient of beta in either direction. When firm size is used as the only portfolio grouping variable (Table AI in Fama and French, 1992), the estimated alphas and betas across portfolios are positively related, causing the estimated coefficient of beta to be upwardly biased. However, when beta is used as the only portfolio grouping variable (Table 2 in Kothari et al., 1995), the estimated alphas and betas across portfolios are negatively related, causing the estimated coefficient of beta to be downward biased. We show that forming portfolios on alphas and betas independently can adequately control for this deviation. © 2014 Inderscience Enterprises Ltd. © 2020 Elsevier B.V., All rights reserved.","Hur, J.; Kumar, R.; Singh, V.",2014,10.1504/ijbsr.2014.058005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901001561&doi=10.1504%2FIJBSR.2014.058005&partnerID=40&md5=6c9b7fb0ff1849da3457d63fab3a0b8b,scopus,"This paper investigates the deviation of the estimated coefficient of beta from the market risk premium in cross-sectional regression of returns on betas. It attributes this deviation to the cross-sectional relationship between estimated alphas and betas, which is influenced by portfolio grouping procedures. The study demonstrates how using firm size or beta as grouping variables can lead to upward or downward bias in the estimated beta coefficient, respectively. The authors propose forming portfolios on alphas and betas independently to control for this deviation.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:40:27.128412
1aeb51a3f8d0fb1b,DEEP NEURAL NETWORKS METHODS FOR ESTIMATING MARKET MICROSTRUCTURE AND SPECULATIVE ATTACKS MODELS: THE CASE OF GOVERNMENT BOND MARKET,"A sovereign bond market offers a wide range of opportunities for public and private sector financing and has drawn the interest of both scholars and professionals as they are the main instrument of most fixed-income asset markets. Numerous works have studied the behavior of sovereign bonds at the microeconomic level, given that a domestic securities market can enhance overall financial stability and improve financial market intermediation. Nevertheless, they do not deepen methods that identify liquidity risks in bond markets. This study introduces a new model for predicting unexpected situations of speculative attacks in the government bond market, applying methods of deep learning neural networks, which proactively identify and quantify financial market risks. Our approach has a strong impact in anticipating possible speculative actions against the sovereign bond market and liquidity risks, so the aspect of the potential effect on the systemic risk is of high importance.",,2025,10.1142/s0217590822480034,,proquest,This study introduces a novel deep learning neural network model to predict speculative attacks and quantify liquidity risks in the government bond market. The model aims to proactively identify potential financial market risks and their impact on systemic risk.,True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T15:41:02.386087
8cc0fdcc0a19bb4a,DETECTING BUSINESS CYCLES FOR HUNGARIAN LEADING AND COINCIDENT INDICATORS WITH A MARKOV SWITCHING DYNAMIC MODEL TO IMPROVE SUSTAINABILITY IN ECONOMIC GROWTH,"This paper applies the hidden Markov switching dynamic regression (MSDR) model to estimate transition probabilities of the Hungarian GDP between recessionary and expansionary periods. The transition probabilities are then compared to the OECD Hungarian binary business cycle indicator to assess the predictive power of the model. The paper proposes a linear model with a mean and a homoscedastic component. The level of symmetricity between the GDP and business cycles is explained by the panel data variables (Unemployment rate, IPI index, Inflation, BUX year-on-year change, and 10-3 Year sovereign bond yield spreads). It is assumed in this paper that by extending the model to encompass an exogenous variable listed in the panel data, essentially making the model bivariate, the maximum likelihood function would capture the business cycle more accurately. The results show that by plugging the unemployment rate as the exogenous variable in the regression, our model’s accuracy is 70%. © 2023 Elsevier B.V., All rights reserved.","Molnár, A.; Vasa, L.; Csiszárik-Kocsir, Á.",2023,10.31181/dmame060120032023m,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164272002&doi=10.31181%2Fdmame060120032023m&partnerID=40&md5=47f287a47b3f2d769b9970cd41c0662f,scopus,"This paper utilizes a hidden Markov switching dynamic regression (MSDR) model to analyze Hungarian GDP, distinguishing between recessionary and expansionary periods. It compares the model's transition probabilities with the OECD's business cycle indicator and explores the relationship between GDP and business cycles using panel data variables like unemployment rate and inflation. The study suggests that incorporating the unemployment rate as an exogenous variable improves the model's accuracy to 70% in capturing business cycles.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:41:06.153346
cc76b502f40b1012,Day trading profit maximization with multi-task learning and technical analysis,"Stock price movements are claimed to be chaotic and unpredictable, and mainstream theories of finance refute the possibility of realizing risk-free profit through predictive modelling. Despite this, a large body of technical analysis work maintains that price movements can be predicted solely from historical market data, i.e., markets are not completely efficient. In this paper we seek to test this claim empirically by developing a novel stochastic trading algorithm in the form of a linear model with a profit maximization objective. Using this method we show improvements over the competitive buy-and-hold baseline over a decade of stock market data for several companies. We further extend the approach to allow for non-stationarity in time, and using multi-task learning to modulate between individual companies and the overall market. Both approaches further improve the predictive profit. Overall this work shows that market movements do exhibit predictable patterns as captured through technical analysis.","Bitvai, Zsolt; Cohn, Trevor",2015,10.1007/s10994-014-5480-x,,wos,"This paper proposes a novel stochastic trading algorithm using multi-task learning and technical analysis to maximize profits in day trading. The algorithm, framed as a linear model with a profit maximization objective, is tested against historical stock market data and shows improvements over the buy-and-hold strategy. Extensions to handle non-stationarity and incorporate market-wide data further enhance predictive profit, suggesting that predictable patterns exist in market movements.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:41:11.731401
7ca6c07a43b0a174,Debt Intolerance: Threshold Level and Composition*,"Fiscal vulnerabilities depend on both the level and composition of government debt. This study examines the role of debt thresholds and debt composition in driving the nonlinear behaviour of long-term interest rates through a novel approach, a panel smooth transition regression with a general logistic model. The main findings are threefold. First, the impact of the expected public debt level on interest rates rises exponentially when the share of foreign private holdings exceeds approximately 20% of government debt denominated in local currency. Second, if the public debt level exceeds a certain level, an increase in foreign private holding of government debt could raise interest rates, offsetting the downward pressure from higher market liquidity. Third, out-of-sample forecasts of this novel non-linear model are more accurate than those of previous methods.","Matsuoka, Hideaki",2022,10.1111/obes.12470,,wos,"This study investigates how the level and composition of government debt influence long-term interest rates using a novel panel smooth transition regression model. It finds that foreign holdings of local currency debt above 20% amplify the impact of debt levels on interest rates, and that increased foreign holdings can raise interest rates beyond a certain debt threshold. The model also demonstrates superior out-of-sample forecasting accuracy compared to previous methods.",True,True,True,gemini-2.5-flash-lite,Olav,M,,2025-10-13T15:41:25.638231
11be73fadb2757bc,Decision analysis under ambiguity,"Abstract In selecting the preferred course of action, decision makers are often uncertain about one or more probabilities of interest. The experimental literature has ascertained that this uncertainty (ambiguity) might affect decision makers' preferences. Then, the decision maker might wish to incorporate ambiguity aversion in the analysis. We investigate the modeling ambiguity attitudes in the solution of decision analysis problems through functionals well-established in the decision theory literature. We obtain the multiple-event problems for subjective expected utility, smooth ambiguity and maximin decision makers. This allows us to establish the conditions under which these alternative decision makers face equivalent problems. Results for certainty equivalents and risk premia in the presence of both risk and ambiguity aversion are obtained. A recent generalization of the classical Arrow-Pratt quadratic approximation allows us to quantify the portions of a premium due to risk-PLXINSERT-, and to ambiguity-aversion. The numerical implementation of the objective functions is addressed, showing that all functionals can be estimated at no additional burden through Monte Carlo simulation. The well known Carter Racing case study is addressed quantitatively to demonstrate the findings. © 2020 Elsevier B.V., All rights reserved.","Borgonovo, E.; Marinacci, M.",2015,10.1016/j.ejor.2015.02.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926408325&doi=10.1016%2Fj.ejor.2015.02.001&partnerID=40&md5=1c2892d7e4971f8f088bace8177405c3,scopus,"This paper investigates how decision-makers incorporate ambiguity aversion into decision analysis problems using established decision theory functionals. It derives multiple-event problems for different decision-maker types (subjective expected utility, smooth ambiguity, maximin) and establishes conditions for problem equivalence. The study quantifies the portions of a premium attributable to risk and ambiguity aversion and demonstrates the application of these findings using a Monte Carlo simulation and the Carter Racing case study.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:41:30.195569
eb29f049a6d970f8,Departures from Rational Expectations and Asset Pricing Anomalies,"We investigate the potential of the consumption CAPM with pessimism, doubt, and the availability heuristic in the agent's beliefs to resolve the equity premium and risk-free rate puzzles. Using the nonlinear GMM estimation techniques, we find that doubt and the availability heuristic play an important role in explaining the cross-section of asset returns. However, when taken alone, these deviations from rational expectations cannot resolve the equity premium and risk-free rate puzzles. This result is robust to the assumption that the expected value of an uncertain prospect is nonlinear in the subjective outcome probabilities.","Semenov, Andrei",2009,10.1080/15427560903373245,,wos,"This study explores how deviations from rational expectations, specifically pessimism, doubt, and the availability heuristic, can explain asset pricing anomalies like the equity premium and risk-free rate puzzles within the consumption CAPM framework. While doubt and the availability heuristic are found to be significant in explaining asset returns, they do not fully resolve these puzzles on their own. The findings remain consistent even when considering nonlinear subjective probabilities.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:41:32.249977
f1cadf4bf4556874,Determinants of cryptocurrency returns: A LASSO quantile regression approach,"We consider a relatively large set of predictors and investigate the determinants of cryptocurrency returns at different quantiles. Our analysis exclusively focuses on the highly volatile period of COVID-19. The innovation in the paper stems from the fact that we employ the LASSO penalty in a quantile regression framework to select informative variables. We find that US government bond indices and small company stock returns, a new predictor introduce in this study, significantly impact the tail behavior of the cryptocurrency returns. © 2022 Elsevier B.V., All rights reserved.","Ciner, C.; Lucey, B.; Yarovaya, L.",2022,10.1016/j.frl.2022.102990,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133923555&doi=10.1016%2Fj.frl.2022.102990&partnerID=40&md5=01a12ed6ee35436c208a8c486eac384a,scopus,This study uses LASSO quantile regression to analyze the determinants of cryptocurrency returns during the COVID-19 pandemic. It identifies US government bond indices and small company stock returns as significant factors influencing the tail behavior of cryptocurrency returns.,True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:41:42.038673
19d08c4e33e25dd9,Development of Adaptive Neuro-Fuzzy Inference System for Assessing Industry Leadership in Accident Situations,"Petroleum activity is characterized as a high-risk activity due to the probability of accidents with material and human losses. The leaders of this segment assume, besides the complex routine tasks, the challenge of making assertive decisions during an accident. This study aims to present an evaluation model of the Industry Leadership Index for Emergencies Situations (ILIE), using the Adaptive Neuro-Fuzzy System (ANFIS). The model was composed of 4 input variables, namely: knowledge, behavior, skill, and attitude; and one output variable, Industry Leadership. The data collection took place in petroleum production units in Brazil, with a sample of 151 respondents through the application of a survey. The observed data were treated in an Excel tabulator and used in the development of the ANFIS model. From this model, it was possible to carry out simulations to predict the impact, which the increase or decrease in the value of each input variable can influence the leader’s profile. The model performed satisfactorily in the Root of the Mean Square Error (RMSE) analysis, being 0.199 in data training and 1.217 in data verification. The results suggest that the ANFIS method can be successfully applied to establish a model to analyze industry leaders prepared for assertive responses in crisis scenarios.",I. C. D. S. Cerqueira; P. P. S. Carvalho; J. L. Moya Rodríguez; S. Á. Filho; F. G. M. Freires,2022,10.1109/access.2022.3206766,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9893569,ieeexplore,"This study developed an Adaptive Neuro-Fuzzy Inference System (ANFIS) to create an Industry Leadership Index for Emergencies Situations (ILIE) in the petroleum industry. The model uses four input variables (knowledge, behavior, skill, attitude) to predict the output variable (Industry Leadership). Data from 151 respondents in Brazilian petroleum units were used to train and verify the ANFIS model, which showed satisfactory performance. The results indicate ANFIS can effectively model industry leaders' preparedness for crisis response.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:41:46.986060
b97c65a22bac2147,"Development of a Machine Learning Model Using Multiple, Heterogeneous Data Sources to Estimate Weekly US Suicide Fatalities","ImportanceSuicide is a leading cause of death in the US. However, official national statistics on suicide rates are delayed by 1 to 2 years, hampering evidence-based public health planning and decision-making.ObjectiveTo estimate weekly suicide fatalities in the US in near real time.Design, Setting, and ParticipantsThis cross-sectional national study used a machine learning pipeline to combine signals from several streams of real-time information to estimate weekly suicide fatalities in the US in near real time. This 2-phase approach first fits optimal machine learning models to each individual data stream and subsequently combines predictions made from each data stream via an artificial neural network. National-level US administrative data on suicide deaths, health services, and economic, meteorological, and online data were variously obtained from 2014 to 2017. Data were analyzed from January 1, 2014, to December 31, 2017.ExposuresLongitudinal data on suicide-related exposures were obtained from multiple, heterogeneous streams: emergency department visits for suicide ideation and attempts collected via the National Syndromic Surveillance Program (2015-2017); calls to the National Suicide Prevention Lifeline (2014-2017); calls to US poison control centers for intentional self-harm (2014-2017); consumer price index and seasonality-adjusted unemployment rate, hourly earnings, home price index, and 3-month and 10-year yield curves from the Federal Reserve Economic Data (2014-2017); weekly daylight hours (2014-2017); Google and YouTube search trends related to suicide (2014-2017); and public posts on suicide on Reddit (2 314 533 posts), Twitter (9 327 472 tweets; 2015-2017), and Tumblr (1 670 378 posts; 2014-2017).Main Outcomes and MeasuresWeekly estimates of suicide fatalities in the US were obtained through a machine learning pipeline that integrated the above data sources. Estimates were compared statistically with actual fatalities recorded by the National Vital Statistics System.ResultsCombining information from multiple data streams, the machine learning method yielded estimates of weekly suicide deaths with high correlation to actual counts and trends (Pearson correlation, 0.811;P < .001), while estimating annual suicide rates with low error (0.55%).Conclusions and RelevanceThe proposed ensemble machine learning framework reduces the error for annual suicide rate estimation to less than one-tenth of that of current forecasting approaches that use only historical information on suicide deaths. These findings establish a novel approach for tracking suicide fatalities in near real time and provide the potential for an effective public health response such as supporting budgetary decisions or deploying interventions.",,2020,10.1001/jamanetworkopen.2020.30932,,proquest,"This study developed a machine learning model using diverse data sources (health services, economic, meteorological, and online data) to estimate weekly US suicide fatalities in near real time. The model showed a high correlation with actual counts and trends, offering a potential for improved public health responses.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:41:54.306371
19c5432dca3228fd,Diffusion copulas: Identification and estimation,"We propose a new semiparametric approach for modelling nonlinear univariate diffusions, where the observed process is a nonparametric transformation of an underlying parametric diffusion (UPD). This modelling strategy yields a general class of semiparametric Markov diffusion models with parametric dynamic copulas and nonparametric marginal distributions. We provide primitive conditions for the identification of the UPD parameters together with the unknown transformations from discrete samples. Likelihood-based estimators of both parametric and nonparametric components are developed and we analyse their asymptotic properties. Kernel-based drift and diffusion estimators are also proposed and shown to be normally distributed in large samples. A simulation study investigates the finite sample performance of our estimators in the context of modelling US short-term interest rates. We also present a simple application of the proposed method for modelling the CBOE volatility index data. (C) 2020 Elsevier B.V. All rights reserved.","Bu, Ruijun; Hadri, Kaddour; Kristensen, Dennis",2021,10.1016/j.jeconom.2020.06.004,,wos,"This paper introduces a novel semiparametric method for modeling nonlinear univariate diffusions, where the observed process is a nonparametric transformation of an underlying parametric diffusion. This approach results in a class of semiparametric Markov diffusion models with parametric dynamic copulas and nonparametric marginal distributions. The authors establish conditions for identifying UPD parameters and unknown transformations from discrete samples, develop likelihood-based estimators for both parametric and nonparametric components, and analyze their asymptotic properties. Kernel-based drift and diffusion estimators are also proposed and shown to have normal distributions in large samples. The study includes a simulation to assess the finite sample performance of these estimators in modeling US short-term interest rates and an application to CBOE volatility index data.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:41:58.445737
d6ed6a2f0e967ff5,Discrete-time implementation of continuous-time filters with application to regime-switching dynamics estimation,"This paper details the implementation in discrete time of filters for a mean-reverting model formulated under a continuous-time framework, whereby a hidden Markov chain governs the model's parameters. Parameter estimates are determined via adaptive filters designed to extract hidden information from an observable time series. An application involving the dynamic behaviour of spot interest rates is considered. More specifically, we present an empirical study aimed at capturing accurately, on the basis of some benchmarks and statistical validation, the evolution of three country-specific rates in the European zone. Our analysis reveals some similar yield-rate and risk characteristics as well as independent market behaviours of the three EU sovereign states. (C) 2019 Elsevier Ltd. All rights reserved.","Grimm, Stefanie; Erlwein-Sayer, Christina; Mamon, Rogemar",2020,10.1016/j.nahs.2019.08.001,,wos,"This paper presents a discrete-time implementation of continuous-time filters for a mean-reverting model governed by a hidden Markov chain. It uses adaptive filters to estimate parameters from observable time series and applies this to model the dynamic behavior of spot interest rates in three European countries, revealing similarities and independent behaviors in their markets.",True,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:42:10.707163
ed308ed2ce635f75,Distributional modeling and forecasting of natural gas prices,"We examine the problem of modeling and forecasting European day‐ahead and month‐ahead natural gas prices. For this, we propose two distinct probabilistic models that can be utilized in risk and portfolio management. We use daily pricing data ranging from 2011 to 2020. Extensive descriptive data analysis shows that both time series feature heavy tails and conditional heteroscedasticity and show asymmetric behavior in their differences. We propose state‐space time series models under skewed, heavy‐tailed distributions to capture all stylized facts of the data. They include the impact of autocorrelation, seasonality, risk premia, temperature, storage levels, the price of European Emission Allowances, and related fuel prices of oil, coal, and electricity. We provide rigorous model diagnostics and interpret all model components in detail. Additionally, we conduct a probabilistic forecasting study with significance tests and compare the predictive performance against literature benchmarks. The proposed day‐ahead (month‐ahead) model leads to a 13% (9%) reduction in out‐of‐sample continuous ranked probability score (CRPS) compared with the best performing benchmark model, mainly due to adequate modeling of the volatility and heavy tails.",,2022,10.1002/for.2853,,proquest,"This paper proposes two probabilistic state-space time series models for forecasting European day-ahead and month-ahead natural gas prices, incorporating factors like temperature, storage levels, and related fuel prices. The models utilize skewed, heavy-tailed distributions to capture the stylized facts of the data, including heavy tails and conditional heteroscedasticity. The proposed models demonstrate improved predictive performance compared to benchmark models, particularly in modeling volatility and heavy tails.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:42:13.794923
4b00fc35dcf2e6b0,Diverging Roads: Theory-Based vs. Machine Learning-Implied Stock Risk Premia,"We compare the performance of theory-based and machine learning (ML) methods for quantifying equity risk premia and assess hybrid strategies that combine the two very different philosophies. The theory-based approach offers advantages at a one-month investment horizon, in particular, if daily frequency risk premium estimates (RPE) are needed. At the one-year horizon, ML has an edge, especially using theory-based RPE as additional feature variables. For a hybrid strategy called Theory with ML Assistance, we employ ML to account for the approximation errors of the theory-based approach. Employing random forests or an ensemble of ML models for theory support yields promising results. © 2025 Elsevier B.V., All rights reserved.","Grammig, J.; Hanenberg, C.; Schlag, C.; Sönksen, J.",2025,10.1093/jjfinec/nbaf005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003374308&doi=10.1093%2Fjjfinec%2Fnbaf005&partnerID=40&md5=a2cce87fcf6aee5edfa0171f297c0c34,scopus,"This study compares theory-based and machine learning (ML) methods for estimating equity risk premia. The theory-based approach is better for short-term (one-month) predictions, especially with daily data. ML performs better at a one-year horizon, particularly when theory-based estimates are used as features. A hybrid strategy, ""Theory with ML Assistance,"" uses ML to correct for approximation errors in the theory-based approach, showing promising results with random forests and ensemble models.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:42:18.593575
4b6d64e1256f6eb5,Do cross-border investors benchmark commercial real estate markets?: Evidence from relative yields and risk premia for a European investment horizon,"Purpose: The purpose of this study is to introduce a new perspective on determinants of cross-border investments in commercial real estate, namely, the relative attractiveness of a target market. So far, the literature has analyzed only absolute measures of investment attractiveness as determinants of cross-border investment flows. Design/methodology/approach: The empirical study uses a classic ordinary least squares estimation for a European panel data set containing 28 cities in 18 countries, with quarterly observations from Q1/2008 to Q3/2018. After controlling for empirically proven explanatory covariates, the model is extended by the new relative measurement based on relative yields/cap rates and relative risk premia. Additionally, the study applies a generalized additive mixed model (GAMM) to investigate a potentially nonlinear relationship. Findings: The study finds on average a ceteris paribus, statistically significant lagged influence of the proxy for relative attractiveness. Nonetheless, a differentiation is needed; relative risk premia are statistically significant, whereas relative yields are not. Moreover, the GAMM confirms a nonlinear relationship for relative risk premia and cross-border transaction volumes. Practical implications: The results are of interest for both academia and market participants as a means of explaining cross-border capital flows. The existing knowledge on determinants is expanded by relative market attractiveness, as well as an awareness of nonlinear relationships. Both insights help to comprehend the underlying transaction dynamics in commercial real estate markets. Originality/value: Whereas the existing body of literature focuses on absolute attractiveness to explain cross-border transaction activity, this study introduces relative attractiveness as an explanatory variable. © 2021 Elsevier B.V., All rights reserved.","Oertel, C.; Willwersch, J.; Cajias, M.",2020,10.1108/jerer-10-2019-0032,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078991288&doi=10.1108%2FJERER-10-2019-0032&partnerID=40&md5=96b180a1e0163261b87854a8d492aa3e,scopus,"This empirical study investigates the influence of relative market attractiveness on cross-border commercial real estate investments in Europe, using panel data from 2008-2018. It introduces relative yields and risk premia as new determinants, finding that relative risk premia significantly impact cross-border transaction volumes, with a nonlinear relationship confirmed by a generalized additive mixed model (GAMM).",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:42:21.810610
adb32e1a86c4dbc0,Do industry returns predict the stock market? A reprise using the random forest,"The prior work reports conflicting evidence on the information content of industry returns for the market index return. We reexamine the out of sample predictive ability of industry returns by considering several relatively advanced methods from the statistical learning literature. We show that when the random forest method, which accounts for both linear and nonlinear dynamics, is used for regression, industry returns indeed contain significant out of sample forecasting power for the market index return. Moreover, our analysis also presents evidence for lead-lag relations among individual industry returns. The reported findings are consistent with the implications of the gradual diffusion of information hypothesis. © 2019 Elsevier B.V., All rights reserved.","Ciner, C.",2019,10.1016/j.qref.2018.11.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057465162&doi=10.1016%2Fj.qref.2018.11.001&partnerID=40&md5=81b05d72c2b849b290de0e292f797a33,scopus,"This study re-examines the predictive power of industry returns for market index returns using advanced statistical learning methods, specifically the random forest. The findings indicate that industry returns do possess significant out-of-sample forecasting power, and also reveal lead-lag relationships among individual industry returns, supporting the gradual diffusion of information hypothesis.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:42:29.080390
e8d85a38742f329b,Do leading indicators forecast US recessions? A nonlinear re-evaluation using historical data,"This paper analyses to what extent a selection of leading indicators is able to forecast U.S. recessions, by means of both dynamic probit models and Support Vector Machine (SVM) models, using monthly data from January 1871 to June 2016. The results suggest that the probit models predict U.S. recession periods more accurately than SVM models up to six months ahead, while the SVM models are more accurate over longer horizons. Furthermore, SVM models appear to distinguish between recessions and tranquil periods better than probit models do. Finally, the most accurate forecasting models are those that include oil, stock returns and the term spread as leading indicators.","Plakandaras, Vasilios; Cunado, Juncal; Gupta, Rangan; Wohar, Mark E.",2017,10.1111/infi.12111,,wos,"This paper evaluates the forecasting ability of leading indicators for US recessions using dynamic probit and Support Vector Machine (SVM) models from 1871 to 2016. Probit models are more accurate for short-term forecasts (up to six months), while SVM models perform better for longer horizons. SVM models also better distinguish recessions from non-recessionary periods. The study identifies oil, stock returns, and the term spread as the most effective leading indicators.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:42:40.328822
b0fbff89929bd3a8,Does Rating Consistency Matter? A Micro-Level Study on the Impact of Corporate ESG Rating Divergence on Equity Financing Costs,"The influence of ESG rating divergence on corporate finance is receiving increasing scholarly attention as ESG concerns become more prominent. However, few studies have systematically examined how ESG rating discrepancies affect corporate equity financing costs. This study addresses this gap by analysing data from Chinese A-share listed firms between 2015 and 2022 to explore the impact of ESG rating divergence on equity financing costs and its underlying mechanisms. The results reveal that: (1) ESG rating divergence significantly raises equity financing costs, suggesting that capital markets impose a risk premium on inconsistent ESG information; (2) this effect is more pronounced in firms with greater external analyst forecast dispersion and more severe internal agency problems; (3) firms that are non-state-owned, face significant financial constraints, or have poor information quality are particularly vulnerable to this effect. The study contributes to the understanding of the economic consequences of ESG rating divergence and offers policy implications for regulators, institutions, investors and firms in navigating ESG-related challenges. © 2025 Elsevier B.V., All rights reserved.","Li, J.; Hou, Y.; Tan, C.; Han, L.",2025,10.1111/acfi.70057,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009813119&doi=10.1111%2Facfi.70057&partnerID=40&md5=bad9947ba156708087e4fd04ac55b870,scopus,"This study investigates the impact of ESG rating divergence on the equity financing costs of Chinese A-share listed firms from 2015-2022. Findings indicate that higher ESG rating divergence leads to increased equity financing costs, particularly for firms with greater analyst forecast dispersion, internal agency problems, non-state ownership, financial constraints, or poor information quality. The research suggests that capital markets penalize inconsistent ESG information.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:42:45.108251
9ed23b41c26184ea,Does a search attention index explain portfolio returns in India?,"Employing asset-pricing models over the period 2012 to 2017, this study examines whether a search attention index (SAI) explains the variation in the weekly excess return of stocks. The study finds that the estimated abnormal return of a portfolio based on search intensity is significantly high for stocks with higher search intensity and low for stocks with lower search intensity. Further, the study observes that, when the SAI is high, the excess returns are high for stocks with a high value, high volatility, and high sensitivity. Interestingly, the study documents that in the Indian market investor attention is irrelevant for stocks with extremely high risk. This study finds that the SAI in India explains the variation in the excess return of stocks as well as the market, size, value, and momentum factors. © 2022 Elsevier B.V., All rights reserved.","Dharani, M.; Hassan, M.K.; Abedin, M.Z.; Ismail, M.A.",2022,10.1016/j.bir.2021.04.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105775150&doi=10.1016%2Fj.bir.2021.04.003&partnerID=40&md5=2e803c6ed55f99bd21aa2e550c14959d,scopus,"This study investigates whether a search attention index (SAI) can explain stock returns in India from 2012 to 2017. It finds that SAI significantly explains variations in weekly excess stock returns, particularly for stocks with high value, volatility, and sensitivity when SAI is high. However, investor attention appears irrelevant for extremely high-risk stocks in the Indian market. The SAI also explains variations related to market, size, value, and momentum factors.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:42:49.072725
d3d12a9e1655b3b2,Does conditioning information matter in estimating continuous time interest rate diffusions?,"We examine an important aspect of empirical estimation of term structure models; the role of conditioning information in dynamic term structure models. The use of both real world or simulated data implicitly incorporates conditioning information. We examine the bias created in estimating the drift by a specific form of conditioning, namely truncation. Using the theory of enlargement of filtrations we provide estimates of the extent of this truncation bias for commonly used short rate models. We find that this truncation bias causes the drift of these models to have a nonlinear structure. © 2018 Elsevier B.V., All rights reserved.","Abhyankar, A.; Basu, D.",2001,10.2307/2676286,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035612944&doi=10.2307%2F2676286&partnerID=40&md5=659a2f593a7c8bb3bb31fd10ef82c8f5,scopus,"This paper investigates the impact of conditioning information on estimating term structure models for interest rates. It analyzes the truncation bias in estimating the drift of short rate models using the theory of enlargement of filtrations, finding that this bias introduces a nonlinear structure to the drift. The study uses both real-world and simulated data.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:42:54.708536
296118b61e9a3e48,"Does high frequency trading affect technical analysis and market efficiency? And if so, how?","In this paper we investigate how high frequency trading affects technical analysis and market efficiency in the foreign exchange (FX) market by using a special adaptive form of the Strongly Typed Genetic Programming (STGP)-based learning algorithm. We use this approach for real one-minute high frequency data of the most traded currency pairs worldwide: EUR/USD, USD/JPY, GBP/USD, AUD/USD, USD/CHF, and USD/CAD. The STGP performance is compared with that of parametric and non-parametric models and validated by two formal empirical tests. We perform in-sample and out-of-sample comparisons between all models on the basis of forecast performance and investment return. Furthermore, our paper shows the relative strength of these models with respect to the actual trading profit generated by their forecasts. Empirical experiments suggest that the STGP forecasting technique significantly outperforms the traditional econometric models. We find evidence that the excess returns are both statistically and economically significant, even when appropriate transaction costs are taken into account. We also find evidence that HFT has a beneficial role in the price discovery process. © 2013 Elsevier B.V. © 2013 Elsevier B.V., All rights reserved.","Manahov, V.; Hudson, R.; Gebka, B.",2014,10.1016/j.intfin.2013.11.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888777057&doi=10.1016%2Fj.intfin.2013.11.002&partnerID=40&md5=c39d0f295369ac49ddb0508cf7876015,scopus,"This study investigates the impact of high-frequency trading (HFT) on technical analysis and market efficiency in the foreign exchange (FX) market. Using a specialized adaptive Strongly Typed Genetic Programming (STGP) algorithm on real one-minute FX data for major currency pairs, the research compares STGP performance against parametric and non-parametric models. The findings indicate that STGP significantly outperforms traditional econometric models in terms of forecast performance and investment return, even after accounting for transaction costs. The study also suggests that HFT plays a positive role in price discovery.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:42:58.840399
c657958f06d1abb7,Dynamic portfolio insurance strategy: A robust machine learning approach,"In this paper, we propose a robust genetic programming (RGP) model for a dynamic strategy of stock portfolio insurance. With portfolio insurance strategy, we divide the money in a risky asset and a risk-free asset. Our applied strategy is based on a constant proportion portfolio insurance strategy. For determining the amount for investing in the risky asset, a critical parameter is a constant risk multiplier that is calculated in our proposed model using RGP to reflect market dynamics. Our model includes four main steps: (1) Selecting the best stocks for constructing a portfolio using a density-based clustering strategy. (2) Enhancing the robustness of our proposed model with an application of the Adaptive Neuro-Fuzzy Inference Systems (ANFIS) for forecasting the future prices of the selected stocks. The findings show that using ANFIS, instead of a regular multi-layer artificial neural network improves the prediction accuracy and our model’s robustness. (3) Implementing the RGP model for calculating the risk multiplier. Risk variables are used to generate equation trees for calculating the risk multiplier. (4) Determining the optimal portfolio weights of the assets using the well-known Markowitz portfolio optimization model. Experimental results show that our proposed strategy outperforms our previous model. © 2021 Elsevier B.V., All rights reserved.","Dehghanpour, S.; Esfahanipour, A.",2018,10.1080/24751839.2018.1431447,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065718194&doi=10.1080%2F24751839.2018.1431447&partnerID=40&md5=5a81c0b59c02568f89020e86bab8376a,scopus,"This paper introduces a robust genetic programming (RGP) model for dynamic stock portfolio insurance. The strategy involves dividing funds between risky and risk-free assets, with a constant proportion portfolio insurance strategy. The model uses RGP to determine a risk multiplier reflecting market dynamics. It includes steps for stock selection via clustering, price forecasting using Adaptive Neuro-Fuzzy Inference Systems (ANFIS) for enhanced robustness, RGP for risk multiplier calculation, and Markowitz optimization for portfolio weights. Experimental results indicate the proposed strategy outperforms previous models.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:43:02.187736
04924b384fc50756,Dynamic relationship of volatility of returns across different markets: evidence from selected next 11 countries,"Purpose: This paper aims to examine whether the volatility of returns in commodity (gold, oil), bond and forex markets is related over time to the volatility of returns in equity markets of Bangladesh, Indonesia, Pakistan, Philippines, Turkey and Vietnam. In addition, the authors analyze the integration of the commodity, bond, forex and equity markets across these markets. Design/methodology/approach: The dynamic conditional correlation GARCH (DCC-GARCH) model is used to capture the time-varying conditional correlation among markets. The authors use daily data of stock prices, oil prices, gold prices, exchange rates and 10 years' bond yields of the six countries from Datastream and investing.com from January 2001 to April 2021. Findings: Findings reveal that the parameters of dynamic correlation are statistically significant which indicates the importance of time-varying co-movements. Estimation of the DCC-GARCH model suggests that the stock market is significantly correlated with bond, forex, gold and oil markets in all six countries. Practical implications: This study has practical implications for policymakers and investment professionals. A better understanding of dynamic linkages among the markets would help in constructing effective hedging and portfolio diversification strategies. Policy makers can get insight to build proper strategies in order to insulate the economy from factors that cause volatility. Originality/value: Several studies have investigated the linkage between commodity and stock markets and the volatility spillover effect, but very little attention is given to study the interrelationship between groups of market segments of different economies. No study has comparatively examined the dynamic relationship of multiple markets of a group of emerging countries simultaneously. © 2024 Elsevier B.V., All rights reserved.","Shafiq, S.; Qureshi, S.S.; Akbar, M.",2024,10.1108/jeas-09-2022-0216,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197194276&doi=10.1108%2FJEAS-09-2022-0216&partnerID=40&md5=16a64a068d49b634b8f06b98b9336458,scopus,"This study investigates the dynamic relationships between the volatility of returns in equity, commodity (gold, oil), bond, and forex markets across six emerging economies (Bangladesh, Indonesia, Pakistan, Philippines, Turkey, and Vietnam). Using daily data from 2001 to 2021 and the DCC-GARCH model, the research finds significant time-varying correlations, indicating that stock markets are correlated with bond, forex, gold, and oil markets in these countries. The findings offer practical implications for hedging, portfolio diversification, and policy-making.",True,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:43:11.324449
116d6c5d4db5f1b9,Dynamics of Money Market Interest Rates in Ghana: Time-Frequency Analysis of Volatility Spillovers,"As the second longest practicing inflation targeting economy in Africa, it is of interest to investigate the degree to which policy interest rate influences other money market rates so as to gauge the overall effectiveness of monetary policy transmission in Ghana. This study evaluates the degree of connectedness among money market rates and also determines the most dominant money market rate(s) in Ghana. The basic finding is that the monetary policy rate has a low-to-moderate influence on volatility dynamics of other money market rates in Ghana across historical time-interval and time-frequency domains. This is a reflection of a generally weak capability of policy interest rate to drive other market rates in Ghana. Both monetary policy rate and Treasury bill rate are net transmitters of shocks, while interbank, lending and saving rates are net receivers of shocks in the money market. However, the Treasury bill emerges as the largest shock transmitter in the money market, across all forecast horizons and analytical domains. The lending rate is the largest shock recipient in the money market, largely from the Treasury bill rate which suggests ample evidence of fiscal dominance in Ghana. The study accentuates the exigency for monetary and fiscal policies to expeditiously address the domestic structural bottlenecks, especially in the financial sector and the fragile fiscal profile, in order to strengthen policy transmission in Ghana.","Akosah, Nana Kwame; Alagidede, Imhotep Paul; Schaling, Eric",2021,10.1111/saje.12287,,wos,"This study analyzes the influence of Ghana's policy interest rate on other money market rates using time-frequency analysis of volatility spillovers. It finds a low-to-moderate influence, indicating weak monetary policy transmission. The Treasury bill rate is the largest shock transmitter, while the lending rate is the largest recipient, suggesting fiscal dominance. The study emphasizes the need for policy reforms to strengthen transmission.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:43:13.804542
d116e668a6034e16,"ECONOMIC INJURY LEVEL, ACTION THRESHOLD, AND A YIELD-LOSS MODEL FOR THE PEA APHID, ACYRTHOSIPHON-PISUM (HOMOPTERA, APHIDIDAE), ON GREEN PEAS, PISUM-SATIVUM","Economic injury level, action threshold, and population development studies with the pea aphid (PA), Acyrthosiphon pisum (Harris), were conducted during 1983-85 [Washington, USA]. Pea aphid densities, simulating those in commercial pea fields, were established using insecticides to manipulate infestation levels. Three experiments, incorporating 12 treatments and six replications, were analyzed. A generalized, nonlinear equation relating pea yield to accumulated aphid feeding days (AFD) is described. The model approximates two phases of a sigmoid infestation-yield curve. An upper maximum plateau and a region of rapidly decreasing yield are approximate. Beyond 1,800 aFD, a lower minimum yield plateau is hypothesized. Economic injury levels calculated for the 3 years'' experiments using the generalized model were 22.2, 18.2, and 12.2 AFD, respectively. Action threshold estimates were determined from linear regression estimates of yield versus aphids per plant at bloom. Action thresholds were 3.6, 0.3, and 0.3 aphids per plant for the years 1983, 1984, 1985, respectively. The pea quality components, tenderometer (TD) and sieve size, were altered by maximum PA densities. High AFD levels increased TD and decreased sieve size significantly when compared with aphid-free controls. Protein content of green peas was not significantly altered by PA feeding.","YENCHO, GC; GETZIN, LW; LONG, GE",1986,10.1093/jee/79.6.1681,,wos,"This study establishes economic injury levels and action thresholds for pea aphids on green peas by developing a yield-loss model based on accumulated aphid feeding days (AFD). The model accounts for different phases of infestation-yield response, and calculated economic injury levels varied across the study years. Action thresholds were also determined, and high aphid densities were found to affect pea quality components like tenderometer readings and sieve size, but not protein content.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:43:16.528851
03cc69643da21feb,ESG in the headlines: Media-driven reputational risk and stock performance,"This study examines the impact of environmental, social, and governance (ESG) reputational risks on stock performance. We use a unique dataset of media-driven ESG reputational risk indicators, covering 4963 Chinese firms from 2009 to 2023. On average, a one-standard-deviation increase in ESG reputational risks is associated with a 4.5 % decrease in simple stock returns, a 14.5 % reduction in excess stock returns relative to the market index, and a 12.2% decline in excess stock returns compared to peer firms of similar size. These negative effects contradict the traditional risk-return relationship predicted by risk premium theory. Further analysis identifies reduced investor confidence and tighter financing constraints as key mechanisms through which ESG reputational risks negatively affect stock returns. Heterogeneity analyses indicate that the negative impact is more pronounced for firms in non-pollution-intensive industries, those facing financing difficulties, and those exposed to environment-related reputational risks.","Zhou, Bole; Ge, Wanjun",2025,10.1016/j.gfj.2025.101127,,wos,"This study investigates how media-driven ESG reputational risks affect stock performance in Chinese firms. It finds that increased ESG risks lead to significant decreases in stock returns, contradicting traditional risk-return theories. The study identifies reduced investor confidence and financing constraints as key mechanisms and notes that the impact is stronger for certain types of firms.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:43:19.739750
ae8ad37dda607103,ESTIMATING AN UPPER BOUND ON THE PRATT RISK-AVERSION COEFFICIENT WHEN THE UTILITY FUNCTION IS UNKNOWN,,"MCCARL, BA; BESSLER, DA",1989,10.1111/j.1467-8489.1989.tb00481.x,,wos,This paper focuses on estimating an upper bound for the Pratt risk-aversion coefficient when the utility function is unknown. The abstract does not provide further details on the methodology or specific applications.,False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:43:22.934836
2058d64637af6983,Early Warning of Systemic Financial Risk of Local Government Implicit Debt Based on BP Neural Network Model,"In recent years, local governments have boosted their local economies by raising large amounts of debt. Even though the state further strictly controls local government debt, the hidden debt formed by the local government borrowing in disguised form can infect systemic financial risks, creating an urgent need to carry out risk warning based on local government hidden debt. The paper uses the macro indicators of local government implicit debt risk at the prefecture-level city level, and introduces the micro indicators of PPP projects, financing platform bank debt, and urban investment debt to establish a BP neural network model. We not only study the contagion effect of local government hidden debt on systemic financial risks, but also predict the systemic financial risks in 2019 and construct an early warning risk system based on the prefecture-level city data from 2015 to 2018. In addition, the early warning effect of local government implicit debt on systemic financial risk under different stress scenarios is investigated. The study found that the implicit debt risk of local governments, the scale of financing platform bank debt, the scale of PPP, and the scale of urban investment bonds have a significant impact on systemic financial risks. The neural network model constructed by introducing these four variables at the same time can better predict the level of systemic financial risk. The model can also accurately predict the changes in systemic financial risks under the stress test of the increase in hidden debt of different local governments, and has a good early warning effect.",,2022,10.3390/systems10060207,,proquest,"This paper develops a BP neural network model to predict systemic financial risks stemming from local government implicit debt. It incorporates macro indicators and micro indicators like PPP projects, financing platform bank debt, and urban investment debt. The model demonstrates an ability to predict risks and their contagion effects, showing a good early warning effect under various stress scenarios.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:43:38.095225
f801db2ab5d662e6,Economic Evaluation and Risk Premium Estimation of Rainfed Soybean under Various Planting Practices in a Semi-Humid Drought-Prone Region of Northwest China,"Economic benefits and risk premiums significantly affect the production system decision making of farmers and government departments. This study evaluated the economic feasibility and estimated the risk premium of 12 rainfed soybean production systems with various planting densities, fertilization rates and planting patterns by considering the impact of soybean price fluctuation. There were two planting densities (D1: 160,000 plants ha−1 and D2: 320,000 plants ha−1), two fertilization rates (F1: 20 kg ha−1 N, 30 kg ha−1 P, 30 kg ha−1 K; F2: 40 kg ha−1 N, 60 kg ha−1 P, 60 kg ha−1 K) and three planting patterns (F+W0: flat cultivation with no irrigation; R+W0: plastic-mulched ridge-furrow cultivation (PMRF) with no irrigation; R+W1: PMRF with supplemental irrigation of 30 mm at the pod-filling stage). Based on the two-year (2019–2020) field data in a semi-humid drought-prone region of northwest China and soybean price fluctuation from January 2014 to June 2021, the net income (NI) was calculated by considering the impact of soybean price fluctuation and assuming constant soybean production costs. The net present value (NPV) method and the stochastic efficiency with respect to a function (SERF) method were used to evaluate the profitability of protective alternatives and the risk of these alternatives. The results showed that the 12 proposed soybean production systems were economically feasible. Reducing the fertilization rate reduced the input costs, but it did not necessarily result in a decrease in soybean yield and NI. The payback period of all production systems was within two years for farmers investing through loans. High-fertilizer and high-density production systems made personal investment obtain the highest economic benefit in this study, which was not the best investment strategy from the perspective of production-to-investment ratio and environmental protection departments. The preferences of farmers with various risk aversion and environmental protection departments in terms of risk premium were also proposed. The economic and risk assessment framework of this study can enhance the understanding of the adjustment of production systems from different perspectives, and provide strategies for promoting the protection of economic, environmental and socially sustainable agricultural systems.",,2023,10.3390/agronomy13112840,,proquest,"This study evaluates the economic feasibility and risk premium of 12 rainfed soybean production systems in a drought-prone region of China, considering various planting densities, fertilization rates, and planting patterns. It uses net income, net present value, and stochastic efficiency with respect to a function to assess profitability and risk under soybean price fluctuations. The findings suggest that while high-input systems may offer higher immediate returns, they are not always the most beneficial from a broader economic or environmental perspective. The research provides a framework for understanding production system adjustments and promoting sustainable agriculture.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:43:42.782724
f010f2916ddce7c3,Empirical causal analysis of flood risk factors on U.S. flood insurance payouts:Implications for solvency and risk reduction,"This paper presents a regression model that quantifies the causal relationship between flood risk factors and the flood insurance payout in the U.S. The flood risk factors that have been considered in this research are flood exposure, infrastructure vulnerability, social vulnerability, and the number of mobile homes. Historical data for the annual flood insurance payout, flood risk factors, and other control variables were collected for six years between 2016 and 2021 and used in a Mixed Effects Regression model to derive the empirical relationships. The regression model expressed the natural logarithm of the annual flood insurance payout in a county based on the flood risk factors and control variables. The paper presents the regression coefficients that quantify the causal influence. It has been found that all four flood risk factors have statistically significant positive influence on the flood insurance payout in a county. However, the extent of the influence is different for different flood risk factors. Among them, flood exposure has the highest influence on the flood insurance payout, which is followed by the number of mobile homes, infrastructure vulnerability, and social vulnerability. Since the federal flood insurance program in the U.S. has a large debt to the U.S. treasury, the government should plan for effective risk reduction that can reduce the flood insurance payout in future to keep the program solvent. The outcomes of this research are expected to facilitate that decision-making process by providing the empirical relationship between flood risk factors and flood insurance payout. © 2024 Elsevier B.V., All rights reserved.","Bhattacharyya, A.; Hastak, M.",2024,10.1016/j.jenvman.2024.120075,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182725293&doi=10.1016%2Fj.jenvman.2024.120075&partnerID=40&md5=b9178af6b532ea7a3bb7c2201e439a3b,scopus,"This study uses a regression model to analyze the causal impact of flood risk factors (exposure, infrastructure vulnerability, social vulnerability, mobile homes) on U.S. flood insurance payouts between 2016-2021. All factors significantly increase payouts, with flood exposure having the largest effect. The findings aim to inform risk reduction strategies for the solvency of the federal flood insurance program.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:43:46.550797
03ea7ad65ff3f52e,Empirical evaluation of the market price of risk using the CIR model,"We describe a simple but effective method for the estimation of the market price of risk. The basic idea is to compare the results obtained by following two different approaches in the application of the Cox-Ingersoll-Ross (CIR) model. In the first case, we apply the non-linear least squares method to cross sectional data (i.e., all rates of a single day). In the second case, we consider the short rate obtained by means of the first procedure as a proxy or the real market short rate. Starting from this new proxy, we evaluate the parameters of the CIR model by means of martingale estimation techniques. The estimate of the market price of risk is provided by comparing results obtained with these two techniques, since this approach makes possible to isolate the market price of risk and evaluate, under the Local Expectations Hypothesis, the risk premium given by the market for different maturities. As a test case, we apply the method to data of the European Fixed Income Market. (c) 2006 Elsevier B.V. All rights reserved.","Bernaschi, M.; Torosantucci, L.; Uboldi, A.",2007,10.1016/j.physa.2006.10.072,,wos,This paper proposes a method to estimate the market price of risk using the Cox-Ingersoll-Ross (CIR) model by comparing two estimation approaches: non-linear least squares on cross-sectional data and martingale estimation techniques using a proxy for the short rate. The method is tested on the European Fixed Income Market.,False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:43:50.593970
d434a775f2150cee,Empirical reverse engineering of the pricing kernel,"This paper proposes an econometric procedure that allows the estimation of the pricing kernel without either any assumptions about the investors preferences or the use of the consumption data. We propose a model of equity price dynamics that allows for (i) simultaneous consideration of multiple stock prices, (ii) analytical formulas for derivatives such as futures, options and bonds, and (iii) a realistic description of all of these assets. The analytical specification of the model allows us to infer the dynamics of the pricing kernel. The model, calibrated to a comprehensive dataset including the S&P 500 index, individual equities, T-bills and gold futures, yields the conditional filter of the unobservable pricing kernel. As a result we obtain the estimate of the kernel that is positive almost surely (i.e. precludes arbitrage), consistent with the equity risk premium, the risk-free discounting, and with the observed asset prices by construction. The pricing kernel estimate involves a highly nonlinear function of the contemporaneous and lagged returns on the S&P 500 index. This contradicts typical implementations of CAPM that use a linear function of the market proxy return as the pricing kernel. Hence, the S&P 500 index does not have to coincide with the market portfolio if it is used in conjunction with nonlinear asset pricing models. We also find that our best estimate of the pricing kernel is not consistent with the standard time-separable utilities, but potentially could be cast into the stochastic habit formation framework of Campbell and Cochrane (J. Political Economy 107 (1999) 205). © 2003 Elsevier B.V. All rights reserved. © 2008 Elsevier B.V., All rights reserved.","Chernov, M.",2003,10.1016/s0304-4076(03)00111-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346937478&doi=10.1016%2FS0304-4076%2803%2900111-8&partnerID=40&md5=887c4b2dc8caa87597889e2ab5275885,scopus,"This paper presents an econometric method to estimate the pricing kernel without assuming investor preferences or using consumption data. It models equity price dynamics, allowing for multiple stock prices, analytical derivative formulas, and realistic asset descriptions. The model infers the pricing kernel's dynamics, calibrated to data including the S&P 500 index, individual equities, T-bills, and gold futures. The estimated kernel is positive, consistent with equity risk premium and observed asset prices, and is a nonlinear function of S&P 500 returns, contradicting typical CAPM implementations. The findings suggest the S&P 500 index may not always represent the market portfolio in nonlinear asset pricing models and that the estimated kernel is not consistent with standard time-separable utilities but might fit a stochastic habit formation framework.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:43:56.437163
9d3cef205fa28fe7,End-to-end transport network digital twins with cloud-native SDN controllers and generative AI [Invited],"This paper explores the potential of network digital twins (NDTs) in networking (both IP Ethernet networks and optical transport networks), highlighting their integration with cloud-native software-defined networking (SDN) controllers and intent-based networking enabled by generative artificial intelligence (GenAI). The proposed framework represents an approach that combines advanced virtualization, real-time analytics, and GenAI. The use of NDTs enables a comprehensive and dynamic digital representation of the physical network, capturing critical aspects, such as topology, traffic patterns, and performance metrics, which permits data-driven decision-making to lead to more efficient networking operations. The incorporation of cloud-native SDN controllers along with an NDT ensures that the system remains scalable, flexible, and responsive to dynamic network conditions. Intent-based networking, powered by GenAI, allows the network to interpret high-level objectives from operators and autonomously translate them into actionable configurations that are enforced by orchestrators and SDN controllers. This eliminates manual intervention, minimizes errors, accelerates the deployment of network services, and provides a means for easier network management. The presented framework significantly enhances automation, enabling predictive maintenance by identifying potential issues before they impact network performance. It optimizes network design by simulating various configurations and testing their feasibility in a risk-free environment. These capabilities collectively improve operational efficiency, reduce downtime, and ensure optimal resource utilization.",A. Abishek; D. Adanza; P. Alemany; L. Gifre; R. Casellas; R. Martinez; R. Munoz; R. Vilalta,2025,10.1364/jocn.550864,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10974926,ieeexplore,"This paper proposes a framework for end-to-end transport network digital twins (NDTs) integrated with cloud-native SDN controllers and generative AI (GenAI). NDTs provide a dynamic digital representation of the physical network, enabling data-driven decisions for efficient operations. Cloud-native SDN controllers ensure scalability and responsiveness, while GenAI-powered intent-based networking translates high-level operator objectives into automated network configurations. This approach enhances automation, enables predictive maintenance, optimizes network design through simulation, reduces downtime, and improves resource utilization.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:44:00.992595
c7ed342d298e889b,Endogenous inattention and risk-specific price underreaction in corporate bonds,"Corporate bond prices are slow to respond to default risk and interest rate shocks, as proxied by firm-level stock returns and Treasury returns, respectively. Furthermore, the underreaction is risk-specific: bonds with better credit quality underreact more to default risk, while those with worse quality underreact more to interest rates. The underreactions imply substantial out-of-sample return predictability, and investors appear to be leaving too much money on the table. The results are consistent with behavioral inattention models in which investors endogenously allocate more attention to payoff-relevant (or salient) risks, and they are not explained by traditional trading friction mechanisms. © 2022 Elsevier B.V., All rights reserved.","Li, J.",2022,10.1016/j.jfineco.2021.09.025,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117362116&doi=10.1016%2Fj.jfineco.2021.09.025&partnerID=40&md5=c050948826a3bbc8282b4b542ccd0387,scopus,"This study investigates the slow response of corporate bond prices to default risk and interest rate shocks, suggesting investors underreact to these risks. The underreaction is risk-specific, with better credit quality bonds underreacting more to default risk and worse quality bonds underreacting more to interest rates. This implies significant out-of-sample return predictability and is explained by behavioral inattention models rather than trading frictions.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:44:07.805449
00660d636f49d59e,Estimating the Term Structure with Linear Regressions: Getting to the Roots of the Problem,"Linear estimators of the affine term structure model are inconsistent since they cannot reproduce the factors used in estimation. This is a serious handicap empirically, giving a worse fit than the conventional ML estimator that ensures consistency. We show that a simple self-consistent estimator can be constructed using the eigenvalue decomposition of a regression estimator. The remaining parameters of the model follow analytically. Estimates from this model are virtually indistinguishable from that of the ML estimator. We apply the method to estimate various models of U.S. Treasury yields. These exercises greatly extend the range of models that can be estimated. © 2021 Elsevier B.V., All rights reserved.","Goliǹski, A.; Spencer, P.",2021,10.1093/jjfinec/nbz031,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099520801&doi=10.1093%2Fjjfinec%2Fnbz031&partnerID=40&md5=fefd9e28937e02b26df6e38c509d65ee,scopus,"This paper proposes a self-consistent estimator for the affine term structure model, which is shown to be consistent and empirically performs similarly to the conventional Maximum Likelihood estimator. The method is applied to estimate U.S. Treasury yield models, expanding the scope of estimable models.",False,True,False,gemini-2.5-flash-lite,Olav,M,,2025-10-13T15:44:33.516277
786cba4cf35e5a45,Estimating time-varying risk premia in UK long-term government bonds,"Simple models of time-varying risk premia are used to measure the risk premia in long-term UK government bonds. The parameters of the models can be estimated using nonlinear seemingly unrelated regression (NL-SUR), which permits efficient use of information across the entire yield curve and facilitates the testing of various cross-sectional restrictions. The estimated time-varying premia are found to be substantially different to those estimated using models that assume constant risk premia. © 2004 Taylor and Francis Ltd. © 2008 Elsevier B.V., All rights reserved.","Steeley, J.M.",2004,10.1080/0960310042000211632,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1542639767&doi=10.1080%2F0960310042000211632&partnerID=40&md5=19ff6b7366f9cad45ca9caab1717eb0d,scopus,This paper estimates time-varying risk premia in UK long-term government bonds using nonlinear seemingly unrelated regression (NL-SUR). The method allows for efficient use of information across the yield curve and testing of cross-sectional restrictions. The estimated time-varying premia differ significantly from those derived from models assuming constant risk premia.,False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:44:45.153839
f0f93a32202f330e,Estimating volatility from ATM options with lognormal stochastic variance and long memory,"In this article we propose a nonlinear state space representation to model At-The-Money (ATM) implied volatilities and to estimate the unobserved Stochastic Volatility (SVOL) for the underlying asset. We derive a polynomial measurement model relating fractionally cointegrated implied and spot volatilities. We then use our state space representation to obtain Maximum Likelihood (ML) estimates of the short-memory model parameters, and for filtering the fractional spot volatility. We are also able to estimate the average volatility risk premia. We applied our methodology to implied volatilities on eurodollar options, from which we filter the unobserved spot local variance. These data arise from Over The Counter (OTC) transactions that account for high liquidity. For these data, we estimated a positive average volatility risk premia, which is consistent with the Intertemporal Capital Asset Pricing Model (ICAPM) setup of Merton (1973). We also had evidence of highly nonlinear relation between eurodollar spot and implied volatilities. From a methodological and computational point of view, the likelihood function, and all the iterative procedures associated with it, converged uniformly in the parameter space at very little computational expense. We illustrated the effectiveness of our approach by evaluating the approximated Information matrix, the Hotelling's T2 test along with other diagnostic procedures. Reprinted by permission of Routledge, Taylor and Francis Ltd.",,2012,10.1080/09603107.2011.624082,,proquest,"This paper proposes a nonlinear state space model to estimate unobserved stochastic volatility (SVOL) from At-The-Money (ATM) implied volatilities. It derives a measurement model relating cointegrated implied and spot volatilities, enabling Maximum Likelihood (ML) estimation of short-memory model parameters and filtering of fractional spot volatility. The study also estimates average volatility risk premia and finds evidence of a nonlinear relationship between eurodollar spot and implied volatilities. The methodology is computationally efficient and validated with diagnostic procedures.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:44:49.437093
65a824924d8fbb16,Estimating yield curves of the U.S. Treasury securities: An interpolation approach,"Following the approach of interpolation, this paper proposes the multiple exponential decay model to fit yield curves for both the U.S. TIPS market and the conventional Treasury security market. Several estimation methods, including the unconstrained/constrained nonlinear minimization, quadratic programming, and the iterative linear least squares, are applied to estimate the unknown parameters according to different curve-fitting purposes. Comparisons between the proposed model and the alternatives show that the multiple exponential decay successfully (1) adapts to a variety of shapes associated with yield curves, (2) (partially) keeps in line with the economic interpretations of Nelson–Siegel summarized by Diebold and Li (), and (3) dominates the competing models in curve-fitting performance measured by mean fitted-price errors over the sample period. In addition, the exact specification of a nonparametric interpolation model is pinned down by applying three statistical tools, which enable us to jointly take into account validity, optimality, and parsimoniousness of the proposed model. © 2020 Elsevier B.V., All rights reserved.","Guo, F.",2019,10.1002/rfe.1039,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055113189&doi=10.1002%2Frfe.1039&partnerID=40&md5=04461965ab8433b380f1e433878baeda,scopus,"This paper proposes a multiple exponential decay model for fitting yield curves of U.S. Treasury securities and TIPS, using various estimation methods. The model demonstrates adaptability to different yield curve shapes, aligns with economic interpretations, and outperforms competing models in fitting accuracy.",True,True,False,gemini-2.5-flash-lite,Olav,M,,2025-10-13T15:45:05.289636
cb24c90ea906f755,Estimation and Forecasting of Sovereign Credit Rating Migration Based on Regime Switching Markov Chain,"Our research aims to develop the regime switching Markov chain (RSMC), a discrete time Markov chain whose underlying regime is depending on a hidden Markov model, which express the dynamics of sovereign credit rating migration. Estimated based on a version of the Expectation-Maximization algorithm, the regime in RSMC indicates either economic expansion or contraction. Then, we apply RSMC to the monthly time series of the sovereign credit rating of 41 nations from January 1994 to December 2018. At first, we confirm that the estimation of RSMC is superior to a homogeneous Markov chain. It implies that the credit rating dynamics are subject to the underlying economic condition. Secondly, we observe that the second tier and non-investment credit ratings in economic contractions are likely to be downgraded. We also detect the continental clustering of economic contractions for the Asian currency and European sovereign debt crises. Lastly, we discover that the forecasting performance of RSMC is superior to that of the benchmark, especially for the second tier and non-investment credit ratings. In conclusion, we claim that RSMC can improve the management of sovereign credit risk exposures.","Oh, Sung Youl; Song, Jae Wook; Chang, Woojin; Lee, Minhyuk",2019,10.1109/access.2019.2934516,,wos,"This study introduces a regime-switching Markov chain (RSMC) model to analyze sovereign credit rating migration, considering underlying economic conditions (expansion/contraction). The RSMC model, estimated using an Expectation-Maximization algorithm, demonstrates superior performance over homogeneous Markov chains. Findings indicate that lower credit ratings are more prone to downgrades during economic contractions, with evidence of continental clustering of these contractions. The RSMC model also shows improved forecasting accuracy, particularly for lower credit ratings, suggesting its utility in managing sovereign credit risk.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:45:26.301730
204aa7a29bdc5b45,"Estimation of the concurrent radiological dosage to humans due to the transfer of 226Ra, 228Ra, and 40K from soil-to-Malaysian traditional medicinal plants","Medicinal plants have been incorporated into various traditional medicine systems worldwide to reduce disease risk, treat illnesses, and provide medicinal remedies. Today, the pharmaceutical industry uses the most active plant compounds in drug synthesis. Possible high levels of naturally occurring radionuclides in medicinal plants have raised public concern about the consequent radiological impact on the consumption of medicinal plants and herbs. This paper reports the first study of soil-to-plant mobilities of natural radionuclides in native medicinal plants in Malaysia. Representative samples of soils and organically grown traditional medicinal plants from western Malaysia were collected and studied using HPGe gamma-ray spectrometry. Average activity concentrations for 226Ra, 228Ra, and 40K in the soils are respectively 57, 84, and 520 Bq/kg, and the respective values in the medicinal plants are 10, 4, and 498 Bq/kg. The respective transfer factors (TFs) for the medicinal leaves are 0.18, 0.05, and 1.18. The TFs of 40K were higher than others due to higher uptake and its essentiality in plant growth. These findings indicate that plant growth habits greatly influenced the radionuclides' uptake. The radioactivities in soils and their corresponding mobilities are in accordance with literature data. To discard any radiological hazards to human health, the estimated threshold consumption rate is found to be approximately 46 kg/y. Annual effective doses and excess lifetime cancer risk for adult members of the public due to the consumption of medicinal plants are found to be negligible. It is suggested that the use of traditional medicinal plants may provide a risk-free and safe means of maintaining public health. © 2025 Elsevier B.V., All rights reserved.","Shuaibu, H.K.; Mohamed, F.; Khandaker, M.U.; Ismail, A.F.; Osman, H.",2024,10.1016/j.radphyschem.2024.111982,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197039271&doi=10.1016%2Fj.radphyschem.2024.111982&partnerID=40&md5=9e5010be32b9a982b5a0f2752b665351,scopus,"This study investigates the transfer of natural radionuclides (226Ra, 228Ra, and 40K) from soil to Malaysian traditional medicinal plants and estimates the resulting radiological dosage to humans. Radionuclide concentrations in soil and plants were measured, and transfer factors were calculated. The findings suggest that the consumption of these medicinal plants poses negligible radiological risks to human health, with an estimated threshold consumption rate of 46 kg/y.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:45:33.500394
aa64a2822074ac0d,Ethical and regulatory challenges in machine learning-based healthcare systems: A review of implementation barriers and future directions,"Machine learning significantly enhances clinical decision-making quality, directly impacting patient care with early diagnosis, personalized treatment, and predictive analytics. Nonetheless, the increasing proliferation of such ML applications in practice raises potential ethical and regulatory obstacles that may prevent their widespread adoption in healthcare. Key issues concern patient data privacy, algorithmic bias, absence of transparency, and ambiguous legal liability. Fortunately, regulations like the General Data Protection Regulation (GDPR), the Health Insurance Portability and Accountability Act (HIPAA), and the FDA AI/ML guidance have raised important ways of addressing things like fairness, explainability, legal compliance, etc.; however, the landscape is far from risk-free. AI liability is another one of the gray areas approaching black, worrying about who is liable for an AI medical error — the developers, the physicians, or the institutions. The study reviews ethical risks and potential opportunities, as well as regulatory frameworks and emerging challenges in AI-driven healthcare. It proposes solutions to reduce bias, improve transparency, and enhance legal accountability. This research addresses these challenges to support the safe, fair, and effective deployment of ML-based systems in clinical practice, guaranteeing that patients can trust, regulators can approve, and healthcare can use them. © 2025 Elsevier B.V., All rights reserved.","Mohammed, S.; Malhotra, N.",2025,10.1016/j.tbench.2025.100215,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007763586&doi=10.1016%2Fj.tbench.2025.100215&partnerID=40&md5=e0577e0306751fa9b17e4ec0fb7e7501,scopus,"This review examines the ethical and regulatory challenges associated with implementing machine learning (ML) in healthcare. It discusses barriers such as data privacy, algorithmic bias, lack of transparency, and legal liability, while also highlighting existing regulations (GDPR, HIPAA, FDA guidance) and proposing solutions for bias reduction, improved transparency, and enhanced accountability to ensure safe and effective deployment of ML systems in clinical practice.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:45:40.439757
12936628b2b341f7,European spreads at the interest rate lower bound,"This paper analyzes the effect of the interest rate lower bound on long-term sovereign bond spreads in the euro area. We specify a joint shadow rate term structure model for the risk-free, the German, and the Italian sovereign yield curves. In our model, the behavior of long-term spreads becomes strongly nonlinear in the underlying factors when interest rates are close to the lower bound, which occurs in the data since the beginning of 2012. We fit the model via Quasi-Maximum Likelihood and show three consequences of the nonlinear behavior of sovereign spreads: (i) they are asymmetrically distributed, (ii) they are affected by (possibly exogenous) changes in the lower bound, and (iii) they become less informative about sovereign risk than when interest rates are far from the lower bound. Shadow spreads, however, still provide reliable information. (C) 2020 The Author(s). Published by Elsevier B.V.","Coroneo, Laura; Pastorello, Sergio",2020,10.1016/j.jedc.2020.103979,,wos,"This paper examines how the interest rate lower bound impacts long-term sovereign bond spreads in the euro area using a joint shadow rate term structure model for risk-free, German, and Italian sovereign yield curves. The model reveals nonlinear behavior of spreads near the lower bound, leading to asymmetric distributions, sensitivity to lower bound changes, and reduced informativeness about sovereign risk. Shadow spreads, however, remain reliable.",True,True,False,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T15:46:21.124245
5a9d202cac09c44d,Evaluating credit rating prediction by using the KMV model and random forest,"PurposeAn increasing number of investors have begun using financial data to develop optimal investment portfolios; therefore, the public financial data shared in the capital market plays a critical role in credit ratings. These data enable investors to understand the credit levels of debtors from a bank perspective; this facilitates predicting the debtor default rate to efficiently evaluate investment risks. The paper aims to discuss these issues.Design/methodology/approachA credit rating model can be developed to reduce the risk of adverse selection and moral hazard caused by information asymmetry in the loan market. In this study, a random forest (RF) was used to evaluate financial variables and construct credit rating prediction models. Data-mining techniques, including an RF, decision tree, neural networks, and support vector machine, were used to search for suitable credit rating forecasting methods. The distance to default from the KMV model was then incorporated into the credit rating model as a research variable to increase predictive power of various data-mining techniques. In addition, four-level and nine-level classification were set to investigate the accuracy rates of various models.FindingsThe experimental results indicated that applying the RF in the variable feature selection process and developing a forecasting model was the most effective method of predicting credit ratings; the four-level and nine-level feature-selection settings achieved 95.5 and 87.8 percent accuracy rates, respectively, indicating that RF demonstrated outstanding feature selection and forecasting capacity.Research limitations/implicationsThe experimental cases were based on financial data from public companies in North America.Practical implicationsPractical implication of this study indicates the most effective financial variables were dividends common/ordinary, cash dividends, volatility assumption, and risk-free rate assumption.Originality/valueThe RF model can be used to perform feature selection and efficiently filter numerous financial variables to obtain crediting rating information instantly.",,2016,10.1108/k-12-2014-0285,,proquest,"This study evaluates credit rating prediction using the KMV model and random forest (RF). The RF model was found to be the most effective for variable selection and forecasting, achieving high accuracy rates in four-level and nine-level classifications. Key financial variables for prediction were identified, and the RF model's ability to efficiently filter variables for instant credit rating information is highlighted.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:46:38.944292
a8cf076865ff9282,Evaluating the combined forecasts of the dynamic factor model and the artificial neural network model using linear and nonlinear combining methods,"The paper evaluates the advantages of combined forecasts from the dynamic factor model (DFM) and the artificial neural networks (ANN). The analysis was based on three financial variables namely the Johannesburg Stock Exchange Return Index, Government Bond Return Index and the Rand/Dollar Exchange Rate in South Africa. The forecasts were based on the out-of-sample period from January 2006 to December 2011. Compared to benchmark autoregressive (AR) models, both the DFM and ANN offer more accurate forecasts with reduced root-mean-square error (RMSE) of around 2–12 % for all variables and over all forecasting horizons. The ANN as a nonlinear combining method outperforms all linear combining methods for all variables and over all forecasting horizons. The results suggest that the ANN combining method can be used as an alternative to linear combining methods to achieve greater forecasting accuracy. The ANN combining method produces out-of-sample forecasts that are substantially more accurate with a sizeable reduction in RMSE of both the AR benchmark model and the best individual forecasting model. We attribute the superiority of the ANN combining method to its ability to capture any existing nonlinear relationship between the individual forecasts and the actual forecasting values. © 2017 Elsevier B.V., All rights reserved.","Babikir, A.; Mwambi, H.",2016,10.1007/s00181-015-1049-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954512445&doi=10.1007%2Fs00181-015-1049-1&partnerID=40&md5=11ab9cf5dd4d813d51fc5ba1afb4d024,scopus,"This study compares the forecasting accuracy of a dynamic factor model (DFM) and an artificial neural network (ANN) model using financial data from South Africa. Both models outperformed benchmark autoregressive models. The ANN, as a nonlinear combining method, demonstrated superior forecasting accuracy compared to linear methods, attributed to its ability to capture nonlinear relationships.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T15:46:54.378056
821367289e61d536,Event-based approach for probabilistic agricultural drought risk assessment under rainfed conditions,"An event-based approach for the probabilistic risk assessment of agricultural drought under rainfed conditions to estimate the economic impact is proposed. The risk parameters are evaluated in an event-based probabilistic framework for a set of hazard events; these results are probabilistically integrated including, in a formal way, all uncertainties related to every part of the process. The hazard is defined as a stochastic or historic set of events, collectively exhaustive and mutually exclusive, that describes the spatial distribution, the annual frequency, and the randomness of the hazard intensity. The risk is expressed in different economic terms: the average annual loss (or pure risk premium) and the loss exceedance curve; these metrics are of particular importance for risk retention (financing) schemes or risk transfer instruments. As an illustrative example, this approach is applied to probabilistic drought risk assessment of maize under rainfed conditions in Mexico. These results are the base of further studies in defining strategies for financial protection against agricultural losses and disasters.",,2015,10.1007/s11069-014-1550-4,,proquest,"This paper proposes an event-based approach for assessing agricultural drought risk under rainfed conditions, focusing on estimating economic impacts. It integrates hazard events, their frequencies, intensities, and associated uncertainties within a probabilistic framework. The risk is quantified using metrics like average annual loss and loss exceedance curves, which are relevant for risk financing and transfer. The approach is demonstrated with a case study on maize drought risk in Mexico, aiming to inform financial protection strategies against agricultural losses.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:46:56.563708
c11bf27ee96c2868,Evolving fuzzy modelling for yield curve forecasting,"Forecasting the term structure of interest rates plays a crucial role in portfolio management, household finance decisions, business investment planning, and policy formulation. This paper aims to address yield curve forecasting and evolving fuzzy systems modelling using data from US and Brazilian fixed income markets. Evolving fuzzy models provide a high level of system adaptation and learn the system dynamic continuously, which is essential for uncertain environments as interest rate markets. Computational experiments show that the evolving fuzzy modelling approaches describe the interest rate behaviour accurately, outperforming traditional econometric techniques in terms of error measures and statistical tests. Moreover, evolving models provide promising results for short and long-term maturities and for both fixed income markets evaluated, highlighting its potential to forecast complex nonlinear dynamics in uncertain environments. © 2020 Elsevier B.V., All rights reserved.","Maciel, L.; Ballini, R.; Gomide, F.",2018,10.1504/ijebr.2018.091047,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045552146&doi=10.1504%2FIJEBR.2018.091047&partnerID=40&md5=7727d894acc5a836f5878e176fffd734,scopus,"This paper explores the use of evolving fuzzy models for yield curve forecasting, applying them to US and Brazilian fixed income markets. The study demonstrates that these adaptive models accurately capture interest rate behavior and outperform traditional econometric methods, showing promise for both short and long-term forecasting in uncertain financial environments.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T15:47:02.207017
8aebf3ff75b87c23,Exchange rate predictability in emerging markets,"This paper uses financial and macroeconomic variables to predict currency returns, by using a two-step procedure. The first step consists of a cointegration equation that explains the exchange rate level as a function of global and domestic financial factors. The second step estimates an error-correction equation, for modeling the expected returns. This approach is a factor model analysis, where a Lasso derived technique is used for variable selection. This paper will focus on the five most frequently traded Latin American currencies, Brazilian Real (BRL), Chilean Peso (CLP), Colombian Peso (COL), Mexican Peso (MXN) and Peruvian Sol (PEN), during the time horizon from December 2001 until February 2016. The first finding is that the Global Exchange Rate Factor offers information about the exchange rate movements. In addition, this paper shows that commodity, equity prices and domestic risk premium are important variables for explaining exchange rates. Moreover, it confirms the existing results for the carry and slope variables. © 2019 Elsevier B.V., All rights reserved.","Baku, E.",2019,10.1016/j.inteco.2018.06.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049573296&doi=10.1016%2Fj.inteco.2018.06.003&partnerID=40&md5=fc8de85fd1f55752a97088fd14aeb2fd,scopus,"This paper investigates the predictability of exchange rates in five Latin American currencies (BRL, CLP, COL, MXN, PEN) from December 2001 to February 2016. It employs a two-step procedure involving cointegration and error-correction equations, utilizing factor model analysis with Lasso for variable selection. Key findings indicate that a Global Exchange Rate Factor, along with commodity prices, equity prices, domestic risk premium, carry, and slope variables, are significant in explaining exchange rate movements.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:47:08.144358
e2d7c330b34a3f2e,Experiments on the application of IOHMMs to model financial returns series,"Input-output hidden Markov models (IOHMMs) are conditional hidden Markov models in which the emission (and possibly the transition) probabilities can be conditioned on an input sequence. For example, these conditional distributions can be linear, logistic, or nonlinear (using for example multilayer neural networks), We compare the generalization performance of several models which are special cases of input-output hidden Markov models on financial time-series prediction tasks: an unconditional Gaussian, a conditional linear Gaussian, a mixture of Gaussians, a mixture of conditional linear Gaussians, a hidden Markov model, and various IOHMMs. The experiments compare these models on predicting the conditional density of returns of market and sector indices. Note that the unconditional Gaussian estimates the first moment with the historical average. The results show that, although for the first moment the historical average gives the best results, for the higher moments, the IOHMMs yielded significantly better performance, as estimated by the out-of-sample likelihood.","Bengio, Y; Lauzon, VP; Ducharme, R",2001,10.1109/72.896800,,wos,"This study compares the performance of various input-output hidden Markov models (IOHMMs) and their special cases for predicting financial time-series returns. While simple models like the historical average performed best for the first moment, IOHMMs significantly outperformed them for higher moments, as measured by out-of-sample likelihood.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:47:12.000888
c053694d929092d7,Exploiting Visual Features in Financial Time Series Prediction,"The possibility to enhance prediction accuracy for foreign exchange rates was investigated in two ways: first applying an outside the box approach to modeling price graphs by exploiting their visual properties, and secondly employing the most efficient methods to detect patterns to classify the direction of movement. The approach that exploits the visual properties of price graphs which make use of density regions along with high and low values describing the shape; hence, the authors propose the name 'Finance Vision.' The data used in the predictive model consists of 1-hour past price values of 4 different currency pairs, between 2003 and 2016. Prediction performances of state-of-the-art methods; Extreme Gradient Boosting, Artificial Neural Network and Support Vector Machines are compared over the same data with the same sets of features. Results show that density based visual features contribute considerably to prediction performance.","Karacor, Adil Gursel; Erkan, Turan Erman",2020,10.4018/ijcini.2020040104,,wos,"This study investigates enhancing foreign exchange rate prediction accuracy by leveraging visual properties of price graphs ('Finance Vision') and pattern detection for movement direction classification. It compares Extreme Gradient Boosting, Artificial Neural Network, and Support Vector Machines using 1-hour past price values of 4 currency pairs (2003-2016). Results indicate that density-based visual features significantly improve prediction performance.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:47:28.923717
aa74a4500fdd27b8,Extrapolating Long-Run Yield Curves: An Innovative and Consistent Approach,"This article proposes a method to build term structures that are consistent with market data and that provide interest rates for which the volatility, on average, decreases as maturities increase. The method is designed for continuous repetitive use and is consistent with work by Diebold and Li, providing reasonable extrapolated rates, with an appropriate level of volatility over time. The Svensson model is adopted, and its parameters are estimated by the combination of a genetic algorithm and a quasi-Newton nonlinear optimization method. We innovate with a new objective function that focuses on both parts of the estimated curves (interpolated and extrapolated). For this purpose, a stability component is added. The new objective function aims to solve the problem of estimating long-term rates not observable in the market, for which the estimates are usually artificially stable or excessively volatile. The results show that the estimation method is able to bring the volatility of extrapolated rates to levels consistent with those observed for the longest liquid rate. Estimation errors are small enough and there is no statistical evidence that they are biased. The method is useful for the insurance market, since it provides interest rates that do not lead to artificially stable or excessively volatile technical provisions. © 2023 Elsevier B.V., All rights reserved.","Signorelli, T.P.; Campani, C.H.; Neves, C.D.R.",2023,10.1080/10920277.2022.2102040,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138326527&doi=10.1080%2F10920277.2022.2102040&partnerID=40&md5=a98fb2ad043814ee043fadaf75c64129,scopus,"This paper introduces a novel and consistent method for constructing long-run yield curves, aiming to provide interest rates with decreasing volatility as maturity increases. It adapts the Svensson model, employing a genetic algorithm and quasi-Newton optimization for parameter estimation. A new objective function is proposed to improve the estimation of unobservable long-term rates, addressing issues of artificial stability or excessive volatility. The method's effectiveness is demonstrated by its ability to produce extrapolated rate volatilities consistent with observed liquid rates, with small and unbiased estimation errors. This approach is particularly beneficial for the insurance market, ensuring stable technical provisions.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T15:47:40.131924
8edd8d9e068332ee,FORECASTING DEMAND FOR MONEY UNDER CHANGING TERM STRUCTURE OF INTEREST-RATES - APPLICATION OF RIDGE REGRESSION,,"WATSON, DE; WHITE, KJ",1976,10.2307/1057334,,wos,"This article applies Ridge Regression to forecast demand for money, considering changes in the term structure of interest rates.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:53:31.217183
1783e8e530cbe052,"Factor Models, Machine Learning, and Asset Pricing","We survey recent methodological contributions in asset pricing using factor models and machine learning. We organize these results based on their primary objectives: estimating expected returns, factors, risk exposures, risk premia, and the stochastic discount factor as well as model comparison and alpha testing. We also discuss a variety of asymptotic schemes for inference. Our survey is a guide for financial economists interested in harnessing modern tools with rigor, robustness, and power to make new asset pricing discoveries, and it highlights directions for future research and methodological advances.","Giglio, Stefano; Kelly, Bryan; Xiu, Dacheng",2022,10.1146/annurev-financial-101521-104735,,wos,"This article surveys recent methodological advancements in asset pricing that utilize factor models and machine learning. It categorizes these advancements by their main goals, including estimating expected returns, factors, risk exposures, risk premia, and the stochastic discount factor, as well as model comparison and alpha testing. The survey also covers various asymptotic schemes for inference and aims to guide financial economists in applying modern, rigorous, and powerful tools for asset pricing research, while also pointing towards future research directions.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:53:54.239867
76f6fcb6934e9db1,Financial conditions and nonlinearities in the European Central Bank (ECB) reaction function: In-sample and out-of-sample assessment,"Our purpose is to investigate how the European Central Bank (ECB) sets interest rates in the context of both linear and nonlinear policy reaction functions. This work contributes to the current debate on central banks having additional objectives over and above control of inflation and output. Three findings emerge. First, the ECB takes financial conditions into account when setting interest rates. Second, amongst Taylor rule models, linear and nonlinear models are empirically indistinguishable within sample, and model specifications with real-time data provide the best description of in-sample ECB interest rate setting behaviour. Third, the 2007-2009 financial crisis witnessed a shift from inflation targeting to output stabilization, and a shift from an asymmetric policy response to financial conditions at high inflation rates to a more symmetric response regardless of the state of inflation. Finally, guidance is provided as regards models for forecasting interest rates in the Eurozone area. Without imposing an a priori choice of the parametric functional form, semiparametric models and autoregressive processes forecast the out-of-sample ECB interest rate setting behaviour better than linear and nonlinear Taylor rule models. (C) 2011 Elsevier B.V. All rights reserved.","Milas, Costas; Naraidoo, Ruthira",2012,10.1016/j.csda.2011.06.032,,wos,"This study examines the European Central Bank's (ECB) interest rate setting behavior using both linear and nonlinear policy reaction functions. It finds that the ECB considers financial conditions, that linear and nonlinear models are empirically similar within sample, and that model specifications with real-time data best describe in-sample behavior. The 2007-2009 financial crisis marked a shift from inflation targeting to output stabilization and a change in the response to financial conditions. For out-of-sample forecasting, semiparametric models and autoregressive processes outperform linear and nonlinear Taylor rule models.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:54:00.449978
ce476a18889360d0,"Financial conditions, macroeconomic factors and disaggregated bond excess returns","Bond excess returns can be predicted by macro factors, however, large parts remain still unexplained. We apply a novel term structure model to decompose bond excess returns into expected excess returns (risk premia) and the innovation part. In order to explore these risk premia and innovations, we complement macro variables by financial condition variables as possible determinants of bond excess returns. We find that the expected part of bond excess returns is driven by macro factors, whereas innovations seem to be mainly influenced by financial conditions, before and after the financial crisis. Thus, financial conditions, such as financial stress, deserve attention when analyzing bond excess returns. © 2015 Elsevier B.V., All rights reserved.","Fricke, C.; Menkhoff, L.",2015,10.1016/j.jbankfin.2015.03.015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930672701&doi=10.1016%2Fj.jbankfin.2015.03.015&partnerID=40&md5=cca2912022d65911e1c5b4829a830f9e,scopus,"This study decomposes bond excess returns into expected excess returns (risk premia) and innovations using a novel term structure model. It investigates the influence of macroeconomic and financial condition variables on these components, finding that macro factors drive expected returns while financial conditions influence innovations, both before and after the financial crisis. The authors suggest that financial conditions, like financial stress, are important for analyzing bond excess returns.",False,True,True,gemini-2.5-flash-lite,Olav,M,,2025-10-13T15:54:16.166592
0726da9e365f79a8,"Financing anomaly, mispricing and cross-sectional return predictability","This study investigates the persistence of financing anomaly in the Chinese stock market. The results show that the long-short portfolio earns an average monthly excess return of 0.88% for the following 12 months based on this anomaly. We provide a behavioral mispricing-based explanation for this anomaly that financing anomaly is stronger in stocks or period with high investor overconfidence. Moreover, we find that financing anomaly is stronger in stocks with more investor attention, indicating that excessive investor attention increases mispricing and reduces market efficiency. However, idiosyncratic volatility cannot capture the arbitrage costs faced by financing anomaly, and investors' lottery-like preference cannot explain financing anomaly. Our findings are robust after controlling for other firm characteristic variables, considering shell value contamination and different sample periods. © 2022 Elsevier B.V., All rights reserved.","Yang, B.; Ye, T.; Ma, Y.",2022,10.1016/j.iref.2022.02.062,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125476546&doi=10.1016%2Fj.iref.2022.02.062&partnerID=40&md5=ce0991486424799573e1de259da53f4a,scopus,"This study examines the financing anomaly in the Chinese stock market, finding it persists and generates excess returns. The anomaly is linked to behavioral mispricing, particularly investor overconfidence and attention, which increase mispricing and reduce market efficiency. The study rules out idiosyncratic volatility and lottery-like preferences as explanations for arbitrage costs and investor behavior, respectively. Findings remain robust across various controls and sample periods.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:54:21.568823
30116f36dc2b4789,Fiscal policy and asset markets: A semiparametric analysis,"Using a flexible semiparametric varying coefficient model specification, this paper examines the role of fiscal policy on the US asset markets (stocks, Corporate and treasury bonds). We consider two possible roles of fiscal deficits (or surpluses): as a separate direct information variable and as a (indirect) conditioning information variable indicating binding constraints oil monetary policy actions. The results show that the impact of monetary policy on the stock market varies, depending oil fiscal expansion or contraction. The impact of fiscal policy on corporate and treasury bond yields follow similar patterns as in the equity market. The results are consistent with the notion of strong interdependence between monetary and fiscal policies. (C) 2008 Elsevier B.V. All rights reserved.","Jansen, Dennis W.; Li, Qi; Wang, Zijun; Yang, Jian",2008,10.1016/j.jeconom.2008.09.007,,wos,"This paper uses a semiparametric varying coefficient model to analyze the impact of fiscal policy on US asset markets (stocks, corporate bonds, and treasury bonds). It investigates fiscal deficits as both a direct information variable and an indicator of monetary policy constraints. The findings suggest that fiscal policy influences the effect of monetary policy on the stock market and that fiscal policy impacts corporate and treasury bond yields similarly to equity markets, indicating a strong interdependence between monetary and fiscal policies.",True,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:54:31.285793
436fe1047a3403b7,Fluctuations in economic and activity and stabilization policies in the CIS,"In this study, a highly flexible form of nonlinear time series models called artificial neural networks (ANNs) are employed to predict fluctuations in economic activity in selected members (Armenia, Azerbaijan, Georgia, Kazakhstan, and Kyrgyzstan) of the Commonwealth of Independent States (CIS) using macroeconomic time series [treasury bill rate (T-bill), long term bond rate (BondLT), money supply (MS), industrial production (IP), spread (10-year treasury bond rate less 3-month treasury bill rate), BRTB (bank rate less 3-month treasury bill rate), and GDP growth rate]. Forecasting recessions being very important though challenging, recessions in the selected countries are modeled recursively 1-10 quarters ahead out-of-sample using ANNs in conjunction with macroeconomic time series for all the countries. The out-of-sample forecast results show that in general no single macroeconomic variable employed appears to be useful for predicting recessions in any of the series. However, for Armenia, the treasury bill rate, industrial production, money supply, and the spread (the yield curve) are candidate variables for predicting recessions 1-10 quarters ahead. For Georgia, Kazakhstan, and Kyrgyzstan, the treasury bill rate and money supply series are candidate variables for predicting recessions 1-10 quarters ahead. Reprinted by permission of Springer",,2011,10.1007/s10614-010-9233-z,,proquest,"This study uses artificial neural networks (ANNs) to forecast economic activity and recessions in Armenia, Azerbaijan, Georgia, Kazakhstan, and Kyrgyzstan. Macroeconomic time series such as T-bill rates, bond rates, money supply, industrial production, and GDP growth were used. The results suggest that while no single variable is universally predictive, specific variables like the treasury bill rate and money supply show potential for forecasting recessions in certain countries.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:54:39.956319
cff0be28d8a28bbf,Fluoride-treated water and the problem of merit goods,"This paper inquires into the fluoride treatment of community water in the United States to determine why and how conflicts in the production, consumption, and distribution of merit goods arise and are resolved. Primary and secondary data were employed to analyze statewide and municipality-level fluoridation initiatives in one key “battleground” state. We find that obstacles to successful fluoridation include a unidimensional policy space, high risk premia assigned by the affected population to health and environmental hazards, concerns over government interference with personal health choices, perceived adequacy of fluoride sources, “customer bundling,” and lack of a critical middle ground for consensus-building. The accessibility and social desirability of merit goods, like fluoridated water, cannot therefore be considered as value-free choices. How consumer demand is expressed, how fluoridation costs and benefits are estimated, how conflicts over its provision and production are resolved, and how the merits of science-based policies can be equally recast in terms of their presumed demerits require serious attention on the part of decision-makers in formulating and implementing health promotion policies.",,2011,10.2166/wp.2010.127,,proquest,"This paper examines the challenges and conflicts associated with community water fluoridation in the United States, analyzing it as a case study for understanding merit goods. It identifies obstacles such as limited policy options, public perception of risks, concerns about government intervention, and difficulties in consensus-building. The study suggests that the provision of merit goods like fluoridated water is not a neutral choice and requires careful consideration of consumer demand, cost-benefit analysis, and the framing of science-based policies.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:54:45.885126
bd71e4915db6a904,Forecasting Government Bond Yields with Neural Networks Considering Cointegration,"This paper discusses techniques that might be helpful in predicting interest rates and tries to evaluate a new hybrid forecasting approach. Results of examining government bond yields in Germany and France reported in this study indicate that a hybrid forecasting approach which combines techniques of cointegration analysis with neural network (NN) forecasting models can produce superior results to the use of NN forecasting models alone. The findings documented in this paper could be a consequence of the fact that examining differenced data under certain conditions will lead to a loss of information and that the inclusion of the error correction term from the cointegration model can help to cope with this problem. The paper also discusses some possibly interesting directions for further research. Copyright © 2015 John Wiley & Sons, Ltd.",,2016,10.1002/for.2385,,proquest,"This paper evaluates a hybrid forecasting approach for government bond yields by combining cointegration analysis with neural networks (NN). The study, focusing on German and French bond yields, suggests this hybrid method outperforms NN models alone, potentially by retaining information lost in differenced data through the inclusion of an error correction term.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T15:54:55.610610
2dbf7991c4f5dbaf,Forecasting Key Macroeconomic Variables of the South African Economy using Bayesian Variable Selection,"This study analyzed the forecasting performances of various multivariate models in predicting 1-8-quarters-ahead of the growth rate of GDP, the consumer price index inflation rate and the three months Treasury bill rate for South Africa over an out-of-sample period of 2000:Q1-2011:Q2, using an in-sample period of 1960:Ql-1999:Q4. The study compared the forecasting performances of the classical and the Minnesota-type Bayesian vector autoregressive (VAR) models with those of linear (fixed-parameter) and nonlinear (time-varying parameter) VARs involving a stochastic search algorithm for variable selection, estimated using Markov Chain Monte Carlo methods. In general, the study finds that variable selection, whether imposed on a time-varying VAR or a fixed parameter VAR, and non-linearity in VARs, play an important part in improving predictions when compared to the linear fixed coefficients classical VAR. However, the results does not indicate marked gains in forecasting power across the different Bayesian models, as well as, over the classical VAR model, possibly because the problem of over parameterization in the classical VAR is not that acute in our three-variable system. Hence, future research would aim to look at VAR models that include over 10 variables.",,2012,10.3923/jas.2012.645.652,,proquest,"This study evaluates the forecasting accuracy of various multivariate models, including Bayesian and classical Vector Autoregressive (VAR) models with time-varying parameters and variable selection, for key South African macroeconomic variables (GDP growth, inflation, T-bill rate). It found that variable selection and non-linearity improve predictions compared to classical VAR, but marked gains across Bayesian models were not observed, possibly due to the small system size. Future research is suggested for larger systems.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:55:43.102834
56a900a32f4d205c,Forecasting NFT coin prices using machine learning: Insights into feature significance and portfolio strategies,"With the rise in popularity of Non-Fungible Tokens (NFTs), the demand for NFT coins has also surged. NFT coins are cryptocurrencies that facilitate NFT ecosystems by supporting NFT trading and platform governance. Accurate price predictions of NFT coins are crucial for risk managing volatility and constructing optimal portfolios. This study employs machine learning techniques to predict the daily price direction of four key NFT coins, namely ENJ, MANA, THETA, and XTZ. The machine learning methods employed include three decision tree-based methods (random forests, extremely randomized trees, XGBoost), support vector machine, Lasso and Naïve Bayes. The findings show that random forests, extremely randomized trees, XGBoost, and support vector machine models have accuracy ranging between 80% and 90% for predictions in the 14 to 21 day range. This adds to the literature showing that machine learning methods have high prediction accuracy for cryptocurrency prices. Conversely, Lasso or Naïve Bayes models yield considerably lower prediction accuracy. Feature importance is assessed using Shapley values. The Shapley value feature importance calculated from random forests highlights that, for 14 and 21-day forecasts, four variables - five-year expected inflation, ten-year bond yields, the interest rate spread, and on balance volume - are consistently highly ranked across all NFT coins. Additionally, the MA50, MA200, and WAD also emerge as important features. These results highlight the importance of including macroeconomic variables which capture business cycle conditions and technical analysis indicators that capture investor psychology as features. NFT coin portfolios constructed using trading signals generated from Extra Trees outperforms a buy and hold portfolio. Extra Trees are easy and fast to implement and investors not making use of this information are likely making sub-optimal investment decisions. © 2023 Elsevier B.V., All rights reserved.","Henriques, I.; Sadorsky, P.",2023,10.1016/j.gfj.2023.100904,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174468022&doi=10.1016%2Fj.gfj.2023.100904&partnerID=40&md5=4aa2aaf1db2dce93e2690576c3902f88,scopus,"This study uses machine learning (ML) models like random forests, extremely randomized trees, XGBoost, and support vector machines to predict the daily price direction of four NFT coins (ENJ, MANA, THETA, XTZ) with 80-90% accuracy for 14-21 day forecasts. Macroeconomic variables (inflation, bond yields, interest rate spread) and technical indicators (on balance volume, moving averages, WAD) are identified as significant features. ML-based trading signals from Extra Trees outperform a buy-and-hold strategy, suggesting potential for improved investment decisions.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:55:47.672609
5afe5ce80e830918,Forecasting Stock Market Crashes via Machine Learning,"This paper uses a comprehensive set of predictor variables from the five largest Eurozone countries to compare the performance of simple univariate and machine learning-based multivariate models in forecasting stock market crashes. In terms of statistical predictive performance, a support vector machine-based crash prediction model outperforms a random classifier and is superior to the average univariate benchmark as well as a multivariate logistic regression model. Incorporating nonlinear and interactive effects is both imperative and foundation for the outperformance of support vector machines. Their ability to forecast stock market crashes outof-sample translates into substantial value-added to active investors. From a policy perspective, the use of machine learning-based crash prediction models can help activate macroprudential tools in time.","Dichtl, Hubert; Drobetz, Wolfgang; Otto, Tizian",2023,10.1016/j.jfs.2022.101099,,wos,"This paper compares univariate and machine learning models (specifically Support Vector Machines) for forecasting stock market crashes in the five largest Eurozone countries. The SVM model demonstrated superior predictive performance compared to simpler models and a random classifier, highlighting the importance of capturing nonlinear and interactive effects. The findings suggest significant value for investors and potential policy applications for macroprudential tools.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:55:51.645036
d93a58f741b11b3d,Forecasting WTI crude oil futures returns: Does the term structure help?,"Nelson-Siegel (NS) factors extracted from the term structure of WTI oil futures are shown to predict subsequent WTI holding period returns in-sample. This in-sample predictability is not diminished by augmenting with macroeconomic indicators or oil market specific predictors. Allowing the decay factor in the Nelson-Siegel model to vary over time improves in-sample predictions at medium horizon return forecasts. We conduct out-of-sample forecasting exercises on models that use NS factors, such as a simple two factor model that uses a composite leading indicator along with the NS decay factor, and a LASSO model that combines NS factors with macroeconomic indicators and oil market specific predictors. These models significantly reduce forecast errors relative to a no change benchmark across a range of return horizons and futures contract maturities. We also find consistent evidence that models that use the NS factors result in trading strategies with higher Sharpe ratios and better skewness properties than buy and hold strategies and historical mean strategies.",,2021,10.1016/j.eneco.2021.105350,,proquest,"This study investigates whether the term structure of WTI crude oil futures can aid in forecasting returns. The Nelson-Siegel (NS) factors extracted from the term structure demonstrate in-sample predictability for subsequent returns, even when augmented with macroeconomic and oil-specific predictors. Varying the decay factor in the NS model further enhances medium-horizon forecasts. Out-of-sample tests using models incorporating NS factors, including a composite leading indicator and a LASSO model, significantly reduce forecast errors compared to a no-change benchmark. Furthermore, trading strategies based on NS factors yield higher Sharpe ratios and improved skewness compared to benchmark strategies.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:55:55.191719
6e5161b9d8618c33,Forecasting cryptocurrencies returns: Do macroeconomic and financial variables improve tail expectation predictions?,"This study aims to jointly predict conditional quantiles and tail expectations for the returns of the most popular cryptocurrencies (Bitcoin, Ethereum, Ripple, Dogecoin and Litecoin) using financial and macroeconomic indicators as explanatory variables. We adopt a Monotone Composite Quantile Regression Neural Network (MCQRNN) model to make one- and five-steps-ahead predictions of Value-at-Risk (VaR) and Expected Shortfall (ES) based on a rolling window and compare the performance of our model against the Historical simulation and the standard ARMA(1,1)-GARCH(1,1) model used as benchmarks. The superior set of models is then chosen by backtesting VaR and ES using a Model Confidence Set procedure. Our results show that the MCQRNN performs better than both benchmark models for jointly predicting VaR and ES when considering daily data. Models with the implied volatility index, treasury yield spread and inflation expectations sharpen the extreme return predictions. The results are consistent for the two risk measures at the 1% and 5% level both, in the case of a long and short position and for all cryptocurrencies.",,2024,10.1007/s11135-023-01761-1,,proquest,"This study uses a Monotone Composite Quantile Regression Neural Network (MCQRNN) to forecast Value-at-Risk (VaR) and Expected Shortfall (ES) for cryptocurrency returns (Bitcoin, Ethereum, Ripple, Dogecoin, and Litecoin). It incorporates financial and macroeconomic indicators and compares the MCQRNN model against historical simulation and ARMA-GARCH benchmarks. The findings indicate that the MCQRNN model outperforms benchmarks for predicting VaR and ES, with implied volatility index, treasury yield spread, and inflation expectations improving tail return predictions.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:55:59.552477
43890536593eb51e,Forecasting day-ahead expected shortfall on the EUR/USD exchange rate: The (I)relevance of implied volatility,"The existing literature provides mixed results on the usefulness of implied volatility for managing risky assets, while evidence for expected shortfall predictions is almost nonexistent. Given its forward-looking nature, implied volatility might be more valuable than backward-looking measures of realized price fluctuations. Conversely, the volatility risk premium embedded in implied volatility leads to overestimating the observed price variation. This paper explores the benefits of augmenting econometric models used in forecasting the expected shortfall, a risk measured endorsed in the Basel III Accord, with information on implied volatility obtained from EUR/USD option contracts. The day-ahead forecasts are obtained from several classes of econometric models: historical simulation, EGARCH, quantile regression-based HAR, joint VaR and ES model, and combination forecasts. We verify whether the resulting expected shortfall forecasts are well-specified and test the models' accuracy. Our results provide evidence that the information provided by forward-looking implied volatility is more valuable than that in backward-looking realized measures. These results hold across multiple model specifications, are stable over time, hold under alternative loss functions, and are more pronounced during periods of higher market uncertainty when risk modeling matters most. (c) 2023 International Institute of Forecasters. Published by Elsevier B.V. All rights reserved.","Lyocsa, Stefan; Plihal, Tomas; Vyrost, Tomas",2024,10.1016/j.ijforecast.2023.11.003,,wos,"This paper investigates the usefulness of implied volatility for forecasting day-ahead expected shortfall on the EUR/USD exchange rate. It compares various econometric models, including historical simulation, EGARCH, quantile regression-based HAR, and a joint VaR and ES model, augmented with implied volatility information. The findings suggest that implied volatility, despite its potential drawbacks, provides valuable forward-looking information that improves expected shortfall forecasts, especially during periods of high market uncertainty.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:56:02.788808
a8145ad99ef09055,Forecasting foreign exchange rates with an intrinsically nonlinear dynamic speed of adjustment model,"Forecasting foreign exchange rates is an important but difficult process; therefore, it is important to use a superior forecasting model. The paper takes up this criterion and proposes to describe and forecast foreign exchange rates by developing an intrinsically nonlinear model with variable and dynamic speeds of adjustment. It is found that the speed of adjusting the random (or expected) to the equilibrium rate is very slow, implying that fiscal policy (statistically insignificat) and monetary policy (statistically significant) may be ineffective to induce changes in the adjustment speed. We also find that the nonlinear dynamic model improves forecasting performance, implying that nonlinearities in the sense of functional forms are exploitable for improved point forecasting of foreign exchange rates.","Lin, WT; Chen, YH",1998,10.1080/000368498325822,,wos,"This paper proposes a nonlinear dynamic model with variable adjustment speeds to forecast foreign exchange rates. The model suggests that the adjustment to equilibrium is slow, and fiscal policy is insignificant while monetary policy is significant in influencing adjustment speed. The nonlinear model demonstrates improved forecasting performance, indicating that exploitable nonlinearities exist for better point forecasting.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:56:05.694459
4f0fd761748ec99f,Forecasting interest rates: a comparative assessment of some second-generation nonlinear models,"Modeling and forecasting of interest rates has traditionally proceeded in the framework of linear stationary methods such as ARMA and VAR, but only with moderate success. We examine here three methods, which account for several specific features of the real world asset prices such as nonstationarity and nonlinearity. Our three candidate methods are based, respectively, on a combined wavelet artificial neural network (WANN) analysis, a mixed spectrum (MS) analysis and nonlinear ARMA models with Fourier coefficients (FNLARMA). These models are applied to weekly data on interest rates in India and their forecasting performance is evaluated vis--vis three GARCH models [GARCH (1,1), GARCH-M (1,1) and EGARCH (1,1)] as well as the random walk model. Both the WANN and MS methods show marked improvement over other benchmark models, and may thus hold out several potentials for real world modeling and forecasting of financial data.","Nachane, Dilip; Clavel, Jose G.",2008,10.1080/02664760701835243,,wos,"This study compares the forecasting performance of three nonlinear models (wavelet artificial neural network, mixed spectrum, and nonlinear ARMA with Fourier coefficients) against GARCH models and a random walk model for Indian interest rates. The wavelet artificial neural network and mixed spectrum methods demonstrated significant improvements over the benchmarks.",True,False,True,gemini-2.5-flash-lite,Olav,M,,2025-10-13T15:56:26.265731
264f50bc1957303c,Forecasting stock market returns with a lottery index: Evidence from China,"This study constructs a Chinese lottery index (LI) based on six popular lottery preference variables by using the partial least squares method and examines the relationship between the LI and future stock market returns during the period from January 2000 to December 2021. We find that the LI can negatively predict stock market excess returns in‐sample and out‐of‐sample. In addition, the LI can generate a large economic gain for a mean–variance investor. Finally, the predictive sources of the LI stem from a cash flow channel and can be explained by the positive volume–volatility relationship and investor attention.",,2024,10.1002/for.3100,,proquest,"This study develops a Chinese lottery index (LI) using lottery preference variables and the partial least squares method. The LI is found to negatively predict future stock market excess returns, offering economic gains for investors. The predictive power is attributed to cash flow, the volume-volatility relationship, and investor attention.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:56:31.514536
019c24267ee95568,Forecasting the term structure of crude oil futures prices with neural networks,"The paper contributes to the limited literature modelling the term structure of crude oil markets. We explain the term structure of crude oil prices using the dynamic Nelson-Siegel model and propose to forecast oil prices using a generalized regression framework based on neural networks. The newly proposed framework is empirically tested on 24. years of crude oil futures prices covering several important recessions and crisis periods. We find 1-month-, 3-month-, 6-month- and 12-month-ahead forecasts obtained from a focused time-delay neural network to be significantly more accurate than forecasts from other benchmark models. The proposed forecasting strategy produces the lowest errors across all times to maturity. © 2017 Elsevier B.V., All rights reserved.","Baruník, J.; Malinská, B.",2016,10.1016/j.apenergy.2015.11.051,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951017088&doi=10.1016%2Fj.apenergy.2015.11.051&partnerID=40&md5=48247872417761781a5c770a7d5dcee8,scopus,"This paper models and forecasts the term structure of crude oil futures prices using a dynamic Nelson-Siegel model and a neural network framework. The neural network approach, specifically a focused time-delay neural network, significantly outperforms benchmark models in forecasting 1- to 12-month-ahead prices, demonstrating its effectiveness across various economic conditions.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:56:35.164061
400f23285a075f7c,Forward contracts for the operation of an electricity industry under spot pricing,A study is reported of the use of forward contracts as risk instruments for electricity industries operating under spot pricing. Forward contracts involve financial transactions or commitments which relate to a physical trade at a later time instance. Price setting and appropriate participant responses are discussed. Simulation studies are used to demonstrate that forward contracts offer participants an opportunity to reduce their risk exposure without removing the incentive to respond to higher spot prices.<>,R. J. Kaye; H. R. Outhred; C. H. Bannister,1990,10.1109/59.49085,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=49085,ieeexplore,"This study investigates the use of forward contracts as risk management tools in electricity industries that operate under spot pricing. It discusses price setting, participant responses, and uses simulations to show that forward contracts can reduce risk exposure without diminishing the incentive to respond to high spot prices.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:56:38.711128
4300334f009b5fe1,"Fractional Black-Scholes option pricing, volatility calibration and implied Hurst exponents in South African context","Background: Contingent claims on underlying assets are typically priced under a framework that assumes, inter alia, that the log returns of the underlying asset are normally distributed. However, many researchers have shown that this assumption is violated in practice. Such violations include the statistical properties of heavy tails, volatility clustering, leptokurtosis and long memory. This paper considers the pricing of contingent claims when the underlying is assumed to display long memory, an issue that has heretofore not received much attention.Aim: We address several theoretical and practical issues in option pricing and implied volatility calibration in a fractional Black-Scholes market. We introduce a novel eight-parameter fractional Black-Scholes-inspired (FBSI) model for the implied volatility surface, and consider in depth the issue of calibration. One of the main benefits of such a model is that it allows one to decompose implied volatility into an independent long-memory component - captured by an implied Hurst exponent - and a conditional implied volatility component. Such a decomposition has useful applications in the areas of derivatives trading, risk management, delta hedging and dynamic asset allocation.Setting: The proposed FBSI volatility model is calibrated to South African equity index options data as well as South African Rand/American Dollar currency options data. However, given the focus on the theoretical development of the model, the results in this paper are applicable across all financial markets.Methods: The FBSI model essentially combines a deterministic function form of the 1-year implied volatility skew with a separate deterministic function for the implied Hurst exponent, thus allowing one to model both observed implied volatility surfaces as well as decompose them into independent volatility and long-memory components respectively. Calibration of the model makes use of a quasi-explicit weighted least-squares optimisation routine.Results: It is shown that a fractional Black-Scholes model always admits a non-constant implied volatility term structure when the Hurst exponent is not 0.5, and that 1-year implied volatility is independent of the Hurst exponent and equivalent to fractional volatility. Furthermore, we show that the FBSI model fits the equity index implied volatility data very well but that a more flexible Hurst exponent parameterisation is required to fit accurately the currency implied volatility data.Conclusion: The FBSI model is an arbitrage-free deterministic volatility model that can accurately model equity index implied volatility. It also provides one with an estimate of the implied Hurst exponent, which could be very useful in derivatives trading and delta hedging.",,2017,10.4102/sajems.v20i1.1532,,proquest,"This paper introduces a novel eight-parameter fractional Black-Scholes-inspired (FBSI) model for implied volatility surface, which decomposes implied volatility into a long-memory component (implied Hurst exponent) and a conditional implied volatility component. The model is calibrated to South African equity index and currency options data. The FBSI model accurately models equity index implied volatility and provides an estimate of the implied Hurst exponent, useful for derivatives trading and hedging. However, a more flexible parameterization is needed for currency options data.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:56:43.909294
129b8cefa68fea23,Functional Autoregression for Sparsely Sampled Data,"We develop a hierarchical Gaussian process model for forecasting and inference of functional time series data. Unlike existing methods, our approach is especially suited for sparsely or irregularly sampled curves and for curves sampled with nonnegligible measurement error. The latent process is dynamically modeled as a functional autoregression (FAR) with Gaussian process innovations. We propose a fully nonparametric dynamic functional factor model for the dynamic innovation process, with broader applicability and improved computational efficiency over standard Gaussian process models. We prove finite-sample forecasting and interpolation optimality properties of the proposed model, which remain valid with the Gaussian assumption relaxed. An efficient Gibbs sampling algorithm is developed for estimation, inference, and forecasting, with extensions for FAR(p) models with model averaging over the lag p. Extensive simulations demonstrate substantial improvements in forecasting performance and recovery of the autoregressive surface over competing methods, especially under sparse designs. We apply the proposed methods to forecast nominal and real yield curves using daily U.S. data. Real yields are observed more sparsely than nominal yields, yet the proposed methods are highly competitive in both settings. Supplementary materials, including R code and the yield curve data, are available online. © 2019 Elsevier B.V., All rights reserved.","Kowal, D.R.; Matteson, D.S.; Ruppert, D.",2019,10.1080/07350015.2017.1279058,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019636100&doi=10.1080%2F07350015.2017.1279058&partnerID=40&md5=8ef118146d36427daf34c568a470d5cd,scopus,"This paper introduces a hierarchical Gaussian process model for functional time series forecasting and inference, particularly effective for sparsely or irregularly sampled data with measurement error. It utilizes a functional autoregression (FAR) with Gaussian process innovations and proposes a nonparametric dynamic functional factor model for enhanced applicability and efficiency. The model demonstrates finite-sample forecasting and interpolation optimality and is estimated using an efficient Gibbs sampling algorithm. Simulations show significant improvements in forecasting and autoregressive surface recovery, especially with sparse data. The method is applied to forecast U.S. nominal and real yield curves.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T15:57:03.407629
07a6141c54a06ec2,Generalist CEOs and Credit Ratings*,"A recent trend is that firms prefer to hire generalist CEOs with transferable skills (across firms or industries) over hiring specialist CEOs, but the consequences of this trend are unclear. In this study, we examine whether credit rating agencies consider a CEO's general skills as a credit risk factor when assessing an entity's overall creditworthiness. We predict and find that generalist CEOs are associated with lower credit ratings, suggesting that the presence of generalist CEOs is a significant credit rating factor. We also find that generalist CEOs are likely to take on more risks, which leads to more volatile performance ex post, and our path analyses confirm default risk is a significant mediator between credit ratings and CEOs' general skills. Our results hold in the presence of additional controls (e.g., CEO characteristics and corporate governance), when applying different fixed‐effect models and different matching methods, and for a subsample with forced CEO turnover. We also find that the negative relationship is attenuated for R&D‐intensive firms and firms in competitive industries. Last, we provide evidence that firms with generalist CEOs face higher borrowing costs, such as bond yields and syndicated loan spreads. Overall, our results contribute to a growing literature on the costs and benefits of hiring generalist CEOs, by providing a full picture of why hiring a generalist CEO may benefit shareholders but also cause misalignments with bondholders' interests.",,2021,10.1111/1911-3846.12662,,proquest,"This study investigates whether credit rating agencies consider CEO generalist skills as a credit risk factor. The findings indicate that generalist CEOs are associated with lower credit ratings, potentially due to increased risk-taking and performance volatility. The research also suggests that firms with generalist CEOs face higher borrowing costs and that this relationship is moderated by firm characteristics like R&D intensity and industry competition. The study contributes to understanding the trade-offs of hiring generalist CEOs for shareholders versus bondholders.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:57:06.125144
abed275e49b577b6,Generalized Forecast Error Variance Decomposition for Linear and Nonlinear Multivariate Models,"We propose a new generalized forecast error variance decomposition with the attractive property that the proportions of the impact accounted for by innovations in each variable sum to unity. Our decomposition is based on the generalized impulse response function, and it can easily be obtained by simulation. The new decomposition is illustrated in an empirical application to US output growth and interest rate spread data.","Lanne, Markku; Nyberg, Henri",2016,10.1111/obes.12125,,wos,"This paper introduces a novel generalized forecast error variance decomposition for multivariate models, applicable to both linear and nonlinear cases. The decomposition ensures that the proportions of impact from each variable's innovations sum to one. It is derived from the generalized impulse response function and can be computed via simulation. The method is demonstrated using US output growth and interest rate spread data.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:57:16.493279
0871d13199519d67,Graph-based stock correlation and prediction for high-frequency trading systems,"In this paper, we have implemented a high-frequency quantitative system that can obtain stable returns for the Chinese A-share market, which has been running for more than 3 months (from March 27, 2020 to June 30, 2020) with the expected results. A number of rules and barriers exist in the Chinese A-share market such as trading restrictions and high fees, as well as scarce and expensive hedging tools. It is difficult to achieve stable absolute returns in such a market. Stock correlation analysis and price prediction play an important role to achieve any profitable trading. The portfolio management and subsequent trading decisions highly depend on the results of stock correlation analysis and price prediction. However, it is nontrivial to analyze and predict any stocks, being time-varying and affected by unlimited factors in a given market. Traditional methods only take some certain factors into consideration but ignore others that may be changed dynamically. In this paper, we propose a novel machine learning model named Graph Attention Long Short-Term Memory (GALSTM) to learn the correlations between stocks and predict their future prices automatically. First, a multi-Hawkes Process is used to initial a correlation graph between stocks. This procedure provides a good training start as the multi-Hawkes Processes will be studied on the most saint feature fluctuations with any correlations being statistically significant. Then an attention-based LSTM is built to learn the weighting matrix underlying the dynamic graph. In addition, we also build matching data process plus portfolio management modules to form a complete system. The proposed GALSTM enables us to expand the scope of stock selection under the premise of controlling risks with limited hedging tools in the A-share market, thereby effectively increasing high-frequency excess returns. We then construct a long and short positions combination, select long positions in the A shares of the entire market, and use stock index futures to short. With GALSTM model, the products managed by our fully automatic quantitative trading system achieved an absolute annual return rate of 44.71% and the standard deviation of daily returns is only 0.42% in three months of operation. Only 1 week loss in 13 weeks of running time. © 2021 Elsevier B.V., All rights reserved.","Yin, T.; Liu, C.; Ding, F.; Feng, Z.; Yuan, B.; Zhang, N.",2022,10.1016/j.patcog.2021.108209,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113573353&doi=10.1016%2Fj.patcog.2021.108209&partnerID=40&md5=971f0fc98ae32339efd70adf33e67ef9,scopus,"This paper introduces a novel machine learning model, GALSTM, for high-frequency stock trading in the Chinese A-share market. The system uses a multi-Hawkes Process to establish stock correlations and an attention-based LSTM to predict future prices. The implemented system achieved a 44.71% annual return rate over three months, demonstrating its effectiveness in managing risk and increasing returns despite market constraints.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:57:22.819786
1943ea8afc4b3783,"Green bonds forecasting: evidence from pre-crisis, Covid-19 and Russian–Ukrainian crisis frameworks","PurposeWithout precedent, green bonds confront, for the first time since their emergence, a twofold crisis context, namely the Covid-19-Russian–Ukrainian crisis period. In this context, this paper aims to investigate the connectedness between the two pioneering bond market classes that are conventional and treasury, with the green bonds market.Design/methodology/approachIn their forecasting target, authors use a Support Vector Regression model on daily S&P 500 Green, Conventional and Treasury Bond Indexes for a year from 2012 to 2022.FindingsAuthors argue that conventional bonds could better explain and predict green bonds than treasury bonds for the three studied sub-periods (pre-crisis period, Covid-19 crisis and Covid-19-Russian–Ukrainian crisis period). Furthermore, conventional and treasury bonds lose their forecasting power in crisis framework due to enhancements in market connectedness relationships. This effect makes spillovers in bond markets more sensitive to crisis and less predictable. Furthermore, this research paper indicates that even if the indicators of the COVID-19 crisis have stagnated and the markets have adapted to this rather harsh economic framework, the forecast errors persist higher than in the pre-crisis phase due to the Russian–Ukrainian crisis effect not yet addressed by the literature.Originality/valueThis study has several implications for the field of green bond forecasting. It not only illuminates the market participants to the best market forecasters, but it also contributes to the literature by proposing an unadvanced investigation of green bonds forecasting in Crisis periods that could help market participants and market policymakers to anticipate market evolutions and adapt their strategies to period specificities.",,2025,10.1108/jes-01-2024-0061,,proquest,"This study investigates the connectedness between conventional, treasury, and green bond markets during pre-crisis, Covid-19, and Russian-Ukrainian crisis periods using a Support Vector Regression model. It finds that conventional bonds are better predictors of green bonds, but both conventional and treasury bonds lose forecasting power during crises due to enhanced market interconnectedness. The Russian-Ukrainian crisis significantly impacts forecast errors even after market adaptation to Covid-19.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:57:36.693768
0cdd1e39bd843201,Hawkes-diffusion process and the conditional probability of defaults in the Eurozone,"This study examines market information embedded in the European sovereign CDS (credit default swap) market by analyzing the sovereign CDSs of 13 Eurozone countries from January 1, 2008, to February 29, 2012, which includes the recent Eurozone debt crisis period. We design the conditional probability of defaults for the CDS prices based on the Hawkes-diffusion process and obtain the theoretical prices of CDS indexes. To estimate the model parameters, we calibrate the model prices to empirical prices obtained from individual sovereign CDS term structure data. The estimated parameters clearly explain both cross-sectional and time-series data. Our empirical results show that the probability of a huge loss event sharply increased during the Eurozone debt crisis, indicating a contagion effect. Even countries with strong and stable economies, such as Germany and France, suffered from the contagion effect. We also find that the probability of small events is sensitive to the state of the economy, spiking several times due to the global financial crisis and the Greek government debt crisis. (C) 2015 Elsevier B.V. All rights reserved.","Kim, Jungmu; Park, Yuen Jung; Ryu, Doojin",2016,10.1016/j.physa.2015.12.087,,wos,"This study uses a Hawkes-diffusion process to analyze sovereign credit default swap (CDS) prices for 13 Eurozone countries from 2008 to 2012, a period encompassing the Eurozone debt crisis. The model estimates the conditional probability of defaults and theoretical CDS index prices, calibrated against empirical data. Findings indicate a contagion effect during the crisis, impacting even strong economies, and that the probability of smaller events is sensitive to economic conditions.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:57:47.950242
03eff6df80dbcb50,Heartbeat classification based on single lead-II ECG using deep learning,"The analysis and processing of electrocardiogram (ECG) signals is a vital step in the diagnosis of cardiovascular disease. ECG offers a non-invasive and risk-free method for monitoring the electrical activity of the heart that can assist in predicting and diagnosing heart diseases. The manual interpretation of the ECG signals, however, can be challenging and time-consuming even for experts. Machine learning techniques are increasingly being utilized to support the research and development of automatic ECG classification, which has emerged as a prominent area of study. In this paper, we propose a deep neural network model with residual blocks (DNN-RB) to classify cardiac cycles into six ECG beat classes. The MIT-BIH dataset was used to validate the model resulting in a test accuracy of 99.51%, average sensitivity of 99.7%, and average specificity of 98.2%. The DNN-RB method has achieved higher accuracy than other state-of-the-art algorithms tested on the same dataset. The proposed method is effective in the automatic classification of ECG signals and can be used for both clinical and out-of-hospital monitoring and classification combined with a single-lead mobile ECG device. The method has also been integrated into a web application designed to accept digital ECG beats as input for analyses and to display diagnostic results.The analysis and processing of electrocardiogram (ECG) signals is a vital step in the diagnosis of cardiovascular disease. ECG offers a non-invasive and risk-free method for monitoring the electrical activity of the heart that can assist in predicting and diagnosing heart diseases. The manual interpretation of the ECG signals, however, can be challenging and time-consuming even for experts. Machine learning techniques are increasingly being utilized to support the research and development of automatic ECG classification, which has emerged as a prominent area of study. In this paper, we propose a deep neural network model with residual blocks (DNN-RB) to classify cardiac cycles into six ECG beat classes. The MIT-BIH dataset was used to validate the model resulting in a test accuracy of 99.51%, average sensitivity of 99.7%, and average specificity of 98.2%. The DNN-RB method has achieved higher accuracy than other state-of-the-art algorithms tested on the same dataset. The proposed method is effective in the automatic classification of ECG signals and can be used for both clinical and out-of-hospital monitoring and classification combined with a single-lead mobile ECG device. The method has also been integrated into a web application designed to accept digital ECG beats as input for analyses and to display diagnostic results.",,2023,10.1016/j.heliyon.2023.e17974,,proquest,"This paper proposes a deep neural network model with residual blocks (DNN-RB) for classifying cardiac cycles into six ECG beat classes using single-lead ECG signals. The model was validated on the MIT-BIH dataset, achieving high accuracy (99.51%), sensitivity (99.7%), and specificity (98.2%), outperforming other state-of-the-art algorithms. The method is suitable for clinical and out-of-hospital monitoring and has been integrated into a web application.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:57:52.520551
4242f44ee4893257,Heterogeneous Demand and Supply for an Insurance‐linked Credit Product in Kenya: A Stated Choice Experiment Approach,"We employ a discrete choice experiment to elicit demand and supply side preferences for insurance‐linked credit, a promising market‐based tool for managing agricultural weather risks and providing access to credit for farmers. We estimate preference heterogeneity using primary data from smallholder farmers and managers of lenders/insurers combined with household socio‐economic survey data in Kenya. We analyse the choice data using maximum simulated likelihood and Hierarchical Bayes estimation of a mixed logit model. Although there are some similarities, we find that there is conflicting demand and supply side preferences for credit terms, collateral requirements, and loan use flexibility. We also analyse willingness to buy and willingness to offer for farmers and suppliers, respectively, for the risk premium for different attributes and their levels. Identifying the preferred attributes and levels for both farmers and financial institutions can guide optimal packaging of insurance and credit providing market participation and adoption motivation for insurance‐bundled credit product.",,2021,10.1111/1477-9552.12401,,proquest,"This study uses a discrete choice experiment to understand the demand and supply preferences for an insurance-linked credit product in Kenya, aimed at managing agricultural weather risks for farmers. It analyzes data from both smallholder farmers and lenders/insurers, employing mixed logit models to identify heterogeneous preferences regarding credit terms, collateral, and loan use. The findings reveal conflicting preferences between demand and supply sides, and the study assesses willingness to buy and offer for different product attributes to guide optimal product design and market adoption.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:57:56.565970
7711efe41665e70e,Hidden persistent disasters and asset prices,"This study analyzes the effects of agents' learning about hidden persistent economic disasters on asset prices. In this study, it is assumed that aggregate consumption follows a hidden Markov regime-switching process and a representative agent infers the current regime, normal regime, or disaster regime, sequentially from the realized path of the past consumption process. In this setting, the fluctuation in the agent's posterior probabilities of the disaster regime augments the volatility of equity returns. By utilizing the stochastic differential utility, this study demonstrates that the current model can help resolve many asset pricing puzzles including the equity premium puzzle, equity volatility puzzle, and risk-free rate puzzle simultaneously. Further, the current model predicts the counter-cyclical pattern in the equity premium and equity-return volatility on the normal regime, although asset returns are negative and highly volatile during disasters. The study also demonstrates that, if the agent's preferences are restricted to time-additive power utility, the consideration of hidden persistent disasters deepens the asset pricing puzzles. © 2013 Springer-Verlag Berlin Heidelberg. © 2014 Elsevier B.V., All rights reserved.","Suzuki, M.",2014,10.1007/s10436-013-0226-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905013524&doi=10.1007%2Fs10436-013-0226-5&partnerID=40&md5=bbf69d9f4aeb9390f5e08124536c8c25,scopus,"This study investigates how agents' learning about hidden persistent economic disasters influences asset prices. It models aggregate consumption with a hidden Markov regime-switching process, where agents infer the current regime (normal or disaster) from past consumption data. The model shows that fluctuating posterior probabilities of the disaster regime increase equity return volatility. The study suggests this model can address asset pricing puzzles like the equity premium, equity volatility, and risk-free rate puzzles. It also predicts counter-cyclical patterns in equity premium and volatility during normal times, contrasting with the negative and volatile returns during disasters. The paper notes that if agents have time-additive power utility, hidden persistent disasters exacerbate asset pricing puzzles.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:57:59.654501
33f5d072b20c21b6,Hierarchical Bayesian Models for Regularization in Sequential Learning,"We show that a hierarchical Bayesian modeling approach allows us to perform regularization in sequential learning. We identify three inference levels within this hierarchy: model selection, parameter estimation, and noise estimation. In environments where data arrive sequentially, techniques such as cross validation to achieve regularization or model selection are not possible. The Bayesian approach, with extended Kalman filtering at the parameter estimation level, allows for regularization within a minimum variance framework. A multilayer perceptron is used to generate the extended Kalman filter nonlinear measurements mapping. We describe several algorithms at the noise estimation level that allow us to implement on-line regularization. We also show the theoretical links between adaptive noise estimation in extended Kalman filtering, multiple adaptive learning rates, and multiple smoothing regularization coefficients.",J. F. G. d. Freitas; M. Niranjan; A. H. Gee,2000,10.1162/089976600300015655,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6789428,ieeexplore,"This paper proposes a hierarchical Bayesian modeling approach for regularization in sequential learning, addressing the limitations of traditional methods like cross-validation in dynamic environments. It outlines three inference levels: model selection, parameter estimation (using extended Kalman filtering), and noise estimation. The authors demonstrate theoretical links between adaptive noise estimation, multiple learning rates, and smoothing regularization coefficients, suggesting on-line regularization capabilities.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:58:07.250616
ef6ce888ac066f40,Hierarchical Bayesian collective risk model: an application to health insurance,"This paper deals with the main statistical steps involved in building an insurance plan, with special emphasis on an application to health insurance. The pure premium is predicted based on the available past information concerning the number and the amount of losses, and also the population exposed to risk. Both the size and the number of losses are treated in a stochastic manner. The claims are assumed to follow a Poisson process and the claim sizes are independent and identically distributed non-negative random variables. The model proposed is a generalization of the collective risk model, usually applied in practice. The evolution of the population at risk is also stochastically described via a nonlinear hierarchical growth model. Furthermore, a theoretical decision framework is adopted for evaluating the premium. Model selection and premium calculation are obtained from the predictive distribution, incorporating all the uncertainties involved.",,2005,10.1016/j.insmatheco.2004.11.006,,proquest,"This paper presents a hierarchical Bayesian collective risk model for health insurance. It predicts pure premiums by stochastically modeling the number and amount of losses, as well as the exposed population. The model generalizes the standard collective risk model and incorporates a hierarchical growth model for the population. Premium evaluation uses a theoretical decision framework and predictive distributions to account for uncertainties.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:58:09.040460
4f40b18a5eb1cbfb,Hoarding the herd: the convenience of productive stocks,"This paper investigates the convenience yield that emerges in markets with productive stocks. We isolate the economic fundamentals giving rise to the yield, and show how these map to the empirical convenience yield measure. A model for price dynamics is derived from an economic model for optimal stock levels. We show how the price process reduces to a simple non-linear first-order Markov process. The model is estimated for the Norwegian market for farmed salmon by Generalized Methods of Moments, where stock growth is approximated by sea-water temperature. Our estimation result supports the theorized role of stock growth as a convenience yield component. Our results are relevant for the functioning of futures markets for commodities such as fish and other animal production where systematic stock growth affects the term structure. © 2014 Wiley Periodicals, Inc. Jrl Fut Mark 35:679-694, 2015 Copyright John Wiley & Sons. Reproduced with permission. An electronic version of this article is available online at http://www.interscience.wiley.com",,2015,10.1002/fut.21679,,proquest,"This paper investigates the convenience yield in markets with productive stocks, deriving a model for price dynamics from an economic model of optimal stock levels. The model is estimated using data from the Norwegian farmed salmon market, where stock growth is approximated by sea-water temperature. The findings suggest that stock growth plays a role in the convenience yield and are relevant for futures markets in commodities like fish and other animal production.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:58:11.720763
13680c9aa7a8de08,How Do Macroeconomic Fundamentals Affect Sovereign Bond Yields? New Evidence from European Forecasts,"Macroeconomic fundamentals impact the long-term insolvency problem of a country. This article empirically assesses the role played by both macroeconomic and fiscal fundamentals, proxied by a set of European Commission's forecasts, in affecting sovereign bond yields. We look at a large panel of 25 European countries between 1992 and 2015. By means panel and time-series approaches, we find that lower short-term interest rates and better fiscal institutions tend to lower bond yields. The better the economic and fiscal outlooks going forward, the lower the yields demanded in international markets. Timing also matters: investors seem to pay more attention to forecasts the shorter the forecast horizon, and they started carrying more weight since the Global Financial Crisis. Finally, the impact of yields' determinants is different across countries, being more prominent in those characterized by economic hardship conditions (Greece, Ireland, Spain, and Portugal). (JEL codes: C23, E44, H68) © 2019 Elsevier B.V., All rights reserved.","Tovar Jalles, J.T.",2019,10.1093/cesifo/ify025,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064515400&doi=10.1093%2Fcesifo%2Fify025&partnerID=40&md5=7535ceeeb760f8019d70ad6693a12d0a,scopus,"This study investigates how macroeconomic and fiscal fundamentals, using European Commission forecasts, influence sovereign bond yields across 25 European countries from 1992 to 2015. Using panel and time-series methods, it finds that lower short-term interest rates and stronger fiscal institutions reduce bond yields. Positive economic and fiscal outlooks also lower yields, with investors paying more attention to shorter forecast horizons, especially since the Global Financial Crisis. The impact varies by country, being more significant in nations facing economic difficulties like Greece, Ireland, Spain, and Portugal.",False,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T15:58:22.569123
09389e7c8bffb0a4,How to Rate the Financial Performance of Private Companies? A Tailored Integrated Rating Methodology Applied to North-Eastern Italian Districts,"This paper contributes to solving the puzzle of assessing the financial performance of private/unlisted companies. The inner characteristics of these companies make the adoption of traditional best practices in estimating risk premia difficult or impossible. Moreover, the lack of market data and comparable information biases the perception of corporate performance and generates the misallocation of credit fundings (both quantities and pricing). Hence, in this paper, we develop an Integrated Rating Methodology (IRM) to estimate a more efficient corporate “return-to-risk” measure. Our IRM is rooted in the seminal “certainty equivalent” model as developed by Lintner in 1965, but we modify it using a shortfall approach, and then compute a “confident equivalent” that is compliant with Fischer Black’s zero-beta model as well as the Basel agreements. An empirical application of the approach is conducted with a sample of 13,583 non-financial SMEs in the north-east regions of Italy, where there is evidence of inefficient bank financing. We back-test our IRM by rating these companies using corporate financial data during the period 2007–2014, which encompasses both the Great Financial Crisis and the European sovereign debt crisis. Our empirical results depict a clear crowding-out effect of credit allocations when we compare our IRM scoring measure with the actual raising ability and the cost of capital relating to these firms. We find that 36% of companies are underfunded, even if they have a superior IRM score, while 27% of them are funded without merit. Interestingly, this last figure is in line with the average non-performing loan ratio provided by official Italian statistics from 2015 to 2020. Therefore, we conclude that our IRM methodology is promising and may be better at estimating risk financing in small private companies (including start-ups) than internal banking models. These initial results will drive our forthcoming research towards creating an IRM 2.0.",,2022,10.3390/jrfm15110493,,proquest,"This paper introduces an Integrated Rating Methodology (IRM) to assess the financial performance of private companies, addressing the challenges posed by limited market data and traditional model limitations. The IRM modifies the certainty equivalent model with a shortfall approach and aligns with Black's zero-beta model and Basel agreements. An empirical application to 13,583 Italian SMEs from 2007-2014 reveals significant misallocations in credit funding, with underfunded superior-scoring companies and overfunded less deserving ones. The study suggests IRM is a promising tool for risk financing in private companies and start-ups.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:58:27.110330
9122c80b394b9c20,Impact of China’s Provincial Government Debt on Economic Growth and Sustainable Development,"Macroeconomic stability is the core concept of sustainable development. However, the coronavirus disease (COVID-19) pandemic has caused government debt problems worldwide. In this context, it is of practical significance to study the impact of government debt on economic growth and fluctuations. Based on panel data of 30 provinces in China from 2012 to 2019, we used the Mann–Kendall method and Kernel Density estimation to analyze the temporal and spatial evolution of China’s provincial government debt ratio and adopted a panel model and HP filtering method to study the impact of provincial government debt on economic growth and fluctuation. Our findings indicate that, during the sample period, China’s provincial government debt promoted economic growth and the regression coefficient (0.024) was significant. From different regional perspectives, the promotion effect of the central region (0.027) is higher than that of the eastern (0.020) and western regions (0.023). There is a nonlinear relationship between China’s provincial government debt and economic growth, showing an inverted “U-shaped” curve. Fluctuations in government debt aggravate economic volatility, with a coefficient of 0.009; tax burden fluctuation and population growth rate aggravate economic changes. In contrast, the optimization of the province’s industrial structure and the improvement of the opening level of provinces slow down economic fluctuations.",,2022,10.3390/su14031474,,proquest,"This study analyzes the impact of China's provincial government debt on economic growth and fluctuations using panel data from 30 provinces between 2012 and 2019. The findings suggest that government debt promoted economic growth, with a nonlinear, inverted U-shaped relationship. Debt fluctuations were found to aggravate economic volatility, while industrial structure optimization and opening-up levels helped slow it down. The study employed the Mann–Kendall method, Kernel Density estimation, a panel model, and HP filtering.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:58:30.252076
7f362baf207aa52c,Implementation Tests of Financial Market Analysis by Text Mining,"In this study, we propose a new text-mining method for long-term market analysis. Using our method, we performe out-of-sample tests using monthly price data of financial markets; Japanese government bond market, Japanese stock market, and the yen-dollar market. First we extract feature vectors from monthly reports of Bank of Japan. Then, trends of each market are estimated by regression analysis using the feature vectors. As a result of comparison with support vector regression, the proposal method could forecast in higher accuracy about both the level and direction of long-term market trends. Moreover, our method showed high returns with annual rate averages as a result of the implementation test.",,2011,10.1527/tjsai.26.313,,proquest,"This study introduces a novel text-mining approach for long-term financial market analysis, specifically applied to the Japanese government bond market, stock market, and yen-dollar market. The method involves extracting feature vectors from Bank of Japan reports and using regression analysis to estimate market trends. The proposed method demonstrated higher accuracy in forecasting long-term trends compared to support vector regression and yielded high returns in implementation tests.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T15:58:36.865724
a28fca24b7615f52,Improving credit risk assessment in P2P lending with explainable machine learning survival analysis,"Recent research using explainable machine learning survival analysis demonstrated its ability to identify new risk factors in the medical field. In this study, we adapted this methodology to credit risk assessment. We used a comprehensive dataset from the Estonian P2P lending platform Bondora, consisting of over 350,000 loans and 112 features with a loan volume of 915 million euros. First, we applied classical (linear) and machine learning (extreme gradient-boosted) Cox models to estimate the risk of these loans and then risk-rated them using risk stratification. For each rating category we calculated default rates, rates of return, and plotted Kaplan–Meier curves. These performance criteria revealed that the boosted Cox model outperformed both the classical Cox model and the platform’s rating. For instance, the boosted model’s highest rating category had an annual excess return of 18% and a lower default rate compared to the platform’s best rating. Second, we explained the machine learning model’s output using Shapley Additive Explanations. This analysis revealed novel nonlinear relationships (e.g., higher risk for borrowers over age 55) and interaction effects (e.g., between age and housing situation) that provide promising avenues for future research. The machine-learning model also found feature contributions aligning with existing research, such as lower default risk associated with older borrowers, females, individuals with mortgages, or those with higher education. Overall, our results reveal that explainable machine learning survival analysis excels at risk rating, profit scoring, and risk factor analysis, facilitating more precise and transparent credit risk assessments. © 2024 Elsevier B.V., All rights reserved.","Bone-Winkel, G.F.; Reichenbach, F.",2024,10.1007/s42521-024-00114-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208063837&doi=10.1007%2Fs42521-024-00114-3&partnerID=40&md5=818beb0a3f30b92b63502ce3db0e3345,scopus,"This study applies explainable machine learning survival analysis, specifically boosted Cox models and Shapley Additive Explanations, to assess credit risk in P2P lending using a large dataset from the Bondora platform. The boosted model outperformed classical methods and the platform's rating in terms of default rates and returns. The explainable AI revealed novel risk factors and confirmed existing ones, leading to more precise and transparent credit risk assessments.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:58:42.530733
345281eb8dc19585,Incorporating economic indicators and market sentiment effect into US Treasury bond yield prediction with machine learning,"Accurate prediction of US Treasury bond yields is crucial for investment strategies and economic policymaking. This paper explores the application of advanced machine learning techniques, specifically Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM) models, in forecasting these yields. By integrating key economic indicators and policy changes, our approach seeks to enhance the precision of yield predictions. Our study demonstrates the superiority of LSTM models over traditional RNNs in capturing the temporal dependencies and complexities inherent in financial data. The inclusion of macroeconomic and policy variables significantly improves the models’ predictive accuracy. This research underscores a pioneering movement for the legacy banking industry to adopt artificial intelligence (AI) in financial market prediction. In addition to considering the conventional economic indicator that drives the fluctuation of the bond market, this paper also optimizes the LSTM to handle situations when rate hike expectations have already been priced-in by market sentiment. © 2024 Elsevier B.V., All rights reserved.","Li, Z.; Wang, B.; Chen, Y.",2024,10.24294/jipd.v8i9.7671,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204238837&doi=10.24294%2Fjipd.v8i9.7671&partnerID=40&md5=7603a9906c2d14dd96d306253e3a6399,scopus,"This paper investigates the use of RNN and LSTM models to predict US Treasury bond yields, incorporating economic indicators and market sentiment. The study finds that LSTM models outperform traditional RNNs and that including macroeconomic and policy variables improves prediction accuracy, highlighting the potential of AI in financial market forecasting.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T15:58:54.141362
2a8b4f58949f786d,Indicator variables for inflation expectations in the Euro area,"In this paper, we model the Euro area market-based inflation expectations extracted from the inflation-linked swaps, and study the macroeconomic information embedded in expected inflation. First, we estimate the Gaussian affine term structure model to decompose the forward ILS-implied inflation rate into inflation expectations and inflation risk premium at one-, two- and three-year horizons. Secondly, from a large panel of macroeconomic series we identify the most significant indicator variables for inflation expectations using the elastic net modification of the LASSO regression. Finally, we measure partial contributions of individual indicator variables to the changes in inflation expectations. Our findings reveal that across horizons considered inflation expectations are correlated to the measures of current inflation of the overall price level and price level of services, the unemployment rate, and the Euro exchange rate. The identified indicators provide a useful information about the evolution of inflation expectations with different intensities at different horizons.",,2021,10.1504/ijse.2021.114615,,proquest,"This paper models Euro area market-based inflation expectations using inflation-linked swaps. It decomposes the implied inflation rate into expectations and risk premium, identifies key macroeconomic indicator variables for inflation expectations using elastic net regression, and measures their contributions to changes in expectations. Findings show correlations with current inflation, unemployment, and exchange rates.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:59:03.744952
716bd56fb97759d4,Inference in Bayesian additive vector autoregressive tree models,"Vector autoregressive (VAR) models assume linearity between the endogenous variables and their lags. This assumption might be overly restrictive and could have a deleterious impact on forecasting accuracy. As a solution we propose combining VAR with Bayesian additive regression tree (BART) models. The resulting Bayesian additive vector autoregressive tree (BAVART) model is capable of capturing arbitrary nonlinear relations between the endogenous variables and the covariates without much input from the researcher. Since controlling for heteroscedasticity is key for producing precise density forecasts, our model allows for stochastic volatility in the errors. We apply our model to two datasets. The first application shows that the BAVART model yields highly competitive forecasts of the U.S. term structure of interest rates. In a second application we estimate our model using a moderately sized Eurozone dataset to investigate the dynamic effects of uncertainty on the economy. © 2023 Elsevier B.V., All rights reserved.","Huber, F.; Rossini, L.",2022,10.1214/21-aoas1488,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127778413&doi=10.1214%2F21-AOAS1488&partnerID=40&md5=742157785b372051b99fe7333df16fb0,scopus,"This paper introduces the Bayesian additive vector autoregressive tree (BAVART) model, which combines Vector Autoregressive (VAR) models with Bayesian Additive Regression Trees (BART) to capture nonlinear relationships between endogenous variables and covariates. The model also incorporates stochastic volatility for density forecasts. The authors demonstrate its effectiveness in forecasting the U.S. term structure of interest rates and in analyzing the dynamic effects of uncertainty on the Eurozone economy.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T15:59:16.645140
dcf76bf9c66b1d50,Inference in asset pricing models with a low-variance factor,"This paper concerns with the effects of including a low-variance factor in an asset pricing model. When a low-variance factor is present, the commonly applied Fama-MacBeth two-pass regression procedure is very likely to yield misleading results. Local asymptotic analysis and simulation evidence indicate that the risk premiums corresponding to all factors are very likely to be unreliably estimated. Moreover, t- and F-statistics are less likely to detect whether the risk premiums are significantly different from zero. We recommend Kleibergen's (2009)FAR statistic when there is a low-variance factor included in an asset pricing model. All rights reserved, Elsevier",,2013,10.1016/j.jbankfin.2012.11.007,,proquest,"This paper investigates the impact of a low-variance factor on asset pricing models, highlighting how it can lead to unreliable risk premium estimations and reduced statistical power in detecting significant risk premiums. It suggests using Kleibergen's FAR statistic in such scenarios.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:59:19.474213
701ad234b9ed241c,Inflation bets or deflation hedges? the changing risks of nominal bonds,"The covariance between US Treasury bond returns and stock returns has moved considerably over time. While it was slightly positive on average in the period 1953 to 2014, it was unusually high in the early 1980s and negative in the early 21st Century, particularly in the downturns of 2001 and 2007 to 2009. This paper specifies and estimates a model in which the nominal term structure of interest rates is driven by four state variables: the real interest rate, temporary and permanent components of expected inflation, and the nominalreal covariance of inflation and the real interest rate with the real economy. The last of these state variables enables the model to fit the changing covariance of bond and stock returns. In the model, a high nominal-real covariance implies a high term premium and a concave yield curve. The decline in this covariance since the early 1980s has driven down our model-implied term premium on 10-year zero-coupon nominal Treasury bonds by about two percentage points. © 2018 Elsevier B.V., All rights reserved.","Campbell, J.Y.; Sunderam, A.; Viceira, L.M.",2017,10.1561/104.00000043,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019756733&doi=10.1561%2F104.00000043&partnerID=40&md5=d1ab4f1d0502d33a087d900282246a09,scopus,"This paper models the nominal term structure of interest rates using four state variables, including real interest rate, expected inflation components, and nominal-real covariance. It explains the changing covariance between US Treasury bond and stock returns over time, noting a decline in the term premium on 10-year nominal Treasury bonds since the early 1980s.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:59:22.129331
475fb32ab8e4049d,Informed Trading and Return Predictability in China: Research Based on Ensemble Neural Network,"We construct a new informed trading index based on the high-frequency trading data of the Chinese A-share market using the ensemble neural network algorithm. We find that the informed trading index is a strong negative predictor of future aggregate stock market returns, with monthly in-sample and out-of-sample (Formula presented.) of 5.45% and 3.53%, respectively, which is far greater than the predictive power of other previously studied informed trading indicators and macroeconomic variables. The asset allocation strategy based on our index can generate large economic gains for the mean-variance investors, with annualized CER (certain equivalent return) gains ranging from 10.91% to 7.80% as the investor’s risk appetite varies. The driving force of the predictive power appears to stem from the liquidity provider role that informed traders play, which decreases the market’s illiquidity risk and lowers the risk premium of equity. Our analysis complements the returns predictability study by adding a new predictor on the one hand and informs market timing strategies on the other. © 2024 Elsevier B.V., All rights reserved.","Li, P.; Yang, L.",2025,10.1080/1540496x.2024.2379471,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200154249&doi=10.1080%2F1540496X.2024.2379471&partnerID=40&md5=45cabd8cde831077ff35ce5455b1167b,scopus,"This study develops an informed trading index for the Chinese A-share market using an ensemble neural network and high-frequency trading data. The index is found to be a significant negative predictor of future aggregate stock market returns, outperforming existing indicators and macroeconomic variables. An asset allocation strategy based on this index yields substantial economic gains for investors. The predictive power is attributed to informed traders' role in reducing market illiquidity risk and equity risk premium. The research contributes to return predictability studies and informs market timing strategies.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:59:29.752210
46aa043b290272aa,Inter-Factor Determinants of Return Reversal Effect with Dynamic Bayesian Network Analysis: Empirical Evidence from Pakistan,"Bayesian Networks are multivariate probabilistic factor graphs that are used to assess underlying factor relationships. From January 2005 to December 2018, the study examines how Dynamic Bayesian Networks can be utilized to estimate portfolio risk and return as well as determine inter-factor relationships among reversal profit-generating components in Pakistan's emerging market (PSX). The goal of this article is to uncover the factors that cause reversal profits in the Pakistani stock market. In visual form, Bayesian networks can generate causal and inferential probabilistic relationships. Investors might update their stock return values in the network simultaneously with fresh market information, resulting in a dynamic shift in portfolio risk distribution across the networks. The findings show that investments in low net profit margin, low investment, and high volatility-based designed portfolios yield the biggest dynamical reversal profits. The main triggering aspects related to generation reversal profits in the Pakistan market, in the long run, are net profit margin, market risk premium, investment, size, and volatility factor. Investors should invest in and build portfolios with small companies that have a low price-to-earnings ratio, small earnings per share, and minimal volatility, according to the most likely explanation.","Haque, Abdul; Rao, Marriam; Qamar, Muhammad Ali Jibran",2022,10.13106/jafeb.2022.vol9.no3.0203,,wos,"This study uses Dynamic Bayesian Networks to analyze inter-factor relationships and estimate portfolio risk and return in Pakistan's stock market from 2005 to 2018. It identifies factors contributing to reversal profits, suggesting that portfolios with low net profit margin, low investment, and high volatility yield the largest reversal profits. Key long-term triggering factors include net profit margin, market risk premium, investment, size, and volatility. The study recommends investing in small companies with low P/E ratios, small EPS, and minimal volatility.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:59:38.425558
471849c4c2e8e8d5,Interest rate models on Lie groups,"This paper examines an alternative approach to interest rate modeling, in which the nonlinear and random behavior of interest rates is captured by a stochastic differential equation evolving on a curved state space. We consider as candidate state spaces the matrix Lie groups; these offer not only a rich geometric structure, butunlike general Riemannian manifoldsalso allow for diffusion processes to be constructed easily without invoking the machinery of stochastic calculus on manifolds. After formulating bilinear stochastic differential equations on general matrix Lie groups, we then consider interest rate models in which the short rate is defined as linear or quadratic functions of the state. Stochastic volatility is also augmented to these models in a way that respects the Riemannian manifold structure of symmetric positive-definite matrices. Methods for numerical integration, parameter identification, pricing, and other practical issues are addressed through examples.","Park, F. C.; Chun, C. M.; Han, C. W.; Webber, N.",2011,10.1080/14697680903468963,,wos,"This paper proposes an alternative approach to interest rate modeling using stochastic differential equations on matrix Lie groups, which offer a rich geometric structure. The models capture nonlinear and random interest rate behavior, with the short rate defined as linear or quadratic functions of the state. Stochastic volatility is incorporated while respecting the manifold structure of symmetric positive-definite matrices. Practical aspects like numerical integration, parameter identification, and pricing are discussed with examples.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T15:59:40.066166
770dcddf8d700ba0,Interest rate term structure modeling using free-knot splines,"In this article a new methodology for estimating the term structure of interest rates is developed. Using polynomial splines, a reliable approximation to term structure may depend crucially upon intelligent selection of numbers and position of spline knots, which can be a combinatorially very complex task. A different approach based on heuristic optimization techniques called genetic algorithms is presented. The optimal spline function takes into account the goodness of fit of the spline function. The new methodology was applied to estimating the term structure using data on zero-coupon Euro market bonds. © 2006 by The University of Chicago. All rights reserved. © 2011 Elsevier B.V., All rights reserved.","Fernández-Rodríguez, F.",2006,10.1086/508009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847039555&doi=10.1086%2F508009&partnerID=40&md5=8d8b9389b44d5169d8cc6a2819340341,scopus,This article proposes a new method for estimating the term structure of interest rates using free-knot splines and genetic algorithms. It addresses the complexity of traditional spline knot selection by employing heuristic optimization to find an optimal spline function that fits the data well. The methodology was applied to Euro market zero-coupon bond data.,True,True,False,gemini-2.5-flash-lite,Olav,M,,2025-10-13T15:59:56.311602
e259919b32e771b4,International evidence on the predictability of return to securitized real estate assets: Econometric models versus neural networks,"The performance of various statistical models and commonly used financial indicators for forecasting securitised real estate returns are examined for five European countries: the UK, Belgium, the Netherlands, France and Italy. Within a VAR framework, it is demonstrated that the gilt-equity yield ratio is in most cases a better predictor of securitized returns than the term structure or the dividend yield. In particular, investors should consider in their real estate return models the predictability of the gilt-equity yield ratio in Belgium, the Netherlands and France, and the term structure of interest rates in France. Predictions obtained from the VAR and univariate time-series models are compared with the predictions of an artificial neural network model. It is found that, whilst no single model is universally superior across all series, accuracy measures and horizons considered, the neural network model is generally able to offer the most accurate predictions for 1-month horizons. For quarterly and half-yearly forecasts, the random walk with a drift is the most successful for the UK, Belgian and Dutch returns and the neural network for French and Italian returns. Although this study underscores market context and forecast horizon as parameters relevant to the choice of the forecast model, it strongly indicates that analysts should exploit the potential of neural networks and assess more fully their forecast performance against more traditional models. © 2004 Elsevier Science B.V., Amsterdam. All rights reserved.","Brooks, C.; Tsolacos, S.",2003,10.1080/0959991032000109517,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0141671708&doi=10.1080%2F0959991032000109517&partnerID=40&md5=551008a304241f4cd4b10fcc00d8a1bf,scopus,"This study examines the performance of econometric models and neural networks in forecasting securitized real estate returns across five European countries. It finds that the gilt-equity yield ratio is often a better predictor than the term structure or dividend yield. While no single model is universally superior, neural networks generally offer the most accurate predictions for 1-month horizons. The study suggests that market context and forecast horizon are important, and analysts should explore the potential of neural networks.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:00:02.372491
835037460463ea4b,Intraday technical trading in the foreign exchange market,"This paper examines the out-of-sample performance of intraday technical trading strategies selected using two methodologies, a genetic program and an optimized linear forecasting model. When realistic transaction costs and trading hours are taken into account, we find no evidence of excess returns to the trading rules derived with either methodology. Thus, our results are consistent with market efficiency. We do find, however, that the trading rules discover some remarkably stable patterns in the data. © 2003 Elsevier Science Ltd. All rights reserved. © 2019 Elsevier B.V., All rights reserved.","Neely, C.J.; Weller, P.A.",2003,10.1016/s0261-5606(02)00101-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037375935&doi=10.1016%2FS0261-5606%2802%2900101-8&partnerID=40&md5=23d064c24d32aa4af4ee08497bfe753f,scopus,"This paper investigates the performance of intraday technical trading strategies in the foreign exchange market, using a genetic program and an optimized linear forecasting model. After accounting for transaction costs and trading hours, no excess returns were found, suggesting market efficiency. However, the trading rules did identify stable patterns in the data.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:00:06.375188
5f6fd09a9aa772ab,Investment Portfolio Allocation and Insurance Solvency: New Evidence from Insurance Groups in the Era of Solvency II,"This study examines the effect of the investment portfolio structure on insurers’ solvency, as measured by the Solvency Capital Requirement ratio. An empirical sample of 88 EU-based insurance groups was analyzed to provide robust evidence of the portfolio’s impact on the Solvency Capital Requirement ratio from 2016 to 2022. Linear regression and supervised machine learning models, particularly extra trees regression, were used to predict the solvency ratios, with the latter outperforming the former. The investigation was supplemented with panel data analysis. Firm-specific factors, including, unit-linked and index-linked liabilities, firm size, investments in property, collective undertakings, bonds and equities, and the ratio of government bonds to corporate bonds and country-specific factors, such as life and non-life market concentration, domestic bond market development, private debt development, household spending, banking concentration, non-performing loans, and CO<inf>2</inf> emissions, were found to have an important effect on insurers’ solvency ratios. The novelty of this research lies in the investigation of the connection of solvency ratios with variables that prior studies have not yet explored, such as portfolio asset allocation, the life and non-life insurance market concentration, and unit-linked and index-linked products, via the employment of a battery of traditional and machine enhanced methods. Furthermore, it identifies the relation of solvency ratios with bond market development and investments in collective undertakings. Finally, it addresses the substantial solvency risks posed by the high banking sector concentration to insurers under Solvency II. © 2024 Elsevier B.V., All rights reserved.","Poufinas, T.; Siopi, E.",2024,10.3390/risks12120191,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213010620&doi=10.3390%2Frisks12120191&partnerID=40&md5=89d1209f8fc21c4be96e743edb1fe259,scopus,"This study investigates how investment portfolio structure affects insurance solvency, measured by the Solvency Capital Requirement (SCR) ratio, using data from 88 EU insurance groups between 2016 and 2022. It employed linear regression and machine learning models, with extra trees regression showing superior predictive performance. Key factors influencing solvency included unit-linked and index-linked liabilities, firm size, investments in property, collective undertakings, bonds, equities, and the ratio of government to corporate bonds. Country-specific factors like market concentration, bond market development, household spending, banking concentration, non-performing loans, and CO2 emissions also played a role. The research highlights novel connections between solvency ratios and variables like portfolio asset allocation, market concentration, unit-linked/index-linked products, bond market development, and investments in collective undertakings. It also addresses solvency risks associated with high banking sector concentration under Solvency II.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:00:22.034081
26003fee30aeb702,Investor attention and Google Search Volume Index: Evidence from an emerging market using quantile regression analysis,"This study investigates whether the investor attention measured by the Google Search Volume Index (GSVI) is effective in forecasting stock returns. The evolving literature on investor attention suggests that higher GSVI can predict higher returns for the first one or two weeks, but with a subsequent price reversal. We use a more recent dataset that covers S&P BSE 500 companies listed on the Indian stock exchange for 2012-2017 and employ the quantile regression approach because it alleviates the statistical problems arising from biased distribution data. The results suggest that a higher GSVI predicts positive and significant returns in the subsequent first and second weeks. Higher quantiles of GSVI experience higher excess returns. The panel cointegration test results support the findings regarding the cointegration of the GSVI and stock returns. Our empirical evidence shows that our model is robust when using a trading strategy based on the Fama-French four-factor model. Thus, the model with GSVI acts as a better predictor of both the direction and magnitude of the excess returns than the model without GSVI.","Swamy, Vighneswara; Dharani, M.; Takeda, Fumiko",2019,10.1016/j.ribaf.2019.04.010,,wos,"This study examines the predictive power of Google Search Volume Index (GSVI) for stock returns in the Indian emerging market (S&P BSE 500, 2012-2017). Using quantile regression, it finds that higher GSVI predicts positive and significant returns in the first two weeks, with higher quantiles of GSVI yielding higher excess returns. Panel cointegration tests support the GSVI-stock return relationship, and a trading strategy based on the Fama-French four-factor model confirms the model's robustness. The GSVI model is shown to be a better predictor of excess returns than models without it.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:00:30.040093
60a0da6f58c3e011,Investor flows and the 2008 boom/bust in oil prices,"This paper explores the impact of investor flows and financial market conditions on returns in crude oil futures markets. I argue that informational frictions and the associated speculative activity may induce prices to drift away from ""fundamental"" values, and may result in price booms and busts. Particular attention is given to the interplay between imperfect information about real economic activity, including supply, demand, and inventory accumulation, and speculative activity in oil markets. Furthermore, I present new evidence that there were economically and statistically significant effects of investor flows on futures prices, after controlling for returns in the United States and emerging-economy stock markets, a measure of the balance sheet flexibility of large financial institutions, open interest, the futures/spot basis, and lagged returns on oil futures. The largest impacts on futures prices were from intermediate-term growth rates of index positions and managed-money spread positions. Moreover, my findings suggest that these effects were through risk or informational channels distinct from changes in convenience yield. Finally, the evidence suggests that hedge fund trading in spread positions in futures impacted the shape of term structure of oil futures prices. © 2014 INFORMS. © 2014 Elsevier B.V., All rights reserved.","Singleton, K.J.",2014,10.1287/mnsc.2013.1756,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894362096&doi=10.1287%2Fmnsc.2013.1756&partnerID=40&md5=7afb35ff36a50061f0a050888e34e1ed,scopus,"This paper investigates how investor flows and financial market conditions influence crude oil futures prices, suggesting that informational frictions and speculation can lead to price booms and busts. It provides evidence that investor flows significantly impact futures prices, distinct from changes in convenience yield, and that hedge fund trading affects the term structure of oil futures prices.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:00:33.413686
2842d62ce44c72c3,Is BRCA Mutation Testing Cost Effective for Early Stage Breast Cancer Patients Compared to Routine Clinical Surveillance? The Case of an Upper Middle-Income Country in Asia,"Objective Previous studies showed that offering BRCA mutation testing to population subgroups at high risk of harbouring the mutation may be cost effective, yet no evidence is available for low- or middle-income countries (LMIC) and in Asia. We estimated the cost effectiveness of BRCA mutation testing in early-stage breast cancer patients with high pre-test probability of harbouring the mutation in Malaysia, an LMIC in Asia. Methods We developed a decision analytic model to estimate the lifetime costs and quality-adjusted life-years (QALYs) accrued through BRCA mutation testing or routine clinical surveillance (RCS) for a hypothetical cohort of 1000 early-stage breast cancer patients aged 40 years. In the model, patients would decide whether to accept testing and to undertake risk-reducing mastectomy, oophorectomy, tamoxifen, combinations or neither. We calculated the incremental cost-effectiveness ratio (ICER) from the health system perspective. A series of sensitivity analyses were performed. Results In the base case, testing generated 11.2 QALYs over the lifetime and cost US$4815 per patient whereas RCS generated 11.1 QALYs and cost US$4574 per patient. The ICER of US$2725/QALY was below the cost-effective thresholds. The ICER was sensitive to the discounting of cost, cost of BRCA mutation testing and utility of being risk-free, but the ICERs remained below the thresholds. Probabilistic sensitivity analysis showed that at a threshold of US$9500/QALY, 99.9% of simulations favoured BRCA mutation testing over RCS. Conclusions Offering BRCA mutation testing to early-stage breast cancer patients identified using a locally-validated risk-assessment tool may be cost effective compared to RCS in Malaysia.",,2018,10.1007/s40258-018-0384-8,,proquest,"This study evaluated the cost-effectiveness of BRCA mutation testing versus routine clinical surveillance for early-stage breast cancer patients in Malaysia, an upper middle-income Asian country. Using a decision analytic model, the study found that BRCA mutation testing was cost-effective, generating more quality-adjusted life-years (QALYs) at an incremental cost-effectiveness ratio (ICER) below the established thresholds. Sensitivity analyses confirmed the robustness of these findings.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:00:37.741039
d398e1bb9cf578bc,Iterative and Recursive Estimation in Structural Nonadaptive Models,"An inference method, called latent backfitting, is proposed. This method appears well suited for econometric models where the structural relationships of interest define the observed endogenous variables as a known function of unobserved state variables and unknown parameters. This nonlinear state-space specification paves the way for iterative or recursive EM-like strategies. In the E steps, the state variables are forecasted given the observations and a value of the parameters. In the M steps, these forecasts are used to deduce estimators of the unknown parameters from the statistical model of latent variables. The proposed iterative/recursive estimation is particularly useful for latent regression models and for dynamic equilibrium models involving latent state variables. Practical implementation issues are discussed through the example of term structure models of interest rates. © 2020 Elsevier B.V., All rights reserved.","Pastorello, S.; Patilea, V.; Renault, E.",2003,10.1198/073500103288619124,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0242318897&doi=10.1198%2F073500103288619124&partnerID=40&md5=89bd95f11924d290104a4169ffefdf21,scopus,"This paper introduces latent backfitting, an inference method for econometric models where structural relationships define observed endogenous variables as a function of unobserved state variables and parameters. It uses iterative or recursive EM-like strategies for forecasting state variables and estimating parameters. The method is applicable to latent regression and dynamic equilibrium models, with an example of term structure models of interest rates.",True,True,True,gemini-2.5-flash-lite,Olav,M,,2025-10-13T16:01:00.023771
e63ce5fd8a54f209,LAND OF ADDICTS? AN EMPIRICAL INVESTIGATION OF HABIT-BASED ASSET PRICING MODELS,"This paper Studies the ability of a general class of habit-based asset pricing models to match the conditional moment restrictions implied by asset pricing theory. We treat the functional form of the habit as unknown, and estimate it along with the rest of the model's finite dimensional parameters. Using quarterly data on consumption growth, assets returns and instruments, our empirical results indicate that the estimated habit function is nonlinear, that habit formation is better described as internal rather than external, and the estimated time-preference parameter and the power utility parameter are sensible. In addition, the estimated habit function generates a positive stochastic discount factor (SDF) proxy and performs well in explaining cross-sectional stock return data. We find that an internal habit SDF proxy can explain a cross-section of size and book-market sorted portfolio equity returns better than (i) the Fama and French (1993) three-factor model, (ii) the Lettau and LUdvigson (2001b) scaled consumption CAPM model, (iii) an external habit SDF proxy, (iv) the classic CAPM, and (v) the classic consumption CAPM. Copyright (C) 2009 John Wiley & Sons, Ltd.","Chen, Xiaohong; Ludvigson, Sydney C.",2009,10.1002/jae.1091,,wos,"This empirical study investigates habit-based asset pricing models by estimating an unknown habit function using quarterly data. The findings suggest a nonlinear, internal habit formation, with sensible parameter estimates. The model effectively explains cross-sectional stock returns, outperforming several benchmark models including the Fama-French three-factor model and scaled consumption CAPM.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:01:04.032971
c7ba5d09c7bc1893,LSTM Framework Design and Volatility Research on Intelligent Forecasting Model for Solving the Parallel Dislocation Problem,"The yield of treasury bonds is the benchmark interest rate in the financial market which is worth predicting and judging. Based on the Long Short-Term Memory (LSTM) neural network model in deep learning, combined with the vector autoregression method (VAR), this paper creatively constructs the VAR-LSTM framework and uses the predicted values of macroeconomic variables and lagged value of the time sequence as input factors to solve the problem of “parallel dislocation” of the fitting results of the traditional LSTM model which significantly improves the prediction accuracy. In order to meet the requirements of active quantitative investment for high precision prediction of stock market index, adaptive noise complete ensemble empirical mode decomposition (EMD) is introduced into the modeling of stock market index prediction. Combined with the efficient modeling ability of long-term and short-term memory network for medium- and long-term dependence of complex series, using the idea of “decomposition-reorganization-prediction-integration”, an integrated prediction method of stock market index CEEMDAN-LSTM is proposed. CEEMDAN is used to decompose and reconstruct the index to obtain its high and low frequency components and trend items. The LSTM prediction models of each component are constructed respectively and the IMF reorganization mode of high frequency subseries is optimized. Then the overall predicted value of the index is obtained by adding and integrating the predicted values of each component. Taking five representative stock market indexes as test data, the prediction results of CEEMDAN-LSTM and mainstream financial time series machine learning modeling methods are compared systematically. The results show that for treasury bond yield series, the prediction accuracy of ARIMA model is higher than that of general LSTM method, while VAR-LSTM model is better than ARIMA model. The prediction error in the training set and the test set is reduced by about 55% and 50% respectively, and the prediction accuracy of the change direction is improved by about 5% and 8% respectively, which has higher application value. The prediction performance of CEEMDAN-LSTM is consistently better than that of existing modeling methods, and has less prediction error and lower lag.",,2021,10.1088/1742-6596/1982/1/012028,,proquest,"This paper proposes two novel forecasting models, VAR-LSTM and CEEMDAN-LSTM, to improve the prediction accuracy of financial time series, specifically treasury bond yields and stock market indexes. The VAR-LSTM model combines Vector Autoregression with Long Short-Term Memory (LSTM) networks to address the ""parallel dislocation"" issue in traditional LSTM models. The CEEMDAN-LSTM model utilizes adaptive noise complete ensemble empirical mode decomposition (CEEMDAN) to decompose and reconstruct stock market indexes, followed by LSTM prediction on individual components. Empirical results demonstrate that both VAR-LSTM and CEEMDAN-LSTM outperform existing methods like ARIMA and general LSTM in terms of prediction accuracy and error reduction.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T16:01:20.184316
0fda4c9a6ad9595b,LSTM–GARCH Hybrid Model for the Prediction of Volatility in Cryptocurrency Portfolios,"In the present work, the volatility of the leading cryptocurrencies is predicted through generalised autoregressive conditional heteroskedasticity (GARCH) models, multilayer perceptron (MLP), long short-term memory (LSTM), and hybrid models of the type LSTM and GARCH, where parameters of the GARCH family are included as features of LSTM models. The study period covered the scenario of the World Health Organization pandemic declaration around March 2020 at hourly frequency. We have found that the different variants of deep neural network models outperform those of the GARCH family in the sense of the hetorerocedastic error, and absolute and squared error (HSE). Under the sharpe ratio, the volatility forecasting of a uniform portfolio at long horizons systematically outperforms the stablecoin Tether, which is considered here as the risk-free asset. Also, including transaction volume helps reduce the value at risk or loss probability for the uniform portfolio. Moreover, in a minimum variance portfolio, it is observed that before the pandemic declaration, a large proportion of the capital was allocated to bitcoin (BTC). In contrast, after March 2020, the portfolio is more diversified with short positions for BTC. Moreover, the MLP models give the best predictive results, although not statistically different in accuracy compared to the LSTM and LSTM–GARCH versions under the Diebold–Mariano test. In sum, MLP models outperform most stylised financial models and are less computationally expensive than more complex neural networks. Therefore, simple learning models are suggested in highly non-linear time series volatility forecasts as it is the cryptocurrency market.",,2024,10.1007/s10614-023-10373-8,,proquest,"This study compares LSTM, GARCH, MLP, and hybrid LSTM-GARCH models for predicting cryptocurrency portfolio volatility during the COVID-19 pandemic. Deep learning models, particularly MLP, generally outperformed GARCH models. The study also found that volatility forecasting for a uniform portfolio exceeded the performance of Tether as a risk-free asset, and including transaction volume reduced risk. Portfolio allocation shifted significantly after the pandemic declaration. MLP models are recommended for their predictive accuracy and computational efficiency in volatile, non-linear markets.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:01:25.776294
7f77847f3cb759f0,"Labor Links, Comovement, and Predictable Returns","Using firms' online job postings, we identify economically related peer firms in the labor market. Firms' labor peers are vastly different from their industry peers, where the overlap is about 20%. Returns of labor-linked firms strongly comove, suggesting common responses to labor market shocks on average. However, industry shocks can affect firms outside the industry through the labor network, leading to substitution effects between labor peers. Lastly, we show that investors do not promptly incorporate news about labor-linked firms, leading to predictable subsequent returns. A long-short strategy exploiting this delay generates an average annualized excess return of 9%. © 2025 Elsevier B.V., All rights reserved.","Liu, Y.; Wu, X.",2025,10.1017/s0022109025101610,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006901938&doi=10.1017%2FS0022109025101610&partnerID=40&md5=bd128e8bf860437c6f596da829c7b48e,scopus,"This study utilizes firms' online job postings to identify labor-related peer firms, which differ significantly from industry peers. It finds that returns of labor-linked firms comove strongly, indicating shared responses to labor market shocks. The research also reveals that industry shocks can propagate through the labor network, causing substitution effects among labor peers. Furthermore, the study demonstrates that investors are slow to react to news about labor-linked firms, resulting in predictable future returns. A trading strategy based on this delay yields an average annualized excess return of 9%.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:01:32.411429
3b8bb9a74be525f1,Leveraging latent representations for milk yield prediction and interpolation using deep learning,"In this study, we propose a lactation model that estimates the daily milk yield by using autoencoders to generate a latent representation of all milk yields observed during the entire lactation cycle, irrespective of the length of the time interval between the different measurements. More specifically, we propose a sequential autoencoder (SAE) to process the sequential data, extract and decode the low-dimensional representations and generate the milk yield sequences. The SAE is compared with a more traditional multilayer perceptron model (MLP) which uses herd and parity information and lagged milk yields as input. Results show that incorporating the recorded daily milk yields, lactation number, herd statistics as well as reproduction and health events the cow encountered during the lactation cycle results in the most qualitative latent representations. Moreover, by leveraging these low-dimensional encodings, the SAE reconstructed the entire milk yield curve with a higher accuracy than the MLP. Hence, we present a framework that is able to infer missing milk yields along the entire lactation curve which facilitates selection and culling decisions as well as the estimation of future earnings and costs. Furthermore, the model allows farmers to enhance their animal monitoring systems as it incorporates the sequence of health and reproduction events to forecast the cow's future productivity.",,2020,10.1016/j.compag.2020.105600,,proquest,"This study proposes a lactation model using sequential autoencoders (SAE) to predict daily milk yield by creating latent representations of lactation cycles. The SAE, which incorporates milk yields, lactation number, herd statistics, and health/reproduction events, outperforms a traditional multilayer perceptron (MLP) in reconstructing milk yield curves and inferring missing yields. This framework aids in selection, culling, financial estimations, and enhances animal monitoring by forecasting productivity.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:01:36.441998
b43fdd1652b6f7bb,Leveraging multi-time-span sequences and feature correlations for improved stock trend prediction,"Accurate stock trend prediction is critical for informed investment decisions and the stability of financial markets. However, existing methodologies often overlook fine-grained stock price volatility and fail to incorporate a comprehensive spectrum of technical indicators, inadequately capturing the complex interrelationships fundamental to technical analysis. This paper proposes MSFCE, a novel framework for stock market trend prediction that enhances feature correlations across multi-time-span sequences. Specifically, MSFCE designs a multi-scale feature encoder to capture both intraday and daily features, which are processed through a Transformer-based dimensionally adaptive encoder. Furthermore, the framework leverages higher-order interactions among technical indicators via a graph attention network, dynamically modeling their interdependencies to improve prediction robustness in dynamic markets. Extensive experiments on the SSE50 and CSI300 datasets demonstrate that MSFCE significantly outperforms existing state-of-the-art methods, consistently exhibiting superior performance across multiple test periods and market conditions. Its strong prediction accuracy and risk management suggest practical applicability in trading strategies, yielding significant excess returns in empirical backtests. © 2024 Elsevier B.V., All rights reserved.","Li, Y.; Zhuang, M.; Wang, J.; Zhou, J.",2025,10.1016/j.neucom.2024.129218,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213080195&doi=10.1016%2Fj.neucom.2024.129218&partnerID=40&md5=34da531abdfb33b8628e81aa8a89243b,scopus,"This paper introduces MSFCE, a novel framework for stock market trend prediction that improves feature correlations across multi-time-span sequences. It uses a multi-scale feature encoder and a Transformer-based encoder to capture intraday and daily features, and a graph attention network to model interdependencies among technical indicators. Experiments on SSE50 and CSI300 datasets show MSFCE outperforms existing methods, demonstrating practical applicability in trading strategies.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:01:41.042393
6603d19b5d0f44e3,Likelihood-based scoring rules for comparing density forecasts in tails,"We propose new scoring rules based on conditional and censored likelihood for assessing the predictive accuracy of competing density forecasts over a specific region of interest, such as the left tail in financial risk management. These scoring rules can be interpreted in terms of Kullback-Leibler divergence between weighted versions of the density forecast and the true density. Existing scoring rules based on weighted likelihood favor density forecasts with more probability mass in the given region, rendering predictive accuracy tests biased toward such densities. Using our novel likelihood-based scoring rules avoids this problem. (C) 2011 Elsevier B.V. All rights reserved.","Diks, Cees; Panchenko, Valentyn; van Dijk, Dick",2011,10.1016/j.jeconom.2011.04.001,,wos,"This paper introduces new scoring rules for comparing density forecasts, focusing on specific regions of interest like financial risk management tails. These rules are based on conditional and censored likelihood and address biases in existing methods that favor forecasts with more mass in the region of interest.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:01:44.986139
1bbee78dc006f8fe,Linear Gaussian affine term structure models with unobservable factors: Calibration and yield forecasting,"This paper provides a significant numerical evidence for out-of-sample forecasting ability of linear Gaussian interest rate models with unobservable underlying factors. We calibrate one, two and three factor linear Gaussian models using the Kalman filter oil two different bond yield data sets and compare their out-of-sample forecasting performance. One-step ahead as well as four-step ahead out-of-sample forecasts are analyzed based on the weekly data. When evaluating the one-step ahead forecasts, it is shown that a one factor model may be adequate when only the short-dated or only the long-dated yields are considered, but two and three factor models performs significantly better when the entire yield spectrum is considered. Furthermore, the results demonstrate that the predictive ability of multi-factor models remains intact far ahead out-of-sample, with accurate predictions available up to one year after the last calibration for one data set and up to three months after the last calibration for the second, more volatile data set. The experimental data denotes two different periods with different yield volatilities, and the stability of model parameters after calibration ill both the cases is deemed to be both significant and practically useful. When it comes to four-step ahead predictions, the quality of forecasts deteriorates for all models, as can be expected, but the advantage of using a multi-factor model as compared to a one factor model is still significant.In addition to the empirical study above, we also suggest a non-linear filter based on linear programming for improving the term structure matching at a given point in time. This method, when used in place of a Kalman filter update, improves the term structure fit significantly with a minimal added computational overhead. The improvement achieved with the proposed method is illustrated for out-of-sample data for both the data sets. This method call be used to model a parameterized yield curve consistently with the underlying short rate dynamics. (c) 2008 Elsevier B.V. All rights reserved.","Date, Paresh; Wang, Chieh",2009,10.1016/j.ejor.2008.01.035,,wos,"This paper investigates the out-of-sample forecasting performance of linear Gaussian interest rate models with unobservable factors. It calibrates one, two, and three-factor models using the Kalman filter on two bond yield datasets and compares their forecasting accuracy for one-step and four-step ahead predictions. The study finds that multi-factor models generally outperform one-factor models, especially when considering the entire yield spectrum. The paper also proposes a non-linear filter based on linear programming to improve term structure matching, which enhances out-of-sample fit with minimal computational cost.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T16:01:57.441423
97d99bf76069f7fb,Linear and non-linear filtering in mathematical finance: a review,This paper presents a review of time series filtering and its applications in mathematical finance. A summary of results of recent empirical studies with market data are presented for yield curve modelling and stochastic volatility modelling. The paper also outlines different approaches to filtering of non-linear time series.,P. Date; K. Ponomareva,2011,10.1093/imaman/dpq008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8133776,ieeexplore,"This review paper discusses time series filtering and its applications in mathematical finance, covering yield curve and stochastic volatility modeling with empirical results. It also explores methods for filtering non-linear time series.",True,True,False,gemini-2.5-flash-lite,Olav,Review,,2025-10-13T16:02:10.576478
3ea0c55e5317e324,Linear and nonlinear predictability in investment style factors: Multivariate evidence,"This paper studies the predictive performance of multivariate models at forecasting the (excess) returns of portfolios mimicking the Market, Size, Value, Momentum, and Low Volatility factors isolated in asset pricing research. We evaluate the accuracy of the point forecasts of a number of linear and regime-switching models in recursive, out-of-sample forecasting experiments. We assess the accuracy of the models using several measures of unbiasedness and predictive accuracy, and using Diebold and Mariano's approach to test whether differences in expected losses from all possible pairs of forecast models are statistically significant. We fail to find evidence that complex statistical models are uniformly more accurate than a naïve constant expected return model for factor-mimicking portfolio (excess) returns. However, we show that it is possible to build simple portfolio strategies that profit from the higher out-of-sample predictive accuracy of forecasting models with Markov switching in conditional mean coefficients. These results appear to be independent of the forecasting horizon and robust to changes in the loss function that captures the investors' objectives. © 2017 Elsevier B.V., All rights reserved.","Chincoli, F.; Guidolin, M.",2017,10.1057/s41260-017-0048-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018248298&doi=10.1057%2Fs41260-017-0048-5&partnerID=40&md5=5b890a8c51620f9b5b10a05c7bfc2bdc,scopus,"This paper investigates the predictive performance of linear and regime-switching models for forecasting excess returns of investment style factors (Market, Size, Value, Momentum, Low Volatility). While complex models did not uniformly outperform a naive model, simple strategies could profit from Markov-switching models. The results were robust across forecasting horizons and loss functions.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:02:27.307623
5ab29547914291e7,"Liquidity shocks, business cycles and asset prices","In the aftermath of the Great Recession, macro models that feature financing constraints have attracted increasing attention. Among these, Kiyotaki et al. (2012) is a prominent example. In this paper, we investigate whether the liquidity shocks and financial frictions proposed by Kiyotaki et al. (2012) can improve the asset pricing predictions of the frictionless RBC model. We study the quantitative business cycle and asset pricing properties in an economy in which agents feature recursive preferences, are subject to a liquidity constraint, and suffer liquidity shocks. We find that the model predicts highly nonlinear time variation and levels of risk premia, which are driven by endogenous fluctuations in equity prices. However, the model fails to account for a basic fact: Periods of scarce liquidity are associated with high asset prices and low expected returns. © 2020 Elsevier B.V., All rights reserved.","Bigio, S.; Schneider, A.",2017,10.1016/j.euroecorev.2017.05.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021406771&doi=10.1016%2Fj.euroecorev.2017.05.004&partnerID=40&md5=7f22dc6c370cebd17926a00725e416ae,scopus,"This paper investigates whether liquidity shocks and financial frictions, as proposed by Kiyotaki et al. (2012), can enhance the asset pricing predictions of the frictionless RBC model. The study incorporates recursive preferences, liquidity constraints, and liquidity shocks. While the model generates significant time variation in risk premia driven by equity prices, it fails to explain the observed phenomenon where periods of scarce liquidity coincide with high asset prices and low expected returns.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:02:29.905767
aa8630690d917199,"Liquidity, volume and volatility in US electricity futures: The case of Palo Verde","Previous research on liquidity has studied the relationships between liquidity, trading activity and volatility, mostly with data from US Treasury securities, stocks and foreign exchange spot markets. Liquidity in futures markets, especially electricity futures, has received little attention. However, liquidity in futures is expected to behave differently to that in spot markets because of the unique asymmetries in futures markets. Liquidity in electricity markets is of interest in countries where these markets are being deregulated. This study estimates these relationships for the Palo Verde electricity futures contract. The results show positive relations between all three pairs of key variables. © 2006 Elsevier B.V., All rights reserved.","Goss, B.",2006,10.1080/17446540500396974,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33744995794&doi=10.1080%2F17446540500396974&partnerID=40&md5=1fbf6045b943bee0079a5367dc3f2a90,scopus,"This study examines the relationships between liquidity, trading volume, and volatility in the Palo Verde electricity futures contract. It finds positive correlations between these variables, contrasting with previous research primarily focused on financial markets like Treasury securities, stocks, and foreign exchange. The research highlights the unique dynamics of futures markets, particularly in deregulating electricity sectors.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:02:31.873170
0082a75f3674363a,Local currency bond risk premia: A panel evidence on emerging markets,"This paper investigates the sources of variation in emerging market (EM) local currency bond risk premium. We find that macroeconomic and financial variables contain valuable information in explaining local currency bond excess returns. Additionally, we extend our analysis to investigate how the influence of different factors change depending on the level of global risk appetite. Although macro fundamentals have an important role in explaining the risk premiums during tranquil times, investors pay less attention to changes in inflation forecast in times of high risk aversion. Positive credit rating changes decrease the bond risk premium in both regimes with a different magnitude. Also, the influence of exchange rate volatility is more pronounced during the time of market stress. © 2019 Elsevier B.V., All rights reserved.","Cepni, O.; Güney, I.E.",2019,10.1016/j.ememar.2019.01.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060970321&doi=10.1016%2Fj.ememar.2019.01.002&partnerID=40&md5=13091778147b29cc4c0611f40f380d9f,scopus,"This paper examines the factors influencing emerging market local currency bond risk premiums, utilizing macroeconomic and financial variables. It finds that while macro fundamentals are important during stable periods, inflation forecast changes are less influential during high risk aversion. Credit rating changes consistently reduce risk premiums, and exchange rate volatility has a greater impact during market stress.",False,True,False,gemini-2.5-flash-lite,Olav,M,,2025-10-13T16:02:52.718591
3353e1adf55ac4eb,Local lagged adapted generalized method of moments and applications,"In this work, an attempt is made for developing the local lagged adapted generalized method of moments (LLGMM). This proposed method is composed of: 1) development of the stochastic model for continuous-time dynamic process; 2) development of the discrete-time interconnected dynamic model for statistic process; 3) utilization of Euler-type discretized scheme for nonlinear and nonstationary system of stochastic differential equations; 4) development of generalized method of moment/observation equations by employing lagged adaptive expectation process; 5) introduction of the conceptual and computational parameter estimation problem; 6) formulation of the conceptual and computational state estimation scheme; and 7) definition of the conditional mean square epsilon -best sub-optimal procedure. The development of LLGMM is motivated by parameter and state estimation problems in continuous-time nonlinear and nonstationary stochastic dynamic model validation problems in biological, chemical, engineering, financial, medical, physical, and social sciences. The byproducts of LLGMM are the balance between model specification and model prescription of continuous-time dynamic process and the development of discrete-time interconnected dynamic model of local sample mean and variance statistic process (DTIDMLSMVSP). DTIDMLSMVSP is the generalization of statistic (sample mean and variance) drawn from the static dynamic population problems. Moreover, it is also an alternative approach to the GARCH (1,1) model and its many related variant models (e.g., EGARCH model, GJR GARCH model). It provides an iterative scheme for updating statistic coefficients in a system of generalized method of moment/observation equations. Furthermore, application of the LLGMM method to stochastic differential dynamic models for energy commodity price, U.S. Treasury bill yield interest rate U.S.-U.K. foreign exchange rate exhibits its unique role and scope.",,2017,10.1080/07362994.2016.1213640,,proquest,"This paper introduces the Local Lagged Adapted Generalized Method of Moments (LLGMM), a novel statistical method for parameter and state estimation in continuous-time nonlinear and nonstationary stochastic dynamic models. LLGMM involves developing stochastic and discrete-time models, utilizing Euler-type discretization, employing lagged adaptive expectation for moment equations, and formulating parameter and state estimation schemes. It is motivated by applications in various scientific fields and offers an alternative to GARCH models. The method is applied to models for energy commodity prices, U.S. Treasury bill yields, and U.S.-U.K. foreign exchange rates.",True,True,True,gemini-2.5-flash-lite,Olav,M,,2025-10-13T16:03:28.600741
bb72826f453f4ef9,Long-term fiscal implications of funding assisted reproduction: a generational accounting model for Spain,"The aim of this study was to assess the lifetime economic benefits of assisted reproduction in Spain by calculating the return on this investment. We developed a generational accounting model that simulates the flow of taxes paid by the individual, minus direct government transfers received over the individual's lifetime. The difference between discounted transfers and taxes minus the cost of either IVF or artificial insemination (AI) equals the net fiscal contribution (NFC) of a child conceived through assisted reproduction. We conducted sensitivity analysis to test the robustness of our results under various macroeconomic scenarios. A child conceived through assisted reproduction would contribute €370,482 in net taxes to the Spanish Treasury and would receive €275,972 in transfers over their lifetime. Taking into account that only 75% of assisted reproduction pregnancies are successful, the NFC was estimated at €66,709 for IVF-conceived children and €67,253 for AI-conceived children. The return on investment for each euro invested was €15.98 for IVF and €18.53 for AI. The long-term NFC of a child conceived through assisted reproduction could range from €466,379 to €-9,529 (IVF) and from €466,923 to €-8,985 (AI). The return on investment would vary between €-2.28 and €111.75 (IVF), and €-2.48 and €128.66 (AI) for each euro invested. The break-even point at which the financial position would begin to favour the Spanish Treasury ranges between 29 and 41 years of age. Investment in assisted reproductive techniques may lead to positive discounted future fiscal revenue, notwithstanding its beneficial psychological effect for infertile couples in Spain.The aim of this study was to assess the lifetime economic benefits of assisted reproduction in Spain by calculating the return on this investment. We developed a generational accounting model that simulates the flow of taxes paid by the individual, minus direct government transfers received over the individual's lifetime. The difference between discounted transfers and taxes minus the cost of either IVF or artificial insemination (AI) equals the net fiscal contribution (NFC) of a child conceived through assisted reproduction. We conducted sensitivity analysis to test the robustness of our results under various macroeconomic scenarios. A child conceived through assisted reproduction would contribute €370,482 in net taxes to the Spanish Treasury and would receive €275,972 in transfers over their lifetime. Taking into account that only 75% of assisted reproduction pregnancies are successful, the NFC was estimated at €66,709 for IVF-conceived children and €67,253 for AI-conceived children. The return on investment for each euro invested was €15.98 for IVF and €18.53 for AI. The long-term NFC of a child conceived through assisted reproduction could range from €466,379 to €-9,529 (IVF) and from €466,923 to €-8,985 (AI). The return on investment would vary between €-2.28 and €111.75 (IVF), and €-2.48 and €128.66 (AI) for each euro invested. The break-even point at which the financial position would begin to favour the Spanish Treasury ranges between 29 and 41 years of age. Investment in assisted reproductive techniques may lead to positive discounted future fiscal revenue, notwithstanding its beneficial psychological effect for infertile couples in Spain.",,2015,10.1016/j.rbms.2016.04.001,,proquest,"This study uses a generational accounting model to assess the long-term fiscal implications of assisted reproduction in Spain. It estimates the net fiscal contribution (NFC) and return on investment (ROI) for children conceived through IVF and artificial insemination, finding positive fiscal returns. The break-even point for the Spanish Treasury is estimated between 29 and 41 years of age.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:03:32.901652
29399bb2b4b0fe0c,Machine learning algorithms applied to the estimation of liquidity: the 10-year United States treasury bond,"PurposeHaving defined liquidity, the aim is to assess the predictive capacity of its representative variables, so that economic fluctuations may be better understood.Design/methodology/approachConceptual variables that are representative of liquidity will be used to formulate the predictions. The results of various machine learning models will be compared, leading to some reflections on the predictive value of the liquidity variables, with a view to defining their selection.FindingsThe predictive capacity of the model was also found to vary depending on the source of the liquidity, in so far as the data on liquidity within the private sector contributed more than the data on public sector liquidity to the prediction of economic fluctuations. International liquidity was seen as a more diffuse concept, and the standardization of its definition could be the focus of future studies. A benchmarking process was also performed when applying the state-of-the-art machine learning models.Originality/valueBetter understanding of these variables might help us toward a deeper understanding of the operation of financial markets. Liquidity, one of the key financial market variables, is neither well-defined nor standardized in the existing literature, which calls for further study. Hence, the novelty of an applied study employing modern data science techniques can provide a fresh perspective on financial markets.",,2024,10.1108/ejmbe-06-2022-0176,,proquest,This study assesses the predictive capacity of liquidity-representative variables using various machine learning models applied to the 10-year United States Treasury bond. It found that private sector liquidity data was more predictive of economic fluctuations than public sector liquidity data. The research highlights the need for better definition and standardization of liquidity in financial markets and offers a fresh perspective using modern data science techniques.,True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T16:03:41.211292
a317ea4fb2e44a47,Machine-Learning-Based Return Predictors and the Spanning Controversy in Macro-Finance,"We propose a two-step machine learning algorithm-the Supervised Adaptive Group LASSO (SAGLasso) method-that is suitable for constructing parsimonious return predictors from a large set of macro variables. We apply this method to government bonds and a set of 917 macro variables and construct a new, transparent, and easy-to-interpret macro variable with significant out-of-sample predictive power for excess bond returns. This new macro factor, termed the SAGLasso factor, is a linear combination of merely 30 selected macro variables out of 917. Furthermore, it can be decomposed into three sublevel factors: a novel housing factor, an employment factor, and an inflation factor. Importantly, the predictive power of the SAGLasso factor is robust to bond yields, namely, the SAGLasso factor is not spanned by bond yields. Moreover, we show that the unspanned variation of the SAGLasso factor cannot be attributed to yield measurement error or macro measurement error. The SAGLasso factor therefore provides a potential resolution to the spanning controversy in the macro-finance literature.",,2023,10.1287/mnsc.2022.4386,,proquest,"This paper introduces the Supervised Adaptive Group LASSO (SAGLasso) method, a two-step machine learning algorithm for building parsimonious return predictors from a large set of macro variables. Applied to government bonds and 917 macro variables, it constructs a new, transparent, and interpretable macro factor (SAGLasso factor) with significant out-of-sample predictive power for excess bond returns. This factor, a linear combination of 30 selected variables, decomposes into housing, employment, and inflation factors. Crucially, its predictive power is robust and not spanned by bond yields, addressing the spanning controversy in macro-finance.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T16:03:57.646735
5080e4a5f3be7be5,Maximum likelihood estimation of partially observed diffusion models,"This paper develops a maximum likelihood (ML) method to estimate partially observed diffusion models based on data sampled at discrete times. The method combines two techniques recently proposed in the literature in two separate steps. In the first step, the closed form approach of Ait-Sahalia (2008) is used to obtain a highly accurate approximation to the joint transition probability density of the latent and the observed states. In the second step, the efficient importance sampling technique of Richard and Zhang (2007) is used to integrate out the latent states, thereby yielding the likelihood function. Using both simulated and real data, we show that the proposed ML method works better than alternative methods. The new method does not require the underlying diffusion to have an affine structure and does not involve infill simulations. Therefore, the method has a wide range of applicability and its computational cost is moderate. (C) 2014 Elsevier B.V. All rights reserved.","Kleppe, Tore Selland; Yu, Jun; Skaug, Hans J.",2014,10.1016/j.jeconom.2014.02.002,,wos,"This paper proposes a maximum likelihood estimation method for partially observed diffusion models using discrete-time sampled data. It combines Ait-Sahalia's (2008) closed-form approximation for transition densities with Richard and Zhang's (2007) importance sampling technique to integrate out latent states and obtain the likelihood function. The method is shown to outperform alternatives on simulated and real data, offering broad applicability due to its independence from affine structures and infill simulations, with moderate computational cost.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:04:00.948237
00a73739f92c0789,Measuring Granger Causality in Quantiles,"We consider measures of Granger causality in quantiles, which detect and quantify both linear and nonlinear causal effects between random variables. The measures are based on nonparametric quantile regressions and defined as logarithmic functions of restricted and unrestricted expectations of quantile check loss functions. They can consistently be estimated by replacing the unknown expectations of check loss functions by their nonparametric kernel estimates. We derive a Bahadur-type representation for the nonparametric estimator of the measures. We establish the asymptotic distribution of this estimator, which can be used to build tests for the statistical significance of the measures. Thereafter, we show the validity of a smoothed local bootstrap that can be used in finite-sample settings to perform statistical tests. A Monte Carlo simulation study reveals that the bootstrap-based test has a good finite-sample size and power properties for a variety of data-generating processes and different sample sizes. Finally, we provide an empirical application to illustrate the usefulness of measuring Granger causality in quantiles. We quantify the degree of predictability of the quantiles of equity risk premium using the variance risk premium, unemployment rate, inflation, and the effective federal funds rate. The empirical results show that the variance risk premium and effective federal funds rate have a strong predictive power for predicting the risk premium when compared to that of the predictive power of the other two macro variables. In particular, the variance risk premium is able to predict the center, lower, and upper quantiles of the distribution of the risk premium; however, the effective federal funds rate predicts only the lower and upper quantiles. Nevertheless, unemployment and inflation rates have no effect on the risk premium. © 2021 Elsevier B.V., All rights reserved.","Song, X.; Taamouti, A.",2021,10.1080/07350015.2020.1739531,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083679836&doi=10.1080%2F07350015.2020.1739531&partnerID=40&md5=e9977f79be5e9c56482a636bb3b4d94d,scopus,"This paper introduces measures of Granger causality in quantiles, utilizing nonparametric quantile regressions to detect and quantify linear and nonlinear causal effects. The measures are estimated using nonparametric kernel estimates and their asymptotic distribution is derived for statistical testing. A smoothed local bootstrap is validated for finite-sample testing, showing good size and power in simulations. An empirical application demonstrates the usefulness of these measures by quantifying the predictability of equity risk premium quantiles using variance risk premium, unemployment rate, inflation, and the effective federal funds rate. The variance risk premium predicts all quantiles, while the federal funds rate predicts lower and upper quantiles, with unemployment and inflation having no significant effect.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:04:04.940603
f17ea1ec214a9104,Measuring Nonlinear Granger Causality in Mean,"We propose model-free measures for Granger causality in mean between random variables. Unlike the existing measures, ours are able to detect and quantify nonlinear causal effects. The new measures are based on nonparametric regressions and defined as logarithmic functions of restricted and unrestricted mean square forecast errors. They are easily and consistently estimated by replacing the unknown mean square forecast errors by their nonparametric kernel estimates. We derive the asymptotic normality of nonparametric estimator of causality measures, which we use to build tests for their statistical significance. We establish the validity of smoothed local bootstrap that one can use in finite sample settings to perform statistical tests. Monte Carlo simulations reveal that the proposed test has good finite sample size and power properties for a variety of data-generating processes and different sample sizes. Finally, the empirical importance of measuring nonlinear causality in mean is also illustrated. We quantify the degree of nonlinear predictability of equity risk premium using variance risk premium. Our empirical results show that the variance risk premium is a very good predictor of risk premium at horizons less than 6 months. We also find that there is a high degree of predictability at the 1-month horizon, that can be attributed to a nonlinear causal effect. Supplementary materials for this article are available online. © 2018 Elsevier B.V., All rights reserved.","Song, X.; Taamouti, A.",2018,10.1080/07350015.2016.1166118,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018176704&doi=10.1080%2F07350015.2016.1166118&partnerID=40&md5=19c7fae0c027a8832819bbcf997de9c1,scopus,"This paper introduces novel model-free measures for Granger causality in mean, capable of detecting and quantifying nonlinear causal effects. These measures are based on nonparametric regressions and estimated using kernel methods. The study derives the asymptotic normality of the estimators, enabling statistical significance tests, and validates a smoothed local bootstrap for finite sample settings. Monte Carlo simulations demonstrate good size and power properties. The authors illustrate the empirical importance by quantifying nonlinear predictability of equity risk premium using variance risk premium, finding significant nonlinear causality at short horizons.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:04:08.596250
bcd2cc2bd87fa5d9,Measuring Risk Premiums Using Financial Reports and Actuarial Disclosures,"Insurance companies increasingly augment their financial reports by releasing actuarial measures the so-called embedded value to supply information about the value of their life insurance activities. Both accounting and actuarial measures differ with respect to the timeliness of profit realisation and its reliability, and their performance in yielding information may differ. This paper asks if and how embedded values help in assessing risk premiums. We estimate multifactor market models in the spirit of Fama and French, and find that actuarial disclosures are superior to financial accounting in estimating these risk premiums. They further add information to financial reports as an estimator for growth opportunities.","Zimmermann, Jochen; Veith, Stefan; Schymczyk, Johannes",2015,10.1057/gpp.2014.17,,wos,"This paper investigates whether actuarial disclosures, specifically embedded values, improve the estimation of risk premiums compared to traditional financial reports for insurance companies. Using multifactor market models, the study finds that actuarial disclosures provide superior information for estimating risk premiums and offer additional insights into growth opportunities.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:04:12.599231
3bf14016cea7e254,Measuring sovereign contagion in Europe,"This paper analyzes sovereign risk shift-contagion, i.e. positive and significant changes in the propagation mechanisms, using bond yield spreads for the major eurozone countries. By emphasizing the use of two econometric approaches based on quantile regressions (standard quantile regression and Bayesian quantile regression with heteroskedasticity) we find that the propagation of shocks in euro's bond yield spreads shows almost no presence of shift-contagion in the sample periods considered (2003-2006, Nov. 2008-Nov. 2011, Dec. 2011-Apr. 2013). Shock transmission is no different on days with big spread changes and small changes. This is the case even though a significant number of the countries in our sample have been extremely affected by their sovereign debt and fiscal situations. The risk spillover among these countries is not affected by the size or sign of the shock, implying that so far contagion has remained subdued. However, the US crisis does generate a change in the intensity of the propagation of shocks in the eurozone between the 2003-2006 pre-crisis period and the Nov. 2008-Nov. 2011 post-Lehman one, but the coefficients actually go down, not up! All the increases in correlation we have witnessed over the last years come from larger shocks and the heteroskedasticity in the data, not from similar shocks propagated with higher intensity across Europe. These surprising, but robust, results emerge because this is the first paper, to our knowledge, in which a Bayesian quantile regression approach allowing for heteroskedasticity is used to measure contagion. This methodology is particularly well-suited to deal with nonlinear and unstable transmission mechanisms especially when asymmetric responses to sign and size are suspected. (C) 2017 Elsevier B.V. All rights reserved.","Caporin, Massimiliano; Pelizzon, Loriana; Ravazzolo, Francesco; Rigobon, Roberto",2018,10.1016/j.jfs.2017.12.004,,wos,"This paper investigates sovereign contagion in Europe using bond yield spreads for major eurozone countries. Employing quantile regression techniques, it finds limited evidence of significant shift-contagion, meaning the propagation mechanisms of shocks did not substantially change during the analyzed periods. While the US crisis influenced shock propagation intensity between pre- and post-Lehman periods, the effect was a decrease in coefficient values, not an increase. The study attributes observed increases in correlation to larger shocks and data heteroskedasticity rather than heightened shock intensity. The authors highlight the novelty of their Bayesian quantile regression approach, which is adept at handling nonlinear and unstable transmission mechanisms.",True,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:04:22.758226
d35fd1822311585b,Measuring the Financial Value of Marketing Strategy with Excess Stock Market Return,"This paper proposes excess stock market return as a way to measure the impact of marketing strategy on firm value. First, it provides an overview of event study method. An event study examines the excess return to a firm's stock price after the release of information that is relevant to the firm's financial success. Second, it shows how excess return captures a marketing strategy's impact on firm value. It presents a model that illustrates how a marketing strategy impacts consumers, future cash flows, firm value, investor's expectations, and excess return. Third, a comparison shows that excess return stacks up well against standard marketing metrics. Excess return yields unbiased estimates, allows direct causal inference, is future oriented, includes all cash flows, accounts for opportunity costs, factors in risk, and takes into account the time value of money.",,2014,10.4018/ijrcm.2014100101,,proquest,"This paper suggests using excess stock market return to quantify the financial impact of marketing strategies on firm value. It outlines the event study method and presents a model demonstrating how marketing strategies influence consumers, future cash flows, firm value, investor expectations, and ultimately, excess returns. The paper argues that excess return is a superior metric compared to standard marketing metrics due to its unbiased estimates, causal inference capabilities, future orientation, and consideration of all cash flows, opportunity costs, risk, and the time value of money.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:04:29.535267
e16d12b73176b407,Metals: resources or financial assets? A multivariate cross-sectional analysis,"Metals are very important resources for industrial production, but recently they have attracted more and more attention from investors. While certainly industrial producers, consumers, and financial investors do have some influence on metal price development, the role of relevant price factors is not yet quite clear. Therefore, in this paper, we examine the explanatory power of various fundamental factors and characteristics known from financial markets, specifically on the expected returns in a unique data sample of 30 metals. We apply-to our knowledge for the first time in this context-the widely accepted method of characteristic-sorted portfolios, extended by the very recent method of two-way portfolio sorts as an alternative to classical multivariate regressions. This mostly nonparametric approach, combined with portfolio aggregation, provides very robust results. Our major finding is that the financial characteristics value and momentum have a very high predictive power for monthly returns of metal portfolios. Metal-specific fundamental factors like stocks, secondary production, apparent consumption, country concentration, mine production, or reserves perform depending on the interpretation moderately well or rather poorly, regarding some economically interpretable transformations and when using multivariate two-way sorts. Hence, from the perspective of expected returns, metals are predominantly assets, while fundamental metal-specific factors still play a non-negligible role. Thus, to a much lesser extent, metals can still be regarded as resources. Overall, the combination of financial characteristics and metal-specific fundamental factors yields the best results. With these robust results, we hope to contribute to a better understanding of metal prices and their underlying factors.","Lutzenberger, Fabian; Gleich, Benedikt; Mayer, Herbert G.; Stepanek, Christian; Rathgeber, Andreas W.",2017,10.1007/s00181-016-1162-9,,wos,"This paper investigates whether metals are primarily industrial resources or financial assets by analyzing the predictive power of financial and fundamental factors on metal returns. Using characteristic-sorted portfolios and two-way sorts, the study finds that financial characteristics like value and momentum significantly predict metal returns, suggesting metals function more as financial assets. While metal-specific factors have some influence, their predictive power is weaker. The findings contribute to understanding metal price dynamics.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:04:35.176347
c4b4f1784aacf26c,Mind the gap: forecasting euro-area output gaps with machine learning,"In this paper, we use the Eurozone yield curve in an effort to forecast the deviations of the euro-area output (IPI) from its long-run trend. We use various short- and long-term interest rates spanning the period from 2004:9 to 2020:6 in monthly frequency. The interest rates are fed to three machine learning methodologies: Decision Trees, Random Forests, and Support Vector Machines (SVM). These Machine Learning methodologies are then compared to an Elastic-Net Logistic Regression (Logit) model from the area of Econometrics. According to the results, the optimal SVM model coupled with the RBF kernel outperforms the competition reaching an in-sample accuracy of 85.29% and an out-of-sample accuracy of 94.74%.",,2022,10.1080/13504851.2021.1963403,,proquest,"This paper forecasts euro-area output gaps using machine learning models (Decision Trees, Random Forests, SVM) trained on Eurozone yield curve data from 2004 to 2020. The optimal SVM model achieved high accuracy in both in-sample and out-of-sample predictions, outperforming an econometric Elastic-Net Logistic Regression model.",True,True,True,gemini-2.5-flash-lite,Olav,M,,2025-10-13T16:05:00.123258
f296d1ee3108d658,Model risk for European-style stock index options,"In empirical modeling, there have been two strands for pricing in the options literature, namely the parametric and nonparametric models. Often, the support for the nonparametric methods is based on a benchmark such as the Black-Scholes (BS) model with constant volatility. In this paper, we study the stochastic volatility (SV) and stochastic volatility random jump (SVJ) models as parametric benchmarks against feedforward neural network (FNN) models, a class of neural network models. Our choice for FNN models is due to their well-studied universal approximation properties of an unknown function and its partial derivatives. Since the partial derivatives of an option pricing formula are risk pricing tools, an accurate estimation of the unknown option pricing function is essential for pricing and hedging. Our findings indicate that FNN models offer themselves as robust option pricing tools, over their sophisticated parametric counterparts in predictive settings. There are two routes to explain the superiority of FNN models over the parametric models in forecast settings. These are normormality of return distributions and adaptive learning.","Gencay, Ramazan; Gibson, Rajna",2007,10.1109/tnn.2006.883005,,wos,"This paper compares the performance of feedforward neural network (FNN) models against parametric models (stochastic volatility and stochastic volatility random jump) for pricing European-style stock index options. The study finds that FNN models are more robust and superior in predictive settings, attributing this to the non-normality of return distributions and adaptive learning capabilities.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:05:04.697212
08344aa9e15b1cee,Model uncertainty in the cross-section of stock returns,"We develop a transparent Bayesian framework to measure uncertainty in asset pricing models. By assigning a modified class of g-priors to the risk prices of asset pricing factors, our method quantifies the trade-off between mean–variance efficiency and parsimony for asset pricing models to achieve high posterior probabilities. Model uncertainty is defined as the entropy of these model probabilities. We prove the model selection consistency property of our procedure, which is missing from the classic g-priors. Acknowledging the possibility of omitting true asset pricing factors in real applications, we also characterize the maximum degree of contamination that the omitted factors can introduce to our model uncertainty measure. Empirically, we find that model uncertainty escalates during major market events and carries a significantly negative risk premium of approximately half the magnitude of the market. Positive shocks to model uncertainty predict persistent outflows from US equity funds and inflows to Treasury funds. © 2025 Elsevier B.V., All rights reserved.",,2025,10.1016/j.jeconom.2025.106066,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011139721&doi=10.1016%2Fj.jeconom.2025.106066&partnerID=40&md5=b5f82e65537b031a0ee7bbc1e2956ee4,scopus,"This paper introduces a Bayesian framework to quantify uncertainty in asset pricing models by using g-priors on risk prices. It measures model uncertainty as the entropy of model probabilities and demonstrates model selection consistency. The study also assesses the impact of omitted factors and finds that model uncertainty increases during market events, carries a negative risk premium, and predicts fund flows.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:05:12.118379
7333ecab1e4cef3e,Modeling and predicting historical volatility in exchange rate markets,"Volatility modeling and forecasting of currency exchange rate is an important task in several business risk management tasks; including treasury risk management, derivatives pricing, and portfolio risk evaluation. The purpose of this study is to present a simple and effective approach for predicting historical volatility of currency exchange rate. The approach is based on a limited set of technical indicators as inputs to the artificial neural networks (ANN). To show the effectiveness of the proposed approach, it was applied to forecast US/Canada and US/Euro exchange rates volatilities. The forecasting results show that our simple approach outperformed the conventional GARCH and EGARCH with different distribution assumptions, and also the hybrid GARCH and EGARCH with ANN in terms of mean absolute error, mean of squared errors, and Theil's inequality coefficient. Because of the simplicity and effectiveness of the approach, it is promising for US currency volatility prediction tasks. © 2021 Elsevier B.V., All rights reserved.","Lahmiri, S.",2017,10.1016/j.physa.2016.12.061,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007518089&doi=10.1016%2Fj.physa.2016.12.061&partnerID=40&md5=9b4dde2750886471bb75756b62a43d05,scopus,"This study proposes a simple and effective approach for predicting historical volatility in currency exchange rates using technical indicators as inputs to artificial neural networks (ANN). The approach was applied to US/Canada and US/Euro exchange rates and outperformed conventional GARCH and EGARCH models, as well as hybrid GARCH-ANN models, in terms of various error metrics. The authors suggest this approach is promising for currency volatility prediction.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:05:15.931538
4a7f853347d7646b,Modeling spot rate using a realized stochastic volatility model with level effect and dynamic drift,"This paper proposes a class of realized stochastic volatility model based on both various realized volatility measures and spot rate. It applies the realized stochastic volatility model (Takahashi, Omori, & Watanabe, 2009, and Koopman & Scharth, 2013) to the spot rate model with dynamic drift and level effect setups (RSVL). A jointly approximated maximum likelihood procedure is used to estimate this model. The simulation results show that the RSVL model can be consistently estimated and noise-and-jump-robust realized volatility measures improve the accuracy of the estimation. This study empirically investigates the Chinese interbank repo market with RSVL model, which manifested the advantage of taking the level effect and nonlinear drift into consideration. The noise-and-jump-robust realized volatility measures (e.g. subsample realized volatility and threshold pre-average realized volatility) decrease the volatility fitting error. The nonparametric testing suggests that the RSVL model with noise-and-jump-robust realized volatility measures has more power on forecasting excess kurtosis and fat tails and predicting dynamics of higher order autocorrelations. (C) 2017 Elsevier Inc. All rights reserved.","Li, Shaoyu; Zheng, Tingguo",2017,10.1016/j.najef.2017.03.003,,wos,"This paper introduces a realized stochastic volatility model (RSVL) for spot rates, incorporating dynamic drift and level effects. The model utilizes realized volatility measures and is estimated using a jointly approximated maximum likelihood procedure. Simulation results indicate that noise-and-jump-robust realized volatility measures enhance estimation accuracy. Empirical analysis on the Chinese interbank repo market demonstrates the RSVL model's effectiveness in capturing level effects and nonlinear drifts. The study also shows that the RSVL model with robust realized volatility measures improves volatility fitting and forecasting of excess kurtosis, fat tails, and higher-order autocorrelation dynamics.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:05:20.057837
7ebc71b7e07716e7,Modeling the Yield Curve of BRICS Countries: Parametric vs. Machine Learning Techniques,"We compare parametric and machine learning techniques (namely: Neural Networks) for in–sample modeling of the yield curve of the BRICS countries (Brazil, Russia, India, China, South Africa). To such aim, we applied the Dynamic De Rezende–Ferreira five–factor model with time–varying decay parameters and a Feed–Forward Neural Network to the bond market data of the BRICS countries. To enhance the flexibility of the parametric model, we also introduce a new procedure to estimate the time varying parameters that significantly improve its performance. Our contribution spans towards two directions. First, we offer a comprehensive investigation of the bond market in the BRICS countries examined both by time and maturity; working on five countries at once we also ensure that our results are not specific to a particular data–set; second we make recommendations concerning modelling and estimation choices of the yield curve. In this respect, although comparing highly flexible estimation methods, we highlight superior in–sample capabilities of the neural network in all the examined markets and then suggest that machine learning techniques can be a valid alternative to more traditional methods also in presence of marked turbulence. © 2022 Elsevier B.V., All rights reserved.","Castello, O.; Resta, M.",2022,10.3390/risks10020036,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124573638&doi=10.3390%2Frisks10020036&partnerID=40&md5=b73ed06482dce2f2b6ac91c5c94bb0ae,scopus,"This study compares parametric and machine learning (Neural Networks) techniques for modeling the yield curve of BRICS countries. It applies the Dynamic De Rezende–Ferreira five–factor model and a Feed–Forward Neural Network to bond market data. The research investigates the bond markets of Brazil, Russia, India, China, and South Africa, offering recommendations on yield curve modeling and estimation choices. The findings suggest that neural networks exhibit superior in-sample capabilities compared to traditional methods, even during periods of market turbulence.",True,True,False,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T16:05:33.304945
9342a41d80edaf33,Modeling the density of US yield curve using Bayesian semiparametric dynamic Nelson-Siegel model,"This paper proposes the Bayesian semiparametric dynamic Nelson-Siegel model for estimating the density of bond yields. Specifically, we model the distribution of the yield curve factors according to an infinite Markov mixture (iMM). The model allows for time variation in the mean and covariance matrix of factors in a discrete manner, as opposed to continuous changes in these parameters such as the Time Varying Parameter (TVP) models. Estimating the number of regimes using the iMM structure endogenously leads to an adaptive process that can generate newly emerging regimes over time in response to changing economic conditions in addition to existing regimes. The potential of the proposed framework is examined using US bond yields data. The semiparametric structure of the factors can handle various forms of non-normalities including fat tails and nonlinear dependence between factors using a unified approach by generating new clusters capturing these specific characteristics. We document that modeling parameter changes in a discrete manner increases the model fit as well as forecasting performance at both short and long horizons relative to models with fixed parameters as well as the TVP model with continuous parameter changes. This is mainly due to fact that the discrete changes in parameters suit the typical low frequency monthly bond yields data characteristics better.",,2020,10.1080/07474938.2019.1690191,,proquest,"This paper introduces a Bayesian semiparametric dynamic Nelson-Siegel model using an infinite Markov mixture to estimate bond yield densities. It models discrete time variation in factor means and covariances, allowing for adaptive regime generation. The model handles non-normalities like fat tails and nonlinear dependencies. Empirical results using US bond yields show improved fit and forecasting performance compared to fixed-parameter and TVP models, attributed to the discrete parameter change approach suiting monthly yield data.",True,True,True,gemini-2.5-flash-lite,Olav,M,,2025-10-13T16:05:50.056067
2cde5aab7ff222dc,Modeling volatility changes in the 10-year Treasury,"This paper examines the daily volatility of changes in the 10-year Treasury note utilizing the iterated cumulative sums of squares algorithm [C. Inclan, G. Tiao, Use of cumulative sums of squares for retrospective detection of changes of variance, J. Am. Stat. Assoc. 89 (1994) 913-923]. The ICSS algorithm can detect regime shifts in the volatility of the interest rate changes. A general model allows for endogenously determined changes in variance while the more restrictive model forces the variance to follow the same process throughout the sample period. A comparison of the out-of-sample volatility forecasting performance of two competing models is made using asymmetric error measures. The asymmetric error statistics penalize models for under- or over-predicting volatility. The results shed light on the importance of ignoring volatility regime shifts when performing out-of-sample forecasts. The findings are important to financial market participants who require accurate forecasts of future volatility in order to implement and evaluate asset performance. (c) 2006 Elsevier B.V. All rights reserved.","Covarrubias, Guillermo; Ewing, Bradley T.; Hein, Scott E.; Thompson, Mark A.",2006,10.1016/j.physa.2006.01.074,,wos,"This paper uses the iterated cumulative sums of squares (ICSS) algorithm to model and detect regime shifts in the daily volatility of 10-year Treasury note changes. It compares the out-of-sample forecasting performance of models that account for volatility regime shifts versus those that do not, using asymmetric error measures. The findings highlight the importance of considering volatility regime shifts for accurate out-of-sample volatility forecasts, which is crucial for financial market participants.",False,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:05:55.727809
c60c32b11706bfaf,Modelling Electricity Swaps with Stochastic Forward Premium Models,"We present a new model for pricing electricity swaps. Two general factors affect contracts but unique risk elements affect each contract. General factors are average swap prices and deterministic trend-seasonal components, and unique elements are forward premiums. Innovations follow MNIG distributions. We estimate the model with data from the European Energy Exchange. The model outperforms four competitors, both in in-sample valuation and in out-of-sample forecasting, and in fitting the term structure of volatilities by market segments. Competitor models are (i) diffusion spot prices, (ii) jump-diffusion spot prices with time dependent volatility, (iii) HJM-based and (iv) Levy multifactor model with NIG distributions. Value-at-Risk measures based on normality strongly underestimate tail risk but our model gives estimates that are more exact.Keywords: Electricity swaps; Stochastic forward premium; Multivariate Normal Inverse Gaussian distribution; Levy processes",,2018,10.5547/01956574.39.2.ibla,,proquest,"This paper introduces a novel model for pricing electricity swaps, incorporating general factors like average swap prices and seasonal trends, alongside unique risk elements represented by forward premiums. The model utilizes Multivariate Normal Inverse Gaussian (MNIG) distributions for innovations and is validated using data from the European Energy Exchange. It demonstrates superior performance compared to five competitor models in valuation, forecasting, and fitting volatility term structures. The model also provides more accurate Value-at-Risk estimates, particularly concerning tail risk, unlike normality-based measures.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:05:59.035351
febabd5bf9c00b9b,Modelling and management of mortality risk: a review,"In the first part of the paper, we consider the wide range of extrapolative stochastic mortality models that have been proposed over the last 15-20 years. A number of models that we consider are framed in discrete time and place emphasis on the statistical aspects of modelling and forecasting. We discuss how these models can be evaluated, compared and contrasted. We also discuss a discrete-time market model that facilitates valuation of mortality-linked contracts with embedded options. We then review several approaches to modelling mortality in continuous time. These models tend to be simpler in nature, but make it possible to examine the potential for dynamic hedging of mortality risk. Finally, we review a range of financial instruments (traded and over-the-counter) that could be used to hedge mortality risk. Some of these, such as mortality swaps, already exist, while others anticipate future developments in the market.","Cairns, Andrew J. G.; Blake, David; Dowd, Kevin",2008,10.1080/03461230802173608,,wos,"This paper reviews stochastic mortality models, focusing on both discrete-time and continuous-time approaches. It discusses model evaluation, forecasting, and the valuation of mortality-linked contracts. The review also covers hedging strategies for mortality risk using existing and potential financial instruments.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:06:02.332448
3e7e2117f4a117be,Modelling dependence structure with Archimedean copulas and applications to the iTraxx CDS index,"In this paper we model the dependence structure between credit default swap (CDS) and jump risk using Archimedean copulas. The paper models and estimates the different relationships that can exist in different ranges of behaviour. It studies the bivariate distributions of CDS index spreads and the kurtosis of equity return distribution. To take into account nonlinear relationships and different structures of dependency, we employ three Archimedean copula functions: Gumbel, Clayton, and Frank. We adopt nonparametric estimation of copula parameters and we find an extreme co-movement of CDS and stock market conditions. In addition, tail dependence indicates the extreme co-movements and the potential for a simultaneous large loss in stock markets and a significant default risk. Ignoring the tail dependence would lead to underestimation of the default risk premium. © 2010 Elsevier B.V. All rights reserved. © 2011 Elsevier B.V., All rights reserved.","Naifar, N.",2011,10.1016/j.cam.2010.10.047,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79251599155&doi=10.1016%2Fj.cam.2010.10.047&partnerID=40&md5=3494ea2f4e142ba65691f60966ea8084,scopus,"This paper models the dependence structure between credit default swap (CDS) and jump risk using Archimedean copulas (Gumbel, Clayton, and Frank). It estimates bivariate distributions of CDS index spreads and equity return kurtosis, employing nonparametric estimation of copula parameters. The study finds extreme co-movement between CDS and stock market conditions, highlighting tail dependence which, if ignored, could lead to underestimation of default risk premium.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:06:07.773718
3f8ab1bb9836d2d3,Monetary reforms and inflation expectations in Japan: Evidence from inflation-indexed bonds,"We assess the impact of news concerning recent Japanese monetary reforms on long-term inflation expectations using an arbitrage-free term structure model of nominal and real yields. Our model accounts for the value of deflation protection embedded in Japanese inflation-indexed bonds issued since 2013, which is sizable and time-varying. Our results suggest that Japanese long-term inflation expectations have remained pos-itive despite extensive spells of deflation, leaving inflation risk premia mostly negative during this period. Moreover, adjusting for deflation protection demonstrates that market responses to policy changes were not as inflationary as they appear under standard modeling procedures. Consequently, the reforms were less disappointingthan is widely perceived.(c) 2021 Elsevier B.V. All rights reserved.","Christensen, Jens H. E.; Spiegel, Mark M.",2022,10.1016/j.jeconom.2021.10.007,,wos,"This study analyzes the effect of Japanese monetary reforms on long-term inflation expectations using an arbitrage-free term structure model applied to nominal and real yields of inflation-indexed bonds issued since 2013. The model incorporates the value of deflation protection, which is significant and fluctuates over time. Findings indicate that Japanese long-term inflation expectations stayed positive even during deflationary periods, with inflation risk premia generally negative. After accounting for deflation protection, the market's reaction to policy changes was less inflationary than suggested by conventional models, implying the reforms were less disappointing than commonly believed.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:06:11.517771
25b9333c8cbad8b3,Monte Carlo Simulations for Resolving Verifiability Paradoxes in Forecast Risk Management and Corporate Treasury Applications,"Forecast risk management is central to the financial management process. This study aims to apply Monte Carlo simulation to solve three classic probabilistic paradoxes and discuss their implementation in corporate financial management. The article presents Monte Carlo simulation as an advanced tool for risk management in financial management processes. This method allows for a comprehensive risk analysis of financial forecasts, making it possible to assess potential errors in cash flow forecasts and predict the value of corporate treasury growth under various future scenarios. In the investment decision-making process, Monte Carlo simulation supports the evaluation of the effectiveness of financial projects by calculating the expected net value and identifying the risks associated with investments, allowing more informed decisions to be made in project implementation. The method is used in reducing cash flow volatility, which contributes to lowering the cost of capital and increasing the value of a company. Simulation also enables more accurate liquidity planning, including forecasting cash availability and determining appropriate financial reserves based on probability distributions. Monte Carlo also supports the management of credit and interest rate risk, enabling the simulation of the impact of various economic scenarios on a company’s financial obligations. In the context of strategic planning, the method is an extension of decision tree analysis, where subsequent decisions are made based on the results of earlier ones. Creating probabilistic models based on Monte Carlo simulations makes it possible to take into account random variables and their impact on key financial management indicators, such as free cash flow (FCF). Compared to traditional methods, Monte Carlo simulation offers a more detailed and precise approach to risk analysis and decision-making, providing companies with vital information for financial management under uncertainty. This article emphasizes that the use of Monte Carlo simulation in financial management not only enhances the effectiveness of risk management, but also supports the long-term growth of corporate value. The entire process of financial management is able to move into the future based on predicting future free cash flows discounted at the cost of capital. We used both numerical and analytical methods to solve veridical paradoxes. Veridical paradoxes are a type of paradox in which the result of the analysis is counterintuitive, but turns out to be true after careful examination. This means that although the initial reasoning may lead to a wrong conclusion, a correct mathematical or logical analysis confirms the correctness of the results. An example is Monty Hall’s problem, where the intuitive answer suggests an equal probability of success, while probabilistic analysis shows that changing the decision increases the chances of winning. We used Monte Carlo simulation as the numerical method. The following analytical methods were used: conditional probability, Bayes’ rule and Bayes’ rule with multiple conditions. We solved truth-type paradoxes and discovered why the Monty Hall problem was so widely discussed in the 1990s. We differentiated Monty Hall problems using different numbers of doors and prizes.",,2025,10.3390/ijfs13020049,,proquest,"This study applies Monte Carlo simulation to address classic probabilistic paradoxes within financial risk management and corporate treasury. It details how this method enhances financial forecast accuracy, investment decision-making, liquidity planning, and risk management (credit, interest rate). The article highlights Monte Carlo simulation's advantage over traditional methods for detailed risk analysis and strategic planning, ultimately supporting corporate value growth by enabling more informed decisions under uncertainty. It specifically uses Monte Carlo simulation to solve veridical paradoxes, such as the Monty Hall problem, employing both numerical and analytical techniques.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:06:20.619701
e148b89b6640d34c,Mortgage convexity,"Most home mortgages in the United States are fixed-rate loans with an embedded prepayment option. When long-term rates decline, the effective duration of mortgage-backed securities (MBS) falls due to heightened refinancing expectations. I show that these changes in MBS duration function as large-scale shocks to the quantity of interest rate risk that must be borne by professional bond investors. I develop a simple model in which the risk tolerance of bond investors is limited in the short run, so these fluctuations in MBS duration generate significant variation in bond risk premia. Specifically, bond risk premia are high when aggregate MBS duration is high. The model offers an explanation for why long-term rates could appear to be excessively sensitive to movements in short rates and explains how changes in MBS duration act as a positive-feedback mechanism that amplifies interest rate volatility. I find strong support for these predictions in the time series of US government bond returns. (C) 2014 Elsevier B.V. All rights reserved.","Hanson, Samuel G.",2014,10.1016/j.jfineco.2014.05.002,,wos,"This paper examines how mortgage-backed securities (MBS) duration fluctuations, driven by prepayment options and interest rate changes, impact bond risk premia. It proposes a model where limited investor risk tolerance amplifies interest rate volatility when MBS duration is high, finding empirical support in US government bond returns.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:06:24.401592
56f3840b8e773856,Multi-Period Portfolio Optimization with Investor Views under Regime Switching,"We propose a novel multi-period trading model that allows portfolio managers to perform optimal portfolio allocation while incorporating their interpretable investment views. This model’s significant advantage is its intuitive and reactive design that incorporates the latest asset return regimes to quantitatively solve managers’ question: how certain should one be that a given investment view is occurring? First, we describe a framework for multi-period portfolio allocation formulated as a convex optimization problem that trades off expected return, risk and transaction costs. Using a framework borrowed from model predictive control introduced by Boyd et al., we employ optimization to plan a sequence of trades using forecasts of future quantities, only the first set being executed. Multi-period trading lends itself to dynamic readjustment of the portfolio when gaining new information. Second, we use the Black-Litterman model to combine investment views specified in a simple linear combination based format with the market portfolio. A data-driven method to adjust the confidence in the manager’s views by comparing them to dynamically updated regime-switching forecasts is proposed. Our contribution is to incorporate both multi-period trading and interpretable investment views into one framework and offer a novel method of using regime-switching to determine each view’s confidence. This method replaces portfolio managers’ need to provide estimated confidence levels for their views, substituting them with a dynamic quantitative approach. The framework is reactive, tractable and tested on 15 years of daily historical data. In a numerical example, this method’s benefits are found to deliver higher excess returns for the same degree of risk in both the case when an investment view proves to be correct, but, more notably, also the case when a view proves to be incorrect. To facilitate ease of use and future research, we also developed an open-source software library that replicates our results. © 2023 Elsevier B.V., All rights reserved.","Oprisor, R.; Kwon, R.",2021,10.3390/jrfm14010003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165752842&doi=10.3390%2Fjrfm14010003&partnerID=40&md5=5f2b6dac5d316cc3ac5eb6c02e9b0cb2,scopus,"This paper presents a multi-period portfolio optimization model that integrates investor views and regime switching. It uses a convex optimization framework and a Black-Litterman model to combine market portfolios with manager views, dynamically adjusting confidence in views based on regime-switching forecasts. The model is tested on 15 years of historical data and aims to improve portfolio performance by providing a quantitative approach to view confidence.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:06:27.762970
fc81822659444a59,Multiobjective Model Predictive Control for portfolio optimization with cardinality constraint,"Model Predictive Control has been shown to be adequate to solve portfolio optimization problems because of its ability to perform the dynamic readjustment of the portfolio considering the market expectations. To consider both wealth and risk and real issues imposed by the financial market, this work proposes a Multiobjective Model Predictive Control strategy with cardinality constraints, besides transaction costs, self-financing, and upper and lower limits. The objective functions are the expected portfolio wealth and the expected Variance and Conditional Value at Risk as the portfolio risk measures. The optimization is performed by a multiobjective genetic algorithm, with operators proposed to control the number of assets in each portfolio and respect the prediction horizon perspective. Finally, an insightful case study is designed using the Brazilian Stock Exchange data in 2019 and 2020. An in-sample analysis explores the relationship between prediction horizon length, cardinality, optimal portfolio composition, risk-free asset, and objective functions on performance. An out-of-sample analysis considers the cumulative wealth, Sharpe ratio, maximum Drawdown, and the monthly accumulated return. Numerical experiments indicate that the proposed strategy outperforms the myopic portfolio selection, beats the primary Brazilian benchmark, a modified Markowitz model, and some top Brazilian investment funds even in crisis times like during the COVID-19 pandemic. © 2022 Elsevier B.V., All rights reserved.","de Melo, M.K.; Cardoso, R.T.N.; Jesus, T.A.",2022,10.1016/j.eswa.2022.117639,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132322733&doi=10.1016%2Fj.eswa.2022.117639&partnerID=40&md5=826d77fde8a0fb668125cdcf76866e6d,scopus,"This paper proposes a Multiobjective Model Predictive Control (MPC) strategy for portfolio optimization, incorporating cardinality constraints, transaction costs, and other financial market realities. It aims to balance expected portfolio wealth with risk measures like Variance and Conditional Value at Risk. A multiobjective genetic algorithm is used for optimization. The strategy is tested using Brazilian Stock Exchange data and shows superior performance compared to benchmarks and investment funds, even during the COVID-19 pandemic.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:06:40.161313
83686a7846be2d41,Multivariate tests of financial models. A new approach,"A variety of financial models are cast as nonlinear parameter restrictions on multivariate regression models, and the framework seems well suited for empirical purposes. Aside from eliminating the errors-in-the-variables problem which has plagued a number of past studies, the suggested methodology increases the precision of estimated risk premiums by as much as 76%. In addition, the approach leads naturally to a likelihood ratio test of the parameter restrictions as a test for a financial model. This testing framework has considerable power over past test statistics. With no additional variable beyond β, the substantive content of the CAPM is rejected for the period 1926-1975 with a significance level less than 0.001. © 1982. © 2014 Elsevier B.V., All rights reserved.","Gibbons, M.R.",1982,10.1016/0304-405x(82)90028-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750176969&doi=10.1016%2F0304-405X%2882%2990028-9&partnerID=40&md5=26284468277d35496a042e65f56a379d,scopus,"This paper proposes a new multivariate approach to testing financial models by casting them as nonlinear parameter restrictions on multivariate regression models. This method addresses errors-in-the-variables issues, improves the precision of risk premium estimates, and introduces a powerful likelihood ratio test for financial models. The Capital Asset Pricing Model (CAPM) is rejected for the period 1926-1975 using this framework.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:06:47.057170
d1cbe8bbd0caa6f9,Negative option values are possible: The impact of treasury bond futures on the cash US Treasury market,"This paper uses a unique financial instrument in the U.S. Treasury market to study the price behavior of the put option embedded in the November 2009-14 callable U.S. Treasury bond. We find that, beginning in August 1993, the estimated option value was persistently negative on nearly every day for the ensuing eight months. We show that the anomalous pricing behavior arose because the underlying callable bond became the cheapest to deliver issue against U.S. Treasury bond futures contracts. Hence, this paper provides direct evidence that derivative assets can significantly distort pricing in the primary asset market.","Jordan, BD; Kuipers, DR",1997,10.1016/s0304-405x(97)00025-1,,wos,"This paper investigates the price behavior of a put option embedded in a callable U.S. Treasury bond by examining the impact of U.S. Treasury bond futures. The study found that the option's value was persistently negative for an extended period, attributed to the callable bond becoming the cheapest to deliver issue against futures contracts. This suggests that derivative assets can significantly influence pricing in the primary asset market.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:06:51.451038
c2c778f7c4011a35,Neural forecasting of the Italian sovereign bond market with economic news,"In this paper, we employ economic news within a neural network framework to forecast the Italian 10‐year interest rate spread. We use a big, open‐source, database known as Global Database of Events, Language and Tone to extract topical and emotional news content linked to bond markets dynamics. We deploy such information within a probabilistic forecasting framework with autoregressive recurrent networks (DeepAR). Our findings suggest that a deep learning network based on long short‐term memory cells outperforms classical machine learning techniques and provides a forecasting performance that is over and above that obtained by using conventional determinants of interest rates alone.",,2022,10.1111/rssa.12813,,proquest,This paper utilizes economic news and a neural network framework (DeepAR with LSTM) to forecast the Italian 10-year interest rate spread. The approach incorporates topical and emotional news content from a large database and demonstrates superior performance compared to classical machine learning techniques and conventional determinants of interest rates.,True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T16:06:57.058384
c828201ee931a14e,Neural network and machine learning use cases: Indian bond market predictions,"This study examines machine learning techniques to investigate how artificial intelligence (AI) affects predicting future trends in the bond market. The bond market offers a global perspective on capital costs for a business by establishing the fair value of the bond issue, which is based on multiple factors. The asset price market, which has employed machine learning (ML) and deep learning (DL) techniques to address the primary forecasting difficulty, surprisingly plays a significant role in predicting fut ure bond market returns. As an outcome, if this gap can be forecast, it can act as the bond market's data-driven long-term direction and yield additional profits. Daily securityspecific data for the 10-to-3-year Indian Treasury Bond (ITB) was gathered from 2013 to 2022 and is available in the global government bonds database. The researchers looked at how well the auto-regressive integrated moving average (ARIMA), linear regression, and deep recurrent neural network-long short-term memory (DLSTM) models could predict bond yields and returns in future bond markets. The empirical results demonstrate that the DLSTM models most fairly predict the price of government bonds over both the short and longer horizons when compared to ARIMA and linear regression. © 2024 Elsevier B.V., All rights reserved.","Antony, J.M.; Sundaram, S.",2024,10.18488/29.v11i1.3667,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195129902&doi=10.18488%2F29.V11I1.3667&partnerID=40&md5=5da04c48a69be5670e5f5b563929cbfc,scopus,"This study investigates the use of machine learning (ML) and deep learning (DL) techniques, specifically auto-regressive integrated moving average (ARIMA), linear regression, and deep recurrent neural network-long short-term memory (DLSTM) models, to predict future trends, yields, and returns in the Indian bond market. Daily data for the 10-to-3-year Indian Treasury Bond (ITB) from 2013 to 2022 was used. The DLSTM model demonstrated the most accurate predictions compared to ARIMA and linear regression.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T16:07:05.306590
62c5869d03cd0ee0,Neural network forecasting in prediction Sharpe ratio: Evidence from EU debt market,"This study analyzes a neural networks model that forecast Sharpe ratio. The developed neural networks model is successful to predict the position of the investor who will be rewarded with extra risk premium on debt securities for the same level of portfolio risk or a greater risk premium than proportionate growth risk. The main purpose of the study is to predict highest Sharpe ratio in the future. Study grouped the data on yields of debt instruments in periods before, during and after world crisis. Results shows that neural networks is successful in forecasting nonlinear time lag series with accuracy of 82% on test cases for the prediction of Sharpe-ratio dynamics in future and investor‘s portfolio position. © 2021 Elsevier B.V., All rights reserved.","Vuković, D.; Vyklyuk, Y.; Matsiuk, N.; Maiti, M.",2020,10.1016/j.physa.2019.123331,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078116317&doi=10.1016%2Fj.physa.2019.123331&partnerID=40&md5=15556954c997b409a337691856374b6f,scopus,"This study applies a neural network model to forecast the Sharpe ratio in the EU debt market, aiming to predict future Sharpe ratio peaks and investor portfolio positions. The model demonstrated an 82% accuracy in forecasting nonlinear time-lag series, analyzing debt instrument yields across periods before, during, and after a world crisis.",True,True,True,gemini-2.5-flash-lite,Olav,M,,2025-10-13T16:07:17.876851
bfdbb84acb713f76,New results on the predictive value of crude oil for US stock returns,"PurposeThe purpose of this study is to clarify the nature of the predictive relationship between crude oil and the US stock market, with particular attention to whether this relationship is driven by time-varying risk premia.Design/methodology/approachThe authors formulate the predictive regression as a state-space model and estimate the time-varying coefficients via the Kalman filter and prediction-error decomposition.FindingsThe authors find that the nature of the predictive relationship between crude oil and the US stock market changed in the latter half of 2008. After mid-2008, the predictive relationship switched signs and exhibited characteristics which make it much more likely that the predictive relationship is due to time-varying risk premia rather than a market inefficiency.Originality/valueThe authors apply a state-space approach to modeling the predictive relationship. This allows one to watch the evolution of the predictive relationship over time. In particular, the authors identify a dramatic shift in the relationship around August 2008. Prior research has not been able to identify shifts in the relationship.",,2018,10.1108/sef-01-2017-0020,,proquest,"This study investigates the predictive relationship between crude oil and the US stock market, employing a state-space model and Kalman filter to estimate time-varying coefficients. The findings indicate a significant shift in this relationship around August 2008, suggesting it is driven by time-varying risk premia rather than market inefficiency. The methodology allows for tracking the evolution of this predictive relationship over time.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:07:20.828838
99693fcac2d04cb6,Non-invasive evaluation of neonatal cerebral status in the newborns of mothers addicted to alcohol and drugs,"The present study aims to assess the effects of alcohol and drug consumption on the cerebral status of a newborn with risk. Although there is a vast literature on the quality of life in terms of health, there is no uniform point of view, since the well-being of a person implies other elements that consider not only health but also the economic and educational environment in which the individual evolves and often these factors are connected. Besides, there is no valid instrument for measuring the quality of life either for an adult or for a child. In most cases, alcohol consumption intensifies in time, significantly decresing the quality of life for the mother and especially for the conception product. The study focuses on showing the The study focuses on highlighting the psychosocial and pharmacological aspects relevant to the diagnosis and management of neonatal cerebral status. The study participants, whose responses were the base for the quantitative analyzes, were individually interviewed using a standardized interview protocol. The interviews were conducted between October 2015 and September 2017. The interview protocol included three sections, in this chapter focusing our attention on the following sections: a) socio-demographic characteristics: age of gestation, sex of the newborn; b) clinical data: Presentation, Weight at Birth, Apgar Score, Cerebral Saturation (rSO<inf>2</inf>), Peripheral saturation (SpO<inf>2</inf>), The extraction fraction (FTOE), Parameters harvested from the umbilical cord at birth (pH, Base excess (BE), pCO<inf>2</inf>, pO<inf>2</inf>, MetHb, COHb), c) risk profile: mother’s alcohol consumption, including during pregnancy and drug use. The study group consisted of 90 infants born full term in Elena Doamna Maternity Hospital in Iasi, between 2015-2017, included in the programme of follow-up of the newborn with risk with the purpose of performing an non-invasive assessment of the fetal and neonatal cerebral status, in order to prevent and establish treatment methods for perinatal asphyxia. Based on the information obtained through the preliminary documentation, 30 newborns with alcohol and / or drug-consuming mothers and 60 neonates with risk-free mothers were selected - the control batch, who accepted to participate in the study.The cases studied showed the homogeneity of the groups depending on the mother’s age and gestational age, as well as the sex of the newborn and the weight at birth (p>0.05). In neonates from mothers at risk, the under-reference level of 1-minute brain saturation, combined with a lower gestational age and the 62.5% probability of performing a caesarean section at low levels of cerebral saturation was noted in 66.7% of newborns.The cut off value of SpO<inf>2</inf>, was established at 70 mL/ 100g/1 min, with a sensitivity of 50.9% and a specificity of 51.3%, after reading the coordinates of ROC curve, but the prediction was not significant from the statistical point of view (p=0.670). The mean level of base excess was al excesului de baze was slightly lower in newborns with the extraction fraction below the cut off value (-4.64 vs -4.18; p=0.560). According to the cases studied, 1 min after birth, 23.3% of the newborns showed an increased level of pCO<inf>2</inf> associated with a reduced level of peripheral saturation (r= -0.231; p=0.05). The correlation between the pO<inf>2</inf> level and the cerebral saturation, recorded 1 min after birth, was direct, but reduced as intensity (r= +0.295; p=0.049). About 27% of the newborns associated increased values of pO<inf>2</inf> with reduced values of the extraction fraction (r=-0.272; p=0.047). The newborns with an extraction fraction over the cut off value had a level of COHb below 1% (p=0.756) more frequently. Newborns from mothers who have consumed alcohol and / or drugs, including during pregnancy, show a reduced level of cerebral saturation and peripheral saturation 1 minute after birth. In 16.7% of newborns, the extraction limit was below the baseline 1 minute after birth. © 2020 Elsevier B.V., All rights reserved.","Crauciuc, D.V.; Crauciuc, E.G.; Iov, C.J.; Furnica, C.; Iov, T.",2018,10.37358/rc.18.11.6708,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062692954&doi=10.37358%2Frc.18.11.6708&partnerID=40&md5=f4546e0e9b4122f43f1e0a1afd543e0f,scopus,"This study non-invasively assesses the cerebral status of newborns exposed to maternal alcohol and drug use. It compares 30 exposed infants with 60 control infants, collecting socio-demographic, clinical (including cerebral saturation, peripheral saturation, and umbilical cord gases), and maternal risk factor data. Results indicate reduced cerebral and peripheral saturation in exposed newborns, with correlations observed between saturation levels and various clinical parameters. The study highlights the impact of maternal substance use on neonatal brain status.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:07:27.595791
3227b5dbadd1f5b3,Non-invasive prenatal testing: A diagnostic innovation shaped by commercial interests and the regulation conundrum,"Non-invasive prenatal testing (NIPT) is grounded in the analysis of free circulating fetal DNA (cfDNA) in pregnant women's blood. The rolling out of this screening method was in large part driven by commercial firms, which hoped to reach a huge potential market by offering a test that was expected to be risk-free, reliable, inexpensive, and able to detect a wide range of genetic traits of the future child. To date, most predictions about the scope and uses of NIPT have not materialized: in 2020 NIPT detects only a limited number of genetic anomalies, while results have to be confirmed by amniocentesis. NIPT has become a commercial success. Nevertheless the implementation of NIPT has tended to diverge across different national settings. In countries that already have state-sponsored screening for Down risk, NIPT has been offered by the state health insurance to women defined as “high risk”, using a variant of the test that detects only three autosomal aneuploidies: trisomy 21, 13 and 18. These countries effectively regulate the supply of NIPT on grounds of cost-effectiveness and reliability. In countries without state-sponsored screening for Down risk, in contrast, multiple versions of NIPT covering a wider range of birth defects are commonly available on the free market, and purchased by women at low as well as high risk of having an affected child. Market-based healthcare systems tend to present women who can afford to pay for NIPT with a largely unregulated choice of technologies – though reimbursement rules imposed by private insurance providers may serve in effect to regulate use by those consumers who cannot afford to pay for tests from their own pockets. This regulatory divergence is shaped by the presence or absence of prior state-sponsored screening programs for Down risk. © 2022 Elsevier B.V., All rights reserved.","Löwy, I.",2022,10.1016/j.socscimed.2020.113064,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086363099&doi=10.1016%2Fj.socscimed.2020.113064&partnerID=40&md5=d7d7b2854549e2fa4d78b2fdc0ca34d0,scopus,"This article discusses the development and implementation of Non-invasive Prenatal Testing (NIPT), a diagnostic innovation based on fetal DNA analysis. It highlights how commercial interests initially drove its rollout with optimistic predictions that have not fully materialized. The implementation of NIPT varies significantly across countries, influenced by existing state-sponsored screening programs and regulatory approaches, leading to different availability and scope of testing in state-funded versus market-based healthcare systems.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:07:35.761048
c2d699d6e932c05d,Nonlinear least squares estimator for generalized diffusion processes with reflecting barriers,"In this paper, we investigate the parameter estimation problem for generalized diffusion processes with two-sided reflected barriers. The estimator is obtained using the nonlinear least squares method based on discretely observed processes. Under certain regularity conditions, we obtain consistency and establish the asymptotic normality of the proposed estimator. Our method can be readily applied to the one-sided reflected diffusion processes. Numerical results, including a two-factor financial model, show that the proposed estimator performs well with large sample sizes. The U.S. treasury rate data is used to illustrate the theoretical results.",,2025,10.1080/17442508.2024.2393257,,proquest,"This paper proposes a nonlinear least squares estimator for generalized diffusion processes with reflecting barriers, using discretely observed data. The estimator is shown to be consistent and asymptotically normal under regularity conditions. The method is applicable to one-sided reflected diffusion processes and is demonstrated with a two-factor financial model and U.S. treasury rate data, showing good performance for large sample sizes.",True,True,False,gemini-2.5-flash-lite,Olav,M,,2025-10-13T16:07:46.634742
8fbc26d4a8606d0f,Nonlinear mean reversion in the term structure of interest rates,"The expectations hypothesis implies that the yield curve provides information on the future change in the short-term interest rate. However, transaction costs exist in the financial market, which prevent investors from realizing the arbitrage opportunity, when the arbitrage does not fully cover the transaction costs. The purpose of this paper is to assess the effect of transaction costs on the predictability of the term structure by using the threshold vector error correction model, which allows for the nonlinear adjustment to the long-run equilibrium relationship. A significant amount of threshold effect is found, and the adjustment coefficients are regime-dependent. The empirical result supports the nonlinear mean reversion in the term structure of interest rates. © 2002 Elsevier Science B.V. All rights reserved. © 2004 Elsevier Science B.V., Amsterdam. All rights reserved.","Seo, B.",2003,10.1016/s0165-1889(02)00124-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038330312&doi=10.1016%2FS0165-1889%2802%2900124-0&partnerID=40&md5=9658712d925b977d5436dcf9eb640c5b,scopus,"This paper investigates the impact of transaction costs on the predictability of the term structure of interest rates. Using a threshold vector error correction model, it demonstrates nonlinear mean reversion and regime-dependent adjustment coefficients, suggesting that transaction costs influence the market's ability to predict future interest rate changes.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T16:07:55.689658
0b74a66f5bfe27e3,Nonlinear support vector machines can systematically identify stocks with high and low future returns,"This paper investigates the profitability of a trading strategy based on training a model to identify stocks with high or low predicted returns. A tail set is defined to be a group of stocks whose volatility-adjusted price change is in the highest or lowest quantile, for example the highest or lowest 5%. Each stock is represented by a set of technical and fundamental features computed using CRSP and Compustat data. A classifier is trained on historical tail sets and tested on future data. The classifier is chosen to be a nonlinear support vector machine (SVM) due to its simplicity and effectiveness. The SVM is trained once per month, in order to adjust to changing market conditions. Portfolios are formed by ranking stocks using the classifier output. The highest ranked stocks are used for long positions and the lowest ranked ones for short sales. The Global Industry Classification Standard is used to build a model for each sector such that a total of 8 long-short portfolios for Energy, Materials, Industrials, Consumer Discretionary, Consumer Staples, Health Care, Financials, and Information Technology are formed. The data range from 1981 to 2010. Without measuring trading costs, but using 91 day holding periods to minimize these, the strategy leads to annual excess returns (Jensen alpha) of 15% with volatilities under 8% using the top 25% of the stocks of the distribution for training long positions and the bottom 25% for the short ones. © 2015 Elsevier B.V., All rights reserved.","Huerta, R.; Corbacho, F.; Elkan, C.",2013,10.3233/af-13016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921366265&doi=10.3233%2FAF-13016&partnerID=40&md5=2deab8efb093476e3de638f71535fbd5,scopus,"This paper explores a trading strategy using nonlinear Support Vector Machines (SVMs) to predict stocks with high and low future returns. The SVM is trained monthly on historical data, using technical and fundamental features, to identify 'tail sets' (top/bottom 5% volatility-adjusted returns). Portfolios are constructed based on SVM rankings, forming long-short positions within industry sectors. The strategy, tested from 1981-2010 with 91-day holding periods, reportedly yields annual excess returns of 15% with under 8% volatility, excluding trading costs.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:08:00.167822
35cf911ff0798991,Nonlinear time‐series analysis of stock volatilities,"The absolute value of the mean‐corrected excess return is used in this paper to measure the volatility of stock returns. We apply various nonlinearity tests available in the literature to show that such volatility series are strongly nonlinear. We then explore the use of threshold autoregressive (TAR) models in describing monthly volatility series. The models built suggest that the volatility series exhibit significant lower‐order serial correlations when the volatility is large, indicating certain volatility clustering in stock returns. Out‐of‐sample forecasts are used to compare the TAR models with linear ARMA models and nonlinear GARCH and EGARCH models. Based on mean squared error and average absolute deviation, the comparisons show that (a) the TAR models consistently outperform the linear ARMA models in multi‐step ahead forecasts for large stocks, (b) the TAR models provide better forecasts than the GARCH and EGARCH models also for the volatilities of large stock returns, and (c) the EGARCH model gives the best long‐horizon volatility forecasts for small stock returns. Copyright © 1992 John Wiley & Sons, Ltd. © 2016 Elsevier B.V., All rights reserved.","Cao, C.Q.; Tsay, R.S.",1992,10.1002/jae.3950070512,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986414605&doi=10.1002%2Fjae.3950070512&partnerID=40&md5=ccecaf8680144f56020bdb3e20ff479f,scopus,"This paper analyzes stock volatilities using nonlinear time-series methods, specifically threshold autoregressive (TAR) models. It demonstrates that stock volatility series are nonlinear and exhibit volatility clustering. The study compares TAR models with linear ARMA and nonlinear GARCH/EGARCH models for out-of-sample forecasting, finding TAR models superior for large stocks, while EGARCH performs best for small stocks at long horizons.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:08:03.081169
ff621dc65c3048e4,Nonlinear trading models through Sharpe Ratio maximization,"While many trading strategies are based on price prediction, traders in financial markets are typically interested in optimizing risk-adjusted performance such as the Sharpe Ratio, rather than the price predictions themselves. This paper introduces an approach which generates a nonlinear strategy that explicitly maximizes the Sharpe Ratio. It is expressed as a neural network model whose output is the position size between a risky and a risk-free asset. The iterative parameter update rules are derived and compared to alternative approaches. The resulting trading strategy is evaluated and analyzed on both computer-generated data and real world data (DAX, the daily German equity index). Trading based on Sharpe Ratio maximization compares favorably to both profit optimization and probability matching (through cross-entropy optimization). The results show that the goal of optimizing out-of-sample risk-adjusted profit can indeed be achieved with this nonlinear approach.While many trading strategies are based on price prediction, traders in financial markets are typically interested in optimizing risk-adjusted performance such as the Sharpe Ratio, rather than the price predictions themselves. This paper introduces an approach which generates a nonlinear strategy that explicitly maximizes the Sharpe Ratio. It is expressed as a neural network model whose output is the position size between a risky and a risk-free asset. The iterative parameter update rules are derived and compared to alternative approaches. The resulting trading strategy is evaluated and analyzed on both computer-generated data and real world data (DAX, the daily German equity index). Trading based on Sharpe Ratio maximization compares favorably to both profit optimization and probability matching (through cross-entropy optimization). The results show that the goal of optimizing out-of-sample risk-adjusted profit can indeed be achieved with this nonlinear approach.",,1997,10.1142/s0129065797000410,,proquest,"This paper presents a nonlinear trading strategy using a neural network to maximize the Sharpe Ratio, optimizing risk-adjusted performance rather than just price prediction. The strategy is evaluated on simulated and real-world data (DAX) and shows favorable comparisons to profit optimization and probability matching methods, achieving optimized out-of-sample risk-adjusted profit.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:08:11.337576
335a8fbd8ef67621,Nonlinearities in the black market zloty-dollar exchange rate: Some further evidence,"This study reappraises the evidence for nonlinear dependence in the monthly black market exchange returns of the Polish zloty, 1955-1990. Predictive asymmetry is reported in conditional variance such that depreciatory shocks have a greater impact on subsequent volatility than appreciatory shocks, jointly with conditional mean nonlinearity of smooth transition between regimes which suggests a simple trading strategy capable of generating positive profit over the sample period. However, support is also found for a competing variance-in-mean model consistent with a time-varying risk premium that is able to rationalize the presence of unexploited profit opportunities, particularly over the latter half of the sample. © 2007 Elsevier B.V., All rights reserved.","McMillan, D.G.; Speight, A.E.H.",2001,10.1080/096031001750071604,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035084341&doi=10.1080%2F096031001750071604&partnerID=40&md5=167b5291532c6c9ef646e89ed01d7817,scopus,"This study examines nonlinearities in the black market zloty-dollar exchange rate from 1955-1990. It finds evidence of predictive asymmetry in conditional variance, where depreciatory shocks have a larger impact on volatility than appreciatory shocks. Additionally, conditional mean nonlinearity suggests a profitable trading strategy. A competing variance-in-mean model, consistent with a time-varying risk premium, also explains unexploited profit opportunities.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:08:15.015424
d67898e84b4d8939,Nonlinearities in the relation between the equity risk premium and the term structure,"This paper investigates the relation between the conditional expected equity risk premium and the slope of the term structure of interest rates. Theoretically, these variables are linked, the relation may be nonlinear, and negative risk premiums are consistent with equilibrium. Given these implications, we employ a nonparametric estimation technique to document the empirical relation between the risk premium and the slope of the term structure using almost two hundred years of data. Of particular interest, the risk premium is increasing in the term structure slope; however, for either small or negative slopes, the risk premium is much more sensitive to changes in interest rates. In addition, the empirical results imply negative expected equity risk premiums for some inverted term structures. Finally, variations in the risk premium do not appear to be related to variations in the variance of equity returns. We illustrate these features in a stylized consumption-based model, and provide the economic intuition behind the results. © 2017 Elsevier B.V., All rights reserved.","Boudoukh, J.; Richardson, M.; Whitelaw, R.F.",1997,10.1287/mnsc.43.3.371,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031096836&doi=10.1287%2Fmnsc.43.3.371&partnerID=40&md5=f1a811b5a39a8ccd4a9ef7c511499e9a,scopus,"This paper examines the nonlinear relationship between the equity risk premium and the term structure slope using nonparametric estimation on nearly 200 years of data. It finds that the risk premium increases with the term structure slope, but is more sensitive to interest rate changes at small or negative slopes, consistent with negative expected equity risk premiums for inverted term structures. The study also illustrates these findings in a stylized consumption-based model.",True,True,False,gemini-2.5-flash-lite,Olav,M,,2025-10-13T16:08:32.540135
c004c1af8e9a15c6,Nonlinearity and Flight‐to‐Safety in the Risk‐Return Trade‐Off for Stocks and Bonds,"We document a highly significant, strongly nonlinear dependence of stock and bond returns on past equity market volatility as measured by the VIX. We propose a new estimator for the shape of the nonlinear forecasting relationship that exploits variation in the cross‐section of returns. The nonlinearities are mirror images for stocks and bonds, revealing flight‐to‐safety: expected returns increase for stocks when volatility increases from moderate to high levels while they decline for Treasuries. These findings provide support for dynamic asset pricing theories in which the price of risk is a nonlinear function of market volatility.",,2019,10.1111/jofi.12776,,proquest,"This study reveals a significant, nonlinear relationship between stock and bond returns and past equity market volatility (VIX). A novel estimator is introduced to capture this nonlinear forecasting relationship. The findings indicate a 'flight-to-safety' phenomenon, where stock returns rise with increasing volatility (moderate to high levels), while Treasury returns fall. This supports dynamic asset pricing theories suggesting a nonlinear price of risk dependent on market volatility.",True,True,True,gemini-2.5-flash-lite,Olav,M,,2025-10-13T16:08:54.164825
3b332c3efa34f2b7,"Nutritional value, elemental bioaccumulation and antioxidant activity of fruiting bodies and mycelial cultures of an unrecorded wild Lactarius hatsudake from Nanyue mountainous region in China","An unrecorded wild mushroom Lactarius hatsudake from Nanyue mountainous region in China was identified. Subsequently, comparative investigation on the nutritional value, elemental bioaccumulation, and antioxidant activity was performed in the fruiting body (FB) and mycelium (MY) samples of this species. It revealed that the contents of moisture (87.66 ± 0.16 g/100 g fw) and ash (6.97 ± 0.16 g/100 g dw) were significantly higher in FB, and the total carbohydrate, fat, and protein concentrations of FB were similar to those in MY. Among nutritionally important elements, FB possessed higher concentrations of potassium (37808.61 ± 1237.38 mg/kg dw), iron (470.69 ± 85.54 mg/kg dw), and zinc (136.13 ± 5.16 mg/kg dw), whereas MY was a better source of magnesium (1481.76 ± 18.03 mg/kg dw), calcium (2203.87 ± 69.61 mg/kg dw), and sodium (277.44 ± 22.93 mg/kg dw). According to the health risk estimation, FB might pose an aluminum-related health problem when a prolonged period of exposure, while MY was risk-free for consumers. The results of antioxidant capacity (1,1-diphenyl-2-picrylhydrazyl (DPPH) and 2,2'-Azino-bis (3-ethylbenzothiazoline-6-sulfonic acid) diammonium salt (ABTS) assays) in FB and MY were within the range of 104.19 ± 5.70 mg ascorbic acid equivalents (AAE)/g to 169.50 ± 4.94 mg AAE/g, and half maximal effective concentration EC50 values ranged from 0.23 ± 0.01 mg/mL to 0.62 ± 0.05 mg/mL. The aqueous extracts of MY demonstrated a strong ABTS radical scavenging capacity with the highest AAE value.An unrecorded wild mushroom Lactarius hatsudake from Nanyue mountainous region in China was identified. Subsequently, comparative investigation on the nutritional value, elemental bioaccumulation, and antioxidant activity was performed in the fruiting body (FB) and mycelium (MY) samples of this species. It revealed that the contents of moisture (87.66 ± 0.16 g/100 g fw) and ash (6.97 ± 0.16 g/100 g dw) were significantly higher in FB, and the total carbohydrate, fat, and protein concentrations of FB were similar to those in MY. Among nutritionally important elements, FB possessed higher concentrations of potassium (37808.61 ± 1237.38 mg/kg dw), iron (470.69 ± 85.54 mg/kg dw), and zinc (136.13 ± 5.16 mg/kg dw), whereas MY was a better source of magnesium (1481.76 ± 18.03 mg/kg dw), calcium (2203.87 ± 69.61 mg/kg dw), and sodium (277.44 ± 22.93 mg/kg dw). According to the health risk estimation, FB might pose an aluminum-related health problem when a prolonged period of exposure, while MY was risk-free for consumers. The results of antioxidant capacity (1,1-diphenyl-2-picrylhydrazyl (DPPH) and 2,2'-Azino-bis (3-ethylbenzothiazoline-6-sulfonic acid) diammonium salt (ABTS) assays) in FB and MY were within the range of 104.19 ± 5.70 mg ascorbic acid equivalents (AAE)/g to 169.50 ± 4.94 mg AAE/g, and half maximal effective concentration EC50 values ranged from 0.23 ± 0.01 mg/mL to 0.62 ± 0.05 mg/mL. The aqueous extracts of MY demonstrated a strong ABTS radical scavenging capacity with the highest AAE value.",,2023,10.1016/j.foodres.2023.113358,,proquest,"This study identifies an unrecorded wild mushroom, Lactarius hatsudake, from China and compares the nutritional value, elemental bioaccumulation, and antioxidant activity of its fruiting bodies (FB) and mycelium (MY). FB had higher moisture and ash content, while nutrient concentrations were similar. FB showed higher levels of potassium, iron, and zinc, whereas MY was richer in magnesium, calcium, and sodium. FB consumption might pose an aluminum-related health risk, unlike MY. Both FB and MY exhibited antioxidant activity, with MY extracts showing stronger ABTS radical scavenging capacity.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:08:58.919398
eae3754b32581403,OP73 Human and financial costs of six early years disadvantages in the UK and Bradford: a birth cohort microsimulation study,"BackgroundEarly childhood disadvantages (up to age 5) can have life-long effects on health and wellbeing. Methods of birth cohort microsimulation can capture these long-term effects and the associated public cost savings, which are hard to estimate using conventional methods (e.g. trials) because the effects may take decades to manifest. We aimed to quantify the long-term effects and public costs of six different early years disadvantages up to age 17 in the UK and Bradford, a multi-ethnic deprived local authority, using a microsimulation model based on the UK Millennium Cohort Study.MethodsUsing a cohort of 15,380 children from the Millenium Cohort Study (MCS) we model early years risk factors (conception to age 5) and subsequent outcomes up to age 17. We choose six risk factors: having a teenage mother, preterm birth, low birthweight (for gestational age), low height (age 5); disability (age 5), and school readiness (age 5). The causal effect parameters used in our microsimulation model are estimated using regressions based on Directed Acyclic Graphs (DAGs) which clearly set out our causal inference assumptions. We quantify a set of policy-relevant outcomes and their annual public costs for the UK and calibrate to Bradford based on local prevalence and population data. We assess the robustness of our findings to alternative assumptions and measures.ResultsThe public cost up to age 17 ranged between £86,058 [44,114–128,002] per 1,000 children for having a teenage mother, or £58,544 for each annual Bradford cohort to £432,920 [263,733–600,108] for school readiness or £1,872,265 for each Bradford cohort. The wellbeing impact ranged from 21 [-37–78] WELLBYs per 1,000 children for low birth weight or 20 per Bradford cohort, to 268 [245–290] for school readiness or 1,160 per Bradford cohort. Each WELLBY is valued by the UK Treasury at £13,000.ConclusionImproving school readiness yielded a larger wellbeing gain and public cost savings per child beneficiary than eliminating any of the other disadvantages we examined, but less than reducing early childhood poverty by moving families from the bottom income quintile to the next. When combined with evidence on short term effects of interventions, comparative long-term estimates of this kind may help policymakers prioritise and justify early years investments.We will report full results for the Uk and Bradford at the meeting, including a ready reckoner table of the long-term benefits and costs of reducing each childhood disadvantage.",,2025,10.1136/jech-2025-ssmabstracts.73,,proquest,"This study uses a birth cohort microsimulation model based on the UK Millennium Cohort Study to quantify the long-term effects and public costs of six early years disadvantages (teenage mother, preterm birth, low birthweight, low height, disability, school readiness) up to age 17 in the UK and Bradford. The findings indicate that improving school readiness offers the greatest wellbeing gain and public cost savings per child, though reducing early childhood poverty is even more impactful. The study suggests these long-term estimates can aid policymakers in prioritizing early years investments.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:09:06.693235
1475fced39bb3621,OPTIMAL ASSET ALLOCATION WITH MULTIVARIATE BAYESIAN DYNAMIC LINEAR MODELS,"We introduce a fast, closed-form, simulation-free method to model and forecast multiple asset returns and employ it to investigate the optimal ensemble of features to include when jointly predicting monthly stock and bond excess returns. Our approach builds on the Bayesian dynamic linear models of West and Harrison (Bayesian Forecasting and Dynamic Models (1997) Springer), and it can objectively determine, through a fully automated procedure, both the optimal set of regressors to include in the predictive system and the degree to which the model coefficients, volatilities and covariances should vary over time. When applied to a portfolio of five stock and bond returns, we find that our method leads to large forecast gains, both in statistical and economic terms. In particular, we find that relative to a standard no-predictability benchmark, the optimal combination of predictors, stochastic volatility and time-varying covariances increases the annualized certainty equivalent returns of a leverage-constrained power utility investor by more than 500 basis points.","Fisher, Jared D.; Pettenuzzo, Davide; Carvalho, Carlos M.",2020,10.1214/19-aoas1303,,wos,"This paper presents a novel, efficient method for modeling and forecasting multiple asset returns using Bayesian dynamic linear models. It automates the selection of optimal regressors, stochastic volatilities, and time-varying covariances. Applied to stock and bond returns, the method significantly improves forecast accuracy and investor returns compared to a benchmark.",True,True,True,gemini-2.5-flash-lite,Olav,M,,2025-10-13T16:11:14.900018
ebf6d44f3b36d6fb,On Gaussian HJM framework for Eurodollar Futures,"One of the standard tools for the theoretical analysis of fixed income securities and their associated derivatives is the term structure model of Heath, Jarrow and Morton. In this paper the question, what specific HJM model is consistent with the observed price of an Eurodollar Futures contract? is discussed. Eurodollar Futures, apart from being the most heavily traded futures are connected to London Inter Bank Offered Rate (LIBOR) and to domestic monetary conditions. The answer to the above question will help in pricing any new derivative on Eurodollar Futures or the one that is not heavily traded. A simple tool to measure the adequacy of different HJM structures that may be used to model Eurodollar Futures price process is suggested. Moreover, the question of estimation of parameters of these models by different methods-method of realized volatility, method of maximum likelihood (ML) and a two-stage method that combines both the realized volatility and ML-is addressed. Although it sounds like a typical statistical procedure, one must be careful in applying standard statistical techniques that are not suitable under arbitrage theory, in particular, ML method. Copyright © 2010 John Wiley & Sons, Ltd. © 2011 Elsevier B.V., All rights reserved.","Raman, B.; Pozdnyakov, V.",2011,10.1002/asmb.845,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051734977&doi=10.1002%2Fasmb.845&partnerID=40&md5=0fae9ebfb31357f6d62e923e89e64c32,scopus,"This paper investigates the Gaussian Heath, Jarrow, and Morton (HJM) framework for modeling Eurodollar Futures, aiming to determine which HJM model aligns with observed Eurodollar Futures prices. It also proposes a method to assess the suitability of different HJM structures for modeling the Eurodollar Futures price process and discusses parameter estimation techniques, including the method of realized volatility, maximum likelihood (ML), and a combined two-stage approach, cautioning against the direct application of standard ML methods under arbitrage theory.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:11:19.170981
01c47e420fd781a1,On a constrained mixture vector autoregressive model,"A mixture vector autoregressive model has recently been introduced to the literature. Although this model is a promising candidate for nonlinear multiple time series modeling, high dimensionality of the parameters and lack of method for computing the standard errors of estimates limit its application to real data. The contribution of this paper is threefold. First, a form of parameter constraints is introduced with an efficient EM algorithm for estimation. Second, an accurate method for computing standard errors is presented for the model with and without parameter constraints. Lastly, a hypothesis-testing approach based on likelihood ratio tests is proposed, which aids in the selection of unnecessary parameters and leads to the greater efficiency at the estimation. A case study employing U.S. Treasury constant maturity rates illustrates the applicability of the mixture vector autoregressive model with parameter constraints, and the importance of using a reliable method to compute standard errors. © 2013 IMACS. Published by Elsevier B.V. All rights reserved. © 2017 Elsevier B.V., All rights reserved.","Wong, C.S.",2013,10.1016/j.matcom.2013.05.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027926931&doi=10.1016%2Fj.matcom.2013.05.001&partnerID=40&md5=5e71ec085d88b327a249856779217765,scopus,"This paper introduces a constrained mixture vector autoregressive (MVAR) model to address limitations of existing MVAR models, such as high dimensionality and lack of standard error computation methods. It proposes an EM algorithm for estimation with parameter constraints, an accurate method for computing standard errors, and a likelihood ratio test for parameter selection. The model's applicability is demonstrated using U.S. Treasury constant maturity rates.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T16:12:09.422504
7cf8480c0b3c77f9,On some filtering problems arising in mathematical finance,"Three situations in which filtering theory is used in mathematical finance are illustrated at different levels of detail. The three problems originate from the following different works: (1) On estimating the stochastic volatility model from observed bilateral exchange rate news, by Mahieu and Schotman (1997). (2) A state space approach to estimate multi-factors CIR models of the term structure of interest rates, by Geyer and Pichler (1996). (3) Risk-minimizing hedging strategies under partial observation in pricing financial derivatives, by Fischer et al. (1996). In the first problem we propose to use a recent nonlinear filtering technique based on geometry to estimate the volatility time series from observed bilateral exchange rates. The model used here is the stochastic volatility model. The filters that we propose are known as projection filters, and a brief derivation of such filters is given. The second problem is introduced in detail, and a possible use of different filtering techniques is hinted at. In fact the filters used for this problem in (2) and part of the literature can be interpreted as projection filters and we will make some remarks on how more general and possibly more suitable projection filters can be constructed. The third problem is only presented briefly. © 1998 Elsevier Science B.V. All rights reserved. © 2018 Elsevier B.V., All rights reserved.","Brigo, D.; Hanzon, B.",1998,10.1016/s0167-6687(98)00008-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032523514&doi=10.1016%2FS0167-6687%2898%2900008-0&partnerID=40&md5=f63a02c53a85f528403e3d4c8fbf372d,scopus,"This paper illustrates three filtering problems in mathematical finance: estimating stochastic volatility from exchange rates, estimating multi-factor CIR models for interest rates, and risk-minimizing hedging strategies under partial observation. It proposes nonlinear filtering techniques, specifically projection filters, for the first problem and discusses their application to the second. The third problem is briefly presented.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:13:07.126064
c2c3870c724130ae,"On the construction of monthly term structures of U.S. interest rates, 1919-1930","This paper presents the methodology used to construct reliable estimates of the term structure of interest rates for the United States during 1919-1930. These monthly term structures are based on individual corporate bonds' price quotations for the majority of U.S. railroad corporations' issues of that era. McCulloch's cubic spline methodology, coupled with Nelson and Siegel's parsimonious estimator, is used to derive curves for three investment-grade risk classes. These estimates compare favorably with Durand's hand-smoothed estimates as well as earlier annual estimates generated by Thies. They provide a consistent basis for a wide range of monetary and financial research on this period. © 1992 Kluwer Academic Publishers. © 2007 Elsevier B.V., All rights reserved.","Baum, C.F.; Thies, C.F.",1992,10.1007/bf00426761,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34249832297&doi=10.1007%2FBF00426761&partnerID=40&md5=29e06ba6e17845fda2889a492cb90335,scopus,"This paper details the construction of monthly U.S. interest rate term structures from 1919-1930, utilizing corporate bond prices and a combination of McCulloch's cubic spline and Nelson-Siegel methodologies. The resulting curves for three investment-grade risk classes are validated against existing estimates and offer a consistent dataset for financial research.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:13:22.986554
84a97f8c3afab555,On the role of liquidity in emerging markets stock prices,"This paper investigates the impact of liquidity on emerging markets' stock prices. Particular attention is given to the estimation of Jensen's alpha and the quantity of risk. Our empirical analysis gives rise to two main issues. The first is related to the presence of an extra premium, i.e. ""alpha puzzle"". The second is the time-varying component of the quantity of risk, i.e. ""beta puzzle"". We find that local liquidity factors do not explain the presence of positive and statistically significant alphas. This puzzle is solved by means of transaction costs. In addition, we show that global liquidity factors, such as VIX and Open Interest, statistically affect the market price of risk. Our empirical finding proves the time varying nature of the global risk factors. Finally, we argue that standard asset pricing models cannot solve the two puzzles simultaneously. © 2012 University of Venice. © 2012 Elsevier B.V., All rights reserved.","Donadelli, M.; Prosperi, L.",2012,10.1016/j.rie.2012.06.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867668206&doi=10.1016%2Fj.rie.2012.06.001&partnerID=40&md5=c2b54628c2c2ef4bebe943d2271295e9,scopus,"This paper examines how liquidity affects stock prices in emerging markets, focusing on Jensen's alpha and risk quantity. It addresses the 'alpha puzzle' (excess premium) and 'beta puzzle' (time-varying risk). The study finds that transaction costs explain the alpha puzzle, while global liquidity factors like VIX and Open Interest influence the market price of risk, demonstrating their time-varying nature. Standard asset pricing models struggle to resolve both puzzles simultaneously.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:13:27.025291
106e9eb0ed46aba0,Option Pricing With Modular Neural Networks,"This paper investigates a nonparametric modular neural network (MNN) model to price the S&P-500 European call options. The modules are based on time to maturity and moneyness of the options. The option price function of interest is homogeneous of degree one with respect to the underlying index price and the strike price. When compared to an array of parametric and nonparametric models, the MNN method consistently exerts superior out-of-sample pricing performance. We conclude that modularity improves the generalization properties of standard feedforward neural network option pricing models (with and without the homogeneity hint).",N. Gradojevic; R. Gencay; D. Kukolj,2009,10.1109/tnn.2008.2011130,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4798200,ieeexplore,"This paper proposes a nonparametric modular neural network (MNN) model for pricing S&P-500 European call options, utilizing modules based on time to maturity and moneyness. The MNN model demonstrates superior out-of-sample pricing performance compared to other models, suggesting that modularity enhances generalization properties.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:13:29.322915
4f8ea2176a7d1090,Option-implied skewness: Insights from ITM-options,"While the standard to calculate model-free option-implied skewness (MFIS) relies on out-of-the-money (OTM) options, we examine the empirical and economic implications of using in-the-money (ITM) options. We find that the positive short-term return predictability of OTM-based MFIS significantly reverses if ITM-options are used instead. While this reversal is inconsistent with an explanation based on skewness preferences, MFIS apparently reflects information that is not timely incorporated in stock prices due to market frictions. Based on these insights, we introduce Delta MFIS as a new measure of additional option-embedded information that significantly predicts subsequent returns beyond a large range of other option-based return predictors. (C) 2021 Elsevier B.V. All rights reserved.","Mohrschladt, Hannes; Schneider, Judith C.",2021,10.1016/j.jedc.2021.104227,,wos,"This study investigates the implications of using in-the-money (ITM) options versus out-of-the-money (OTM) options for calculating model-free option-implied skewness (MFIS). The authors find that using ITM options reverses the short-term return predictability observed with OTM-based MFIS. This suggests that MFIS captures information not immediately reflected in stock prices due to market frictions. They propose a new measure, Delta MFIS, which demonstrates significant predictive power for subsequent returns, outperforming other option-based predictors.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:13:31.601584
b9e208171342ca97,Out-of-sample forecasts and nonlinear model selection with an example of the term structure of interest rates,"It is well known that goodness-of-fit measures lead to overfilling. We compare the small-sample properties of linear and several nonlinear models using a Monte Carlo study. A large number of linear series are generated and conventional methods of fitting nonlinear models are applied to each. The best linear and nonlinear models are compared using in-sample and out-of-sample criteria. Out-of-sample forecasts are shown to be superior for selecting the proper specification. The experiment is repeated using a nonlinear model and the in-sample fit and forecasts of the various models are compared. An example is provided using the term structure of interest rates. © 2018 Elsevier B.V., All rights reserved.","Liu, Y.; Enders, W.",2003,10.2307/1061692,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037280360&doi=10.2307%2F1061692&partnerID=40&md5=d77d1d9208071e7118c7356f32f054cf,scopus,"This paper compares linear and nonlinear models for forecasting, using Monte Carlo simulations and an example of the term structure of interest rates. It demonstrates that out-of-sample forecasts are superior for model specification selection compared to in-sample measures. The study highlights the issue of overfitting with goodness-of-fit measures and provides evidence for the effectiveness of nonlinear models in certain scenarios.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T16:13:45.496360
80d155e7f0df34e2,PREDICTING STOCK RETURNS AND VOLATILITY WITH INVESTOR SENTIMENT INDICES: A RECONSIDERATION USING A NONPARAMETRIC CAUSALITY-IN-QUANTILES TEST,"Evidence of monthly stock returns predictability based on popular investor sentiment indices, namely SBW and SPLS as introduced by Baker and Wurgler (2006, 2007) and Huang et al. (2015) respectively are mixed. While, linear predictive models show that only SPLS can predict excess stock returns, nonparametric models (which accounts for misspecification of the linear frameworks due to nonlinearity and regime changes) finds no evidence of predictability based on either of these two indices for not only stock returns, but also its volatility. However, in this paper, we show that when we use a more general nonparametric causality-in-quantiles model of Balcilar et al., (forthcoming), in fact, both SBW and SPLS can predict stock returns and its volatility, with SPLS being a relatively stronger predictor of excess returns during bear and bull regimes, and SBW being a relatively powerful predictor of volatility of excess stock returns, barring the median of the conditional distribution. © 2018 Elsevier B.V., All rights reserved.","Balcilar, M.; Gupta, R.; Kyei, C.",2018,10.1111/boer.12119,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018969715&doi=10.1111%2Fboer.12119&partnerID=40&md5=ad35656b372c4c8565d3b24a65a4d8f0,scopus,"This paper investigates the predictability of stock returns and volatility using investor sentiment indices (SBW and SPLS). While previous linear and some nonparametric models yielded mixed or no results, this study employs a more general nonparametric causality-in-quantiles model. The findings indicate that both SBW and SPLS can predict stock returns and volatility, with SPLS showing stronger prediction for excess returns in different market regimes and SBW predicting volatility, except for the median.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:13:51.793733
f7928e8fe95d86ae,"Particle filtering, learning, and smoothing for mixed-frequency state-space models","A particle filter approach for general mixed-frequency state-space models is considered. It employs a backward smoother to filter high-frequency state variables from low-frequency observations. Moreover, it preserves the sequential nature of particle filters, allows for non-Gaussian shocks and nonlinear state-measurement relation, and alleviates the concern over sample degeneracy. Simulation studies show that it outperforms the commonly used state-augmented approach for mixed-frequency data for filtering and smoothing. In an empirical exercise, predictive mixed-frequency regressions are employed for Treasury bond and US dollar index returns with quarterly predictors and monthly stochastic volatility. Stochastic volatility improves model inference and forecasting power in a mixed-frequency setup but not for quarterly aggregate models. © 2019 Elsevier B.V., All rights reserved.","Leippold, M.; Yang, H.",2019,10.1016/j.ecosta.2019.07.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070197669&doi=10.1016%2Fj.ecosta.2019.07.001&partnerID=40&md5=4f3f599d8db72b3c54cacc8133757f86,scopus,"This paper proposes a particle filter method for mixed-frequency state-space models, utilizing a backward smoother to handle high-frequency state variables with low-frequency observations. The method supports non-Gaussian shocks and nonlinear relationships, and it is shown to outperform the state-augmented approach in simulations. An empirical application to Treasury bond and US dollar index returns demonstrates that stochastic volatility improves inference and forecasting in a mixed-frequency context, but not in quarterly aggregate models.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T16:14:08.357670
2614c15f385e0a14,Positive forward rates in the maximum smoothness framework,In this paper we present a nonlinear dynamic programming algorithm for the computation of forward rates within the maximum smoothness framework. The algorithm implements the forward rate positivity constraint for a one-parametric family of smoothness measures and it handles price spreads in the constraining data set. We investigate the outcome of the algorithm using the Swedish Bond market showing examples where the absence of the positive constraint leads to negative interest rates. Furthermore we investigate the predictive accuracy of the algorithm as we move along the family of smoothness measures. Among other things we observe that the inclusion of spreads not only improves the smoothness of forward curves but also significantly reduces the predictive error.,"Manzano, J; Blomvall, J",2004,10.1088/1469-7688/4/2/011,,wos,"This paper introduces a nonlinear dynamic programming algorithm to compute forward rates within the maximum smoothness framework, incorporating a positivity constraint for forward rates and handling price spreads. The algorithm is tested on the Swedish Bond market, demonstrating that the absence of the positivity constraint can lead to negative interest rates. The study also examines the predictive accuracy across different smoothness measures, finding that including spreads improves curve smoothness and reduces predictive error.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:14:18.066639
a316a8dab4798b82,Predictability and Financial Sufficiency of Health Insurance in Colombia: An Actuarial Analysis With a Bayesian Approach,"Every year, the Colombian government provides a prospective premium, known as the capitation payment unit (CPU), for each affiliated person (according to sex, region, and age) to each health insurance company, in order to manage the corresponding risk in health. This article studies the prediction capacity for the health expenditure for the more than 20 million affiliates to the contributory regime of health, as well as the CPU's financial sufficiency, using an actuarial approach. Using the pure risk premium method and generalized linear models, both classic and Bayesian, the CPU is estimated; these results are compared to actual expenditure by an index of forecasting ability. It is concluded that the use of historical information about expenditure on health, as well as the Bayesian inference, among the other methodological innovations developed, provides an advantage for obtaining more accurate prospective values. These technical recommendations seek to support an improvement in the public budget allocation of more than 6 billion dollars per year to the Colombian health system.","Espinosa, Oscar; Bejarano, Valeria; Ramos, Jeferson",2024,10.1080/10920277.2023.2197475,,wos,"This actuarial analysis investigates the predictive power and financial sufficiency of Colombia's capitation payment unit (CPU) for health insurance. Using both classic and Bayesian generalized linear models, the study estimates the CPU and compares it to actual health expenditures. The findings suggest that incorporating historical expenditure data and employing Bayesian inference leads to more accurate prospective values, potentially improving public budget allocation for the Colombian health system.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:14:21.674784
559a1b5288affd81,Predictability of bull and bear markets: A new look at forecasting stock market regimes (and returns) in the US,"The empirical literature of stock market predictability mainly suffers from model uncertainty and parameter instability. To meet this challenge, we propose a novel approach that combines dimensionality reduction, regime-switching models, and forecast combination to predict excess returns on the S&P 500. First, we aggregate the weekly information of 146 popular macroeconomic and financial variables using different principal component analysis techniques. Second, we estimate Markov-switching models with time-varying transition probabilities using the principal components as predictors. Third, we pool the models in forecast clusters to hedge against model risk and to evaluate the usefulness of different specifications. Our weekly forecasts respond to regime changes in a timely manner to participate in recoveries or to prevent losses. This is also reflected in an improvement of risk-adjusted performance measures as compared to several benchmarks. However, when considering stock market returns, our forecasts do not outperform common benchmarks. Nevertheless, they do add statistical and, in particular, economic value during recessions or in declining markets. © 2023 Elsevier B.V., All rights reserved.","Haase, F.; Neuenkirch, M.",2023,10.1016/j.ijforecast.2022.01.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125423203&doi=10.1016%2Fj.ijforecast.2022.01.004&partnerID=40&md5=2a9da8940804b1ea4ce1e3e635eb98ee,scopus,"This study proposes a novel approach to predict stock market excess returns by combining dimensionality reduction, regime-switching models, and forecast combination. The method uses principal components from macroeconomic and financial variables to estimate Markov-switching models with time-varying transition probabilities. While the forecasts show timely responses to regime changes and improve risk-adjusted performance against benchmarks, they do not outperform benchmarks for stock market returns but add value during recessions or declining markets.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:14:24.634925
85aad9d7cb4653db,Predicting Equity Premium: A New Momentum Indicator Selection Strategy With Machine Learning,"We propose a new momentum-determined indicator-switching (N-MDIS) strategy, harnessing the power of machine learning to enhance the accuracy of equity premium prediction. Specifically, we re-examine the regime-dependent feature of univariate predictive regression relative to the benchmark. Furthermore, we investigate the prediction mechanism of the momentum-determined indicator-switching (MDIS) strategy and validate the significance of market regime information for the MDIS. Our findings demonstrate an overwhelmingly superior ex-post forecasting performance compared with the MDIS. More notably, our empirical results substantiate that machine learning greatly aids in momentum indicator selection. The results show that the N-MDIS with machine learning generates more accurate ex-ante equity premium forecasts than both MDIS strategy and N-MDIS strategy with logistic regression, yielding statistically and economically significant results. Moreover, our new approach exhibits robust forecasting performance across a series of robustness tests.","Qu, Yong; Yuan, Ying",2025,10.1002/for.3200,,wos,"This study introduces a novel momentum-determined indicator-switching (N-MDIS) strategy that utilizes machine learning to improve equity premium prediction. The strategy re-examines regime-dependent features of predictive regressions and validates the importance of market regime information for the MDIS. The N-MDIS with machine learning demonstrates superior ex-post forecasting performance compared to existing methods, including the MDIS strategy and N-MDIS with logistic regression, providing accurate ex-ante equity premium forecasts with statistically and economically significant results. The approach also shows robust forecasting performance.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:14:27.298506
88b65f3a8cabb4d8,Predicting bond risk premiums with machine learning: Evidence from China,"This study evaluates the ability of machine-learning algorithms to forecast bond risk premiums in the Chinese market. Using a comprehensive set of macro-, firm- and bond-level predictors, we find that machine learning, especially neural network, delivers markedly higher out-of-sample performance than traditional linear benchmarks. The local per-capita fiscal expenditure (EXPEND), bond credit ratings (CREDIT), and profitability- and intangible-related firm characteristics emerge as the most informative variables. Predictive gains are especially pronounced for low-rated issues, non-state-owned enterprises, and periods of heightened economic policy uncertainty. Incorporating machine-learning-based forecasts also helps to enhance credit rating accuracy. Collectively, our findings highlight the value of non-linear machine learning modeling techniques for bond pricing in emerging markets. © 2025 Elsevier B.V., All rights reserved.","Chai, B.; Jiang, F.; Lin, Y.; You, T.",2025,10.1016/j.pacfin.2025.102882,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011378841&doi=10.1016%2Fj.pacfin.2025.102882&partnerID=40&md5=e8cc6284881f9db8972654ad641eaf0d,scopus,"This study investigates the effectiveness of machine learning (ML) algorithms, particularly neural networks, in predicting bond risk premiums in China. The ML models, utilizing macro-, firm-, and bond-level data, significantly outperform traditional linear models in out-of-sample predictions. Key predictors include fiscal expenditure, credit ratings, and firm characteristics. The predictive gains are most substantial for lower-rated bonds, non-state-owned enterprises, and during periods of high economic policy uncertainty. The study also notes that ML-based forecasts improve credit rating accuracy, underscoring the utility of non-linear ML for bond pricing in emerging markets.",True,False,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T16:14:41.352018
fd7272563aec0c21,Predicting daily oil prices: Linear and non-linear models,"In this paper, we assess the accuracy of linear and nonlinear models in predicting daily crude oil prices. Competing forecasts of crude oil prices are generated from parsimonious linear models which require no parameter estimation, as well as linear and nonlinear models. Two of the linear models that we employ exploit the informational content of oil demand and the increasing correlation between oil and equity prices and are novel to the literature. The nonlinear model that we consider is an artificial neural network. More specifically, we consider a bagged neural network, a neural network trained using the genetic algorithm as well as a neural network with fuzzy logic. We find that some of the linear models outperform the random walk in terms of out-of-sample statistical forecast accuracy. Our findings also suggest that while the buy-and-hold strategy dominates some of the models in terms of dollar payoffs and risk-adjusted returns under a long-only strategy, all the models that we consider generate higher dollar payoffs than the buy-and-hold strategy under the short-only strategy. An investor obtains the largest profits by trading based on the moving average convergence divergence which is a technical indicator.","Dbouk, Wassim; Jamali, Ibrahim",2018,10.1016/j.ribaf.2018.01.003,,wos,"This paper compares the predictive accuracy of linear and nonlinear models for daily crude oil prices. It includes novel linear models incorporating oil demand and oil-equity correlations, and nonlinear models such as bagged neural networks, genetic algorithm-trained neural networks, and fuzzy logic neural networks. Some linear models outperform the random walk, and while buy-and-hold is superior for long-only strategies, all models yield better payoffs than buy-and-hold for short-only strategies. Trading based on the moving average convergence divergence indicator yields the largest profits.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:14:47.009358
2589459f5fd7391f,Predicting real growth and the probability of recession in the Euro area using the yield spread,"Although the spread has been established as a leading indicator of economic activity, recent studies in US and European Union (EU) countries have documented, theoretically and empirically, that the term spread-output growth relationship may not be stable over time and it may be subjected to nonlinearities. Using aggregate data for the Euro area over the period 1970:1-2000:4, we applied linear regression as well as nonlinear models to examine the predictive accuracy of the term spread-output growth relationship. Our results confirm the ability of the yield curve as a leading indicator. Moreover, significant nonlinearity with respect to time and past annual growth is detected, outperforming the linear model in out-of-sample forecasts of 1-year-ahead annual growth. Furthermore, probit models that use the EMU and US yield spreads are successful in predicting EMU recessions. © 2004 International Institute of Forecasters. Published by Elsevier B.V. All rights reserved. © 2005 Elsevier B.V., All rights reserved.","Duarte, A.; Venetis, I.A.; Paya, I.",2005,10.1016/j.ijforecast.2004.09.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-14844296381&doi=10.1016%2Fj.ijforecast.2004.09.008&partnerID=40&md5=1b54c1e1fff7743dddb40cdaf2e3bfbb,scopus,"This study investigates the predictive accuracy of the term spread for economic growth and recession probability in the Euro area using both linear and nonlinear models. The findings confirm the yield curve's role as a leading indicator, with nonlinear models showing superior out-of-sample forecasting performance for annual growth. Probit models utilizing yield spreads also proved successful in predicting Euro area recessions.",True,True,True,gemini-2.5-flash-lite,Olav,M,,2025-10-13T16:15:14.014893
f69c831266f5be91,Predicting the Canadian Yield Curve Using Machine Learning Techniques,"This study applies machine learning methods to predict the Canadian yield curve using a comprehensive set of macroeconomic variables. Lagged values of the yield curve and a wide array of Canadian and international macroeconomic variables are utilized across various machine learning models. Hyperparameters are estimated to minimize mispricing across government bonds with different maturities. The Group Lasso algorithm outperforms the other models studied, followed by Lasso. In addition, the majority of the models outperform the Random Walk benchmark. The feature importance analysis reveals that oil prices, bond-related factors, labor market conditions, banks’ balance sheets, and manufacturing-related factors significantly drive yield curve predictions. This study is one of the few that uses such a broad array of macroeconomic variables to examine Canadian macro-level outcomes. It provides valuable insights for policymakers and market participants, with its feature importance analysis highlighting key drivers of the yield curve.",,2025,10.3390/ijfs13030170,,proquest,"This study uses machine learning techniques, including Group Lasso and Lasso, to predict the Canadian yield curve by incorporating a wide range of macroeconomic variables. The Group Lasso model demonstrated superior performance compared to other models and a Random Walk benchmark. Key predictors identified include oil prices, bond-related factors, labor market conditions, banks' balance sheets, and manufacturing indicators. The research offers significant insights for policymakers and market participants by identifying crucial drivers of the yield curve.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T16:15:28.025978
0f8cf13e7b3aab14,Predicting the milk yield curve of dairy cows in the subsequent lactation period using deep learning,"Existing lactation models predict milk yields based on a fixed amount of observed milk production in early lactation. In contrast, this study proposes a model to predict the entire lactation curve of dairy cows by leveraging historical milk yield information observed in the preceding cycle. More specifically, we present a deep learning framework to encode the model inputs, predict the latent representation of the milk yield sequences and generate the corresponding lactation curves. Results show that the proposed framework outperforms the baseline models and that during the first 26 days of lactation, the model's predictions are more accurate than those of a state-of-the-art lactation model which is able to leverage the observed milk yields. As a result, the framework presented in this study allows farmers to increase their forecast horizon with respect to predicting its herd's total production and hence facilitates optimal herd management. Additionally, the model can be used to compare a cow's actual and expected milk yield over the entire course of the lactation cycle. This in turn can help to accelerate disease detection and enhance current animal monitoring systems. Finally, as the model incorporates the impact of health and reproduction events as well as herd management on the cow's productivity, future earnings and costs can be estimated more accurately.","Liseune, Arno; Salamone, Matthieu; Van den Poel, Dirk; van Ranst, Bonifacius; Hostens, Miel",2021,10.1016/j.compag.2020.105904,,wos,"This study introduces a deep learning framework to predict the entire lactation curve of dairy cows using historical milk yield data from the previous lactation cycle. The model outperforms baseline methods and shows higher accuracy in early lactation compared to a state-of-the-art model. It aids in optimizing herd management, accelerating disease detection, enhancing animal monitoring, and improving financial forecasting by incorporating health, reproduction, and management events.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:15:35.493605
70093db6f3573ba2,Predictive power of Markovian models: Evidence from US recession forecasting,"This paper provides extensions to the application of Markovian models in predicting US recessions. The proposed Markovian models, including the hidden Markov and Markov models, incorporate the temporal autocorrelation of binary recession indicators in a traditional but natural way. Considering interest rates and spreads, stock prices, monetary aggregates, and output as the candidate predictors, we examine the out-of-sample performance of the Markovian models in predicting the recessions 1-12 months ahead, through rolling window experiments as well as experiments based on the fixed full training set. Our study shows that the Markovian models are superior to the probit models in detecting a recession and capturing the recession duration. But sometimes the rolling window method may affect the models' prediction reliability as it could incorporate the economy's unsystematic adjustments and erratic shocks into the forecast. In addition, the interest rate spreads and output are the most efficient predictor variables in explaining business cycles.","Tian, Ruilin; Shen, Gang",2019,10.1002/for.2579,,wos,"This paper extends the use of Markovian models (hidden Markov and Markov models) for forecasting US recessions, incorporating temporal autocorrelation of recession indicators. It compares their out-of-sample performance against probit models using interest rates, spreads, stock prices, monetary aggregates, and output as predictors. The Markovian models demonstrate superiority in recession detection and duration capture, although rolling window methods can impact reliability. Interest rate spreads and output are identified as the most effective predictors.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:15:40.152853
7b8a48fc4285407a,Predictive power of investor sentiment for Bitcoin returns: Evidence from COVID-19 pandemic,"In this paper, we examine the impact of investor sentiment on Bitcoin returns. Using a large dataset of messages discussed on social media and several financial indicators, we create a sentiment indicator based on computational text analysis and driven by the principal component analysis (PCA) method. We utilize a vector autoregressive analysis and other analytical methods to examine the sentiment index-bitcoin return nexus. Our findings reveal that the sentiment index is a strong predictor of cryptocurrency market returns in the short term. Furthermore, we confirm that during the COVID-19 pandemic, investors' sentiments significantly impacted Bitcoin returns. Our results show that the proposed sentiment index can generate excess returns for investors who utilize it as a return predictor. Our empirical findings suggest important policy implications.","Bouteska, Ahmed; Mefteh-Wali, Salma; Dang, Trung",2022,10.1016/j.techfore.2022.121999,,wos,"This study investigates the predictive power of investor sentiment on Bitcoin returns, particularly during the COVID-19 pandemic. The authors construct a sentiment indicator using social media data and financial indicators via principal component analysis (PCA) and computational text analysis. Their findings indicate that this sentiment index is a significant short-term predictor of cryptocurrency returns, with a notable impact observed during the pandemic.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:15:43.323404
fba39a36693c57bd,Pricing Asian and Barrier Options Using a Combined Heston Model and Monte Carlo Simulation Approach with Artificial Intelligence,"The computation of fair values for exotic options often necessitates complex pricing techniques, which remain sparsely addressed in academic literature. Predominantly, the assessment of fair value for vanilla options relies on methodologies such as the Black-Scholes model or Monte Carlo simulations. This study proposes an innovative, dynamic approach to pricing, leveraging artificial intelligence in conjunction with the Heston model and a Monte Carlo simulation engine. This approach aims to furnish estimates of the prices for Barrier and Asian options. To enhance the accuracy of the model, calibration was performed employing a supervised machine learning algorithm, a continuous risk-free curve, and a dynamic implied volatility surface, derived from the current market data of vanilla options on S&P 500 futures. The amalgamation of these models yields instantaneous pricing for exotic option derivatives, contingent on the investor's determination of time to maturity and barrier levels. The efficacy of the model was evaluated by comparing the output prices to theoretical model predictions and a selection of over-the-counter traded options. Our findings indicate that the proposed dynamic, integrated approach substantially reduces the disparity between the theoretical models and current market prices. The prices calculated by our model demonstrate a marginal error of merely 0.33% in comparison to market prices, a significant improvement over the considerably larger error of 3.12% exhibited by traditional models. © 2023 Elsevier B.V., All rights reserved.","Khalife, D.; Yammine, J.; Rahal, S.; Freiha, S.",2023,10.18280/mmep.100519,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175234385&doi=10.18280%2Fmmep.100519&partnerID=40&md5=dd008174b8ed9190c2398219abc45cbb,scopus,"This study introduces a novel method for pricing exotic options like Barrier and Asian options by combining the Heston model, Monte Carlo simulation, and artificial intelligence. The model is calibrated using supervised machine learning, a risk-free curve, and a dynamic implied volatility surface derived from S&P 500 futures data. The approach aims to provide instantaneous pricing and was validated by comparing its output to theoretical predictions and over-the-counter traded options, showing a significant reduction in pricing error compared to traditional models.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:15:46.794081
488001700e2f9649,Pricing currency options with support vector regression and stochastic volatility model with jumps,"This paper presents an efficient currency option pricing model based on support vector regression (SVR). This model focuses on selection of input variables of SVR. We apply stochastic volatility model with jumps to SVR in order to account for sudden big changes in exchange rate volatility. We use forward exchange rate as the input variable of SVR, since forward exchange rate takes interest rates of a basket of currencies into account. Therefore, the inputs of SVR will include moneyness (spot rate/strike price), forward exchange rate, volatility of the spot rate, domestic risk-free simple interest rate, and the time to maturity. Extensive experimental studies demonstrate the ability of new model to improve forecast accuracy. © 2010 Elsevier Ltd. All rights reserved. © 2011 Elsevier B.V., All rights reserved.","Wang, P.",2011,10.1016/j.eswa.2010.05.037,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956613198&doi=10.1016%2Fj.eswa.2010.05.037&partnerID=40&md5=c18e5dcbda6e97cf6ca8e0968802b1d1,scopus,"This paper proposes a currency option pricing model using Support Vector Regression (SVR) combined with a stochastic volatility model that incorporates jumps. The model utilizes forward exchange rate, moneyness, spot rate volatility, domestic risk-free interest rate, and time to maturity as inputs for SVR. The authors claim improved forecast accuracy through experimental validation.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:15:50.375156
da32f56a08ba4a8f,Pricing participating longevity-linked life annuities: a Bayesian Model Ensemble approach,"Participating longevity-linked life annuities (PLLA) in which benefits are updated periodically based on the observed survival experience of a given underlying population and the performance of the investment portfolio are an alternative insurance product offering consumers individual longevity risk protection and the chance to profit from the upside potential of financial market developments. This paper builds on previous research on the design and pricing of PLLAs by considering a Bayesian Model Ensemble of single population generalised age-period-cohort stochastic mortality models in which individual forecasts are weighted by their posterior model probabilities. For the valuation, we adopt a longevity option decomposition approach with risk-neutral simulation and investigate the sensitivity of results to changes in the asset allocation by considering a more aggressive lifecycle strategy. We calibrate models using Taiwanese (mortality, yield curve and stock market) data from 1980 to 2019. The empirical results provide significant valuation and policy insights for the provision of a cost effective and efficient risk pooling mechanism that addresses the individual uncertainty of death, while providing appropriate retirement income and longevity protection.Participating longevity-linked life annuities (PLLA) in which benefits are updated periodically based on the observed survival experience of a given underlying population and the performance of the investment portfolio are an alternative insurance product offering consumers individual longevity risk protection and the chance to profit from the upside potential of financial market developments. This paper builds on previous research on the design and pricing of PLLAs by considering a Bayesian Model Ensemble of single population generalised age-period-cohort stochastic mortality models in which individual forecasts are weighted by their posterior model probabilities. For the valuation, we adopt a longevity option decomposition approach with risk-neutral simulation and investigate the sensitivity of results to changes in the asset allocation by considering a more aggressive lifecycle strategy. We calibrate models using Taiwanese (mortality, yield curve and stock market) data from 1980 to 2019. The empirical results provide significant valuation and policy insights for the provision of a cost effective and efficient risk pooling mechanism that addresses the individual uncertainty of death, while providing appropriate retirement income and longevity protection.",,2022,10.1007/s13385-021-00279-w,,proquest,"This paper develops a Bayesian Model Ensemble approach to price participating longevity-linked life annuities (PLLA). It uses generalized age-period-cohort stochastic mortality models, a longevity option decomposition approach with risk-neutral simulation, and Taiwanese data from 1980-2019. The study investigates the impact of asset allocation strategies on valuation and provides insights for retirement income and longevity protection.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:15:58.263143
5d8fab955f1e2b4c,Pricing with finite dimensional dependence,"We consider derivative pricing in factor models, where the factor is Markov with Finite Dimensional Dependence (FDD). The FDD condition allows for explicit formulas for derivative prices and their term structure and in this respect is a serious competitor of models with affine dynamic factors. The approach is illustrated by a comparison of the prices of realized and integrated volatility swaps. We show that the usual practice of replacing a payoff written on the realized volatility by the payoff written on the integrated volatility can imply pricing errors which are not negligible when the volatility of the volatility is large.",,2015,10.1016/j.jeconom.2015.02.027,,proquest,"This paper explores derivative pricing within factor models where the factor exhibits Finite Dimensional Dependence (FDD). The FDD condition enables the derivation of explicit formulas for derivative prices and their term structure, presenting a viable alternative to models with affine dynamic factors. The study illustrates this approach by comparing prices of realized and integrated volatility swaps, demonstrating that substituting integrated volatility for realized volatility can lead to significant pricing errors, especially when the volatility of volatility is high.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:16:04.195538
e8ba8666d5c2c61f,Probabilistic assessment of earthquake insurance rates for Turkey,"A probabilistic model is presented to obtain a realistic estimate of earthquake insurance rates for reinforced concrete buildings in Turkey. The model integrates information on seismic hazard and information on expected earthquake damage on engineering facilities in a systematic way, yielding to estimates of earthquake insurance premiums. In order to demonstrate the application of the proposed probabilistic method, earthquake insurance rates are computed for reinforced concrete buildings constructed in five cities located in different seismic zones of Turkey. The resulting rates are compared with the rates currently charged by the insurance companies. The earthquake insurance rates are observed to be sensitive to the assumptions on seismic hazard and damage probability matrices and to increase significantly with increasing violation of the code requirements. © Springer 2005. © 2008 Elsevier B.V., All rights reserved.","Yücemen, M.S.",2005,10.1007/s11069-004-6485-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23844495206&doi=10.1007%2Fs11069-004-6485-8&partnerID=40&md5=bc0c07df0ae19c758040a226d34e044e,scopus,"This paper presents a probabilistic model to estimate earthquake insurance rates for reinforced concrete buildings in Turkey. It combines seismic hazard data with expected earthquake damage information to calculate premiums. The model's application is demonstrated by computing rates for buildings in five Turkish cities and comparing them to current market rates. The study highlights the sensitivity of these rates to seismic hazard assumptions and damage probabilities, noting a significant increase in rates with non-compliance to building code requirements.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:16:07.196573
cdbf8ff96c734ee2,Psychological Pathways to Fraud: Understanding and Preventing Fraud in Organizations,"In response to calls for more research on how to prevent or detect fraud (ACAP, Final Report of the Advisory Committee on the Auditing Profession, United States Department of the Treasury, Washington, DC, 2008; AICPA, SAS No. 99: Consideration of Fraud in a Financial Statement Audit, New York, NY, 2002; Carcello et al., Working Paper, University of Tennessee, Bentley University and Kennesaw State University, 2008; Wells, Journal of Accountancy, 2004), we develop a framework that identifies three psychological pathways to fraud, supported by multiple theories relating to moral intuition and disengagement, rationalization, and the role played by negative affect. The purpose of developing the framework is twofold: (1) to draw attention to important yet under-researched aspects of ethical decision-making, and (2) to increase our understanding of the psychology of committing fraud. Our framework builds on the existing fraud triangle (PCAOB, Consideration of fraud in a financial statement audit. AU Section 316, www.pcaobus.org, 2005) which is used by auditors to assess fraud risk. The fraud triangle is composed of three factors that, together, predict the likelihood of fraud within an organization: opportunity, incentive/pressure, and attitude/rationalization. We find that, when faced with the opportunity and incentive/pressure, there are three psychological pathways to fraud nestled within attitude/rationalization: (1) lack of awareness, (2) intuition coupled with rationalization, and (3) reasoning. These distinctions are important for fraud prevention because each of these paths is driven by a different psychological mechanism. This framework is useful in a number of ways. First, it identifies certain insidious situational factors in which individuals commit fraud without recognizing it. Second, it extends our knowledge of rationalization by theorizing that individuals use rationalization to avoid or reduce the negative affect that accompanies performing an unethical behavior. Negative affect is important because individuals wish to avoid it. Third, it identifies several other methods fraudsters use to reduce negative affect, each of which could serve as potential ""psychological red flags"" and helps predict future fraudulent behavior. Finally, our framework can be used as a theoretical foundation to explore several interventions designed to prevent fraud.[PUBLICATION ABSTRACT]",,2011,10.1007/s10551-011-0741-0,,proquest,"This paper proposes a framework identifying three psychological pathways to fraud within organizations, building upon the existing fraud triangle. These pathways—lack of awareness, intuition coupled with rationalization, and reasoning—are driven by different psychological mechanisms and are important for fraud prevention. The framework also highlights the role of negative affect and rationalization in ethical decision-making and suggests potential psychological red flags for predicting fraudulent behavior.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:16:12.985062
3dc61fee5d9bbaf9,Purebred or hybrid?: Reproducing the volatility in term structure dynamics,"This paper investigates the ability of mixtures of affine, quadratic, and non-linear models to track the volatility in the term structure of interest rates. Term structure dynamics appear to exhibit pronounced time varying or stochastic volatility. Ahn et al. (Rev. Financial Stud. xx (2001) xxx) provide evidence suggesting that term structure models incorporating a set of quadratic factors are better able to reproduce term structure dynamics than affine models, although neither class of models is able to fully capture term structure volatility. In this study, we combine affine, quadratic and non-linear factors in order to maximize the ability of a term structure model to generate heteroskedastic volatility. We show that this combination entails a tradeoff between specification of heteroskedastic volatility and correlations among the factors. By combining factors, we are able to gauge the cost of this tradeoff. Using efficient method of moments (Gallant and Tauchen, Econometric Theory 12 (1996) 657), we find that augmenting a quadratic model with a non-linear factor results in improvement in fit over a model comprised solely of quadratic factors when the model only has to confront first and second moment dynamics. When the full dynamics are confronted, this result reverses. Since the non-linear factor is characterized by stronger dependence of volatility on the level of the factor, we conclude that flexibility in the specification of both level dependence and correlation structure of the factors are important for describing term structure dynamics. © 2003 Elsevier B.V. All rights reserved. © 2008 Elsevier B.V., All rights reserved.","Ahn, D.-H.; Dittmar, R.F.; Gallant, A.R.; Gao, B.",2003,10.1016/s0304-4076(03)00106-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346937479&doi=10.1016%2FS0304-4076%2803%2900106-4&partnerID=40&md5=ea1e59d19022c6cce6ceb6b3b72bdb0e,scopus,"This paper explores the effectiveness of combining affine, quadratic, and non-linear models to capture the volatility in term structure dynamics of interest rates. It finds that while augmenting a quadratic model with a non-linear factor improves fit for first and second moment dynamics, this advantage is lost when considering full dynamics, suggesting the importance of both level dependence and correlation structure in factor specification.",True,True,False,gemini-2.5-flash-lite,Olav,M,,2025-10-13T16:16:31.987120
e80e3012da1e8609,QuantFactor REINFORCE: Mining Steady Formulaic Alpha Factors With Variance-Bounded REINFORCE,"Alpha factor mining aims to discover investment signals from the historical financial market data, which can be used to predict asset returns and gain excess profits. Powerful deep learning methods for alpha factor mining lack interpretability, making them unacceptable in the risk-sensitive real markets. Formulaic alpha factors are preferred for their interpretability, while the search space is complex and powerful explorative methods are urged. Recently, a promising framework is proposed for generating formulaic alpha factors using deep reinforcement learning, and quickly gained research focuses from both academia and industries. This paper first argues that the originally employed policy training method, i.e., Proximal Policy Optimization (PPO), faces several important issues in the context of alpha factors mining. Herein, a novel reinforcement learning algorithm based on the well-known REINFORCE algorithm is proposed. REINFORCE employs Monte Carlo sampling to estimate the policy gradient—yielding unbiased but high variance estimates. The minimal environmental variability inherent in the underlying state transition function, which adheres to the Dirac distribution, can help alleviate this high variance issue, making REINFORCE algorithm more appropriate than PPO. A new dedicated baseline is designed to theoretically reduce the commonly suffered high variance of REINFORCE. Moreover, the information ratio is introduced as a reward shaping mechanism to encourage the generation of steady alpha factors that can better adapt to changes in market volatility. Evaluations on real assets data indicate the proposed algorithm boosts correlation with returns by 3.83%, and a stronger ability to obtain excess returns compared to the latest alpha factors mining methods, which meets the theoretical results well.",J. Zhao; C. Zhang; M. Qin; P. Yang,2025,10.1109/tsp.2025.3576781,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11024173,ieeexplore,"This paper proposes QuantFactor REINFORCE, a novel reinforcement learning algorithm for mining interpretable formulaic alpha factors. It addresses limitations of PPO in alpha factor mining by using REINFORCE with a new baseline and information ratio reward shaping to reduce variance and encourage steady factors. Evaluations show improved correlation with returns and excess return generation compared to existing methods.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:16:36.395103
d59e983e8c52c31d,RETRACTED: Wheat Futures Prices Prediction in China: A Hybrid Approach (Retracted Article),"Stocks markets play their financial roles of price shocks and hedging just when they are proficient. The imperative highlights of productive market are that one cannot make extraordinary profit from the stocks markets. This research investigates whether China wheat futures price can be predicted by employing artificial intelligence neural network. This would add to our knowledge whether wheat futures market is resourceful and would enable traders, sellers, and investors to improve cost-effective trading strategy. We utilize the traditional financial model to forecast the wheat futures price and acquire out of sample point estimates. We additionally assess the robustness of our outcomes by applying several alternative forecasting techniques such as artificial intelligence with one hidden layer and autoregressive integrated moving average (ARIMA) model. Furthermore, the statistical significance of our point estimation was further tested through the Mariano and Diebold test. Considering random walk forecast as the bench mark, we used a number of economic indicators, trader's expectation towards futures prices, and lagged value of futures price of wheat in order to forecast the evaluation of wheat futures price. The computable significance of out of sample estimations recommends that our ANN with one hidden layer has the best anticipating presentation among all the models considered in this exploration and has the estimating power in foreseeing wheat futures returns. Furthermore, this investigation discovers that the futures price of wheat can be predicted, and the wheat futures market of China is not productive.","Sun, Yunpeng; Guo, Jin; Shan, Shan; Khan, Yousaf Ali",2021,10.1155/2021/5545802,,wos,"This retracted article investigates the predictability of China's wheat futures prices using artificial intelligence (ANN) and ARIMA models. The study found that the ANN model with one hidden layer outperformed other methods, suggesting that wheat futures prices can be predicted and the market is not efficient. The research aimed to inform trading strategies.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:16:41.779738
8b07c85a311ecc17,Rare Disasters and Risk Attitudes: International Differences and Implications for Integrated Assessment Modeling,"Evaluation of public policies with uncertain economic outcomes should consider society's preferences regarding risk. However, the preference models used in most integrated assessment models, including those commonly used to inform climate policy, do not adequately characterize the risk attitudes revealed by typical investment decisions. Here, we adopt an empirical approach to risk preference description using international historical data on investment returns and the occurrence of rare economic disasters. We improve on earlier analyses by employing a hierarchical Bayesian inference procedure that allows for nation-specific estimates of both disaster probabilities and preference parameters. This provides a stronger test of the underlying investment model than provided by previous calibrations and generates some compelling hypotheses for further study. Specifically, results suggest that society is substantially more averse to risk than typically assumed in integrated assessment models. In addition, there appear to be systematic differences in risk preferences among nations. These results are likely to have important implications for policy recommendations: higher aversion to risk increases the precautionary value of taking action to avoid low-probability, high-impact outcomes. However, geographically variable attitudes toward risk indicate that this precautionary value could vary widely across nations, thereby potentially complicating the negotiation of transboundary agreements focused on risk reduction. © 2012 Society for Risk Analysis. © 2013 Elsevier B.V., All rights reserved.; MEDLINE® is the source for the MeSH terms of this document.","Ding, P.; Gerst, M.D.; Bernstein, A.; Howarth, R.B.; Borsuk, M.E.",2012,10.1111/j.1539-6924.2012.01872.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869495004&doi=10.1111%2Fj.1539-6924.2012.01872.x&partnerID=40&md5=3d9232bba36bfd98b20035479434effc,scopus,"This study empirically investigates societal risk attitudes using international historical data on investment returns and rare economic disasters. It employs a hierarchical Bayesian inference procedure to estimate nation-specific disaster probabilities and preference parameters, suggesting higher risk aversion than commonly assumed in integrated assessment models. The findings highlight systematic differences in risk preferences among nations, which could impact policy recommendations and the negotiation of transboundary agreements for risk reduction.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:16:45.434385
d600bf6f8813eb90,Rating Crop Insurance Contracts with Nonparametric Bayesian Model Averaging,Crop insurance is plagued by relatively little historical information but significant spatial information. We investigate the efficacy of using nonparametric Bayesian model averaging (BMA) to incorporate extraneous information into the estimated premium rates. Nonparametric BMA is particularly suited to this application because it does not make any assumptions about parametric form or the extent to which yields are similar. We evaluate the proposed estimator under small-to-medium sample sizes and various geographical restrictions on the distance of spatial smoothing for policy relevance. The nonparametric BMA consistently decreases error and enables statistically significant and economically important rents to be captured.,,2020,10.22004/ag.econ.302453,,proquest,"This paper proposes a nonparametric Bayesian model averaging (BMA) approach to improve crop insurance premium rate estimation by incorporating spatial information, especially when historical data is limited. The method is evaluated for its effectiveness in reducing errors and capturing economic rents under various conditions.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:16:48.751425
9f0348c54278ded0,Real-time Bayesian learning and bond return predictability,"The paper examines statistical and economic evidence of out-of-sample bond return predictability for a real-time Bayesian investor who learns about parameters, hidden states, and predictive models over time. We find some statistical evidence using information contained in forward rates. However, such statistical predictability can hardly generate any economic value for investors. Furthermore, we find that strong statistical and economic evidence of bond return predictability from fully-revised macroeconomic data vanishes when real-time macroeconomic information is used. We also show that highly levered investments in bonds can improve short-run bond return predictability. (C) 2021 Elsevier B.V. All rights reserved.","Wan, Runqing; Fulop, Andras; Li, Junye",2022,10.1016/j.jeconom.2020.04.052,,wos,"This paper investigates bond return predictability for a real-time Bayesian investor. While some statistical predictability is found using forward rates, it offers little economic value. Predictability using fully-revised macroeconomic data disappears with real-time information. Highly leveraged bond investments can enhance short-run predictability.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T16:17:03.207232
d709c1992299932e,Reconciling interest rates evidence with theory: Rejecting unit roots when the HD(1) is a competing alternative,"The paper introduces the HD(1), a Markovian process of order one with reversion rates that are faster the farther the process is from equilibrium. The aHD(1) approximation is introduced to allow for an estimation -calibration procedure based on available ARMA routines. Critical values of unit root tests with aHD(1) alternative are tabulated for the signed likelihood -ratio statistic. Revisiting the non-stationarity of interest rates stylized fact, the aHD(1) is found to be preferred to ARMA, SETAR and RCA and the resulting tests to reject the unit root hypothesis for all rates and yields considered.","Palandri, Alessandro",2024,10.1016/j.jbankfin.2024.107113,,wos,"This paper proposes the HD(1) model, a Markovian process with faster reversion rates when further from equilibrium, and its approximation aHD(1) for estimation. It introduces critical values for unit root tests against the aHD(1) alternative and applies them to interest rates. The findings suggest the aHD(1) is superior to other models (ARMA, SETAR, RCA) and leads to the rejection of the unit root hypothesis for all examined interest rates and yields.",False,True,False,gemini-2.5-flash-lite,Olav,M,,2025-10-13T16:17:31.873301
4e8eda2d3ff61821,Regime Changes and Financial Markets,"Regime-switching models can match the tendency of financial markets to often change their behavior abruptly and the phenomenon that the new behavior of financial variables often persists for several periods after such a change. Although the regimes captured by regime-switching models are identified by an econometric procedure, they often correspond to different periods in regulation, policy, and other secular changes. In empirical estimates, the means, volatilities, autocorrelations, and cross-covariances of asset returns often differ across regimes in a manner that allows regime-switching models to capture the stylized behavior of many financial series including fat tails, heteroskedasticity, skewness, and time-varying correlations. In equilibrium models, regimes in fundamental processes, such as consumption or dividend growth, strongly affect the dynamic properties of equilibrium asset prices and can induce non-linear risk-return trade-offs. Regime switches also lead to potentially large consequences for investors' optimal portfolio choice.","Ang, Andrew; Timmermann, Allan",2012,10.1146/annurev-financial-110311-101808,,wos,"This paper discusses regime-switching models and their application to financial markets. These models can capture abrupt changes in market behavior and the persistence of new behaviors. The identified regimes often align with changes in regulation, policy, or other secular shifts. Empirical estimates show differences in means, volatilities, autocorrelations, and cross-covariances of asset returns across regimes, which helps capture stylized behaviors like fat tails, heteroskedasticity, skewness, and time-varying correlations. In equilibrium models, regime switches in fundamental processes impact asset prices and investor portfolio choices.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:17:34.537688
3aba891800eef614,Relationship between expected treasury bill and Eurodollar interest rates: A fractional cointegration analysis,"In this paper, we extend Booth and Tse's (BT) 1995 analysis of fractional cointegration between the expected Eurodollar and Treasury bill interest rates implied by their respective futures contracts. The definition of fractional cointegration suggested by Cheung and Lai (1993) and used by BT is refined so that it requires the cointegrating relationship to be stationary as well as mean-reverting. In addition to the Geweke and Porter-Hudak method used by BT, a more efficient Maximum Likelihood (ML) method is used to estimate the cointegrating relationship. The LM (Engle (1982)) test indicates the possible existence of a heteroscedastic cointegrating relationship. Therefore, we use heteroscedastic models (GARCH and Exponential GARCH) to represent the cointegrating regression instead of the simple homoscedastic model used by BT. The empirical evidence cannot reject the null hypothesis of a stationary fractional cointegration relationship between the Eurodollar and Treasury bill interest rates. © 2001 Kluwer Academic Publishers. © 2018 Elsevier B.V., All rights reserved.","Shrestha, K.; Welch, R.L.",2001,10.1023/a:1008340408261,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846540251&doi=10.1023%2FA%3A1008340408261&partnerID=40&md5=899be94c4ad1bef4c2b113c4e3aaee7f,scopus,"This paper investigates the fractional cointegration between expected Eurodollar and Treasury bill interest rates using refined definitions and advanced estimation methods (Maximum Likelihood, GARCH, Exponential GARCH). The findings suggest a stationary fractional cointegration relationship between these interest rates, contradicting previous homoscedastic models.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:17:49.937277
8239e0ec15a172c7,Research on Early Warning of Banking Crises from the Perspective of Credit Structures,"The relationship between credit expansion and banking crises is complex and cannot be fully explained by total credit alone. A systematic analysis of the relationship between the amount and structure of total credit and banking crises is important for an objective prediction of the influence of potential financial risks. This paper, drawing on data from 15 selected countries, delves into the power of credit indicators in the early warning of banking crises from the perspectives of industrial structure, sector structure, and term structure of credit. Various machine learning methods were used, including Logistic Regression, Random Forest, Decision Tree, Support Vector Machine (SVM), Bagging, and Boosting models. The empirical findings indicate that credit expansion plays a crucial role in triggering banking crises. However, total credit is better suited for the early warning of short-term banking crises, whereas credit structure is more useful for the early warning of long-term banking crises. Moreover, in an early warning system, identifying key early warning indicators is more meaningful than merely increasing the number of indicators. Machine learning can somewhat enhance the early warning power, but it may not always be robust. Therefore, more attention should be paid to potential systemic banking crises resulting from an imbalance in credit structure while regulating the total credit threshold. © 2024 Elsevier B.V., All rights reserved.","Yuqin, Z.; Zixuan, L.; Wu, W.",2024,10.19873/j.cnki.2096-0212.2024.03.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210133695&doi=10.19873%2Fj.cnki.2096-0212.2024.03.003&partnerID=40&md5=0337e2a748df7f5e03df0a89befbb474,scopus,"This paper investigates the role of credit structures in the early warning of banking crises, using data from 15 countries and applying various machine learning methods. It finds that while credit expansion is a trigger, total credit is better for short-term warnings and credit structure for long-term warnings. The study emphasizes the importance of identifying key indicators and suggests focusing on credit structure imbalances for systemic crisis prevention.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:18:02.021661
a75b710a9b4a9b93,Research on RMB exchange rate forecast based on the neural network model and the Nelson–Siegel model,"This paper expands the neural network model to predict exchange rate based on the factors extracted from the Nelson–Siegel model. Based on the theory about exchange rate forecasting, interest could be used to predict the movement of exchange rate. Therefore, this paper analyzes the interest rate term structure factors based on the US and China yield curves data, then uses the Nelson–Siegel model to extract the factors of the interest rate term structure. Finally, the factors of yield curves are used as input data to of the neural network model. And the mean forecasting squared errors, mean absolute errors, mean absolute percentage errors of neural network model, Nelson–Siegel regression model, and ARIMA model are compared. The results show that the neural network model has a superior ability to explain the exchange rate fluctuations of the CNY and USD, and the prediction ability is better than the exchange rate prediction ability of the Nelson–Siegel regression model and ARIMA model.",,2020,10.1057/s41283-020-00062-3,,proquest,"This paper proposes a neural network model for RMB exchange rate forecasting, utilizing factors extracted from the Nelson-Siegel model applied to US and China yield curves. The model's predictive performance is compared against the Nelson-Siegel regression model and ARIMA, showing superior accuracy for the neural network.",True,True,True,gemini-2.5-flash-lite,Olav,M,,2025-10-13T16:18:33.182458
2d91d5fdaf6e3910,Research on e-commerce platform credit supervision resilience based on stochastic catastrophe theory; 基于随机突变的电商平台信用监管弹性研究,"With the development of the Internet economy, e-commerce platforms such as Taobao, DiDi, and Uber have received widespread attention. These enterprises have changed people′s lifestyles and have played an important role in promoting consumption and employment. However, the strong growth of the platform market comes with issues and scandals, including false or misleading information, poor quality, fake products, and counterfeits. The increasing number of such disputes has harmed the interests of consumers and has had a significant adverse social impact. Thus, how to achieve effective e-commerce platform supervision has become the focus of the government, practitioners, and researchers. E-commerce platform credit supervision requires collaboration and coordination between the government and e-commerce platforms. However, the collaborative regulation system is vulnerable and subject to various internal and external factors, such as interaction attributes, utility perception, and incentives. Resilience research explores whether changes in these factors lead to sudden changes in participants′ strategy selection, where resilience refers to the ability of the regulation system to maintain the current equilibrium state. Probing the mechanism of credit supervision resilience evolution can help capture the nonlinearity and randomness in the platform-government relationship from a quantitative perspective and provide decision support for developing credit supervision strategies. However, existing research on e-commerce platform credit supervision focuses on finding equilibrium strategies and their existence conditions, and “resilience” is rarely mentioned. In addition, existing resilience research is rarely based on a catastrophe perspective, and sudden changes in credit supervision are difficult to describe. Therefore, this study integrates resilience theory and stochastic catastrophe theory to propose a catastrophe-based resilience measure for the collaborative regulation system, investigate how system resilience is affected by how the system is regulated, and recommend preventive measures to effectively avoid unexpected radical changes. In the first part, the general framework and research context of the study are presented. In the second part, the catastrophe of supervision behavior is described using an evolutionary game and stochastic catastrophe theory. First, an evolutionary game model between the government and e-commerce platforms is proposed, and a payoff matrix is established. On this foundation, the Gaussian White noise is used to show the random disturbance in the game, and Itô stochastic differential equations are introduced to develop a stochastic dynamic system. Subsequently, a probability density function is introduced to build the stochastic cusp catastrophe model, and the catastrophe set is found to explain the catastrophe of the regulation system. In the third section, with the areas of catastrophes identified, the concept of credit regulation resilience is presented based on Holling′s definition of resilience, and a catastrophe-based resilience measure for the collaborative regulation system is proposed. In the fourth part, simulation experiments are conducted to explore the influence of excess return, punishment, the degree of collaboration between the government and e-commerce platforms, and the degree of public and media participation, and the effectiveness of the model is verified with practical cases. Finally, the advice on e-commerce platform credit supervision is presented in terms of results. Some important conclusions and managerial insights are derived. First, catastrophe occurs suddenly whenthe parameters cross the borderline of the catastrophe set. Therefore, controlling the relevant parameters away from the catastrophe set can maintain the effectiveness of the regulation system. Second, the resilience evolution process is consistent with the catastrophe process: the occurrence of catastrophe leads to rapid changes in resilience. Therefore, it is possible to predict the state of the regulation system based on resilience and intervene in time to control the occurrence of catastrophes when the value of resilience is found to change rapidly or be close to zero. Third, different parameters have different effects on resilience. Punishment has a significant impact on resilience, and the two share a “U”-shaped relationship. In addition, under the constraint of catastrophe threshold, resilience decreases with increasing excess returns and increases with increasing collaboration degree and public and media participation. Therefore, it would be more effective to combine resilience and catastrophe conditions to optimize the regulatory strategy. In general, this research integrates resilience theory and stochastic catastrophe theory to study e-commerce platform credit supervision issues, which provides new ideas for e-commerce platform credit supervision research. The study analyzes the impact of system parameter changes on catastrophe degree from a quantitative perspective through resilience measurement, and the conclusion can provide a basis for e-commerce platform supervision warnings and policy optimization. © 2024 Elsevier B.V., All rights reserved.","Xiaochao, W.; She, S.; Guihua, N.",2024,10.13587/j.cnki.jieem.2024.01.020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185965736&doi=10.13587%2Fj.cnki.jieem.2024.01.020&partnerID=40&md5=a3635447abe5d98ab6aa9b0b3e59b0d9,scopus,"This study integrates resilience theory and stochastic catastrophe theory to analyze the credit supervision resilience of e-commerce platforms. It proposes a catastrophe-based resilience measure for collaborative regulation systems involving government and platforms. The research uses an evolutionary game model with Gaussian White noise and Itô stochastic differential equations to develop a stochastic cusp catastrophe model. Simulation experiments explore the influence of factors like excess return, punishment, collaboration degree, and public/media participation on system resilience. The findings suggest controlling parameters away from the catastrophe set, monitoring resilience for early warnings of catastrophic changes, and optimizing regulatory strategies by considering both resilience and catastrophe conditions. The study aims to provide quantitative insights and decision support for e-commerce platform credit supervision.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:18:37.929461
4ee8fbad2db12a73,Research on the Volatility of the Cotton Market under Different Term Structures: Perspective from Investor Attention,"This study performed comprehensive investigations of the complex interconnections between investor attention and cotton futures price volatility under different term structures. In this paper, in-sample analysis, out-of-sample forecast, influencing mechanisms, as well as nonlinear connections are fully explored using several linear model specifications. The results can be summarized as follows: first, investor attention is the Granger causality of the cotton futures volatility and shows significant linear impacts on cotton volatility; second, models incorporated with investor attention significantly improve the prediction accuracy of cotton volatility in the long term compared with the commonly used AR benchmark model; third, the influence of investor attention on cotton volatility may occur through open interest; and fourth, investor attention presents nonlinear impacts on cotton volatility as well. Overall, the results of this article can provide strong supporting evidence for the important roles of investor attention in asset pricing applications. © 2023 Elsevier B.V., All rights reserved.","Zhou, Q.; Zhu, P.; Wu, Y.; Zhang, Y.",2022,10.3390/su142114389,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146714935&doi=10.3390%2Fsu142114389&partnerID=40&md5=44ec11308eaba61a09e9858a300f5c37,scopus,"This study investigates the relationship between investor attention and cotton futures price volatility across different term structures. It employs linear models to analyze Granger causality, forecast accuracy improvements, the mediating role of open interest, and nonlinear impacts of investor attention on volatility. The findings suggest investor attention plays a significant role in asset pricing.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:18:42.071300
a70a5d5938d86e6e,Revealing the correlation between economic indicators and gold prices for forecasting: Medium term forecast framework with data patch,"Predicting gold prices through the analysis of key economic indicators such as inflation rates, Government Bond Yields, and the U.S. Dollar Index, alongside historical Gold Prices, is crucial for enabling investors to better understand market dynamics and make vital decisions to maximize returns. However, previous studies have faced challenges in extracting hidden factors related to gold price prediction from diverse economic indicators, and the comprehensive exploration of gold price data is yet to be fully achieved. To address this, the present study introduces a mid to long-term gold price prediction model named DPformer. This model utilizes a patching strategy to investigate the relationships between different economic indicators and Gold Prices. It also employs a decomposition approach to discover the mid to long-term trend characteristics and yearly seasonal patterns of Gold Prices. The core of the model integrates a Transformer module, which is solely based on an Encoder structure, and enhances it with multiple attention mechanisms and convolutions. This enhancement allows the improved Transformer model to more effectively capture the long-term dependencies of Gold Prices. The empirical results demonstrate that DPformer consistently outperforms a suite of advanced models widely adopted in terms of mid to long-term forecasting accuracy, including LSTM, GRU, Transformer, DLinear, and PatchTST. Notably, for the 30-step gold price prediction task, DPformer achieves a 21.78 % reduction in Mean Squared Error compared to PatchTST. Moreover, by quantitatively analyzing how various economic indicators influence gold price forecasts, this study provides substantial support for investors in making informed decisions at critical moments. © 2025 Elsevier B.V., All rights reserved.","Bao, G.; Niu, Y.; Cui, B.; Ji, W.",2026,10.1016/j.eswa.2025.129594,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105015297138&doi=10.1016%2Fj.eswa.2025.129594&partnerID=40&md5=b39221e28ac6635c40a32273e8371cef,scopus,"This study introduces DPformer, a Transformer-based model for mid to long-term gold price forecasting. It incorporates a patching strategy to analyze relationships between economic indicators (inflation, bond yields, USD index) and gold prices, and uses a decomposition approach to capture trends and seasonal patterns. DPformer demonstrates superior forecasting accuracy compared to several advanced models, including LSTM, GRU, Transformer, DLinear, and PatchTST, with significant improvements in Mean Squared Error.",True,True,True,gemini-2.5-flash-lite,Olav,M,,2025-10-13T16:19:00.579553
91b7a341ab735f04,Revisiting the Dynamic Linkages of Treasury Bond Yields for the BRICS: A Forecasting Analysis,"We examined the dynamic linkages among money market interest rates in the so-called “BRICS” countries (Brazil, Russia, India, China, and South Africa) by using weekly data of the overnight, one-, three-, and six- months, as well as of one year, Treasury bills rates covering the period from January 2005 to August 2019. A long-run relationship among interest rates was established by employing the Vector Error Correction modeling (VECM), which revealed the validation of the Expectation Hypothesis Theory (EH) of the term structure of interest rates, taking into account long-run deviations from equilibrium and inherent nonlinearities. We unveiled short-run dynamic adjustments for the term structure of the BRICS, subject to regime switches. We then used Markov Switching Vector Error Correction models (MS-VECM) to forecast them dynamically during an out-of-sample period of May 2016 through August 2019. The MSIH-VECM forecasts were found to be superior to the VECM approaches. The novelty of our paper is mainly due to the exploration of the possibility of parameter instability as a crucial factor, which might explain the rejection of the restricted version of the cointegration space, and on the dynamic out-of-sample forecasts of the term structure over a more recent time span in order to assess further the usefulness of our nonlinear MS-VECM characterization of the term structure, capturing the effects of the global and domestic financial crisis. © 2022 Elsevier B.V., All rights reserved.","Bekiros, S.; Avdoulas, C.",2020,10.3390/forecast2020006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124554003&doi=10.3390%2Fforecast2020006&partnerID=40&md5=143c1143e1b1541385f42e9f6876da88,scopus,"This study investigates the dynamic relationships between Treasury bill rates in BRICS countries from 2005 to 2019. Using Vector Error Correction Models (VECM) and Markov Switching VECM (MS-VECM), the research validates the Expectation Hypothesis Theory and reveals regime switches affecting short-run adjustments. The MS-VECM model demonstrated superior forecasting accuracy compared to standard VECM, particularly in capturing effects of financial crises. The paper highlights parameter instability and provides dynamic out-of-sample forecasts.",True,True,True,gemini-2.5-flash-lite,Olav,M,,2025-10-13T16:19:29.238682
a83ca3456d3a610f,Reweighted Price Relative Tracking System for Automatic Portfolio Optimization,"In this paper, we propose a novel reweighted price relative tracking (RPRT) system for automatic portfolio optimization (APO). In the price prediction stage, it automatically assigns separate weights to the price relative predictions according to each asset's performance, and these weights will also be automatically updated. In the portfolio optimizing stage, a novel tracking system with a generalized increasing factor is proposed to maximize the future wealth of next period. Besides, an efficient algorithm is designed to solve the portfolio optimization objective, which is applicable to large-scale and time-limited situations. Extensive experiments on six benchmark datasets from real financial markets with diverse assets and different time spans are conducted. RPRT outperforms other state-of-the-art systems in cumulative wealth, mean excess return, annual percentage yield, and some typical risk metrics. Moreover, it can withstand considerable transaction costs and runs fast. It indicates that RPRT is an effective and efficient APO system.",Z. -R. Lai; P. -Y. Yang; L. Fang; X. Wu,2020,10.1109/tsmc.2018.2852651,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8411138,ieeexplore,"This paper introduces a novel Reweighted Price Relative Tracking (RPRT) system for automatic portfolio optimization (APO). The system features automatic weighting of price relative predictions based on asset performance and a tracking system with a generalized increasing factor to maximize future wealth. An efficient algorithm is provided for large-scale, time-limited scenarios. Experiments on benchmark datasets show RPRT outperforms existing systems in cumulative wealth, returns, and risk metrics, while being robust to transaction costs and fast.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:19:33.512856
4496875730ddc067,Risk Assessments and Risk Premiums in the Eurodollar Market,"Increasing awareness of the potential risks involved in lending to heavily indebted governments focuses attention on credit pricing in the Eurodollar market. This paper utilizes a recent survey of country-by-country risk assessments as perceived by lenders to show that a systematic relationship exists between these assessments and interest rates in the Euromarket. The relationship is derived from an underlying model described in the paper. The estimated parameters verify a number of hypotheses, providing insights on the loss rates lenders expect to incur in case of default.",,1982,10.1111/j.1540-6261.1982.tb02217.x,,proquest,"This paper investigates the relationship between lender perceptions of country-specific risks and interest rates in the Eurodollar market. It uses a survey of risk assessments to demonstrate a systematic link, derived from a theoretical model, and estimates parameters to understand expected loss rates in case of default.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:19:37.580742
6d8b6b03952fab6a,"Risk, Return and Volatility Feedback: A Bayesian Nonparametric Analysis","In this paper, we let the data speak for itself about the existence of volatility feedback and the often debated risk–return relationship. We do this by modeling the contemporaneous relationship between market excess returns and log-realized variances with a nonparametric, infinitely-ordered, mixture representation of the observables’ joint distribution. Our nonparametric estimator allows for deviation from conditional Gaussianity through non-zero, higher ordered, moments, like asymmetric, fat-tailed behavior, along with smooth, nonlinear, risk–return relationships. We use the parsimonious and relatively uninformative Bayesian Dirichlet process prior to overcoming the problem of having too many unknowns and not enough observations. Applying our Bayesian nonparametric model to more than a century’s worth of monthly US stock market returns and realized variances, we find strong, robust evidence of volatility feedback. Once volatility feedback is accounted for, we find an unambiguous positive, nonlinear, relationship between expected excess returns and expected log-realized variance. In addition to the conditional mean, volatility feedback impacts the entire joint distribution.",,2018,10.3390/jrfm11030052,,proquest,"This paper uses a Bayesian nonparametric model to analyze the relationship between market excess returns and realized variances. The model allows for non-Gaussian distributions and nonlinear risk-return relationships. The analysis of over a century of US stock market data reveals strong evidence of volatility feedback and a positive, nonlinear relationship between expected excess returns and expected log-realized variance after accounting for volatility feedback.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:19:41.914483
98742387c3e762c5,Risk-neutral valuation of participating life insurance contracts in a stochastic interest rate environment,"Over the last years, the valuation of life insurance contracts using concepts from financial mathematics has become a popular research area for actuaries as well as financial economists. In particular, several methods have been proposed of how to model and price participating policies, which are characterized by an annual interest rate guarantee and some bonus distribution rules. However, despite the long terms of life insurance products, most valuation models allowing for sophisticated bonus distribution rules and the inclusion of frequently offered options assume a simple Black-Scholes setup and, more specifically, deterministic or even constant interest rates.We present a framework in which participating life insurance contracts including predominant kinds of guarantees and options can be valuated and analyzed in a stochastic interest rate environment. In particular, the different option elements can be priced and analyzed separately. We use Monte Carlo and discretization methods to derive the respective values.The sensitivity of the contract and guarantee values with respect to multiple parameters is studied using the bonus distribution schemes as introduced in [Bauer, D., Kiesel, R., Kling, A., Russ, J., 2006. Risk-neutral valuation of participating life insurance contracts. Insurance: Math. Econom. 39, 171-183]. Surprisingly, even though the value of the contract as a whole is only moderately affected by the stochasticity of the short rate of interest, the value of the different embedded options is altered considerably in comparison to the value under constant interest rates. Furthermore, using a simplified asset portfolio and empirical parameter estimations, we show that the proportion of stock within the insurer's asset portfolio substantially affects the value of the contract. Published by Elsevier B. V.","Zaglauer, Katharina; Bauer, Daniel",2008,10.1016/j.insmatheco.2007.09.003,,wos,"This paper presents a framework for valuing participating life insurance contracts in a stochastic interest rate environment, using Monte Carlo and discretization methods. It analyzes the sensitivity of contract and guarantee values to various parameters, finding that stochastic interest rates significantly impact embedded options, and that the proportion of stocks in the insurer's portfolio affects contract value. The study contrasts these findings with models assuming constant interest rates.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:19:46.610512
e0a69d7554298af5,Risk-return relationship in equity markets: Using a robust GMM estimator for GARCH-M models,"While most asset pricing models postulate a positive relationship between excess returns and risk, there is no consensus on the nature of the relationship due to conflicting empirical evidence. The relationship is particularly ambiguous within a GARCH-M framework. This paper demonstrates that such a conflict can be attributed primarily to the downward bias of standard estimators that neglect additive outliers (AO) commonly observed in financial returns, and proposes a feasible estimation method (RGMME) for the GARCH-M model based upon a robust variant of the GMM. Monte Carlo experiments demonstrate that AOs cause more serious bias in the ML and GMM estimates of the relationship coefficient than previously expected. Therefore, in the presence of AOs, the RGMME appears superior to other standard estimators in terms of the root mean square error criterion. There is strong evidence favouring the RGMME over standard estimators based on its empirical application. In particular, it is substantially evident from the results of the RGMME that there is support for a positive relationship between excess returns and conditional volatility for all three major equity markets. © 2009 Elsevier B.V., All rights reserved.","Park, B.-J.",2009,10.1080/14697680801898584,https://www.scopus.com/inward/record.uri?eid=2-s2.0-61449224631&doi=10.1080%2F14697680801898584&partnerID=40&md5=299f690c41cd8d9b3024ed2a38d344d4,scopus,"This paper addresses the conflicting empirical evidence on the risk-return relationship in equity markets within a GARCH-M framework. It proposes a robust GMM estimator (RGMME) to account for additive outliers, which are shown to bias standard estimators. Monte Carlo experiments and empirical applications suggest that RGMME is superior and provides strong evidence for a positive relationship between excess returns and conditional volatility in major equity markets.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:19:52.627089
95cf4c32af8b022e,SCORING: FAILURE RISK MANAGEMENT TOOL FOR SMES IN ALGERIA,"Algerian public banks need to implement credit risk management techniques tailored to the specific characteristics of SMEs to prevent the deterioration of the banks' solvency due to the degradation of the quality of their SME portfolios. In this regard, our main objective is to highlight the interest that credit risk management will have within the People's Credit of Algeria by developing a Credit Scoring model based on the logistic regression technique, using a sample of 226 SMEs. The study results demonstrate the importance of the logistic regression model in classifying companies and its predictive ability for default, with a good classification rate of 91.2%.",,2024,10.2478/bsaft-2024-0018,,proquest,"This study develops a credit scoring model using logistic regression for Algerian SMEs to manage credit risk in public banks. The model achieved a 91.2% classification rate, demonstrating its effectiveness in predicting default.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:19:56.345205
10c0455867adbe4c,SF-Transformer: A Mutual Information-Enhanced Transformer Model with Spot-Forward Parity for Forecasting Long-Term Chinese Stock Index Futures Prices,"The complexity in stock index futures markets, influenced by the intricate interplay of human behavior, is characterized as nonlinearity and dynamism, contributing to significant uncertainty in long-term price forecasting. While machine learning models have demonstrated their efficacy in stock price forecasting, they rely solely on historical price data, which, given the inherent volatility and dynamic nature of financial markets, are insufficient to address the complexity and uncertainty in long-term forecasting due to the limited connection between historical and forecasting prices. This paper introduces a pioneering approach that integrates financial theory with advanced deep learning methods to enhance predictive accuracy and risk management in China’s stock index futures market. The SF-Transformer model, combining spot-forward parity and the Transformer model, is proposed to improve forecasting accuracy across short and long-term horizons. Formulated upon the arbitrage-free futures pricing model, the spot-forward parity model offers variables such as stock index price, risk-free rate, and stock index dividend yield for forecasting. Our insight is that the mutual information generated by these variables has the potential to significantly reduce uncertainty in long-term forecasting. A case study on predicting major stock index futures prices in China demonstrates the superiority of the SF-Transformer model over models based on LSTM, MLP, and the stock index futures arbitrage-free pricing model, covering both short and long-term forecasting up to 28 days. Unlike existing machine learning models, the Transformer processes entire time series concurrently, leveraging its attention mechanism to discern intricate dependencies and capture long-range relationships, thereby offering a holistic understanding of time series data. An enhancement of mutual information is observed after introducing spot-forward parity in the forecasting. The variation of mutual information and ablation study results highlights the significant contributions of spot-forward parity, particularly to the long-term forecasting. Overall, these findings highlight the SF-Transformer model’s efficacy in leveraging spot-forward parity for reducing uncertainty and advancing robust and comprehensive approaches in long-term stock index futures price forecasting.",,2024,10.3390/e26060478,,proquest,"This paper proposes the SF-Transformer model, which integrates spot-forward parity with the Transformer architecture, to improve the accuracy of long-term Chinese stock index futures price forecasting. The model leverages mutual information from variables like stock index price, risk-free rate, and dividend yield, demonstrating superior performance over LSTM, MLP, and arbitrage-free pricing models in predicting prices up to 28 days ahead. The study highlights the significant contribution of spot-forward parity, especially for long-term forecasting.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:20:03.780491
a1733dd5c20d41d0,SPCM: A Machine Learning Approach for Sentiment-Based Stock Recommendation System,"Recommendation systems play a pivotal role in delivering user preference information. However, they often face the challenge of information cocoons due to repeated content delivery, particularly prevalent in stock recommendations that are susceptible to investor sentiment. In response to the information cocoons, we propose the Sentiment and Price Combined Model (SPCM), which leverages sentiment features and price factors to predict stock price movements. This novel framework combines collective sentiment analysis with state-of-the-art BERT transformer models and advanced machine learning techniques. Over a three-year period, we collected 40 million stock comments from the Guba platform, extracting investor sentiment conveyed in text information and investigating the impact of metrics such as homophily on stock recommendations. Experimental results indicate that both the volume of posts and the agreement index affect the effectiveness of investor sentiment, while homophily reduces the accuracy of participants’ stock price judgments. The recognition accuracy of the BERT-based sentiment analysis model reaches an impressive 84.12%, and the portfolio constructed by SPCM yields a cumulative return four times that of the industry benchmark. Furthermore, homogeneous quantitative metrics also enhance diversification in stock selection.",J. Wang; Z. Chen,2024,10.1109/access.2024.3357114,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10411881,ieeexplore,"This paper introduces the Sentiment and Price Combined Model (SPCM), a machine learning approach using sentiment analysis (BERT) and price factors to predict stock price movements and improve stock recommendations. The model was trained on 40 million stock comments and demonstrated a significant improvement in portfolio returns compared to the benchmark.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:20:13.484297
996f7b5a24667f4b,"Sentiment, Attention, and Earnings Pricing","We find that investor sentiment restrains the predictability of earnings news on announcement returns but the constraining effect of sentiment on the predictive power of earnings news diminishes as sentiment falls. We document that investor attention works as an important channel in the relation between investor sentiment and announcement returns. Investor attention enhances the immediate price reaction to earnings news by curbing the impact of sentiment on the predictive power of earnings news. Our findings reflect the joint effect of attention and sentiment on the source of excess returns documented in the prior earnings-based market anomaly literature. © 2024 Elsevier B.V., All rights reserved.","Cai, Q.; Yung, K.",2024,10.1080/15427560.2022.2100381,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134382035&doi=10.1080%2F15427560.2022.2100381&partnerID=40&md5=380f51696ca6525d0a7916caf01ca30e,scopus,"This study investigates how investor sentiment and attention influence the pricing of earnings news and announcement returns. It finds that sentiment initially restrains predictability but this effect lessens with falling sentiment. Investor attention acts as a channel, enhancing immediate price reactions by mitigating sentiment's impact. The research highlights the combined effect of attention and sentiment on excess returns, contributing to the understanding of earnings-based market anomalies.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:20:21.714731
b426a797302dce5e,Short rate nonlinearities and regime switches,"Using non-parametric estimation methods, various authors have shown distinct non-linearities in the drift and volatility function of the US short rate, which are inconsistent with standard affine term structure models. We document how a regime-switching model with state-dependent transition probabilities between regimes can replicate the patterns found by the non-parametric studies. To do so, we use data from the UK and Germany in addition to US data and include term spreads in some of our models. We also examine the drift and volatility function of the term spread. (C) 2002 Elsevier Science B.V. All rights reserved.","Ang, A; Bekaert, G",2002,10.1016/s0165-1889(01)00042-2,,wos,"This paper uses a regime-switching model with state-dependent transition probabilities to replicate non-linearities in the US short rate's drift and volatility, as identified by non-parametric methods. The study extends its analysis to UK and German data, incorporating term spreads and examining their drift and volatility functions.",True,True,False,gemini-2.5-flash-lite,Olav,M,,2025-10-13T16:20:50.115038
967c7f9c55b36c26,Shrinkage drift parameter estimation for multi-factor ornstein-uhlenbeck processes,"We consider some inference problems concerning the drift parameters of multi-factors Vasicek model (or multivariate Ornstein-Uhlebeck process). For example, in modeling for interest rates, the Vasicek model asserts that the term structure of interest rate is not just a single process, but rather a superposition of several analogous processes. This motivates us to develop an improved estimation theory for the drift parameters when homogeneity of several parameters may hold. However, the information regarding the equality of these parameters may be imprecise. In this context, we consider Stein-rule (or shrinkage) estimators that allow us to improve on the performance of the classical maximum likelihood estimator (MLE). Under an asymptotic distributional quadratic risk criterion, their relative dominance is explored and assessed. We illustrate the suggested methods by analyzing interbank interest rates of three European countries. Further, a simulation study illustrates the behavior of the suggested method for observation periods of small and moderate lengths of time. Our analytical and simulation results demonstrate that shrinkage estimators (SEs) provide excellent estimation accuracy and outperform the MLE uniformly. An over-ridding theme of this paper is that the SEs provide powerful extensions of their classical counterparts. Copyright © 2009 John Wiley & Sons, Ltd. © 2010 Elsevier B.V., All rights reserved.","Nkurunziza, S.; Ahmed, S.E.",2010,10.1002/asmb.775,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950822342&doi=10.1002%2Fasmb.775&partnerID=40&md5=87b9e60a35435f4613bac68331ed6461,scopus,"This paper proposes shrinkage estimators for the drift parameters of multi-factor Ornstein-Uhlenbeck processes, motivated by interest rate modeling. The study explores the performance of these estimators against the classical maximum likelihood estimator under quadratic risk, using interbank interest rates from three European countries for illustration. Simulation studies assess the estimators' behavior with short and moderate observation periods, concluding that shrinkage estimators offer improved accuracy and outperform the MLE.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:20:53.301173
2bf6170d727cf04b,Simultaneous nonparametric inference of time series,"We consider kernel estimation of marginal densities and regression functions of stationary processes. It is shown that for a wide class of time series, with proper centering and scaling, the maximum deviations of kernel density and regression estimates are asymptotically Gumbel. Our results substantially generalize earlier ones which were obtained under independence or beta mixing assumptions. The asymptotic results can be applied to assess patterns of marginal densities or regression functions via the construction of simultaneous confidence bands for which one can perform goodness-of-fit tests. As an application, we construct simultaneous confidence bands for drift and volatility functions in a dynamic short-term rate model for the U.S. Treasury yield curve rates data. © Institute of Mathematical Statistics, 2010. © 2010 Elsevier B.V., All rights reserved.","Liu, W.; Wu, W.B.",2010,10.1214/09-aos789,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955161966&doi=10.1214%2F09-AOS789&partnerID=40&md5=ab2390352cc6b054754f9e082002cee6,scopus,"This paper presents a method for simultaneous nonparametric inference of time series, focusing on kernel estimation of marginal densities and regression functions for stationary processes. It demonstrates that for a broad range of time series, the maximum deviations of kernel estimates asymptotically follow a Gumbel distribution. These findings extend previous work by relaxing independence or beta mixing assumptions. The authors apply these asymptotic results to construct simultaneous confidence bands for assessing marginal densities and regression functions, enabling goodness-of-fit tests. An application is provided for constructing confidence bands for drift and volatility functions in a dynamic short-term rate model using U.S. Treasury yield curve rates data.",True,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:21:12.800728
fcf25faf491d1950,Smoothing spline nonlinear nonparametric regression models,"Almost all of the current nonparametric regression methods, such as smoothing splines, generalized additive models, and varying-coefficients models, assume a linear relationship when nonparametric functions are regarded as parameters. In this article we propose a general class of smoothing spline nonlinear nonparametric models that allow nonparametric functions to act nonlinearly. They arise in many fields as either theoretical or empirical models. Our new estimation methods are based on an extension of the Gauss-Newton method to infinite-dimensional spaces and the backfilling procedure. We extend the generalized cross-validation and generalized maximum likeli-hood methods to estimate smoothing parameters. We establish connections between some nonlinear nonparametric models and nonlinear mixed-effects models. We derive approximate Bayesian confidence intervals for inference. We illustrate the methods with an application to term structure of interest rates and conduct simulations to evaluate the finite-sample performance of our methods. © 2012 Elsevier B.V., All rights reserved.","Ke, C.; Wang, Y.",2004,10.1198/016214504000000755,https://www.scopus.com/inward/record.uri?eid=2-s2.0-10844220603&doi=10.1198%2F016214504000000755&partnerID=40&md5=279118ed9878d6bf94922de6cfe6fa44,scopus,"This paper introduces a new class of smoothing spline nonlinear nonparametric models that allow nonparametric functions to act nonlinearly, extending current methods that assume linearity. The proposed estimation methods utilize an extension of the Gauss-Newton method and a backfilling procedure. The authors also extend generalized cross-validation and generalized maximum likelihood for smoothing parameter estimation, establish connections to nonlinear mixed-effects models, and derive approximate Bayesian confidence intervals. The methods are demonstrated with an application to the term structure of interest rates and evaluated through simulations.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T16:33:30.843966
b10f6a1cfc7ff655,Sovereign Default Forecasting in the Era of the COVID-19 Crisis,"The COVID-19 crisis has revealed the economic vulnerability of various countries and, thus, has instigated the systematic exploration and forecasting of sovereign default risks. Multivariate statistical and stochastic process-based sovereign default risk forecasting has a 50-year developmental history. This article describes a continuous, non-homogeneous Markov chain method as the basis for a COVID-19-related sovereign default risk forecast model. It demonstrates the estimation of sovereign probabilities of default (PDs) over a five-year horizon period with the developed model reflecting the impact of the COVID-19 crisis. The COVID-19-adopted Markov model estimates PDs for most countries, including those that are advanced with AAA and AA ratings, to suggest that no sovereign nation's economy is secure from the financial impact of the COVID-19 pandemic. The dynamics of the estimated PDs are indicative of contemporary evidence as experienced in the recent financial crisis. The empirical results of this article have policy implications for foreign investors, sovereign lenders, export finance institutions, foreign trade experts, risk management professionals, and policymakers in the field of finance. The developed model can be used to timely recognize potential problems with sovereign entities in the current COVID-19 crisis and to take appropriate mitigating actions.","Kristof, Tamas",2021,10.3390/jrfm14100494,,wos,"This article proposes a continuous, non-homogeneous Markov chain model to forecast sovereign default risks, specifically incorporating the impact of the COVID-19 crisis. The model estimates probabilities of default over a five-year horizon, suggesting no nation is immune to the pandemic's financial effects. The findings have implications for various financial stakeholders and can aid in early risk recognition and mitigation.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:33:46.880005
f727ea2b065264c6,Sovereign debt spreads in EMU: The time-varying role of fundamentals and market distrust,"This paper provides further analysis on the determinants of sovereign debt spreads for peripheral Eurozone countries since the start of EMU, paying special attention to episodes that characterized the global financial crisis aftermath starting in 2007. More specifically, the purpose of our research is to disentangle the role of fundamental variables and market perception about variations on risk in order to explain the evolution of sovereign spreads in EMU during the recent crisis. Our results, in line with previous literature, show the importance of three groups of observable variables, namely, changes in risk-aversion of creditors, fiscal indebtedness and liquidity variables. In addition, our model includes unobserved components that are estimated through the Kalman filter as time-varying deviation from fixed-mean parameters of spread determinants. This shows the importance of expectations (market sentiments), amplifying (or reducing) the relative importance of the spread determinants over time through the time-varying behavior of the parameters around their steady-state estimates. © 2017 Elsevier B.V., All rights reserved.","Paniagua, J.; Sapena, J.; Tamarit, C.",2017,10.1016/j.jfs.2016.06.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003968366&doi=10.1016%2Fj.jfs.2016.06.004&partnerID=40&md5=30cea9ab8a37d258da1d42b867fa74c8,scopus,"This paper analyzes the determinants of sovereign debt spreads in peripheral Eurozone countries, focusing on the period since the start of EMU and the aftermath of the global financial crisis. It disentangles the roles of fundamental variables (risk aversion, fiscal indebtedness, liquidity) and market perception (expectations, market sentiments) in explaining spread variations. The study uses a model with unobserved components estimated via the Kalman filter to capture time-varying parameters, highlighting how market sentiments amplify or reduce the importance of spread determinants.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:33:49.529403
6d7c5e9a1bb1743e,Sovereign default risk premia: Evidence from the default swap market,"This study explores the risk premia embedded in sovereign default swaps using a term structure model. The risk premia remunerate investors for unexpected changes in the default intensity. A number of interesting results emerge from the analysis. First, the risk premia contribution to spreads decreases over the sample, 2003-07, and rebounds at the start of the 'credit crunch.' Second, daily risk premia co-move with US macro variables and corporate default risk. Third, global factors explain most of Latin American countries' premia, and local factors best explain European and Asian premia. The importance of global factors grows over time. Finally, conditioning on lagged local and global variables at a weekly frequency, sovereign risk premia are highly predictable. (c) 2012 Elsevier B.V. All rights reserved.","Zinna, Gabriele",2013,10.1016/j.jempfin.2012.12.006,,wos,"This study analyzes sovereign default swap data from 2003-07 to understand risk premia. It finds that risk premia, which compensate investors for unexpected changes in default intensity, decreased over the sample and then increased during the credit crunch. Daily risk premia correlate with US macro variables and corporate default risk. Global factors significantly influence Latin American countries' premia, while local factors are more important for European and Asian countries, with the influence of global factors increasing over time. The study also demonstrates that sovereign risk premia are predictable using lagged local and global variables at a weekly frequency.",False,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:33:54.824328
cba55a63862ae3c0,Spatiotemporal adaptive neural network for long-term forecasting of financial time series,"Optimal decision-making in social settings is often based on forecasts from time series (TS) data. Recently, several approaches using deep neural networks (DNNs) such as recurrent neural networks (RNNs) have been introduced for TS forecasting and have shown promising results. However, the applicability of these approaches is being questioned for TS settings where there is a lack of quality training data and where the TS to forecast exhibit complex behaviors. Examples of such settings include financial TS forecasting, where producing accurate and consistent long-term forecasts is notoriously difficult. In this work, we investigate whether DNN-based models can be used to forecast these TS conjointly by learning a joint representation of the series instead of computing the forecast from the raw time-series representations. To this end, we make use of the dynamic factor graph (DFG) to build a multivariate autoregressive model. We investigate a common limitation of RNNs that rely on the DFG framework and propose a novel variable-length attention-based mechanism (ACTM) to address it. With ACTM, it is possible to vary the autoregressive order of a TS model over time and model a larger set of probability distributions than with previous approaches. Using this mechanism, we propose a self-supervised DNN architecture for multivariate TS forecasting that learns and takes advantage of the relationships between them. We test our model on two datasets covering 19 years of investment fund activities. Our experimental results show that the proposed approach significantly outperforms typical DNN-based and statistical models at forecasting the 21-day price trajectory. We point out how improving forecasting accuracy and knowing which forecaster to use can improve the excess return of autonomous trading strategies. © 2021 Elsevier B.V., All rights reserved.","Chatigny, P.; Patenaude, J.-M.; Wang, S.",2021,10.1016/j.ijar.2020.12.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101315001&doi=10.1016%2Fj.ijar.2020.12.002&partnerID=40&md5=eed6e32a846204aecf5b2e3a39ad4c48,scopus,"This paper proposes a novel self-supervised deep neural network architecture using a dynamic factor graph and a variable-length attention-based mechanism (ACTM) for multivariate time series forecasting. The model learns joint representations of financial time series to improve long-term forecasting accuracy, outperforming traditional DNN and statistical models on investment fund activity data for 21-day price trajectory prediction.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:34:13.039010
e1687aa7566d8268,Spot price forecasting for best trading strategy decision support in the Iberian electricity market,"The increasing volatility in electricity markets has reinforced the need for better trading strategies by both sellers and buyers to limit the exposure to losses. Accordingly, this paper proposes an electricity trading strategy based on a mid-term forecast of the average spot price and a risk premium analysis based on this forecast. This strategy can help traders (buyers and sellers) decide whether to trade in the futures market (of varying monthly maturity) or to wait and trade in the spot market. The forecast model consists of an Artificial Neural Network trained with the Long Short Term Memory architecture to predict the average monthly spot prices, using only market price-related data as input variables. Statistical analysis verified the correlation and dependency between variables. The forecast model was trained, validated and tested with price data from the Iberian Electricity Market (MIBEL), in particular the Spanish zone, between January 2015 and August 2019. The last year of this period was reserved for testing the performance of the proposed forecast model and trading strategy. For comparison purposes, the results of a forecasting model trained with the Extreme Learning Machine over the same period are also presented. In addition, the forecasted value of the average monthly spot price was used to perform a risk premium analysis. The results were promising, as they indicated benefits for traders adopting the proposed trading strategy, proving the potential of the forecast model and the risk premium analysis based on this forecast. © 2023 Elsevier B.V., All rights reserved.","Magalhães, B.G.; Bento, P.M.R.; Pombo, J.A.N.; Calado, M.R.A.; Mariano, S.J.P.S.",2023,10.1016/j.eswa.2023.120059,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152124621&doi=10.1016%2Fj.eswa.2023.120059&partnerID=40&md5=2fc811ab19d0dccddc007edf8afeaadc,scopus,"This paper proposes an electricity trading strategy using a Long Short Term Memory (LSTM) Artificial Neural Network to forecast average monthly spot prices in the Iberian Electricity Market (MIBEL). The strategy aims to help traders decide between trading in the futures or spot market by incorporating a risk premium analysis based on the price forecast. The model was trained and tested on data from January 2015 to August 2019, and its performance was compared to an Extreme Learning Machine model. The results suggest the proposed strategy offers benefits to traders.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:34:17.823681
e89622193853160a,Statistical-mechanical aids to calculating term-structure models,"Recent work in statistical mechanics has developed new analytical and numerical techniques to solve coupled stochastic equations. This paper describes application of the very fast simulated reannealing and path-integral methodologies to the estimation of the Brennan and Schwartz two-factor term-structure (time-dependent) model of bond prices. It is shown that these methodologies can be utilized to estimate more complicated n-factor nonlinear models. Applications to other systems are stressed. © 1990 The American Physical Society. © 2015 Elsevier B.V., All rights reserved.","Ingber, L.",1990,10.1103/physreva.42.7057,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000885755&doi=10.1103%2FPhysRevA.42.7057&partnerID=40&md5=7a538a086e652ffbed77c37d63eb9224,scopus,"This paper applies statistical mechanics techniques, specifically very fast simulated reannealing and path-integral methodologies, to estimate the Brennan and Schwartz two-factor term-structure model of bond prices. The authors demonstrate that these methods can be extended to estimate more complex n-factor nonlinear models and suggest broader applications.",True,True,False,gemini-2.5-flash-lite,Olav,M,,2025-10-13T16:34:35.939037
9efd0961480d44fa,Stock Market Over-reaction: The South African Evidence,"It has been suggested that stock markets over-react and that investors pay too much attention to recent “dramatic” news. If over-reaction does occur and prices overshoot then there should be a subsequent revision in the opposite direction. This paper outlines empirical research into the over-reaction hypothesis on the Johannesburg Stock Exchange using data over the period July 1974 to June 1989 for two hundred and four relatively well traded securities. The results are consistent with the over-reaction hypothesis and indicate substantial weak form inefficiencies in the South African stock market in the long-term. The performance of portfolios of shares formed on the basis of prior return data can be predicted and, on average, portfolios of prior ‘losers’ outperformed prior ‘winners’ by about twenty percent over the three years after portfolio formation. Finally, comparison between the empirical results and a similar study for the New York Stock Exchange calls into some question the hypothesis that exceptionally large returns in January in the USA are due to investor tax loss selling. There is evidence of both a January effect and an asymmetric excess returns effect for the South African market but it is less pronounced than for the American market. © 1992, Taylor & Francis Group, LLC. All rights reserved. © 2015 Elsevier B.V., All rights reserved.","Page, M.J.; Way, C.V.",1992,10.1080/10293523.1992.11082314,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649803751&doi=10.1080%2F10293523.1992.11082314&partnerID=40&md5=9cd756ed9454bc64c6b0446e9333f57b,scopus,"This paper investigates the stock market over-reaction hypothesis on the Johannesburg Stock Exchange using data from July 1974 to June 1989. The findings suggest that the market does over-react, leading to inefficiencies where portfolios of 'losers' outperform 'winners' by approximately 20% over three years. The study also notes a less pronounced January effect and asymmetric excess returns in South Africa compared to the US market.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:34:39.511485
f529c768b7421166,Stock Ranking with Multi-Task Learning,"Stock prediction, aiming at predicting the future trends of stocks, plays a key role in stock investment. Towards the investment target, the primary task is selecting the stocks with potentials to obtain the highest excess returns, always regarded as stock ranking. List-wise stock ranking is able to consider the relative comparisons of multiple stocks, approaching the essence of stock ranking most. However, most existing methods fail in list-wise stock ranking, because the information complexity and small number of samples bring in training difficulties. To address these limitations, a novel Deep Multi-Task Learning (DMTL) solution is proposed, called Multi-Task Stock Ranking (MTSR). It utilizes the joint learning framework of DMTL to learn the list-wise stock ranking with the enhancements of auxiliary tasks. With DMTL, the easily-trained tasks act as learning guider, providing extra gradient backpropagation, to help learn the hardly-trained list-wise ranking task. Additionally, Task Relation Attention is utilized to capture the dynamic task relations to achieve better knowledge transfer between tasks. The experiments conducted on real-world stock datasets demonstrate the superiority of MTSR over several state-of-the-art methods. © 2022 Elsevier B.V., All rights reserved.","Ma, T.; Tan, Y.",2022,10.1016/j.eswa.2022.116886,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127127011&doi=10.1016%2Fj.eswa.2022.116886&partnerID=40&md5=8ec2a8a3cf191d9de530898998c026d3,scopus,"This paper proposes a novel Deep Multi-Task Learning (DMTL) solution called Multi-Task Stock Ranking (MTSR) to address the challenges in list-wise stock ranking. MTSR utilizes a joint learning framework with auxiliary tasks to enhance the learning of the primary stock ranking task, incorporating Task Relation Attention for dynamic knowledge transfer. Experiments on real-world stock data show MTSR outperforms existing methods.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:34:55.555362
ab47e4e16b2d8b40,Stock market risk and return: An equilibrium approach,"Empirical evidence that expected stock returns are weakly related to volatility at the market level appears to contradict the intuition that risk and return are positively related. We investigate this issue in a general equilibrium exchange economy characterized by a regime-switching consumption process with time-varying transition probabilities between regimes. When estimated using consumption data, the model generates a complex, nonlinear and time-varying relation between expected returns and volatility, duplicating the salient features of the risk/return trade-off in the data. The results emphasize the importance of time-varying investment opportunities and highlight the perils of relying on intuition from static models.","Whitelaw, RF",2000,10.1093/rfs/13.3.521,,wos,"This paper investigates the relationship between stock market risk and return using a general equilibrium model with a regime-switching consumption process. The model generates a complex, nonlinear, and time-varying relationship between expected returns and volatility, which aligns with empirical observations and challenges static model intuitions.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:34:57.712363
6444a892b0e70ae6,Stock return predictability in emerging markets: Does the choice of predictors and models matter across countries?,"This study aims to examine return predictability in 24 emerging markets disaggregated in different regions. We propose four specifications, including a benchmark model. Then, an augmented model appropriate for each country, including a large set of potential factors, is evaluated. Furthermore, a dynamic multifactor model is investigated for all countries. Finally, we relax the symmetric hypothesis in asset return predictability based on a non-parametric non-linear approach: the projection pursuit regression model. Our study reveals three main findings. First, we reject all previous findings supporting a standard model of asset return predictability that is valuable for all countries, as we show that each country has specific domestic factors (both macroeconomic and financial) useful to predict future returns. Second, our empirical framework shows that asset return predictability might be robustly modelled based on non-linear specification based on the projection pursuit regression model. Our findings' explanatory power of out-of-sample estimations is economically relevant. Our results are useful for investors and policy-makers for portfolio diversification and regulation policies.","Hadhri, Sinda; Ftiti, Zied",2017,10.1016/j.ribaf.2017.04.057,,wos,"This study investigates stock return predictability in 24 emerging markets, exploring the impact of predictor choice and model specification. It evaluates benchmark, augmented, and dynamic multifactor models, and introduces a non-parametric non-linear approach (projection pursuit regression). Key findings indicate that domestic factors are crucial for predicting returns in each country, and that non-linear specifications offer robust and economically relevant out-of-sample predictability. The results are relevant for investors and policymakers.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:35:01.106748
d83e112304124b7e,Stock return prediction with multiple measures using neural network models,"In the field of empirical asset pricing, the challenges of high dimensionality, non-linear relationships, and interaction effects have led to the increasing popularity of machine learning (ML) methods. This study investigates the performance of ML methods when predicting different measures of stock returns from various factor models and investigates the feature importance and interaction effects among firm-specific variables and macroeconomic factors in this context. Our findings reveal that neural network models exhibit consistent performance across different stock return measures when they rely solely on firm-specific characteristic variables. However, the inclusion of macroeconomic factors from the financial market, real economic activities, and investor sentiment leads to substantial improvements in the model performance. Notably, the degree of improvement varies with the specific measures of stock returns under consideration. Furthermore, our analysis indicates that, after the inclusion of macroeconomic factors, there is a dissimilarity in model performance, variable importance, and interaction effects among macroeconomic and firm-specific variables, particularly concerning abnormal returns derived from the Fama–French three- and five-factor models compared with excess returns. This divergence is primarily attributed to the extent to which these factor models remove the variance associated with the macroeconomic variables. These findings collectively offer valuable insights into the efficacy of neural network models for stock return predictions and contribute to a deeper understanding of the intricate relationship between factor models, stock returns, and macroeconomic conditions in the domain of empirical asset pricing.",,2024,10.1186/s40854-023-00608-w,,proquest,"This study explores the use of neural network models for predicting stock returns, considering both firm-specific characteristics and macroeconomic factors. The research finds that incorporating macroeconomic data significantly enhances prediction performance, with the degree of improvement varying based on the specific stock return measure used. The study also highlights differences in model performance and variable importance when predicting abnormal returns versus excess returns, attributing this to how well factor models account for macroeconomic variance.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:35:08.557880
47139e88004ecf9f,Stock return prediction: Stacking a variety of models,"We employ an ensemble learning approach, “stacking”, to refine and combine a variety of linear and nonlinear individual stock return prediction models. In an application of forecasting U.S. market excess return, stacking with a simple structure can outperform the traditional historical mean benchmark, Mallows model averaging, simple combination forecast, complete subset regression, combination elastic net forecast, and several other models in terms of both in- and out-of-sample performance measures on a consistent basis. More importantly, we find that the out-of-sample gains of stacking are especially evident during extreme downside market movements. Overall, stacking can generate substantive improvements in market excess return predictability. © 2022 Elsevier B.V., All rights reserved.","Zhao, A.B.; Cheng, T.",2022,10.1016/j.jempfin.2022.04.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129550064&doi=10.1016%2Fj.jempfin.2022.04.001&partnerID=40&md5=d484a59706ba72cf605ca0e12ac292de,scopus,"This study utilizes an ensemble learning technique called stacking to improve stock return prediction by combining various linear and nonlinear models. The approach demonstrates superior performance compared to traditional benchmarks and other combination methods, particularly in predicting excess market returns and during periods of significant market downturns.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:35:12.400635
9968a1106eb5e0f0,Stripping the Swiss discount curve using kernel ridge regression,"We analyze and implement the kernel ridge regression (KR) method developed in Filipovic et al. (Stripping the discount curve—a robust machine learning approach. Swiss Finance Institute Research Paper No. 22–24. SSRN. https://ssrn.com/abstract=4058150, 2022) to estimate the risk-free discount curve for the Swiss government bond market. We show that the insurance industry standard Smith–Wilson method is a special case of the KR framework. We recapitulate the curve estimation methods of the Swiss Solvency Test (SST) and the Swiss National Bank (SNB). In an extensive empirical study covering the years 2010–2022 we compare the KR curves with the SST and SNB curves. The KR method proves to be robust, flexible, transparent, reproducible and easy to implement, and outperforms the benchmarks in- and out-of-sample. We show the limitations of all methods for extrapolating the yield curve and propose possible solutions for the extrapolation problem. We conclude that the KR method is the preferred method for estimating the discount curve. © 2024 Elsevier B.V., All rights reserved.","Camenzind, N.; Filipovic, D.",2024,10.1007/s13385-024-00386-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195370618&doi=10.1007%2Fs13385-024-00386-4&partnerID=40&md5=f8b724d5222c2422b156f081fb57b69f,scopus,"This paper implements and analyzes the kernel ridge regression (KR) method to estimate the risk-free discount curve for the Swiss government bond market. It demonstrates that the Smith-Wilson method is a special case of KR and compares KR with the Swiss Solvency Test (SST) and Swiss National Bank (SNB) methods. The study finds KR to be robust, flexible, transparent, reproducible, and superior to benchmarks in empirical tests from 2010-2022. Limitations in yield curve extrapolation are discussed, and KR is recommended as the preferred method.",True,True,False,gemini-2.5-flash-lite,Olav,M,,2025-10-13T16:35:36.923669
76f158d70acb9ec7,Structural models of corporate bond pricing with maximum likelihood estimation,"This paper empirically examines the proxy, volatility-restriction (VR) and maximum likelihood (ML) approaches to implementing structural corporate bond pricing models, and documents that ML estimation is the best among the three implementation methods. Empirical studies using either the proxy approach or the VR method conclude that barrier-independent models significantly underestimate corporate bond yields. Although barrier-dependent models tend to overestimate the yield on average, they generate a sizable degree of underestimation. The present paper shows that the proxy approach is an upwardly biased estimator of the corporate assets and makes the empirical framework work systematically against structural models of corporate bond pricing. The VR approach may generate inconsistent corporate bond prices or may fail to give a positive corporate bond price for some structural models. When the Merton, LS, BD and LT models are implemented with ML estimation, we find substantial improvement in their performances. Our empirical analysis shows that the LT model is very accurate for predicting short-term bond yields, whereas the LS and BD models are good predictors for medium-term and long-term bonds. The Merton model however significantly overestimates short-term bond yields and underestimates long-term bond yields. Unlike empirical studies in the past, the Merton model implemented with ML estimation does not consistently underestimate corporate bond yields. All rights reserved, Elsevier",,2008,10.1016/j.jempfin.2008.01.001,,proquest,"This paper compares three methods for implementing structural corporate bond pricing models: proxy, volatility-restriction (VR), and maximum likelihood (ML). It finds that ML estimation performs best. The study demonstrates that the proxy approach is a biased estimator and the VR approach can lead to inconsistent prices. When implemented with ML, the Merton, LS, BD, and LT models show improved performance. The LT model is accurate for short-term yields, LS and BD for medium- to long-term yields, and the Merton model's performance improves significantly compared to previous studies.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:35:44.286070
cd2aff9d49f38305,"Supply, demand, and risk premiums in electricity markets","We model the impact of supply and demand on risk premiums in electricity futures, using daily data for 2003-2014. The model provides a satisfactory fit and allows for unspanned economic risk not embedded in futures prices. Model-implied spot risk premiums and forward biases are large, negative, highly time-varying, and exhibit plausible seasonal patterns. They differ from existing models, especially in periods of market turmoil, have not decreased in size over time, and help predict future returns. Both demand and supply have an economically significant impact on risk premiums. The risk premium associated with supply is characterized by large positive outliers. (c) 2021 Elsevier B.V. All rights reserved.","Jacobs, Kris; Li, Yu; Pirrong, Craig",2022,10.1016/j.jbankfin.2021.106390,,wos,"This study models the impact of supply and demand on risk premiums in electricity futures using daily data from 2003-2014. The model, which fits the data well, reveals unspanned economic risk and shows that model-implied spot risk premiums and forward biases are large, negative, time-varying, and have seasonal patterns. These premiums differ from existing models, especially during market turmoil, have not diminished over time, and aid in predicting future returns. Both supply and demand significantly influence risk premiums, with supply-related risk premiums exhibiting large positive outliers.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:35:48.038921
5b3f2c4540a8a7c5,THE REGIME-DEPENDENT EVOLUTION of CREDIBILITY: A FRESH LOOK at Hong Kong'S LINKED EXCHANGE RATE SYSTEM,"An estimated Markov-switching DSGE modeling framework that allows for parameter shifts across regimes is employed to test the hypothesis of regime-dependent credibility of Hong Kong's linked exchange rate system. The baseline model distinguishes two regimes with respect to the time-series properties of the risk premium. Regime-dependent impulse responses to macroeconomic shocks reveal substantial differences in spreads. To test the sensitivity of the results, a number of robustness checks are performed. The findings contribute to efforts at modeling exchange rate regime credibility as a nonlinear process with two distinct regimes. © 2019 Elsevier B.V., All rights reserved.","Blagov, B.; Funke, M.",2019,10.1017/s136510051700075x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045762185&doi=10.1017%2FS136510051700075X&partnerID=40&md5=e7efb5e1e34ec2ce7ac00bf000c579d0,scopus,"This study uses a Markov-switching DSGE model to analyze the credibility of Hong Kong's linked exchange rate system, considering parameter shifts across different economic regimes. The model identifies two distinct regimes based on risk premium time-series properties and shows regime-dependent responses to macroeconomic shocks, impacting spreads. Robustness checks confirm these findings, suggesting that exchange rate regime credibility can be modeled as a nonlinear process with two regimes.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:35:51.351311
45482d9d94cd349f,THE STRUCTURE OF SKEWNESS PREFERENCES IN ASSET PRICING MODELS WITH HIGHER MOMENTS: AN EMPIRICAL TEST,"In this paper, the authors employ a nonlinear formulation to examine empirically the structural content of the three moment capital asset pricing model (CAPM). Whereas previous research focused on the coefficients of beta and co‐skewness, this paper presents empirical results on the market risk premium and elasticity coefficient components of these two coefficients. The results indicate that although the estimated coefficient of coskewness gives important information on the marginal rate of substitution between skewness and expected return, the elasticity coefficient can provide additional (albeit different) information on skewness preference that is independent of the effects of the market risk premium. This research also shows how the non‐linear formulation provides a direct linkage between the twomoment and three‐moment CAPM versions and thus provides an empirical test of the theoretical conditions under which skewness preference is consistent with the two‐moment CAPM empiricial results. Copyright © 1988, Wiley Blackwell. All rights reserved © 2016 Elsevier B.V., All rights reserved.","Sears, R.S.; Wei, K.C.J.",1988,10.1111/j.1540-6288.1988.tb00772.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0013178861&doi=10.1111%2Fj.1540-6288.1988.tb00772.x&partnerID=40&md5=4187f72b11f820ea8b517dd3bcad4551,scopus,"This paper empirically tests a nonlinear formulation of the three-moment capital asset pricing model (CAPM). It investigates the market risk premium and elasticity coefficient components of beta and co-skewness, finding that the elasticity coefficient offers independent information on skewness preference. The nonlinear formulation also links the two- and three-moment CAPM versions, allowing for an empirical test of theoretical conditions.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:35:54.166645
c3de53dd92c70087,Targeting Long Rates in a Model with Segmented Markets,"This paper develops a model of segmented financial markets in which the net worth of financial institutions limits the degree of arbitrage across the term structure. The model is embedded into the canonical Dynamic New Keynesian (DNK) framework. We estimate the model using data on the term premium. Our principal results include the following. First, the estimated segmentation coefficient implies a nontrivial effect of central bank asset purchases on yields and real activity. Second, there are welfare gains to having the central bank respond to the term. premium, e.g., including the term premium in the Taylor Rule. Third, a policy that directly targets the term premium sterilizes the real economy from shocks originating in the financial sector. A term-premium peg can have sigmficant welfare effects. (ILL E12, E23, E31, E43, E52, E58)","Carlstrom, Charles T.; Fuerst, Timothy S.; Paustian, Matthias",2017,10.1257/mac.20150179,,wos,"This paper presents a model of segmented financial markets where institutional net worth restricts arbitrage across the term structure. Embedded within the Dynamic New Keynesian (DNK) framework, the model is estimated using term premium data. Key findings suggest that central bank asset purchases significantly impact yields and economic activity, and that central bank policy should consider the term premium for welfare gains and to insulate the real economy from financial shocks.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:35:58.694465
bf72d6fc89554dd0,Technical indicators and aggregate stock returns: An updated look,"We provide updated analyses of technical indicators and aggregate stock return forecasting. We construct 105 new technical indicators as big data predictors and adopt eight advanced shrinkage methods in our forecasting analyses. Our evidence suggests that the refinements of 105 technical factors successfully overcome those of Neely et al.’s (2014) 14 technical variables to a large extent and challenge the forecasting role of Welch and Goyal's (2008) 14 popular macroeconomic variables when ENet and Lasso are used. The excellent performance of the forecasting information based on 105 technical indicators generates sufficiently high in-sample and out-of-sample R-squared values and economically sizable gains in forecasting the excess returns of the composite Standard & Poor 500 market. The corresponding evidence remains robust to changes in the business cycle, forecasting horizons, and alternative evaluation periods. © 2025 Elsevier B.V., All rights reserved.","Shi, Q.",2025,10.1016/j.mulfin.2025.100898,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216555986&doi=10.1016%2Fj.mulfin.2025.100898&partnerID=40&md5=d990bba991d2907cab343d027f7fee3d,scopus,"This study updates the analysis of technical indicators for aggregate stock return forecasting. It introduces 105 new technical indicators and employs eight advanced shrinkage methods. The findings indicate that these new indicators significantly improve forecasting compared to previous work and challenge the predictive power of popular macroeconomic variables, particularly when using ENet and Lasso. The results show high in-sample and out-of-sample R-squared values and economically significant gains in forecasting S&P 500 excess returns, with robustness across different business cycles, forecasting horizons, and evaluation periods.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:36:06.914912
f0b7502b7b81fe47,Temperature effects on crop yields in heat index insurance,"Heat can cause substantial yield losses in crop production and climate change is increasing the risk of this kind of damage. Weather index insurance can help to reduce the financial losses resulting from heat exposure. This paper introduces crop-specific payout functions based on restricted cubic splines in heat index insurance. The use of restricted cubic splines is a cutting-edge method to reflect empirically estimated temperature effects on crop yields and to estimate temperature-related yield losses. The integration of these temperature effects in payout functions facilitates insurance design and allows hourly temperatures to be used as the underlying index. An empirical analysis is used to assess heat stress effects for a panel of East German winter wheat and winter rapeseed producers, to calibrate insurance contracts accordingly and simulate the resulting risk reducing capacities. We find that the insurance scheme introduced here leads to statistically and economically significant out-of-sample risk reducing capacities for farmers, i.e. risk premiums are reduced by up to approximately 20% at the median, in comparison to the uninsured status and at the actuarially fair premium. Moreover, we highlight that policy-makers can support the cost-efficient provision of market-based weather index insurance by fostering data collection and data","Bucheli, Janic; Dalhaus, Tobias; Finger, Robert",2022,10.1016/j.foodpol.2021.102214,,wos,"This paper introduces crop-specific payout functions for heat index insurance using restricted cubic splines to model temperature effects on crop yields. An empirical analysis on East German winter wheat and rapeseed producers demonstrates the insurance's risk-reducing capacity, potentially lowering risk premiums by up to 20%. The study suggests policy support for data collection to enhance market-based weather index insurance.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:36:11.778418
d20a95b2f68502a7,Testing different forms of efficiency for Dhaka Stock Exchange,"The Efficient-Market Hypothesis (EMH) asserts that efficient markets are informationally efficient or all information (market, public or private) should reflect on stock prices. No one could earn excess profit using any kind of information in efficient market. There are three forms of efficiency in markets: strong, semi-strong and weak. We tested EMH for Dhaka Stock Exchange (DSE) for the period 2003 2005. We used the excess return market model to test the semi-strong form efficiency of DSE. Two forecasting techniques, Autoregressive Integrated Moving Average (ARIMA) and neural network, are used to test the weak form efficiency of DSE. We get excess return for many stocks listed in DSE, demonstrating that DSE is not an efficient market in semi-strong form. Besides, the DSE market index is not random and the trend could be captured by ARIMA and neural network techniques. Therefore, the DSE is also not an efficient market in weak form.",,2011,10.1504/ijfsm.2011.038325,,proquest,"This study tests the Efficient-Market Hypothesis (EMH) for the Dhaka Stock Exchange (DSE) from 2003-2005. It uses the excess return market model to assess semi-strong form efficiency and ARIMA and neural network techniques to evaluate weak form efficiency. The findings indicate that the DSE is not efficient in either the semi-strong or weak form, as excess returns were observed and market index trends could be captured by forecasting models.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:36:21.386692
00b59708a2208b21,Testing predictability and nonlinear dependence in the Indian stock market,"This paper suggests a systematic approach to studying predictability and nonlinear dependence in the context of the Indian stock market, one of the most important emerging stock markets in the world. The proposed approach considers nonlinear dependence in returns and envisages appropriate specification of both the conditional first- and second-order moments, so that final conclusions are free from any probable statistical consequences of misspecification. To this end, a number of rigorous tests are applied on the returns, based on four major daily indices of the Indian stock market. It is found that the Indian stock market is predictable, and this observed lack of efficiency is due to serial correlation, nonlinear dependence, day-of-the week effects, parameter instability, conditional heteroskedasticity (GARCH), daily-level seasonality in volatility, the short-term interest rate (in some subperiods of some indices), and some dynamics in the higher-order moments.","Sarkar, N; Mukhopadhyay, D",2005,10.1080/1540496x.2005.11052624,,wos,"This paper investigates predictability and nonlinear dependence in the Indian stock market using a systematic approach that accounts for nonlinearities in returns and specifies conditional moments. Applying rigorous tests to daily index returns, the study finds the market to be predictable due to factors like serial correlation, nonlinear dependence, day-of-the-week effects, parameter instability, GARCH, seasonality in volatility, interest rates, and higher-order moment dynamics.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:36:25.560552
1144b886f92f558b,The Case for Causal Factor Investing,"Researchers use factor models to obtain unbiased estimates of the premia harvested by assets exposed to certain risk characteristics. These estimates are unbiased only if the factor models are correctly specified. Choosing the correct model specification requires knowledge of the causal graph that characterizes the underlying data-generating process. Following the current econometric canon, however, factor researchers choose their model specifications using associational (noncausal) arguments, such as the model's explanatory power, instead of applying causal inference procedures, such as do-calculus. As a result, factor investing models are likely misspecified, and the estimates of risk premia are biased. This article explains the dire consequences of factor investing's specification errors and calls for the need to rebuild the discipline under the more scientific foundations of causal factor investing. © 2024 Elsevier B.V., All rights reserved.","de Prado, M.; Lipton, A.; Zoonekynd, V.",2024,10.3905/jpm.2024.51.1.146,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209686202&doi=10.3905%2Fjpm.2024.51.1.146&partnerID=40&md5=b31ceb8daae39541bf7abf6b6d68e29d,scopus,"This article argues that current factor investing models are likely misspecified due to the use of associational rather than causal inference methods, leading to biased estimates of risk premia. It calls for a shift towards causal factor investing for more scientifically sound foundations.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:36:28.606172
1aec91d9d8da5a64,The Dynamic Impact of Macro Factors on the Performance of Blended Real Estate Equity Strategies,"This article uses a small number of macro factors to model the performance of private US core real estate and associated blended strategies that incorporate listed and private non-core components. The macro factors selected are economic growth, real rate, expected inflation, the term structure, and credit spreads. Private real estate performance was de-smoothed using a nonlinear modeling approach that accounted for differing smoothing effects during identifiable regimes through market cycles. The estimated linear factor loadings are aligned with economic intuition and expectations, including real estate’s inflation hedging characteristics. Using threshold regression modeling to capture nonlinearities in the relationships, a smaller number of the factors were found to be of greater statistical significance. The impact of these factors is found to evolve over time, particularly during phases of market disruption. Although linear factor modeling remains the common approach to estimate risk–return exposures for asset allocation and portfolio risk management processes, the results suggest that these linear models should be adapted to consider these shifting relationships and resulting implications. © 2025 Elsevier B.V., All rights reserved.","Farrelly, K.; Moss, A.",2025,10.3905/jpm.2025.51.11.142,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017711928&doi=10.3905%2Fjpm.2025.51.11.142&partnerID=40&md5=ef0473a31b5e01f9ac2af83908260360,scopus,"This study models the performance of blended real estate equity strategies using key macro factors like economic growth, real rates, inflation, term structure, and credit spreads. It employs a nonlinear approach to de-smooth private real estate performance, accounting for market cycle variations. The findings indicate that while linear factor models are common, nonlinearities and evolving factor impacts, especially during market disruptions, necessitate model adaptation.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:36:33.554942
7267afe860c9d705,The Informational Content of the Term Spread in Forecasting the US Inflation Rate: A Nonlinear Approach,"The difficulty in modelling inflation and the significance in discovering the underlying data-generating process of inflation is expressed in an extensive literature regarding inflation forecasting. In this paper we evaluate nonlinear machine learning and econometric methodologies in forecasting US inflation based on autoregressive and structural models of the term structure. We employ two nonlinear methodologies: the econometric least absolute shrinkage and selection operator (LASSO) and the machine-learning support vector regression (SVR) method. The SVR has never been used before in inflation forecasting considering the term spread as a regressor. In doing so, we use a long monthly dataset spanning the period 1871:1-2015:3 that covers the entire history of inflation in the US economy. For comparison purposes we also use ordinary least squares regression models as a benchmark. In order to evaluate the contribution of the term spread in inflation forecasting in different time periods, we measure the out-of-sample forecasting performance of all models using rolling window regressions. Considering various forecasting horizons, the empirical evidence suggests that the structural models do not outperform the autoregressive ones, regardless of the model's method. Thus we conclude that the term spread models are not more accurate than autoregressive models in inflation forecasting. Copyright © 2016 John Wiley & Sons, Ltd.",,2017,10.1002/for.2417,,proquest,"This paper evaluates nonlinear machine learning (LASSO, SVR) and econometric methodologies for forecasting US inflation using the term spread. The study utilizes a long monthly dataset from 1871 to 2015 and compares the performance of these models against ordinary least squares regression using rolling window regressions. The findings indicate that structural models do not outperform autoregressive models, and term spread models are not more accurate than autoregressive models for inflation forecasting.",True,True,True,gemini-2.5-flash-lite,Olav,M,,2025-10-13T16:37:14.114746
53693a3db3c1fcab,The Nonlinear Nature of Country Risk and its Implications for DSGE Models,"Country risk premia can substantially affect macroeconomic dynamics. We concentrate on one of their most important determinants- A country's net foreign asset (NFA) position and-in contrast to the existing research-investigate its nonlinear link to risk premia. The importance of this particular nonlinearity is two-fold. First, it allows to identify the NFA level above which the elasticity becomes much (possibly dangerously) higher. Second, such a nonlinear relationship is a standard ingredient of dynamic stochastic general equilibrium (DSGE) models, but its proper calibration/estimation is missing. Our estimation shows that indeed the link is highly nonlinear and helps to identify the NFA position where the nonlinearity kicks in at approximately-70% to-75% of GDP. We also provide a proper calibration of the risk premium-NFA relationship which can be used in DSGE models and demonstrate that its slope matters significantly for economic dynamics in such a model. © 2020 Elsevier B.V., All rights reserved.","Brzoza-Brzezina, M.; Kotłowski, J.",2020,10.1017/s136510051800038x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052611952&doi=10.1017%2FS136510051800038X&partnerID=40&md5=37354b76d72bae70019c8812aed9cfcb,scopus,"This paper investigates the nonlinear relationship between a country's net foreign asset (NFA) position and its risk premium, identifying a critical NFA level (approximately -70% to -75% of GDP) where the elasticity significantly increases. The authors calibrate this nonlinear relationship for use in Dynamic Stochastic General Equilibrium (DSGE) models, demonstrating its substantial impact on economic dynamics.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:37:18.548743
e160e6b80df70894,The October 2014 United States Treasury bond flash crash and the contributory effect of mini flash crashes,"We investigate the causal uncertainty surrounding the flash crash in the U.S. Treasury bond market on October 15, 2014, and the unresolved concern that no clear link has been identified between the start of the flash crash at 9:33 and the opening of the U.S. equity market at 9:30. We consider the contributory effect of mini flash crashes in equity markets, and find that the number of equity mini flash crashes in the three-minute window between market open and the Treasury Flash Crash was 2.6 times larger than the number experienced in any other three-minute window in the prior ten weekdays. We argue that (a) this statistically significant finding suggests that mini flash crashes in equity markets both predicted and contributed to the October 2014 U.S. Treasury Bond Flash Crash, and (b) mini-flash crashes are important phenomena with negative externalities that deserve much greater scholarly attention.",,2017,10.1371/journal.pone.0186688,,proquest,"This paper examines the October 2014 U.S. Treasury bond flash crash, investigating a potential link between mini flash crashes in equity markets and the Treasury market event. The study finds a statistically significant increase in equity mini flash crashes preceding the Treasury flash crash, suggesting they contributed to it and highlighting the need for further research into these phenomena.",False,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:37:24.538949
bb9b735a815837e5,The Predictability of the Exchange Rate When Combining Machine Learning and Fundamental Models,"In 1983, Meese and Rogoff showed that traditional economic models developed since the 1970s do not perform better than the random walk in predicting out-of-sample exchange rates when using data obtained after the beginning of the floating rate system. Subsequently, whether traditional economical models can ever outperform the random walk in forecasting out-of-sample exchange rates has received scholarly attention. Recently, a combination of fundamental models with machine learning methodologies was found to outcompete the predictability of random walk (Amat et al. 2018). This paper focuses on combining modern machine learning methodologies with traditional economic models and examines whether such combinations can outperform the prediction performance of random walk without drift. More specifically, this paper applies the random forest, support vector machine, and neural network models to four fundamental theories (uncovered interest rate parity, purchase power parity, the monetary model, and the Taylor rule models). We performed a thorough robustness check using six government bonds with different maturities and four price indexes, which demonstrated the superior performance of fundamental models combined with modern machine learning in predicting future exchange rates in comparison with the results of random walk. These results were examined using a root mean squared error (RMSE) and a Diebold–Mariano (DM) test. The main findings are as follows. First, when comparing the performance of fundamental models combined with machine learning with the performance of random walk, the RMSE results show that the fundamental models with machine learning outperform the random walk. In the DM test, the results are mixed as most of the results show significantly different predictive accuracies compared with the random walk. Second, when comparing the performance of fundamental models combined with machine learning, the models using the producer price index (PPI) consistently show good predictability. Meanwhile, the consumer price index (CPI) appears to be comparatively poor in predicting exchange rate, based on its poor results in the RMSE test and the DM test. © 2023 Elsevier B.V., All rights reserved.","Zhang, Y.; Hamori, S.",2020,10.3390/jrfm13030048,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091877651&doi=10.3390%2Fjrfm13030048&partnerID=40&md5=352d6320ec6190df592c437a799b8a70,scopus,"This paper investigates the predictability of exchange rates by combining traditional economic models with machine learning techniques (random forest, support vector machine, neural network). The study applies these combinations to four fundamental theories (uncovered interest rate parity, purchase power parity, monetary model, Taylor rule) and evaluates their performance against a random walk benchmark using RMSE and Diebold-Mariano tests. Results indicate that fundamental models augmented with machine learning generally outperform the random walk, with the producer price index showing better predictability than the consumer price index.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:37:35.679442
f4423359be40fb95,The Price of Predictability: Estimating Inconsistency Premiums in Social Interactions,"For financial decision-making, people trade off the expected value (return) and the variance (risk) of an option, preferring higher returns to lower ones and lower risks to higher ones. To make decision-makers indifferent between a risky and risk-free option, the expected value of the risky option must exceed the value of the risk-free option by a certain amount-the risk premium. Previous psychological research suggests that similar to risk aversion, people dislike inconsistency in an interaction partner's behavior. In eight experiments (total N = 2,412) we pitted this inconsistency aversion against the expected returns from interacting with an inconsistent partner. We identified the additional expected return of interacting with an inconsistent partner that must be granted to make decision-makers prefer a more profitable, but inconsistent partner to a consistent, but less profitable one. We locate this inconsistency premium at around 31% of the expected value of the risk-free option.","Gerten, Judith; Zuern, Michael K.; Topolinski, Sascha",2022,10.1177/0146167221998533,,wos,"This study investigates inconsistency aversion in social interactions, analogous to risk aversion in financial decision-making. Through eight experiments with 2,412 participants, the researchers quantified the 'inconsistency premium' – the additional expected return needed to make individuals indifferent between interacting with a profitable but inconsistent partner and a consistent but less profitable one. The findings suggest this premium is approximately 31% of the expected value of the risk-free option.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:37:45.972142
6baa570f6471f39a,The Relationship Between Volatility and Sovereign Credit Risk in the Emerging Markets: A Nonlinear ARDL Approach,"This study investigates the short- and long-run nexus between the volatility index of VIX and sovereign credit risk represented by CDS spread in emerging markets, namely Turkey, China, Russia, Brazil, and Mexico. The emerging markets are at the center of investors' interest due to high return opportunities. The relationship between volatility and sovereign credit risk has been studied many times via linear models. However, financial series exhibit asymmetric dynamics, as volatility clustering, excess kurtosis, and others. Thus, we use nonlinear autoregressive distributed lags (NARDL) analysis to capture nonlinear relations between the volatility and the sovereign credit risks of these countries by using daily data from 04.01.2010 to 29.11.2019. The bounds test of the NARDL model confirms the cointegration between VIX and CDS spreads of the countries under study. The analysis of estimated NARDL parameters shows that negative shocks of the volatility index have a long-lasting impact on CDS spreads. Chinese CDS spread are more sensitive to VIX index changes in the short run. The effect of a decrease in volatility on Russian CDS spread is higher than the effect of an increase. Turkish and Brazilian CDS spreads are more reactive to increase in the VIX, whereas Mexican CDS is less sensitive. These findings show that investors, arbitrageurs and speculators should consider global indicators when taking a position on sovereign bonds of emerging markets.","Yigit, Fatih; Aliyev, Fuzuli",2022,10.21121/eab.1064521,,wos,"This study examines the short- and long-run relationship between the VIX volatility index and sovereign credit risk (CDS spread) in emerging markets (Turkey, China, Russia, Brazil, Mexico) using a nonlinear autoregressive distributed lags (NARDL) approach. The NARDL model captures asymmetric dynamics, revealing that negative VIX shocks have a lasting impact on CDS spreads. The study also highlights country-specific short-run sensitivities and asymmetric effects of volatility changes on CDS spreads.",True,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:37:50.313872
b81f992522b901f7,The Role of Regime Shifts in the Term Structure of Interest Rates: Further Evidence from an Emerging Market,"In this paper, we investigate the interrelationships among Turkish interest rates having different maturities by using a regime-switching vector error correction model. We find a relationship of long-run equilibrium among interest rates having various maturities. Furthermore, we conclude that term structure dynamics exhibit significant nonlinearity. A forecasting experiment also reveals that the nonlinear term structure models fare better in forecasting than other linear specifications. However, we cannot conclude that interest rate adjustments are made in an asymmetric way in the long run. Adapted from the source document.",,2012,10.2753/ree1540-496x4806s504,,proquest,"This paper examines the relationship between Turkish interest rates of different maturities using a regime-switching vector error correction model. It finds a long-run equilibrium and significant nonlinearity in term structure dynamics, with nonlinear models outperforming linear ones in forecasting. However, asymmetric long-run interest rate adjustments were not confirmed.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T16:38:11.209179
03ad76077d8f372b,The SR approach: A new estimation procedure for non-linear and non-Gaussian dynamic term structure models,This paper suggests a new approach for estimating linear and non-linear dynamic term structure models with latent factors. We impose no distributional assumptions on the factors which therefore may be non-Gaussian. The novelty of our approach is to use many observables (yields or bond prices) in the cross-section dimension. This implies that the latent factors can be determined quite accurately by a sequence of cross-section regressions. We also show how output from these regressions can be used to obtain model parameters by a two- or three-step moment-based estimation procedure.,,2015,10.1016/j.jeconom.2014.10.002,,proquest,"This paper introduces a new estimation method for dynamic term structure models, allowing for non-linear and non-Gaussian latent factors. The approach leverages a large number of observable variables (yields or bond prices) to accurately determine these factors through cross-section regressions, which are then used for parameter estimation via a moment-based procedure.",True,True,False,gemini-2.5-flash-lite,Olav,M,,2025-10-13T16:38:28.874703
9478f7d5a656d74f,The Stochastic Stationary Root Model,"We propose and study the stochastic stationary root model. The model resembles the cointegrated VAR model but is novel in that: (i) the stationary relations follow a random coefficient autoregressive process, i.e., exhibhits heavy-tailed dynamics, and (ii) the system is observed with measurement error. Unlike the cointegrated VAR model, estimation and inference for the SSR model is complicated by a lack of closed-form expressions for the likelihood function and its derivatives. To overcome this, we introduce particle filter-based approximations of the log-likelihood function, sample score, and observed Information matrix. These enable us to approximate the ML estimator via stochastic approximation and to conduct inference via the approximated observed Information matrix. We conjecture the asymptotic properties of the ML estimator and conduct a simulation study to investigate the validity of the conjecture. Model diagnostics to assess model fit are considered. Finally, we present an empirical application to the 10-year government bond rates in Germany and Greece during the period from January 1999 to February 2018.",,2018,10.3390/econometrics6030039,,proquest,"This paper introduces the stochastic stationary root (SSR) model, an extension of the cointegrated VAR model. The SSR model features random coefficient autoregressive processes for stationary relations, leading to heavy-tailed dynamics, and incorporates measurement error. Due to the lack of closed-form likelihood expressions, the authors propose particle filter-based approximations for estimation and inference, enabling a stochastic approximation of the Maximum Likelihood (ML) estimator. The study includes a simulation to validate asymptotic properties and model diagnostics. An empirical application to German and Greek government bond rates from 1999-2018 is presented.",True,True,False,gemini-2.5-flash-lite,Olav,M,,2025-10-13T16:39:04.153229
5311c7fe6ee66e24,"The UK Companies Act of 2006, the Sarbanes-Oxley Act of 2002, and important reviews of 2009. Implications for the certainty equivalent coefficient net present value criterion","Purpose - The purpose of this paper is to draw attention to the fact that the certainty equivalent coefficient net present value criterion, CEC(NPV), in disregarding a fundamental requirement for the calculation of cash flows for purposes of discounted cash flow analysis, invalidates this capital budgeting criterion from the perspective of sound research methodology. The paper also investigates the impact of the UK Companies Act of 2006, the Sarbanes-Oxley Act of 2002, and important reviews such as the Turner Review of 2009, the Walker Review of 2009, and the Review of the Combined Code of 2009 on this operationally invalid capital budgeting criterion, as well as its impact on the process of financial managerial decision making. Design/methodology/approach - The CEC(NPV) as a discounted cash flow capital budgeting criterion was examined from the perspective of the axioms of cash flow estimation as well as from the definition of the cost of capital in order to ascertain the contribution of this criterion to financial management. The relevant sections of the UK Companies Act of 2006, the Sarbanes-Oxley Act of 2002, the Turner Review of 2009, the Walker Review of 2009, and the Review of the Combined Code of 2009 were studied in order to establish whether the CEC(NPV) was able to satisfy the requirements of this legislation and these important reviews. Findings - The CEC(NPV) is construct invalid and does not measure what it purports to measure: it over-states financial viability. As a consequence, it does not meet the requirements of sound research methodology and therefore is at odds with the UK Companies Act of 2006, the Sarbanes-Oxley Act of 2002, and falls foul of the Turner Review of 2009, the Walker Review of 2009, the 2009 Review of the Combined Code issued by the Financial Reporting Council. As such it cannot be endorsed by the Financial Services Authority. Originality/value - The paper usefully shows that the CEC(NPV) denies financial managers application of Fisherian analysis for resolving conflicts in the rankings of mutually exclusive projects, and, the comparison of project cost of capital with their respective internal rates of return. Comparisons of the internal rate of return, not with the risk-free rate (that is assumed to be a constant and which exhibits minimal variability in comparison with the cost of capital), but with the cost of capital cost of capital, are a sine qua non for managerial decision making, especially capital budgeting.",,2010,10.1108/17542431011093162,,proquest,"This paper critiques the certainty equivalent coefficient net present value (CEC(NPV)) criterion for capital budgeting, arguing it is methodologically flawed due to incorrect cash flow calculations. It examines the implications of the UK Companies Act 2006, Sarbanes-Oxley Act 2002, and several 2009 reviews (Turner, Walker, Combined Code) on this criterion, concluding that CEC(NPV) is construct invalid, overstates financial viability, and conflicts with these legislative and review requirements. The paper highlights that CEC(NPV) hinders financial managers from applying Fisherian analysis for project ranking and comparing internal rates of return with the cost of capital.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:39:08.231491
6b9cd4fca22e9660,The UK and the Eurozone,"The article reviews the case for the UK to join the Eurozone by way of presenting a review of HM Treasury's widely well-regarded ""Euro Report"" (2003). The review provides an opportunity to rehearse and update the elements of optimum currency area (OCA) theory. In particular, the study draws attention to fresh estimates of the trade effect of the UK's adhesion to the Eurozone, the small size of which sharply contrasts with earlier estimates. They substantially remove a challenge to the Report's negative conclusion. The study sets the review in the perspective of public opinion surveys and HM Government's decisions. © 2006 Oxford University Press. © 2008 Elsevier B.V., All rights reserved.","Artis, M.",2006,10.1093/cesifo/ifj002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750451943&doi=10.1093%2Fcesifo%2Fifj002&partnerID=40&md5=bd9251b7434142022294a2c53adc8d2b,scopus,"This article reviews the UK's potential accession to the Eurozone, analyzing the HM Treasury's 2003 ""Euro Report"" and updating elements of optimum currency area theory. It highlights new estimates of the trade effect of UK adhesion, which are smaller than previously thought and support the report's negative conclusion. The study also considers public opinion and government decisions.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:39:11.399077
e6571fb417c7b72e,The Yield Curve as a Recession Leading Indicator. An Application for Gradient Boosting and Random Forest,"Most representative decision-tree ensemble methods have been used to examine the variable importance of Treasury term spreads to predict US economic recessions with a balance of generating rules for US economic recession detection. A strategy is proposed for training the classifiers with Treasury term spreads data and the results are compared in order to select the best model for interpretability. We also discuss the use of SHapley Additive exPlanations (SHAP) framework to understand US recession forecasts by analyzing feature importance. Consistently with the existing literature we find the most relevant Treasury term spreads for predicting US economic recession and a methodology for detecting relevant rules for economic recession detection. In this case, the most relevant term spread found is 3-month–6-month, which is proposed to be monitored by economic authorities. Finally, the methodology detected rules with high lift on predicting economic recession that can be used by these entities for this propose. This latter result stands in contrast to a growing body of literature demonstrating that machine learning methods are useful for interpretation comparing many alternative algorithms and we discuss the interpretation for our result and propose further research lines aligned with this work. © 2022 Elsevier B.V., All rights reserved.","Delgado, P.C.; Congregado, E.; Golpe, A.A.; Vides, J.C.",2022,10.9781/ijimai.2022.02.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127570531&doi=10.9781%2Fijimai.2022.02.006&partnerID=40&md5=9f07996466ae0db75c8618f7c33d0667,scopus,This study applies gradient boosting and random forest models to predict US economic recessions using Treasury term spreads. It identifies the 3-month–6-month term spread as the most relevant predictor and proposes a methodology for economic authorities to use these findings for recession detection. The study also discusses the interpretability of the machine learning models using SHAP values.,True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:39:41.215777
0b2ab325246838cf,The commodity risk premium and neural networks,"The paper uses linear and nonlinear predictive models to study the linkage between a set of 128 macroeconomic and financial predictors and the risk premium of commodity futures contracts. The linear models use shrinkage methods based on either naive averaging or principal components. The nonlinear models use feedforward deep neural networks (DNN) either as stand-alone or in conjunction with a long short-term memory network (LSTM). Out of the four specifications considered, the LSTM-DNN architecture best captures the risk premium, which underscores the need to estimate models that are both nonlinear and recurrent. The superior performance of the LSTM-DNN portfolio persists after accounting for transaction costs or illiquidity and is unrelated to previously-documented commodity risk factors. © 2023 Elsevier B.V., All rights reserved.","Rad, H.; Low, R.K.Y.; Miffre, J.; Faff, R.",2023,10.1016/j.jempfin.2023.101433,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174155469&doi=10.1016%2Fj.jempfin.2023.101433&partnerID=40&md5=2b9b49f1dc40abe23ee81ebf9bd51eb2,scopus,"This paper investigates the relationship between macroeconomic and financial predictors and the commodity futures risk premium using both linear and nonlinear predictive models. It finds that a combination of Long Short-Term Memory (LSTM) and Deep Neural Networks (DNN) architecture outperforms other models in capturing the risk premium, even after accounting for transaction costs and illiquidity. The study highlights the importance of nonlinear and recurrent models for this prediction task.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:39:45.208807
e1060b1fe4f87af2,The composition of CMBS risk,"This paper identifies the put-option, liquidity availability proportion, and shadow liquidity risk premia embedded within commercial mortgage backed securities (CMBS) using reduced form and structural generalization models. These risk values are then interpreted as trading signals which are tested with automated trading strategies that buy undervalued and sell overvalued CMBS from November 2007 through June 2015. All three signals generate substantial positive trading profits in testing for the reduced form model but not for the structural generalization. The risk signals constructed independently of market pricing provide more profitable automated trading insights than those constructed from interactions between modeled risk measures and market spreads. In my tests of the information content of the risk signals with respect to future macroeconomic indicators, I find statistically significant evidence in keeping with recent studies. While I cannot reject CMBS efficiency, this paper's disclosure of new risk measures, the profitability of automated strategies based on those risk measures, and the statistical significance of their forward guidance capabilities, together contributes to our understanding of CMBS risk and the credit spread puzzle debate. (C) 2016 Elsevier B.V. All rights reserved.","Christopoulos, Andreas D.",2017,10.1016/j.jbankfin.2016.12.005,,wos,"This paper develops and tests three risk measures for Commercial Mortgage-Backed Securities (CMBS): put-option, liquidity availability proportion, and shadow liquidity risk premia. These measures are used as trading signals in automated strategies. The reduced form model's signals were profitable, while the structural generalization model's were not. The risk signals, independent of market pricing, outperformed those derived from market interactions. The signals also showed predictive power for future macroeconomic indicators, contributing to the understanding of CMBS risk and the credit spread puzzle.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:39:49.554922
d9e7ca11936aaf69,The decoupling between public debt fundamentals and bond spreads after the European sovereign debt crisis,"We contribute to the literature that documents empirically that the relationship between public debt fundamentals and sovereign bond spreads in Spain, France, and Italy (versus Germany) weakened after the 2010–2012 episode of sovereign debt markets’ significant distress. To construct our measure of public debt fundamentals, we build on the literature that combines the Value at Risk approach with the estimation of the correlation pattern of public debt dynamics’ macroeconomic determinants via Vector Auto Regressions (VARs) to estimate the probability distribution of alternative debt trajectories. Since we incorporate in the VAR new information in a sequential manner, we are able to retrieve time-varying probabilities that characterize the expected behaviour of debt at a given point in time in the future. We then empirically confront such probabilistic indicators with market-derived sovereign bond spreads.",,2023,10.1080/00036846.2022.2120959,,proquest,"This study investigates the weakened relationship between public debt fundamentals and sovereign bond spreads in Spain, France, and Italy compared to Germany, following the 2010-2012 European sovereign debt crisis. The authors develop a measure of public debt fundamentals by combining the Value at Risk approach with Vector Auto Regressions (VARs) to estimate time-varying probabilities of future debt trajectories. These probabilistic indicators are then empirically compared with market-derived sovereign bond spreads.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T16:40:13.650176
8f0d28acc3ae4ac3,The disappearance of style in the US equity market,"This article investigates the modelling of style returns in the United States and the returns to style 'tilts' based on forecasts of enhanced future style returns. We use hidden Markov model to build our forecasts for data from 1975 to 1998. We do not include more recent observations as the subsequent trend and volatility sways the analysis. Our finding that style returns are less forecastible in the late 1990s is consistent with the hypothesis that style returns are the result of anomalies rather than risk premia. The erosion of anomalous returns as public awareness of their presence is translated into strategies that arbitrage away the excess returns seems to be a hypothesis consistent with our modelling results. Reprinted by permission of Routledge, Taylor and Francis Ltd.",,2007,10.1080/09603100701217978,,proquest,"This study models US equity style returns and the effectiveness of style 'tilts' using forecasts derived from a hidden Markov model (1975-1998). The findings suggest style returns became less predictable in the late 1990s, supporting the hypothesis that these returns stem from anomalies rather than risk premia, and that increased awareness and arbitrage are eroding these anomalies.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:40:17.040565
056f1ead8d9317b9,The dynamic interaction of speculation and diversification,"A discrete time model of a financial market is developed, in which heterogeneous interacting groups of agents allocate their wealth between two risky assets and a riskless asset. In each period each group formulates its demand for the risky assets and the risk-free asset according to myopic mean-variance maximizazion. The market consists of two types of agents: fundamentalists, who hold an estimate of the fundamental values of the risky assets and whose demand for each asset is a function of the deviation of the current price from the fundamental, and chartists, a group basing their trading decisions on an analysis of past returns. The time evolution of the prices is modelled by assuming the existence of a market maker, who sets excess demand of each asset to zero at the end of each trading period by taking an offsetting long or short position, and who announces the next period prices as functions of the excess demand for each asset and with a view to long-run market stability. The model is reduced to a seven-dimensional nonlinear discrete-time dynamical system, that describes the time evolution of prices and agents' beliefs about expected returns, variances and correlation. The unique steady state of the model is determined and the local asymptotic stability of the equilibrium is analysed, as a function of the key parameters that characterize agents' behaviour. In particular it is shown that when chartists update their expectations sufficiently fast, then the stability of the equilibrium is lost through a supercritical Neimark-Hopf bifurcation, and self-sustained price fluctuations along an attracting limit cycle appear in one or both markets. Global analysis is also performed, by using numerical techniques, in order to understand the role played by the chartists' behaviour in the transition to a regime characterized by irregular oscillatory motion and coexistence of attractors. It is also shown how changes occurring in one market may affect the price dynamics of the alternative risky asset, as a consequence of the dynamic updating of agents' portfolios. © 2005 Taylor & Francis Group Ltd. © 2005 Elsevier B.V., All rights reserved.","Chiarella, C.; Dieci, R.; Gardini, L.",2005,10.1080/1350486042000260072,https://www.scopus.com/inward/record.uri?eid=2-s2.0-16644376238&doi=10.1080%2F1350486042000260072&partnerID=40&md5=eb95c9185d3b76bd89c850f8dda27d54,scopus,"This paper presents a discrete-time model of a financial market with heterogeneous agents who trade between risky and riskless assets. Agents are either fundamentalists (trading based on fundamental values) or chartists (trading based on past returns). The model uses a market maker to set prices and analyzes the stability of the unique steady state. It shows that rapid chartist behavior can lead to price fluctuations and limit cycles, and explores the transition to irregular oscillatory motion and the coexistence of attractors. The study also examines how changes in one market can affect another due to portfolio updates.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:40:22.501357
3f2dc0f0809e9321,The efficacy of neural networks in predicting returns on stock and bond indices,"This paper uses two recently developed tests to identify neglected nonlinearity in the relationship between excess returns on four asset classes and several economic and financial variables. Having found some evidence of possible nonlinearity, it was then investigated whether the predictive power of these variables could be enhanced by using neural network models instead of linear regression or GARCH models. Some evidence of nonlinearity in the relationships between the explanatory variables and large stocks and corporate bonds was found. It was also found that the GARCH models are conditionally efficient with respect to neural network models, but the neural network models outperform GARCH models if financial performance measures are used. In resonance with the results reported for the tests for neglected nonlinearity, it was found that the neural network forecasts are conditionally efficient with respect to linear regression models for large stocks and corporate bonds, whereas the evidence is not statistically significant for small stocks and intermediate-term government bonds. This difference persists even when financial performance measures for individual asset classes are used for comparison. © 2018 Elsevier B.V., All rights reserved.","Desai, V.S.; Bharati, R.",1998,10.1111/j.1540-5915.1998.tb01582.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032392191&doi=10.1111%2Fj.1540-5915.1998.tb01582.x&partnerID=40&md5=a3740dd5ed940d8a119fb152e4c41aab,scopus,"This study investigates the efficacy of neural networks in predicting returns on stock and bond indices, comparing them against linear regression and GARCH models. Evidence of nonlinearity was found in the relationships between explanatory variables and large stocks and corporate bonds. Neural networks outperformed GARCH models when financial performance measures were used, and showed conditional efficiency against linear regression for large stocks and corporate bonds, though not for smaller stocks or intermediate-term government bonds.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:40:47.362673
33d0e0f20fc3942d,The expected inflation channel of government spending in the postwar U.S.,"There exist sticky price models in which the output response to a government spending change can be large if the central bank is nonresponsive to inflation. According to this ""expected inflation channel,"" government spending drives up expected inflation, which in turn, reduces the real interest rate and leads to an increase in private consumption. This paper examines whether the channel was important in the post-WWII U.S., with particular attention to the 2009 Recovery Act period. First, we show that a model calibrated to have a large output multiplier requires a large response of expected inflation to a government spending shock. Next, we show that this large response is inconsistent with structural vector autoregression evidence from the Federal Reserve's passive policy period (1959-1979). Then, we study expected inflation measures during the Recovery Act period in conjunction with a panel of professional forecaster surveys, a cross-country comparison of bond yields and fiscal policy news announcements. We show that the expected inflation response was too small to engender a large output multiplier. © 2020 Elsevier B.V., All rights reserved.","Dupor, B.; Li, R.",2015,10.1016/j.euroecorev.2014.11.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920172590&doi=10.1016%2Fj.euroecorev.2014.11.004&partnerID=40&md5=6d2a17fcdbe357ec1335ae439f96aa03,scopus,"This paper investigates the expected inflation channel of government spending in the postwar U.S., focusing on the 2009 Recovery Act period. It finds that while sticky price models suggest a large output multiplier can arise from this channel, empirical evidence from the passive policy period and the Recovery Act period indicates that the expected inflation response was too small to generate such a multiplier.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:40:59.180826
62cb6b8984b2f540,The hawk eye scan: Halyomorpha halys detection relying on aerial tele photos and neural networks,"This paper faces the challenge of monitoring the Brown Marmorated Stink Bug (H.halys) (Halyomorpha halys) within orchards, utilizing drones, and computer vision. H.halys is an invasive species originating from East Asia, that is extremely polyphagous and poses a significant threat to various crops. Our first contribution is a drone navigation protocol, which ensures risk-free drone flights in cluttered orchard environments, preserving image quality and avoiding obstacles. We then create a pioneering H.halys dataset consisting of aerial telephotos captured in the field autonomously by the drone. The dataset allows the development and evaluation for the first time of multiple ML models for H.halys detection in the field. We trained YOLOV5, YOLOV8, RETINANET, and FASTER-RCNN models using different learning methodologies, exploiting different percentages of images without the bug, and using different slicing procedures for the images. The Medium YOLOV5 model trained with all images containing a bug detects the largest number of H.halys on the testing set and overall performs the best, while RETINANET and FASTER-RCNN provide the best trade-off between precision and recall. Models vary in their ability to handle occluded H.halys and bug-free images, which are common since the presence of the bug cannot be predicted before capturing a photo. These results show promising potential for automating H.halys monitoring, despite the image complexity and the early dataset stage. Our work marks a significant step towards enhancing smart agriculture practices due to the simplicity of the data acquisition process and the off-the-shelf hardware selection. © 2025 Elsevier B.V., All rights reserved.","Palazzetti, L.; Rangarajan, A.K.; Dinca, A.; Boom, B.; Popescu, D.; Offermans, P.; Pinotti, C.M.",2024,10.1016/j.compag.2024.109365,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202194488&doi=10.1016%2Fj.compag.2024.109365&partnerID=40&md5=6343a88c6f0066b44a1045827b3a882c,scopus,"This paper introduces a drone-based system using aerial photography and machine learning (YOLOV5, YOLOV8, RETINANET, FASTER-RCNN) for detecting the invasive Brown Marmorated Stink Bug (Halyomorpha halys) in orchards. It details a drone navigation protocol, a novel dataset of aerial images, and evaluates various ML models for bug detection, with YOLOV5 showing the best detection rate and others offering good precision-recall trade-offs. The study highlights the potential for automated pest monitoring in agriculture.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:41:04.644711
19ee415ed7fb651b,"The impact of contagion effects of media reports, investors' sentiment and attention on the stock market based on HAR-RV model","In this paper, the Shanghai Securities Composite Index and 18 A-share listed companies are used to justify the impact of contagion effects of media reports, investors' sentiment and attention on stock market. Five indicators are built: The news media optimistic tendency, investors' attention, investors' sentiment, investors' sentiment disagreement and media sentiment disagreement. Furthermore, theoretical models are constructed based on HAR-RV model to analyze the contagion structure between media sentiment and investors' sentiment and its impact on the performance of stock market. Additionally, the reverse silence spiral theory is proposed to analyze the regulatory role of sentiment disagreement in the contagion effects according to the information communication theory. The empirical results demonstrate the following conclusions. (1) The optimism degree of media reports positively affects investors' subjective sentiment and increases their transaction volume. (2) Strengthening investors' attention to corporate-related information is the main path by which media sentiment interferes with investors' sentiment. (3) Media sentiment will indirectly affect the excess return and volatility of stocks through investors' sentiment and their transactions. (4) Media sentiment disagreement has weakened the influence of media sentiment on investors' attention and sentiment. Investors' sentiment disagreement has alleviated its impact on the excess returns and volatility of stocks.","Lei, Bolin; Song, Yuping",2023,10.1142/s242478632350010x,,wos,"This study investigates the influence of media reports, investor sentiment, and attention on the stock market using the HAR-RV model. It constructs indicators for media optimism, investor attention, sentiment, and disagreement. The findings suggest that media optimism boosts investor sentiment and trading volume, with investor attention acting as a key mediator. Media sentiment indirectly impacts stock returns and volatility via investor sentiment and trading. Sentiment disagreement, both in media and among investors, mitigates these contagion effects.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:41:09.591511
029c751e000c5cf6,The impact of corporate social responsibility on financial distress: evidence from developing economy,"Purpose: This study aims to explore the role of corporate social responsibility (CSR) on the likelihood of financial distress for a sample of 139 Pakistan Stock Exchange (PSX) listed firms throughout 2008–2019. Design/methodology/approach: The dynamic generalized method of moments (GMM) estimator is used to examine the impact of CSR on financial distress. The investment in CSR is measured through a multidimensional financial approach which comprises the sum of the contribution made by the company in the form of charitable donation, employees’ welfare and research and development, while the Altman Z-score is used as an indicator of financial distress. The higher the Z-score, the lower will be the probability of financial distress. Findings: The authors find a significant positive impact of CSR on financial distress in GMM model. This finding is consistent with the shareholder view and over-investment hypothesis of CSR as management makes an investment in CSR to get personal benefits, which resultantly leads the firm toward financial distress state. Further, this positive relationship remains present for firms having strong involvement in foreign business through exports. Research limitations/implications: Like other studies, the present study is not free from limitations. First, financial firms are skipped from the sample, although literature witnesses a lot of studies highlight the financial firms’ commitment to achieving CSR goals. Second, financial distress occurs in different stages, and this study fails to establish a linkage between CSR engagement at different stages of financial distress. In the future, researchers can make valuable addition by covering these missing links in present studies. Practical implications: Findings suggest several practical implications. For policymakers, they should encourage firms to adopt more socially responsible behavior as it not only prevents them from distress but also comes with better investment behavior, minimize bankruptcies and make economies more strong and stable. Second, results suggest corporate managers emphasize socially responsible behavior as its benefits are beyond the “societal benefits” as it lessens financial distress through lower cost of debt, lesser financial constraints and reduced cost of information asymmetry, and it minimizes the cost of capital. Lastly, investors make risk premium assessments related to future earnings by determining the likelihood of financial distress in the future. Originality/value: The study extends the body of existing literature on CSR and the likelihood of financial distress in Pakistan, which is according to the best knowledge of the authors, not yet studied before. The results suggest that policymakers may pay special attention to the quality of CSR while predicting corporate financial distress. © 2021 Elsevier B.V., All rights reserved.","Farooq, M.; Noor, A.",2021,10.1108/par-10-2020-0196,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107835740&doi=10.1108%2FPAR-10-2020-0196&partnerID=40&md5=c3473de3b8ddfd1041890bffc2ae76da,scopus,"This study investigates the relationship between corporate social responsibility (CSR) and financial distress in 139 listed firms on the Pakistan Stock Exchange from 2008 to 2019. Using the dynamic generalized method of moments (GMM) estimator, the research measures CSR through charitable donations, employee welfare, and R&D, and financial distress via the Altman Z-score. The findings indicate a significant positive impact of CSR on financial distress, supporting the over-investment hypothesis where CSR investments may benefit management personally, leading to financial difficulties. This effect is stronger for firms with substantial export involvement. The study acknowledges limitations such as excluding financial firms and not differentiating CSR's impact across various stages of financial distress. Practical implications suggest policymakers should encourage CSR for economic stability and corporate managers should view CSR as a strategy to reduce debt costs, financial constraints, and information asymmetry, ultimately lowering the cost of capital. Investors can use CSR quality to assess future earnings risk.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:41:20.572257
34a2d6abf168cbd6,The impact of the Covid-19 related media coverage upon the five major developing markets,"This paper analyses the influence of the Covid-19 coverage by the social media upon the shape of the sovereign yield curves of the five major developing countries, namely Federative Republic of B razil, Russian Federation, Republic of India, People's Republic of China, and the Republic of South Africa (BRICS). The coherenc e between the level, slope, and the curvature of the sovereign yield term structures and the Covid-19 medi a coverage is found to vary between low and high ranges, depending on the phases of the pandemic. The empirical estimations of the yield-curve factors a re performed by means of the Diebold-Li modified version of the Nelson-Siegel model. The intervals of low coherence reveal the capacity of the two latent factors, level and slope, to be used for creating cross-factor diversification strategies, workable under crisis conditions, as evidenced on the example of the ongoing pandemic. Diverse coherence patterns are reported on a per-country basis, highlighting a promising potential of sovereign debt investments for designing cross-country and cross-factor fixed-income strategies, capable of hedging downside risks. © 2021 Elsevier B.V., All rights reserved.","Umar, Z.; Gubareva, M.; Sokolova, T.",2021,10.1371/journal.pone.0253791,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109009430&doi=10.1371%2Fjournal.pone.0253791&partnerID=40&md5=c2e168481e34ba89a700029c35741b67,scopus,"This paper examines how COVID-19 media coverage has affected the sovereign yield curves of five major developing countries (BRICS). It uses a modified Nelson-Siegel model to analyze the relationship between media coverage and the yield curve's level, slope, and curvature, finding that coherence varies with pandemic phases. The study suggests that latent factors like level and slope can be used for diversification and risk hedging strategies, with country-specific patterns highlighted.",True,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:41:27.700753
7fa232dd8733f7ab,The implied volatility smirk of commodity options,"This paper studies the implied volatility (IV) smirks in four commodity markets by adopting Zhang and Xiang's methodology. First, we document the term structure and dynamics of IV smirks. Overall, the commodity IV curves are negatively skewed with a positive curvature. Then we analyze the commodity and S&P 500 returns' predictability based on in-sample and out-of-sample tests and find that the information embedded in IV smirks can significantly predict monthly commodity and S&P 500 returns. For example, the risk-neutral fourth cumulant (FC) from the crude oil market outperforms all of the standard predictors in predicting the S&P 500 returns.","Jia, Xiaolan; Ruan, Xinfeng; Zhang, Jin E.",2021,10.1002/fut.22161,,wos,"This paper analyzes implied volatility (IV) smirks in four commodity markets, finding negatively skewed IV curves with positive curvature. It demonstrates that information from IV smirks can predict monthly returns in commodity and S&P 500 markets, with the risk-neutral fourth cumulant from crude oil showing superior predictive power for S&P 500 returns.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:41:30.411092
38aa2888d66b462f,The information content of a nonlinear macro-finance model for commodity prices,"State-of-the-art term structure models of commodity prices have serious difficulties extrapolating the prices of long-maturity futures contracts from short-dated contracts. This situation is problematic for valuing real commodity-linked assets. We estimate a nonlinear four-factor continuous time model of commodity price dynamics. The model nests many previous specifications. To estimate the model, we use crude oil prices and inventories. The inventory data and nonlinear price dynamics have a large impact on oil price forecasts. The additional factor in our model compared with current three-factor models has a significant impact on model-implied long-maturity futures prices. © 2021 Elsevier B.V., All rights reserved.","Khan, S.; Khokher, Z.; Simin, T.",2017,10.1093/rfs/hhw087,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027014860&doi=10.1093%2Frfs%2Fhhw087&partnerID=40&md5=9dc58ffacd4b0157ed4a02671873fc2f,scopus,"This paper estimates a nonlinear four-factor continuous time model for commodity price dynamics, addressing difficulties in extrapolating long-maturity futures prices from short-dated contracts. Using crude oil prices and inventories, the model shows that inventory data and nonlinear dynamics significantly impact oil price forecasts, and the additional factor improves long-maturity futures price implications compared to existing three-factor models.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:41:35.811901
af84a9d1919ec35b,The performance of variance ratio unit root tests under nonlinear stationary TAR and STAR processes: Evidence from Monte Carlo simulations and applications,"This paper investigates the performance of variance ratio unit root tests under nonlinear stationary three-regime threshold autoregressive (TAR) and smooth transition autoregressive (STAR) processes that are significant for some economic theories and variables. Variance ratio unit root tests are effective tools in empirical analysis because they can theoretically consider broad classes of nonlinear stationary processes under the null or alternative hypothesis. Nevertheless, our Monte Carlo simulations demonstrate that these tests perform poorly (with severe size distortions or low power) under stationary TAR and STAR processes. To verify our Monte Carlo results, we apply these tests to yield spreads such as the TAR and STAR processes.","Maki, Daiki",2008,10.1007/s10614-007-9107-1,,wos,"This paper evaluates the effectiveness of variance ratio unit root tests when applied to nonlinear stationary TAR and STAR processes, which are relevant in economic contexts. Through Monte Carlo simulations, the study finds that these tests exhibit poor performance, characterized by significant size distortions or low power, when applied to stationary TAR and STAR processes. The findings are further validated by applying the tests to yield spreads, which are modeled as TAR and STAR processes.",True,True,False,gemini-2.5-flash-lite,Olav,M,,2025-10-13T16:42:02.722473
edcd8b865beeefe6,The price of power: The valuation of power and weather derivatives,"Pricing contingent claims on power presents numerous challenges due to (1) the unique behavior of power prices, and (2) time-dependent variations in prices. We propose and implement a model in which the spot price of power is a function of two state variables: demand (load) and fuel price. in this model, any power derivative price must satisfy a PDE with boundary conditions that reflect capacity limits and the non-linear relation between load and the spot price of power. Moreover, since power is non-storable and demand is not a traded asset, the power derivative price embeds a market price of risk. Using inverse problem techniques and power forward prices from the PJM market, we solve for this market price of risk function. During 1999-2001, the upward bias in the forward price was as large as $50/MWh for some days in July. By 2005, the largest estimated upward bias had fallen to $19/MWh. These large biases are plausibly due to the extreme right skewness of power prices: this induces left skewness in the payoff to short forward positions, and a large risk premium is required to induce traders to sell power forwards. This risk premium suggests that the power market is not fully integrated with the broader financial markets. (C) 2008 Published by Elsevier B.V.","Pirrong, Craig; Jermakyan, Martin",2008,10.1016/j.jbankfin.2008.04.007,,wos,"This paper proposes a model for pricing power derivatives, considering the unique behavior of power prices and time-dependent variations. It incorporates demand (load) and fuel price as state variables, leading to a PDE with boundary conditions reflecting capacity limits and non-linear relationships. The model also accounts for the market price of risk due to power's non-storability and demand not being a traded asset. Using PJM market data, the study estimates this risk premium, finding significant upward biases in forward prices between 1999-2001, which decreased by 2005. These biases are attributed to the right skewness of power prices, suggesting the power market is not fully integrated with broader financial markets.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:42:06.167478
6b731db9ed4214e8,The relation between the equity risk premium and the bond maturity premium in the UK: 1900-2006,"Using a rich data set for the UK for over a century, we find that the relation between the equity risk premium and the government bond maturity premium is nonlinear and subject to stochastic regime switching. We identify a regime in which both premia are jointly characterized by low volatility and another regime in which both premia are characterized by high volatility. The occurrence of the high volatility regime chronologically coincides with major changes in the pound exchange rate. The low volatility regime has a higher probability of turning up over two consecutive years than the high volatility regime, but it is not perceived by investors to be an absorbing regime. The lagged maturity premium is a strong predictor of the equity risk premium only in the regime of low volatility. In addition, the lagged equity premium is a predictor of the maturity premium also in the low volatility regime. This result on regime-dependent bidirectional predictability is robust to alternative definitions of the equity premium, and to the inclusion of real interest rate and real growth effects. © 2008 Springer Science+Business Media, LLC. © 2009 Elsevier B.V., All rights reserved.","Kanas, A.",2009,10.1007/s12197-008-9038-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-65249147174&doi=10.1007%2Fs12197-008-9038-2&partnerID=40&md5=8f24282c2b2092e99ed1ca80871e306d,scopus,"This study investigates the relationship between the equity risk premium and the government bond maturity premium in the UK from 1900-2006. It identifies two distinct regimes: one with low volatility and another with high volatility, the latter coinciding with significant changes in the pound's exchange rate. The research finds that the lagged maturity premium predicts the equity risk premium, and vice versa, specifically within the low volatility regime. This bidirectional predictability is robust to various definitions and the inclusion of other economic factors.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T16:42:58.411411
ec4e0a59fc7c80b4,The role of supervised learning in the decision process to fair trade US municipal debt,"Determining a fair price and an appropriate timescale to trade municipal debt is a complex decision. This research uses data informatics to explore transaction characteristics and trading activity of investment grade US municipal bonds. Using the relatively recent data stream distributed by the Municipal Securities Rulemaking Board, we provide an institutional summary of market participants and their trading behavior. Subsequently, we focus on a sample of AAA bonds to derive a new methodology to estimate a trade-weighted benchmark municipal yield curve. The methodology integrates the study of ridge regression, artificial neural networks, and support vector regression. We find an enhanced radial basis function artificial neural network outperforms alternate methods used to estimate municipal term structure. This result forms the foundation for establishing a decision theory on optimal municipal bond trading. Using multivariate modeling of a liquidity domain measured across three dependent variables, we investigate the proposed decision theory by estimating weekly production-theoretic bond liquidity returns to scale. Across the three liquidity measures and for almost all weeks investigated, bond trading liquidity is elastic with respect to the modeled factors. This finding leads us to conclude that an optimal trading policy for municipal debt can be implemented on a weekly timescale using the elasticity estimates of bond price, trade size, risk, days-to-maturity, and the macroeconomic influences of labor in the workforce and building activity.",,2018,10.1007/s40070-018-0079-2,,proquest,"This research explores transaction characteristics and trading activity of investment grade US municipal bonds using data informatics. It proposes a new methodology to estimate a trade-weighted benchmark municipal yield curve by integrating ridge regression, artificial neural networks, and support vector regression. An enhanced radial basis function artificial neural network was found to outperform other methods. The study also investigates a decision theory on optimal municipal bond trading by modeling bond liquidity returns to scale, concluding that an optimal trading policy can be implemented weekly using elasticity estimates.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:43:23.998616
ebe956d6dd174735,The term structure of policy rules,"A formula is derived that links the coefficients of the monetary policy rule for the short-term interest rate to the coefficients of the implied affine equations for long-term interest rates. The formula predicts that an increase in the coefficients in the monetary policy rule will lead to an increase in the coefficients in the affine equations. Empirical evidence for such a prediction is provided. The curve of the response coefficients by maturity is also predicted by the formula. The formula's predictive accuracy and its closed form make it a useful tool for studying the policy implications of embedding no-arbitrage affine theories into macro models. All rights reserved, Elsevier",,2009,10.1016/j.jmoneco.2009.09.004,,proquest,"This paper derives a formula connecting coefficients of monetary policy rules for short-term interest rates to implied affine equations for long-term interest rates. It predicts that increased policy rule coefficients lead to increased affine equation coefficients, supported by empirical evidence. The formula also predicts the response coefficients curve by maturity and is useful for embedding no-arbitrage affine theories into macro models.",False,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:43:29.714235
75536a9b5e21dd96,The volatility structure of the fixed income market under the HJM framework: A nonlinear filtering approach,"The dynamics for interest rate processes within the well-known multi-factor Heath, Jarrow and Morton (HJM) specification are considered. Despite the flexibility of and the notable advances in theoretical research about the HJM model, the number of empirical studies of it is still very sparse. This paucity is principally due to the difficulties in estimating models in this class, which are not only high-dimensional, but also nonlinear and involve latent state variables. The estimation of a fairly broad class of HJM models as a nonlinear filtering problem is undertaken by adopting the local linearization filter, which is known to have some desirable statistical and numerical features, so enabling the estimation of the model via the maximum likelihood method. The estimator is then applied to the US, the UK and the Australian markets. Different two- and three-factor models are found to be the best for each market, with the factors being the level, the slope and the twist effect. The contribution of each factor towards overall variability of the interest rates and the financial reward each factor claims are found to differ considerably from one market to another. (C) 2008 Elsevier B.V. All rights reserved.","Chiarella, Carl; Hung, Hing; To, Thuy-Duong",2009,10.1016/j.csda.2008.07.036,,wos,"This paper estimates a broad class of Heath, Jarrow, and Morton (HJM) interest rate models using a nonlinear filtering approach, specifically the local linearization filter. The method is applied to US, UK, and Australian markets, identifying two- and three-factor models (level, slope, twist) as optimal. The study finds significant cross-market differences in factor contributions to interest rate variability and risk premia.",True,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:44:14.391597
e901b823bc8df6c4,Trading Macro-Cycles of Foreign Exchange Markets Using Hybrid Models,"Most existing studies on forecasting exchange rates focus on predicting next-period returns. In contrast, this study takes the novel approach of forecasting and trading the longer-term trends (macro-cycles) of exchange rates. It proposes a unique hybrid forecast model consisting of linear regression, multilayer neural network, and combination models embedded with technical trading rules and economic fundamentals to predict the macro-cycles of the selected currencies and investigate the predicative power and market timing ability of the model. The results confirm that the combination model has a significant predictive power and market timing ability, and outperforms the benchmark models in terms of returns. The finding that the government bond yield differentials and CPI differentials are the important factors in exchange rate forecasts further implies that interest rate parity and PPP have strong influence on foreign exchange market participants.",,2021,10.3390/su13179820,,proquest,"This study proposes a hybrid model combining linear regression, multilayer neural networks, and technical/fundamental rules to forecast and trade longer-term exchange rate macro-cycles. The model demonstrates significant predictive power and market timing ability, outperforming benchmarks. Government bond yield differentials and CPI differentials are identified as key factors influencing exchange rates, suggesting the importance of interest rate parity and PPP.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T16:44:48.650045
edae0558b51d4ae1,Transaction costs and nonlinear adjustment towards equilibrium in the US treasury bill market,"This paper uses nonlinear error correction models to study yield movements in the US Treasury Bill Market. Nonlinear error correction arises because portfolio adjustment is an 'on-off' process, which occurs only when disequilibrium in the bill market is large enough to induce investors to incur the transaction costs associated with buying/selling bills. This, together with heterogeneity of transaction costs, implies that the strength of aggregate error correction depends on both the distribution of costs and the extent of disequilibrium in the market. Smooth transition models are used to describe an aggregate adjustment process which is strong when the market is distant from equilibrium, but becomes weaker as the market approaches equilibrium. Linearity tests indicate that the types of nonlinearities that would be induced by transactions costs are statistically significant, and estimated models which incororate these nonlinearities outperform their linear counterparts, both in sample and out of sample. © Blackwell Publishers 1997. © 2018 Elsevier B.V., All rights reserved.","Anderson, H.M.",1997,10.1111/1468-0084.00078,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031523523&doi=10.1111%2F1468-0084.00078&partnerID=40&md5=97619a45ee86a6fafd0456ff928a5b07,scopus,"This paper investigates yield movements in the US Treasury Bill Market using nonlinear error correction models. It posits that nonlinear adjustment occurs due to transaction costs, where portfolio adjustments are triggered only when market disequilibrium is significant enough to overcome these costs. The study employs smooth transition models to capture this aggregate adjustment process, which is stronger at greater distances from equilibrium. Empirical tests confirm the statistical significance of these nonlinearities and show that models incorporating them outperform linear models.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T16:45:15.855737
7bf6f432caa24633,Turning point vs trend,"At the beginning of the paper some shortcomings of the existing forecasting systems are demonstrated on examples of products of the EU forecasting service and of the Macroeconomic Prospect Team of the Treasury of the UK. And apart of it the smoothed lines of Nobel Prize winner of 2010 Professor Pissarides are considered in comparison with clear forecasts of turning points received by the author on the same time series. Then a description of forecasting of the recession of the early 1990s in the UK is given, as a part of forecasting of innovative growth. It is underlined that statistics must show explicitly ‘the height of technological leap’ and provide separate parameters of old and new technologies. And that the current focusing of attention on the most advanced technologies only should be broadened to all technologies which actually are being implementing in the economy. Comparison with the Cambridge Multisectoral Dynamic Model of the British Economy shows how peculiarities of reflection of new technologies could affect ability of seeing turning points. At the end some remarks are contributed to the current discussion between the competing schools. Positive aspects of the “Great Recession” of 2008 – 2010 are highlighted along with their similarity with previous crises. At that an attempt to restore the “shattered intellectual structure” of Alan Greenspan is made. © 2021 Elsevier B.V., All rights reserved.","Ryaboshlyk, V.",2011,10.14254/2071-8330.2011/4-1/6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969842369&doi=10.14254%2F2071-8330.2011%2F4-1%2F6&partnerID=40&md5=9e9f89c66430456f736ae5a0762ab880,scopus,"This paper critiques existing forecasting systems, using examples from the EU and UK Treasury. It compares smoothed trend lines with identified turning points, discusses forecasting the UK's early 1990s recession as part of innovative growth, and emphasizes the need for statistics to differentiate between old and new technologies. The author contrasts their approach with the Cambridge Multisectoral Dynamic Model, highlighting how technology reflection impacts turning point identification. The paper also touches on the 2008-2010 ""Great Recession"" and attempts to reconstruct Alan Greenspan's economic framework.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:45:19.573812
41a74f0a8de13787,U.S. vertically integrated electric utility greenhouse gas emissions and carbon risk premiums around the Paris Accord,"We study the pricing of greenhouse gas emissions of vertically integrated producers of electricity around the Paris Accord (PA). We study whether emissions are priced by financial markets, providing a market-based incentive for firms to reduce their carbon footprints and if the heightened attention on climate change post-Paris Accord (PA) impacts the size of the “carbon risk premium.” We focus on electricity generators, because they are responsible for the largest share of emissions and emissions reductions in the U.S. and are highly exposed to regulatory, physical, and stranded asset risks. We find the cost of carbon risk is reflected in the returns of vertically integrated electric utilities. The post-PA period provides the strongest evidence that carbon risk is priced. We find that equity markets provide incentives for power producers to reduce emissions, as reductions in emissions are associated with reductions in required returns on equity (increases in equity market values). The challenge for regulators is how to respond in rate cases. Lowering a utility's regulated return to reflect lower market estimates of the return on equity would dilute the market-based incentive for emissions reductions. Adding a longer-term return incentive for continued investment in emissions reductions would reinforce the market incentive.",,2024,10.1016/j.enpol.2024.114346,,proquest,"This study investigates whether financial markets price greenhouse gas emissions of vertically integrated electric utilities in the U.S. around the Paris Accord. The findings suggest that equity markets do reflect the cost of carbon risk, providing incentives for power producers to reduce emissions, with stronger evidence of this pricing in the post-Paris Accord period. The study also discusses the implications for regulators regarding rate cases and maintaining market-based incentives for emissions reductions.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:45:23.828781
fa8f409ad6d08349,Unconventional Monetary Policy in the Euro Zone,"The European Central Bank adopted a policy of quantitative easing early in 2015, long after the US and UK, and after implementing a succession of measures to increase liquidity in the Euro zone financial markets, none of which proved sufficient eventually. The paper draws out lessons for the Euro zone from US and UK experience. Numerous event studies have been undertaken to uncover the effects of QE on yields on and prices of financial assets. Estimated effects on long-term government bond yields are then converted into the size of the cut in the policy rate that would normally have been needed to produce them. From these implicit cuts in policy rates, estimates of the effect on GDP and inflation are generated. Euro zone QE appears to have had a much smaller effect on bond yields for the core members states than did QE in the US or UK. Therefore its effects on output and inflation are likely to be proportionately smaller. Its effects on long-term government bond yields in periphery members are greater. QE is compressing interest differential among Euro zone member states. The dangers of QE to which various commentators draw attention, that it creates a danger of inflation in the future, that it creates asset price bubbles, that it allows zombie firms and banks to survive, slowing down the process of adjustment, seem remote. Meanwhile it makes a useful contribution to cutting the costs of debt service and allowing member states more fiscal room for maneouvre.",,2016,10.1007/s11079-016-9393-0,,proquest,"This paper analyzes the effectiveness of the European Central Bank's quantitative easing (QE) policy, implemented in 2015, by drawing lessons from US and UK experiences. It uses event studies to estimate QE's impact on bond yields, GDP, and inflation, finding a smaller effect in the Eurozone core compared to the US/UK, but a greater effect on periphery member states, leading to compressed interest rate differentials. The paper dismisses common concerns about QE, such as future inflation or asset bubbles, and highlights its benefits in reducing debt servicing costs and increasing fiscal flexibility for member states.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:45:26.886243
1198155656cc7f11,Unconventional monetary policy in a nonlinear quadratic model,"After the financial market meltdown and the Great Recession of the years 2007–9, the financial market-macro link has become an important issue in monetary policy modeling. We develop a dynamic model that contains a nonlinear Phillips curve, a dynamic output equation, and a nonlinear credit flow equation – capturing the importance of credit cycles, risk premia, and credit spreads. Our Nonlinear Quadratic Model (NLQ) model has three dynamic state equations and a quadratic objective function. It can be used to evaluate the response of central banks to the Great Recession in moving from conventional to unconventional monetary policy. We solve the model with a new numerical procedure using estimated parameters for the euro area. We conduct simulations to explore the (de)stabilizing effects of the nonlinearities in the model. We demonstrate that credit flows, risk premia, and credit spreads play an important role as an amplification mechanism and in affecting the transmission of monetary policy. We thereby highlight the importance of the natural rate of interest as an anchor for a central bank target and the weight it places on the credit flows for the effectiveness of unconventional monetary policy. Our model is similar in structure compared to larger scale macro-econometric models which many central banks employ.",,2020,10.1515/snde-2019-0099,,proquest,"This paper introduces a Nonlinear Quadratic (NLQ) dynamic model to analyze the impact of unconventional monetary policy, particularly in response to the 2007-9 Great Recession. The model incorporates a nonlinear Phillips curve, an output equation, and a credit flow equation, emphasizing credit cycles, risk premia, and credit spreads. Using estimated parameters for the euro area and a novel numerical procedure, the study simulates the model to assess the stabilizing effects of nonlinearities and the role of credit flows in monetary policy transmission. The findings underscore the significance of the natural rate of interest and credit flows for effective unconventional monetary policy, drawing parallels to larger-scale models used by central banks.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:45:34.934290
51c6a633d70ff311,Understanding the determinants of bond excess returns using explainable AI,"Recent empirical evidence indicates that bond excess returns can be predicted using machine learning models. However, although the predictive power of machine learning models is intriguing, they typically lack transparency. This paper introduces the state-of-the-art explainable artificial intelligence technique SHapley Additive exPlanations (SHAP) to open the black box of these models. Our analysis identifies the key determinants that drive the predictions of bond excess returns produced by machine learning models and recognizes how these determinants relate to bond excess returns. This approach facilitates an economic interpretation of the predictions of bond excess returns made by machine learning models and contributes to a thorough understanding of the determinants of bond excess returns, which is critical for the decisions of market participants and the evaluation of economic theories. © 2023 Elsevier B.V., All rights reserved.","Beckmann, L.; Debener, J.; Kriebel, J.",2023,10.1007/s11573-023-01149-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160660732&doi=10.1007%2Fs11573-023-01149-5&partnerID=40&md5=e58f5c87140bb001abb38c4aebeb2e78,scopus,"This paper applies the SHapley Additive exPlanations (SHAP) technique to machine learning models predicting bond excess returns. It aims to increase transparency by identifying key determinants driving these predictions and facilitating economic interpretation, thereby enhancing understanding for market participants and economic theory evaluation.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T16:45:54.522784
398476d4b3717e7e,Unemployment insurance and mortgages,"We consider mortgages including the possibility of an unemployment insurance. The insurance company pays the cash flows of the credit as soon as the borrower becomes unemployed, for a maximal number of payments fixed in the contract. We develop a probabilistic model for describing the cash flows paid by the insurance company. We jointly take into account unemployment, job search and prepayment phenomena. With such a model it is possible to study the probabilistic properties of the cash flow pattern as a function of the age of the credit. Finally, we discuss the estimation of the parameters of such a model and its use for pricing the insurance contract. (C) 1997 Elsevier Science B.V.","Gourieroux, C; Scaillet, O",1997,10.1016/s0167-6687(97)00003-6,,wos,"This paper models mortgages with unemployment insurance, where the insurer covers payments upon borrower unemployment for a fixed period. It incorporates unemployment, job search, and prepayment dynamics to analyze cash flow patterns and price insurance contracts.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:45:57.235001
2648f39614791bbd,Unit-root tests and asymmetric adjustment with an example using the term structure of interest rates,"This article develops critical values to test the null hypothesis of a unit root against the alternative of stationarity with asymmetric adjustment. Specific attention is paid to threshold and momentum threshold autoregressive processes. The standard Dickey-Fuller tests emerge as a special case. Within a reasonable range of adjustment parameters, the power of the new tests is shown to be greater than that of the corresponding Dickey-Fuller test. The use of the tests is illustrated using the term structure of interest rates. It is shown that the movements toward the long-run equilibrium relationship are best estimated as an asymmetric process.","Enders, W; Granger, CWJ",1998,10.2307/1392506,,wos,"This paper introduces new unit-root tests that account for asymmetric adjustment, outperforming standard Dickey-Fuller tests in power. The methodology is applied to the term structure of interest rates, revealing asymmetric adjustments towards long-run equilibrium.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:46:02.614347
3c76378dff70bc76,Univariate and multivariate forecasting of the electricity futures curve using Dynamic Recurrent Neural Networks,"In recent years international power markets have witnessed high uncertainty and extraordinary volatility which, given the inherent complexity of the market, has made the Electricity Price Forecasting (EPF) process increasingly difficult. Therefore the development of a proper forecasting framework suitable for both stable and volatile periods has assumed an increasing importance for market players and policymakers in both strategic planning and risk management. At present, the majority of the studies on electricity price forecasting focused on the analysis of spot markets, neglecting the importance of derivative price modeling to mitigate the risks induced by market downturns and turmoil. Our study nests within this research stream and analyzes the potential of a set of state-of-the-art Machine Learning (ML) models for the prediction of the term structure of electricity futures prices. The objective is to define an ML-based framework capable of ensuring high predictive performance of the term structure during both stable and extremely turbulent conditions. In this regard we examined the predictive capabilities of a variety of Dynamic Recurrent Neural Networks (DRNNs) including: Nonlinear Autoregressive Neural Networks (NAR-NNs), NAR with Exogenous Inputs (NARX-NNs), Long Short-Term Memory (LSTM-NNs), Stacked Long Short-Term Memory (ST-LSTM-NNs), Bidirectional Long Short-Term Memory (BI-LSTM-NNs) and Encoder–Decoder Long Short-Term Memory Neural Networks (ED-LSTM-NNs). The models were applied to both low fluctuating and volatile sets of daily futures prices of the European Energy Exchange (EEX) for univariate as well as multivariate forecasting. Additionally, we compared this set of networks to baseline models commonly used in the EPF literature, including classical statistical and ML methods. Empirical results highlighted that DRNN models predictions are consistent with futures prices trends observed under different market regimes and outperform the competitors’ performance. Overall, main outcomes of the study may be summarized as follows: LSTM-based models seem to have the highest predictive power, with robust performance under various conditions. In detail the Multivariate BI-LSTM-NN performs better under quiet market conditions ensuring an accuracy level of 98.11 %, while the Univariate ED-LSTM-NN ensures superior predictive performance in presence of turmoil, achieving a 95.33 % accuracy. © 2025 Elsevier B.V., All rights reserved.","Castello, O.; Resta, M.",2025,10.1016/j.apenergy.2025.126082,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006717802&doi=10.1016%2Fj.apenergy.2025.126082&partnerID=40&md5=e71b13e9cb03bed028f944d93dbc5d7e,scopus,"This study investigates the use of Dynamic Recurrent Neural Networks (DRNNs), including various LSTM architectures, for forecasting electricity futures prices. The models were applied to both stable and volatile market conditions, comparing their performance against baseline methods. Results indicate that LSTM-based models, particularly Multivariate BI-LSTM-NN and Univariate ED-LSTM-NN, demonstrate strong predictive power under different market regimes.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:46:07.966419
a89249e7f1f4fe1a,Using Particle Swarm Optimization Algorithm to Calibrate the Term Structure Model,"One of the advantages of stochastic differential equations (SDE) is that they can follow a variety of different trends so that they can establish complex dynamic systems in the economic and financial fields. Although some estimation methods have been proposed to identify the unknown parameters in virtue of the results in the SDE model to speed up the process, these solutions only focus on using explicit approach to solve SDEs, and therefore they are not reliable to deal with data source merged being large and varied. Thus, this study makes progress in creating a new implicit way to fill in the gaps of accurately calibrating the unknown parameters in the SDE model. Essentially, the primary goal of the article is to generate rigid SDE simulation. Meanwhile, the particle swarm optimization method serves a purpose to search and simultaneously obtain the optimal estimation of the model unknown parameters in the complicated experiment of parameter space in an effective way. Finally, in an interest rate term structure model, it is verified that the method effectively deals with parameter estimation in the SDE model.",,2021,10.1155/2021/8893940,,proquest,This study proposes a new implicit method using the Particle Swarm Optimization algorithm to calibrate unknown parameters in a stochastic differential equation (SDE) model for interest rate term structure. The method aims to provide rigid SDE simulation and effectively estimates parameters in complex parameter spaces.,True,True,False,gemini-2.5-flash-lite,Olav,M,,2025-10-13T16:46:41.898669
eaf9d6fda873b291,Value-at-Risk via mixture distributions reconsidered,"Value-at-Risk (VaR) has evolved as one of the most prominent measures of downside risk in financial markets. Zhang and Cheng [M.-H. Zhang, Q.-S. Cheng, An Approach to VaR for capital markets with Gaussian mixture, Applied Mathematics and Computation 168 (2005) 1079-1085] proposed an approach to VaR for daily returns based on Gaussian mixtures, which have become rather popular in empirical economics and finance since the seminal paper of Hamilton [J.D. Hamilton, A new approach to the economic analysis of nonstationary time series and the business cycle, Econometrica 57 (2) (1989) 357-384]. However, they do not conduct tests to assess the accuracy of the mixture-implied VaR measures. Recently, Guidolin and Timmermann [M. Guidolin, A. Timmermann, Term structure of risk under alternative econometric specifications, Journal of Econometrics, 131 (2006) 285-308] showed that Markov mixture models do well in measuring VaR at a monthly frequency, but the results may not hold for daily returns due to their more pronounced non-Gaussian features. This paper provides an extensive application of various Markov mixture models to VaR for daily returns of major European stock markets, including out-of-sample backtesting. To accommodate the properties of daily returns, we consider both Gaussian and Student's t mixtures, and we compare the performance of both uni- and multivariate models under different parameter updating schemes. We find that a univariate mixture of two Student's t distributions performs best overall. However, by the example of the recent turmoil in financial markets, we also highlight a weak point of the approach. © 2009 Elsevier Inc. All rights reserved. © 2009 Elsevier B.V., All rights reserved.","Haas, M.",2009,10.1016/j.amc.2009.08.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349981419&doi=10.1016%2Fj.amc.2009.08.005&partnerID=40&md5=37f9b7ead5757561295b093089af7ee0,scopus,"This paper evaluates various Markov mixture models for calculating Value-at-Risk (VaR) for daily stock market returns, comparing Gaussian and Student's t mixtures. A univariate mixture of two Student's t distributions performed best overall, though a weakness was identified during recent market turmoil.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:46:46.286670
e81f47b356497a31,Valuing GM technologies using real options: the case of drought tolerant wheat in Australia,"In this article we seek to estimate the value of a partially-developed crop technology from the perspective of the firm developing the technology. Firms need this value estimation to decide whether their technology will earn a sufficient return in the market to justify investing in it. However, determining the (ex-ante) value of the technology before it is commercialised is challenging as the technology is not yet in the market and hence the demand function has not yet been defined. An alternative valuation method is required. We use risk premiums, Monte Carlo simulation and real options analysis and we demonstrate this combination of valuation tools on wheat that is currently being developed in Australia to be drought tolerant. The results indicate that this drought tolerant wheat variety is likely to be adopted by farmers in most regions and has a pre-commercialisation value that justifies continued investment in its development. We also identified South Australia as a region in which the new variety would not be sufficiently valuable to farmers to see them adopt it and we consider possible explanations for this outcome.",,2018,10.1080/09537325.2018.1474194,,proquest,"This article estimates the value of a drought-tolerant wheat technology in Australia using real options analysis, Monte Carlo simulation, and risk premiums. The study suggests the technology is likely to be adopted by farmers and justifies continued investment, although adoption is unlikely in South Australia.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:46:52.920099
5a945e5c94a9bfc5,Valuing the Treasury's Capital Assistance Program,"T he Capital Assistance Program (CAP) was created by the U. S. government in February 2009 to provide backup capital to large financial institutions unable to raise sufficient capital from private investors. Under the terms of the CAP, a participating bank receives contingent capital by issuing preferred shares to the Treasury combined with embedded options for both parties: The bank gets the option to redeem the shares or convert them to common equity, with conversion mandatory after seven years; the Treasury earns dividends on the preferred shares and gets warrants on the bank's common equity. We develop a contingent claims framework in which to estimate market values of these CAP securities. The interaction between the competing options held by the buyer and issuer of these securities creates a game between the two parties, and our approach captures this strategic element of the joint valuation problem and clarifies the incentives it creates. We apply our method to the 18 publicly held bank holding companies that participated in the Supervisory Capital Assessment Program (the stress test) launched together with the CAP. On average, we estimate that compared to a market transaction, the CAP securities carry a net value of approximately 30% of the capital invested for a bank participating to the maximum extent allowed under the terms of the program. We also find that the net value varies widely across banks. We compare our estimates with abnormal stock price returns for the stress test banks at the time the terms of the CAP were announced; we find correlations between 0.78 and 0.85, depending on the precise choice of period and set of banks included. These results suggest that our valuation aligns with shareholder perception of the value of the program, prompting questions about industry reactions and the overall impact of the program.","Glasserman, Paul; Wang, Zhenyu",2011,10.1287/mnsc.1110.1351,,wos,"This paper values the U.S. Treasury's Capital Assistance Program (CAP) using a contingent claims framework that accounts for the strategic interaction between the bank and the Treasury regarding embedded options. The authors estimate that CAP securities provided a net value of approximately 30% of the invested capital to participating banks, with significant variation across institutions. The valuation aligns with abnormal stock returns observed around the CAP announcement, suggesting market recognition of the program's value.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:46:55.681971
2a226e9e781cf124,Variable Selection and Oversampling in the Use of Smooth Support Vector Machines for Predicting the Default Risk of Companies,"In the era of Basel II a powerful tool for bankruptcy prognosis is vital for banks. The tool must be precise but also easily adaptable to the bank's objectives regarding the relation of false acceptances (Type I error) and false rejections (Type II error). We explore the suitability of smooth support vector machines (SSVM), and investigate how important factors such as the selection of appropriate accounting ratios (predictors), length of training period and structure of the training sample influence the precision of prediction. Moreover, we show that oversampling can be employed to control the trade-off between error types, and we compare SSVM with both logistic and discriminant analysis. Finally, we illustrate graphically how different models can be used jointly to support the decision-making process of loan officers. Copyright (C) 2008 John Wiley & Sons, Ltd.","Haerdle, Wolfgang; Lee, Yuh-Jye; Schaefer, Dorothea; Yeh, Yi-Ren",2009,10.1002/for.1109,,wos,"This study investigates the use of smooth support vector machines (SSVM) for predicting company default risk, comparing it with logistic and discriminant analysis. It explores the impact of variable selection, training period length, and sample structure on prediction accuracy. The research also demonstrates how oversampling can be used to manage the trade-off between Type I and Type II errors, offering a tool for loan officers to support decision-making.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:47:02.493226
8ac774f2ab9a9ab4,Volatility Expectations and Returns,"We provide evidence that agents have slow-moving beliefs about stock market volatility that lead to initial underreaction to volatility shocks followed by delayed overreaction. These dynamics are mirrored in the VIX and variance risk premiums, which reflect investor expectations about volatility, and are also supported in both surveys and firm-level option prices. We embed these expectations into an asset pricing model and find that the model can account for a number of stylized facts about market returns and return volatility that are difficult to reconcile, including a weak or even negative risk-return trade-off.","Lochstoer, Lars A.; Muir, Tyler",2022,10.1111/jofi.13120,,wos,"This study investigates how slow-moving beliefs about stock market volatility influence investor reactions to volatility shocks, leading to initial underreaction and subsequent overreaction. These patterns are observed in market indicators like the VIX and variance risk premiums, and are further supported by survey data and option prices. The authors incorporate these findings into an asset pricing model, which helps explain several established market return and volatility characteristics, including a weak or negative risk-return trade-off.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:47:09.316701
339ab8be530e9be4,Volatility in equity markets and monetary policy rate uncertainty,"Asset pricing models assume the risk-free rate to be a key factor for equity prices. Hence, there should be a strong link between monetary policy rate uncertainty and equity return volatility, both in theory and data. This paper uses regression-based projections for realized variance to examine the relationship between short horizon forecasts of equity variance and proxies for monetary policy rate uncertainty. By assessing various projection models for UK, US and euro area equity indices, we show that the proxies for monetary policy rate uncertainty have a significant and positive predictive power for the equity return variance. Adding monetary policy rate uncertainty variables can significantly improve forecasting models for equity variance and volatility at weekly, monthly and even quarterly horizons. The findings imply that market views of short-term interest rate developments may indeed be embedded in equity prices and their variations. (C) 2017 The Bank of England. Published by Elsevier B.V. All rights reserved.","Kaminska, Iryna; Roberts-Sklar, Matt",2018,10.1016/j.jempfin.2017.09.008,,wos,"This paper investigates the relationship between monetary policy rate uncertainty and equity return volatility using regression-based projections for realized variance. The study finds that proxies for monetary policy rate uncertainty have significant predictive power for equity return variance across UK, US, and euro area equity indices, improving forecasting models at various horizons. The results suggest that market expectations of short-term interest rate movements are reflected in equity prices and their fluctuations.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:47:12.017970
40ca5d0204bc68fe,War discourse and global equity returns,"This study investigates the asset pricing implications of war risks in global stock markets. We employ a novel war discourse index developed by Hirshleifer et al. (2023a), which captures market attention to war through news. Extending this approach to both developed and emerging markets, we uncover a significantly positive relation between war risks and global stock market excess returns, which is robust to a range of sensitivity checks. Our findings indicate that investor attention to war risks significantly influences equity premium in global markets. © 2024 Elsevier B.V., All rights reserved.","Wang, J.; Fang, Y.; Hu, X.; Zhong, A.",2024,10.1016/j.frl.2024.106068,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203652926&doi=10.1016%2Fj.frl.2024.106068&partnerID=40&md5=5190fb5a9dcd24cdbbd57eb075a3d053,scopus,"This study examines the relationship between war risks, measured by a war discourse index based on news attention, and global stock market returns. The research finds a significant positive correlation between war risks and excess returns in both developed and emerging markets, suggesting that investor attention to war influences the equity premium.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:47:16.685922
68a8425f93fcd105,Wasserstein barycenter regression for estimating the joint dynamics of renewable and fossil fuel energy indices,"In order to characterize non-linear system dynamics and to generate term structures of joint distributions, we propose a flexible and multidimensional approach, which exploits Wasserstein barycentric coordinates for histograms. We apply this methodology to study the relationships between the performance in the European market of the renewable energy sector and that of the fossil fuel energy one. Our methodology allows us to estimate the term structure of conditional joint distributions. This optimal barycentric interpolation can be interpreted as a posterior version of the joint distribution with respect to the prior contained in the past histograms history. Once the underlying dynamics mechanism among the set of variables are obtained as optimal Wasserstein barycentric coordinates, the learned dynamic rules can be used to generate term structures of joint distributions.",,2023,10.1007/s10287-023-00436-4,,proquest,"This paper introduces a novel approach using Wasserstein barycentric coordinates for histograms to analyze the joint dynamics of renewable and fossil fuel energy indices in the European market. The method estimates the term structure of conditional joint distributions, offering a flexible way to characterize non-linear system dynamics and generate future distribution terms based on historical data.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:47:24.515209
df6c8aefb227da2b,Wavelet Neural Network Model for Yield Spread Forecasting,"In this study, a hybrid method based on coupling discrete wavelet transforms (DWTs) and artificial neural network (ANN) for yield spread forecasting is proposed. The discrete wavelet transform (DWT) using five different wavelet families is applied to decompose the five different yield spreads constructed at shorter end, longer end, and policy relevant area of the yield curve to eliminate noise from them. The wavelet coefficients are then used as inputs into Levenberg-Marquardt (LM) ANN models to forecast the predictive power of each of these spreads for output growth. We find that the yield spreads constructed at the shorter end and policy relevant areas of the yield curve have a better predictive power to forecast the output growth, whereas the yield spreads, which are constructed at the longer end of the yield curve do not seem to have predictive information for output growth. These results provide the robustness to the earlier results.",,2017,10.3390/math5040072,,proquest,"This study proposes a hybrid model combining Discrete Wavelet Transforms (DWT) and Artificial Neural Networks (ANN) to forecast yield spreads. The DWT decomposes yield spreads into wavelet coefficients, which are then used as inputs for an ANN to predict output growth. The findings indicate that shorter-term and policy-relevant yield spreads are better predictors of output growth than longer-term spreads.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T16:47:44.364081
139d782a705519c0,Weight of the Default Component of CDS Spreads: Avoiding Procyclicality in Credit Loss Provisioning Framework,"The current expected loss calculations have recently attracted considerable attention in the research on credit risk modeling, impairment provisioning, and financial networks’ stability. A new CDS-based approach to estimate current expected credit loss is proposed for low default portfolios, containing credit exposures to corporate issuers covered by publicly traded CDS contracts. First, a fraction of CDS spread related to a pure default compensation for different CDS maturities is assessed. Our results contrast with previous research. Second, based on the obtained historical weights of the default risk premium, a forward-looking term structure of the probabilities of default implied by the current CDS quotes is derived. The proposed approach covers both investment and noninvestment grade debt. The resulting framework is applied to a sample of corporate bonds. The developed methodology provides a useful tool, on one hand, for credit risk managers and balance-sheet preparers and, on the other hand, for regulators of financial markets as it sheds light on how procyclicality could be avoided in provisions.",,2019,10.1155/2019/7820618,,proquest,This paper proposes a new Credit Default Swap (CDS)-based approach to estimate current expected credit loss for low-default portfolios. It assesses the fraction of CDS spread related to pure default compensation and derives a forward-looking term structure of probabilities of default. The methodology aims to avoid procyclicality in provisioning and is applicable to both investment and non-investment grade debt.,False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:47:48.042633
ec360be29d8293a3,What Drives Short Rate Dynamics? A Functional Gradient Descent Approach,"Functional gradient descent (FGD), a recent technique coming from computational statistics, is applied to the estimation of the conditional moments of the short rate process with the goal of finding the main drivers of the drift and volatility dynamics. FGD can improve the accuracy of some reasonable starting estimates obtained using classical short rate models introduced in the literature. It exploits the predictive information of an enlarged set of variables, including yields at other maturities, time, and macroeconomic indicators. Fitting this methodology to the time series of monthly US 3-month Treasury bill rates, we find that the drift dynamics react mostly in a nonlinear way to changes in macroeconomic variables, whereas volatility dynamics are subjected to time-dependent regime-switches. Finally we show the superior performance of the final predictions obtained by applying FGD in a forecasting exercise.","Audrino, Francesco",2012,10.1007/s10614-011-9310-y,,wos,"This paper applies Functional Gradient Descent (FGD), a statistical technique, to estimate the conditional moments of the short rate process, aiming to identify key drivers of drift and volatility. FGD enhances initial estimates from classical models by incorporating a wider range of variables, including other maturities' yields, time, and macroeconomic indicators. Applied to US 3-month Treasury bill rates, the study finds nonlinear responses of drift to macroeconomic changes and time-dependent regime switches in volatility. The method demonstrates superior forecasting performance.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T16:49:00.041291
190ed5fb32b9d0d7,Yield Curve Modeling: Applicability of the Traditional Factor Models for Sri Lanka Government Bonds,"The main aim of this study is to evaluate the effectiveness of two widely discussed yield curve models, the Nelson-Siegel model and the Nelson-Siegel-Svensson model, in estimating Sri Lanka Government Bond yields. The parameters for both models were estimated using the YieldCurve package in R-Studio. The average R-squared values were 96.25% for the Nelson-Siegel model and 98.75% for the Nelson-Siegel-Svensson model. However, neither model consistently achieved high R-squared values across the entire sample period. The Nelson-Siegel-Svensson model demonstrated greater consistency in R-squared values compared to the Nelson-Siegel model throughout the period. But the R-squared value declined in 2022 compared with the previous period for both models as the yield curve accompanied more twists and turns with volatile economic conditions. These results suggest that there is significant potential for developing more representative yield curve models or enhancing existing models by incorporating additional influential factors. The monetary authorities and Investment banks of the country would pay more attention to data-driven decision-making in the future to set up economic and monetary targets as well as to achieve hurdle rates for the client’s portfolios. Having an accurate yield curve model would be a play major role in this regard. © 2025 Elsevier B.V., All rights reserved.","Dayarathne, K.P.N.S.; Thayasivam, U.",2025,10.1007/s42979-025-04075-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008883628&doi=10.1007%2Fs42979-025-04075-1&partnerID=40&md5=bfd2bb858b8bff203220d422f64d79a5,scopus,"This study evaluates the Nelson-Siegel and Nelson-Siegel-Svensson yield curve models for Sri Lanka Government Bonds. While both models show high average R-squared values, their performance varied, particularly during volatile economic conditions in 2022. The Nelson-Siegel-Svensson model was more consistent. The findings suggest a need for more representative models or the incorporation of additional factors, highlighting the importance of accurate yield curve modeling for monetary authorities and investment banks.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:49:22.657985
4feca7e995018566,Yield Curve Point Triplets in Recession Forecasting,"Several studies have highlighted the yield curve's ability to forecast economic activity. These studies use the information provided by the slope of the yield curve-i.e., pairs of short- and long-term interest rates. In this paper, we construct three models for forecasting the positive and negative deviations of real US GDP from its long-run trend over the period from 1976Q3 to 2011Q4: one that uses only pairs of interest rates and two that draw on more than two points from the yield curve. We employ two alternative forecasting methodologies: the probit model, which is commonly used in this line of literature, and the support vector machines (SVM) approach from the area of machine learning. Our results show that we can achieve a 100% out-of-sample forecasting accuracy for negative output gaps (recessions) with both methodologies and an overall accuracy (both inflationary and unemployment gaps) of 80% in the case of the best SVM model. The forecasting performance of our model strengthens the existing evidence that the yield curve can be a useful tool for gauging future economic activity. © 2021 Elsevier B.V., All rights reserved.","Gogas, P.; Papadimitriou, T.; Chrysanthidou, E.",2015,10.1111/infi.12067,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940045257&doi=10.1111%2Finfi.12067&partnerID=40&md5=9fac416464a2ce0a8f7fa26d4a7513a9,scopus,"This paper constructs three models to forecast US GDP deviations from its long-run trend, utilizing yield curve information (pairs and triplets of interest rates). It compares the commonly used probit model with the machine learning approach of Support Vector Machines (SVM). The study achieves 100% out-of-sample accuracy in forecasting recessions and 80% overall accuracy for gaps using the best SVM model, reinforcing the yield curve's utility in economic forecasting.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:50:08.014022
7aed82193826f66e,Yield curve estimation of the nelson-siegel class model by using hybrid method with L-BFGS-B iterations approach,"This paper discussed about model extension in determines the yield curve. Determine of yield curve using Nelson-Siegel class model. This class model consisting of: 3-factor model, 4-factor model, the 5-factor model, and 6-factor model. 6-factor model is a model extended from 5-factor models. The extension aims to increase the level of accuracy in determine the yield curve. Nelson-Siegel class model is model that more difficult to estimate because it has two shape the parameters, i.e. the linear and nonlinear parameters. Extension of this model is done by adding the fourth hump into 5-factor model. In addition, we obtain new model, this model have local minimum multiple so that it is more difficult to be estimated. To estimate this model, we propose estimation using a hybrid method. Hybrid method is combines method of estimation the nonlinear least squares with constrained optimization, and then continued with L-BFGS-B iteration approach. Estimation of the class model was done by full estimation, i.e. estimating the linear parameters and nonlinear parameters simultaneously. Then, we calculated MSE, AIC, and BIC. The purpose of calculating this component is to determine the best of model. The best model obtainable if the models have component value which is smaller than the other models. This paper uses data from Indonesian government bonds. Based on data processing, we obtained the best model i.e. 6- factors model. © 2015 Elsevier B.V., All rights reserved.","Muslim; Rosadi, D.; Gunardi, G.; Abdurakhman, n.",2015,10.12988/ams.2015.43209,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929934326&doi=10.12988%2Fams.2015.43209&partnerID=40&md5=68f1e179c410a82458ca47a5b40c3e7f,scopus,"This paper proposes a hybrid estimation method combining nonlinear least squares with constrained optimization and L-BFGS-B iterations to estimate extended Nelson-Siegel class models (3- to 6-factor) for yield curve determination. The 6-factor model, an extension of the 5-factor model with an added hump, was found to be the best model based on MSE, AIC, and BIC using Indonesian government bond data.",True,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-13T16:50:35.257413
ab0c1b13706fa110,Yield curve extrapolation with machine learning,"Yield curve extrapolation to unobservable tenors is a key technique for the market-consistent valuation of actuarial liabilities required by Solvency II and forthcoming similar regulations. Since the regulatory method, the Smith-Wilson method, is inconsistent with observable yield curve dynamics, parsimonious parametric models, the Nelson-Siegel model and its extensions, are often used for yield curve extrapolation in risk management. However, it is difficult for the parsimonious parametric models to extrapolate yield curves without excessive volatility because of their limited ability to represent observed yield curves with a limited number of parameters. To extend the representational capabilities, we propose a novel yield curve extrapolation method using machine learning. Using the long short-term memory architecture, we achieve purely data-driven yield curve extrapolation with better generalization performance, stability, and consistency with observed yield curve dynamics than the previous parsimonious parametric models on US and Japanese yield curve data. In addition, our method has model interpretability using the backpropagation algorithm. The findings of this study prove that neural networks, which have recently received considerable attention in mortality forecasting, are useful for yield curve extrapolation, where they have not been used before. © 2025 Elsevier B.V., All rights reserved.","Akiyama, S.; Matsuyama, N.",2025,10.1017/asb.2024.27,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210168150&doi=10.1017%2Fasb.2024.27&partnerID=40&md5=5439066d1de45cd8165ba925b1874d67,scopus,"This study proposes a novel yield curve extrapolation method using Long Short-Term Memory (LSTM) neural networks, a machine learning technique. The method aims to improve upon traditional models like Smith-Wilson and Nelson-Siegel by offering better generalization, stability, and consistency with observed yield curve dynamics. The research demonstrates the effectiveness of this data-driven approach on US and Japanese yield curve data, highlighting its potential for market-consistent valuation of actuarial liabilities under regulations like Solvency II. The authors also note the interpretability of their model through backpropagation and suggest that neural networks, successful in mortality forecasting, are also valuable for yield curve extrapolation.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-13T16:50:45.245519
