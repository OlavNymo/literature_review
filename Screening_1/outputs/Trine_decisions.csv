paper_id,title,abstract,authors,year,doi,link,database,llm_summary,i1_hint,i2_hint,i3_hint,model,reviewer,decision,notes,timestamp
71de55dd11e1d89d,A Hybrid Methodology Using Machine Learning Techniques and Feature Engineering Applied to Time Series for Medium- and Long-Term Energy Market Price Forecasting,"In the electricity market, the issue of contract negotiation prices between generators/traders and buyers is of particular relevance, as an accurate contract modeling leads to increased financial returns and business sustainability for the various participating agents, encouraging investments in specialized sectors for price forecasting and risk analysis. This paper presents a methodology applied in experiments on energy forward curve scenarios using a set of techniques, including Long Short-Term Memory (LSTM), Extreme Gradient Boosting (XGBoost), Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors (SARIMAX), and Feature Engineering to generate a 10-year projection of the Conventional Long-Term Price. The model validation proved to be effective, with errors of only 4.5% by Root Mean Square Error (RMSE) and slightly less than 2% by Mean Absolute Error (MAE), for a time series spanning from 7 January 2012 to 31 August 2024, in the Brazilian energy market.",,2025,10.3390/en18061387,,proquest,"This paper proposes a hybrid methodology combining Long Short-Term Memory (LSTM), Extreme Gradient Boosting (XGBoost), SARIMAX, and Feature Engineering for medium- and long-term energy market price forecasting. The model achieved low error rates (4.5% RMSE, <2% MAE) in predicting the Brazilian energy market's conventional long-term price over a 10-year projection.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:27:27.201753
7af7d2c3acf27b2d,A Markov-modulated tree-based gradient boosting model for auto-insurance risk premium pricing,"In most sub Saharan African countries,the mechanism for pricing auto-insurance policies is tariff based. This means that the key factor that influences price changes is usually based on regulation and legislative dynamics. Additionally, where ratemaking is risk based, analysis has in most cases focused on internal historical data or claims history, particularly in the sub Saharran Africa. These policy regimes have led to unfair price distortions among policyholders and have increased risk of portfolios for most insurance companies. In this study we consider geographical location risk that influence auto-insurance claim process for an insurance company. The study develops a Markov-modulated tree-based gradient boosting (MMGB) model for pricing auto-insurance premiums. The Markov-modulated tree-based gradient boosting model is a Tweedie general linear model (GLM) based pricing algorithm with a compound Poisson-Gamma distribution whose rate varies according to accident risk in a Markovian process. Thus, the study extends the existing premium pricing framework by integrating a geographical location risk factor into the main pricing framework. The study applies the model to a motor insurance data set from Ghana. The results show that the proposed method is superior to other competing models because it generates relatively fair premium predictions for the non-life auto-insurance companies, helping to mitigate more the insured risk for the firm and the industry. © 2020 Elsevier B.V., All rights reserved.","Arku, D.; Doku-Amponsah, K.; Howard, N.K.",2020,10.3233/rda-180050,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085768703&doi=10.3233%2FRDA-180050&partnerID=40&md5=43274c399c2abe9303985a4ed9506140,scopus,"This study proposes a Markov-modulated tree-based gradient boosting (MMGB) model for auto-insurance premium pricing, incorporating geographical location risk. The model, a Tweedie GLM with a compound Poisson-Gamma distribution, uses a Markovian process for accident risk and is applied to motor insurance data from Ghana. Results indicate the MMGB model provides fairer premium predictions and better risk mitigation compared to other models.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:27:37.268421
94f2194dac2bf593,A Multi-Objective Portfolio Selection Model With Fuzzy Value-at-Risk Ratio,"Considering nonstatistical uncertainties and/or insufficient historical data in security return forecasts, fuzzy set theory has been applied in the past decades to build portfolio selection models. Meanwhile, various risk measurements such as variance, entropy, and Value-at-Risk have been proposed in fuzzy environments to evaluate investment risks from different perspectives. Sharpe ratio, also known as the reward-to-variability ratio, which measures the risk premium per unit of the nonsystematic risk (asset deviation), has received great attention in modern portfolio theory. In this study, the Sharpe ratio in fuzzy environments is introduced, whereafter, a fuzzy Value-at-Risk ratio is proposed. Compared with Sharpe ratio, Value-at-Risk ratio is an index with dimensional knowledge that reflects the risk premium per unit of the systematic risk (the greatest loss under a given confidence level). On the basis of the two ratios, a multi-objective model is built to evaluate their joint impact on portfolio selection. Then, the proposed model is solved by a fuzzy simulation based multi-objective particle swarm optimization algorithm, where the global best of each iteration is determined by an improved dominance times based method. Finally, the algorithm superiority is justified via comparing with existing solvers on benchmark problems, and the model effectiveness is exemplified by using three case studies on portfolio selection.",B. Wang; Y. Li; S. Wang; J. Watada,2018,10.1109/tfuzz.2018.2842752,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8370687,ieeexplore,"This paper proposes a multi-objective portfolio selection model incorporating fuzzy Value-at-Risk ratio and Sharpe ratio. It addresses uncertainties in security return forecasts using fuzzy set theory and introduces a novel fuzzy Value-at-Risk ratio to measure systematic risk. The model is solved using a fuzzy simulation-based multi-objective particle swarm optimization algorithm, and its effectiveness is demonstrated through case studies.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:27:43.707252
512099f188d82d66,"A Network and Machine Learning Approach to Factor, Asset, and Blended Allocation","The main idea of this article is to approach and compare factor and asset allocation portÂfolios using both traditional and alternative allocation techniques: Inverse variance optimization, minimum-variance optimization, and centrality-based techniques from network science. Analysis of the interconnectedÂness between assets and factors shows that their relaÂtionship is strong. The authors compare the allocation techniques, considering centrality and hierarchal-based networks. They demonstrate the advantages of graph theory to explain the advantages to portfolio manageÂment and the dynamic nature of assets and factors with their ""importance score."" They find that asset allocaÂtion can be efficiently derived using directed networks, dynamically driven by both US Treasuries and curÂrency returns with significant centrality scores. AlterÂnatively, the inverse variance weight estimation and correlation-based networks generate factor allocation with favorable risk-return parameters. Furthermore, factor allocation is driven mostly by the importance scores of the Fama-French-Carhart factors: SMB, HML, CMA, RMW, and MOM. The authors confirm previous results and argue that both factors and assets are interconnected with different value and momentum factors. Therefore, a blended strategy comprising factors and assets can be defensible for investors. As argued in previous research, factors are much more overcrowded than assets. Therefore, the centrality scores help to identify the crowded expoÂsure and build diversified allocation. The authors run LASSO regressions and show how the network-based allocation can be implemented using machine learning. © 2020 Elsevier B.V., All rights reserved.","Konstantinov, G.; Chorus, A.; Rebmann, J.",2020,10.3905/jpm.2020.1.147,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088367857&doi=10.3905%2Fjpm.2020.1.147&partnerID=40&md5=6e1dee8ae9c04309ccd1b8d63a1dbd71,scopus,"This article proposes a network and machine learning approach to compare factor and asset allocation portfolios using traditional and alternative techniques. It analyzes the interconnectedness between assets and factors, demonstrating the advantages of graph theory for portfolio management and identifying key drivers for asset and factor allocation. The study suggests a blended strategy of factors and assets can be beneficial, using machine learning (LASSO regressions) to implement network-based allocation.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:27:57.422600
3c7ed3cc4ae5de19,A Neural Network Architecture for Maximizing Alpha in a Market Timing Investment Strategy,"In finance, assuming more risk often corresponds to the expectation of higher, compensating returns. In this setting, alpha stands out as one of the most prevalent and refined measures of risk-adjusted return ever postulated, allowing for the estimation of the excess return that cannot be explained by the risk factors impacting an asset. This article introduces a neural network architecture designed to formulate an investment strategy with the explicit goal of maximizing alpha. The strategy, centered around market timing, determines on a daily basis, based on past returns of the risky asset, whether to fully invest in the risky asset or opt for the risk-free alternative. The neural network architecture comprises two components: a policy network for strategy implementation and an evaluation network for long-term alpha computation during parameter optimization. Employing value-weighted US size decile portfolios as risky assets, the study achieves significant out-of-sample alphas ranging from 3.6% to 8.2% per year under the  $q^{5}$  asset pricing model (with a transaction cost assumption of one basis point). By construction, these alphas are not generated by risky asset growth. Robustness tests yield similar results with equal-weighted decile portfolios or under the Fama and French six-factor asset pricing model. Variations in transaction cost, number of past returns used as inputs, policy network design, or training sample size produce similar outcomes. This study underscores the effectiveness of reinforcement learning-inspired techniques in uncovering alpha in financial markets.",J. H. Ospina-Holguín; A. M. Padilla-Ospina,2024,10.1109/access.2024.3446708,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10640102,ieeexplore,"This article presents a neural network architecture for a market timing investment strategy aimed at maximizing alpha. The strategy uses a policy network and an evaluation network to decide daily between investing in a risky asset or a risk-free alternative, based on past returns. The study achieved significant out-of-sample alphas (3.6%-8.2% annually) using US size decile portfolios and the $q^{5}$ asset pricing model, with robustness checks confirming these results across different portfolio types, factor models, and parameter variations. The findings highlight the potential of reinforcement learning-inspired methods for alpha generation.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:28:25.401538
88f2b0ee6b8ed45b,A New Credit Spread to Predict Economic Activities in China,"In recent years, the relationship between bond spreads and macro economy has been studied extensively by economists in western countries. However, few attentions were paid on this topic in China. This essay regards Chinese bond market as a complex system and constructs bond indices for China with the bottom-up approach. The authors use the data of 3,205 non-financial corporate bonds from February 2010 to October 2017 and construct a new spread noted as the PE_SOE spread. The authors find that the PE_SOE spread has a negative impact on economic activities and has the best predictive ability at short-run forecasting horizons, owing to the institutional superiority of the state-owned enterprises in China. The Treasury bond yields are found to have the best predictive ability at long-run horizons. Both spread shock and Treasury yield shock could lead to deflation and declines in economic activities, and the Treasury yield shock has a more severe and persistent impact on the economy due to the financial accelerator mechanism. PE_SOE spread is proved to be a better indicator for Chinese corporate bond market and can be widely used not only in future Chinese economic studies, but also for Chinese government’s macroeconomic monitoring and warning.",,2019,10.1007/s11424-019-8033-3,,proquest,"This study constructs a new credit spread, the PE_SOE spread, using data from 3,205 non-financial corporate bonds in China from February 2010 to October 2017. The PE_SOE spread negatively impacts economic activities and is a strong short-run predictor, while Treasury bond yields are better long-run predictors. Both spread and Treasury yield shocks can lead to deflation and economic decline, with Treasury yield shocks having a more severe and persistent impact. The PE_SOE spread is identified as a superior indicator for the Chinese corporate bond market and useful for macroeconomic monitoring.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:28:53.853948
d4e4962a4745e30c,A Risk-Free Discount Rate Prediction Model for Mineral Project Evaluation Using a Hybrid Discrete Wavelet Transform and Artificial Neural Network,"The discount rate input parameter of Net Present Value (NPV) in mineral project evaluation is a function of a risk-free rate and risk premium component. To obtain a reliable NPV, it is important to estimate each of these components. This study employs a hybrid approach to predict risk-free rate using Discrete Wavelet Transform and Artificial Neural Network (DWT-ANN). The DWT-ANN model was tested using London Interbank Offered Rate (LIBOR) dataset from 1986 to 2020. The results showed that Discrete Wavelet Transform-Radial Basis Function Neural Network (DWT-RBFNN) of the three different hybrid algorithms developed and applied performed best in predicting the risk-free rate. This is because it achieved the lowest root mean square error of 0.0376 and the highest correlation coefficient of 0.9995. The DWT-RBFNN model can be a useful alternative tool for predicting risk-free rate, which is a key input parameter for the determination of discount rate. © 2022 Elsevier B.V., All rights reserved.","Gyebuni, R.; Ziggah, Y.Y.; Mireku-Gyimah, D.",2022,10.1155/2022/9984679,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139564882&doi=10.1155%2F2022%2F9984679&partnerID=40&md5=1dc2d0577c07a01ce69bdf0d8deb8b22,scopus,"This study proposes a hybrid Discrete Wavelet Transform and Artificial Neural Network (DWT-ANN) model to predict the risk-free rate, a crucial component for determining the discount rate in mineral project evaluation. The DWT-RBFNN model demonstrated superior performance in predicting the risk-free rate using LIBOR data from 1986-2020, achieving a low RMSE of 0.0376 and a high correlation coefficient of 0.9995.",True,False,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T15:31:00.866451
339df4defc6a0a72,A Semi-Static Replication Method for Bermudan Swaptions under an Affine Multi-Factor Model,"We present a semi-static replication algorithm for Bermudan swaptions under an affine, multi-factor term structure model. In contrast to dynamic replication, which needs to be continuously updated as the market moves, a semi-static replication needs to be rebalanced on just a finite number of instances. We show that the exotic derivative can be decomposed into a portfolio of vanilla discount bond options, which mirrors its value as the market moves and can be priced in closed form. This paves the way toward the efficient numerical simulation of xVA, market, and credit risk metrics for which forward valuation is the key ingredient. The static portfolio composition is obtained by regressing the target option’s value using an interpretable, artificial neural network. Leveraging the universal approximation power of neural networks, we prove that the replication error can be arbitrarily small for a sufficiently large portfolio. A direct, a lower bound, and an upper bound estimator for the Bermudan swaption price are inferred from the replication algorithm. Additionally, closed-form error margins to the price statistics are determined. We practically study the accuracy and convergence of the method through several numerical experiments. The results indicate that the semi-static replication approaches the LSM benchmark with basis point accuracy and provides tight, efficient error bounds. For in-model simulations, the semi-static replication outperforms a traditional dynamic hedge. © 2023 Elsevier B.V., All rights reserved.","Hoencamp, J.H.; Jain, S.; Kandhai, D.",2023,10.3390/risks11100168,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175481739&doi=10.3390%2Frisks11100168&partnerID=40&md5=8b7a20248b55d866fc0a12d7eb46c0ab,scopus,"This paper introduces a semi-static replication method for Bermudan swaptions using an affine, multi-factor term structure model. Unlike dynamic replication, this method requires rebalancing only at discrete intervals. The approach decomposes the swaption into a portfolio of vanilla discount bond options, allowing for closed-form pricing. It utilizes an artificial neural network for static portfolio composition and demonstrates that replication error can be minimized with a sufficiently large portfolio. The method provides estimators for swaption prices and error margins, showing accuracy comparable to the LSM benchmark and outperforming traditional dynamic hedging in simulations.",True,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:31:54.606328
d67f32ecdbb90051,A Structural Credit Risk Model with Jumps Based on Uncertainty Theory,"This study, within the framework of uncertainty theory, employs an uncertain differential equation with jumps to model the asset value process of a company, establishing a structured model of uncertain credit risk that incorporates jumps. This model is applied to the pricing of two types of credit derivatives, yielding pricing formulas for corporate zero-coupon bonds and Credit Default Swap (CDS). Through numerical analysis, we examine the impact of asset value volatility and jump magnitude on corporate default uncertainty, as well as the influence of jump magnitude on the pricing of zero-coupon bonds and CDS. The results indicate that an increase in volatility levels significantly enhances default uncertainty, and an expansion in the magnitude of negative jumps not only directly elevates default risk but also leads to a significant increase in the value of zero-coupon bonds and the price of CDS through a risk premium adjustment mechanism. Therefore, when assessing corporate default risk and pricing credit derivatives, the disturbance of asset value jumps must be considered a crucial factor.",,2025,10.3390/math13060897,,proquest,This study develops a structural credit risk model using uncertainty theory and uncertain differential equations with jumps to price corporate zero-coupon bonds and Credit Default Swaps (CDS). Numerical analysis shows that asset value volatility and negative jumps increase default uncertainty and impact derivative pricing. The study emphasizes considering asset value jumps in default risk assessment and derivative pricing.,False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:32:46.608576
84a48054af586950,A Study of Stock Market Predictability Based on Financial Time Series Models,"In today’s era of economic globalization and financial integration, the stock market is constantly complex, showing many deviations that cannot be explained by classical financial analysis, but at the same time, some classic financial statistical features have striking similarities. This suggests that although the stock market is intricate, there are universal laws that can be found through data mining to find its underlying operating rules. In this paper, we construct financial time series models such as ARIMA, ARCH, and GARCH to predict the stock market price fluctuations and trends. The ARIMA model is used to fit the linear financial time series, and the GARCH model is used to fit the nonlinear time series residuals. The results show that the integrated tree model based on the idea of weight voting has high accuracy in predicting stock market bulls and bears, with XGBoost prediction accuracy up to 96%, and the neural network model is also very effective, with an accuracy rate of over 90%.",,2022,10.1155/2022/8077277,,proquest,"This paper explores stock market predictability using financial time series models like ARIMA, ARCH, and GARCH. It applies these models, including XGBoost and neural networks, to predict price fluctuations and trends, achieving high accuracy rates.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:33:04.982714
4229103779a54ec5,A TVM-Copula-MIDAS-GARCH model with applications to VaR-based portfolio selection,"This paper develops a novel time-varying multivariate Copula-MIDAS-GARCH (TVM-Copula-MIDAS-GARCH) model with exogenous explanatory variables to model the joint distribution of returns. The model accounts for mixed frequency factors that affect the time-varying dependence structure of financial assets. Furthermore, we examine the effectiveness of the proposed model in VaR-based portfolio selection. We conduct an empirical analysis on estimating the 90%, 95%, 99% VaRs of the portfolio constituted of the Shanghai Composite Index, Shanghai SE Fund Index, and Shanghai SE Treasury Bond Index. The empirical results show that the proposed TVM-Copula-MIDAS-GARCH model is effective to investigate the nonlinear time-varying dependence among those three indices and performs better in portfolio selection. © 2020 Elsevier B.V., All rights reserved.","Jiang, C.; Ding, X.; Xu, Q.; Tong, Y.",2020,10.1016/j.najef.2019.101074,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072602228&doi=10.1016%2Fj.najef.2019.101074&partnerID=40&md5=73863b38a88325f20a4b8f7eddc0f364,scopus,"This paper introduces a new TVM-Copula-MIDAS-GARCH model to capture the time-varying dependence structure of financial assets using mixed-frequency factors. The model's effectiveness is demonstrated through an empirical analysis of VaR-based portfolio selection using the Shanghai Composite Index, Shanghai SE Fund Index, and Shanghai SE Treasury Bond Index, showing improved performance in nonlinear dependence investigation and portfolio selection.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:33:11.220482
209637058300c847,A Unified Framework for Fast Large-Scale Portfolio Optimization,"We introduce a unified framework for rapid, large-scale portfolio optimization that incorporates both shrinkage and regularization techniques. This framework addresses multiple objectives, including minimum variance, mean-variance, and the maximum Sharpe ratio, and also adapts to various portfolio weight constraints. For each optimization scenario, we detail the translation into the corresponding quadratic programming (QP) problem and then integrate these solutions into a new open-source Python library. Using 50 years of return data from US mid to large-sized companies, and 33 distinct firm-specific characteristics, we utilize our framework to assess the out-of-sample monthly rebalanced portfolio performance of widely-adopted covariance matrix estimators and factor models, examining both daily and monthly returns. These estimators include the sample covariance matrix, linear and nonlinear shrinkage estimators, and factor portfolios based on Asset Pricing (AP) Trees, Principal Component Analysis (PCA), Risk Premium PCA (RP-PCA), and Instrumented PCA (IPCA). Our findings emphasize that AP-Trees and PCA-based factor models consistently outperform all other approaches in out-of-sample portfolio performance. Finally, we develop new (Formula presented.) and (Formula presented.) regularizations of factor portfolio norms which not only elevate the portfolio performance of AP-Trees and PCA-based factor models but they have a potential to reduce an excessive turnover and transaction costs often associated with these models. © 2025 Elsevier B.V., All rights reserved.","Deng, W.; Polak, P.; Safikhani, A.; Shah, R.",2024,10.1080/26941899.2023.2295539,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003413482&doi=10.1080%2F26941899.2023.2295539&partnerID=40&md5=09ebcd357e426caf6aa3db4d044e82bc,scopus,"This paper presents a unified framework for fast, large-scale portfolio optimization, integrating shrinkage and regularization techniques to handle multiple objectives (minimum variance, mean-variance, maximum Sharpe ratio) and various constraints. The framework is translated into quadratic programming problems and implemented in an open-source Python library. Empirical analysis using 50 years of US company data and 33 firm characteristics compares the out-of-sample performance of various covariance matrix estimators and factor models, finding AP-Trees and PCA-based models to be superior. New regularizations are proposed to improve performance and reduce turnover.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:33:22.118005
8467d8f718f5b82c,A class of Gaussian hybrid processes for modeling financial markets,"This paper proposes a one-factor model of financial markets using a class of Gaussian process that can be decomposed into a Brownian motion and an Ornstein-Uhlenbeck process. It is shown that this ""hybrid"" process is obtained as a continuous-time scaling limit of the differenced first-order autoregressive integrated moving average (ARIMA(1,1,1)) process. Parameter estimations using an ARIMA(1,1,1) framework and its variance ratio test show the accuracy of the proposed model. Construction of the one-factor commodity futures price model is presented as an application. A multidimensional extension of the hybrid process is also presented in the Appendix. © 2008 Springer Science+Business Media, LLC. © 2008 Elsevier B.V., All rights reserved.","Itoh, Y.",2007,10.1007/s10690-007-9058-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-41149159008&doi=10.1007%2Fs10690-007-9058-5&partnerID=40&md5=d41636f4abc62db5bad174cc42210d9d,scopus,"This paper introduces a novel one-factor financial market model based on a Gaussian process that combines Brownian motion and an Ornstein-Uhlenbeck process. The model is derived as a limit of ARIMA(1,1,1) processes and its parameters can be estimated using ARIMA frameworks and variance ratio tests. An application to commodity futures pricing is demonstrated, and a multidimensional extension is also provided.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:33:42.451979
23dde43cefbb3d8b,A comparison of linear regression and neural network methods for predicting excess returns on large stocks,"Recent studies have shown that there is predictable variation in returns of financial assets over time. We investigate whether the predictive power of the economic and financial variables employed in the above studies can be enhanced if the statistical method of linear regression is replaced by feedforward neural networks with backpropagation of error. A shortcoming of backpropagation networks is that too many free parameters allow the neural network to fit the training data arbitrarily closely resulting in an ""overfitted"" network. Overfitted networks have poor generalization capabilities. We explore two methods that attempt to overcome this shortcoming by reducing the complexity of the network. The results of our experiments confirm that an ""overfitted"" network, while making better predictions for within-sample data, makes poor predictions for out-of-sample data. The methods for reducing the complexity of the network, explored in this paper, clearly help improve out-of-sample forecasts. We show that one cannot say that the linear regression forecasts are conditionally efficient with respect to the neural networks forecasts with any degree of confidence. However, one can say that the neural networks forecasts are conditionally efficient with respect to the linear regression forecasts with some confidence. © 2020 Elsevier B.V., All rights reserved.","Desai, V.S.; Bharati, R.",1998,10.1023/a:1018993831870,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032376229&doi=10.1023%2Fa%3A1018993831870&partnerID=40&md5=9522b8d59588be335e6542c062a56e6f,scopus,"This study compares linear regression and neural networks for predicting excess stock returns, finding that while neural networks can overfit, methods to reduce complexity improve out-of-sample forecasts. Neural network forecasts are found to be conditionally efficient with respect to linear regression forecasts.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:33:49.651257
6260d09b870d4639,A comparison of tests of nonlinear cointegration with application to the predictability of US interest rates using the term structure,"We test whether there are nonlinearities in the response of short- and long-term interest rates to the spread in interest rates, and assess the out-of-sample predictability of interest rates using linear and nonlinear models. We find strong evidence of nonlinearities in the response of interest rates to the spread. Nonlinearities are shown to result in more accurate short-horizon forecasts, especially of the spread. © 2004 International Institute of Forecasters. Published by Elsevier B.V. All rights reserved. © 2008 Elsevier B.V., All rights reserved.","Clements, M.P.; Galvão, A.B.",2004,10.1016/j.ijforecast.2003.09.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1842659918&doi=10.1016%2Fj.ijforecast.2003.09.001&partnerID=40&md5=0815b049531bbb3f1e71069935d68952,scopus,"This study investigates nonlinearities in the relationship between short- and long-term interest rates and their impact on interest rate predictability. The authors find significant nonlinearities that improve short-horizon forecasts, particularly for the interest rate spread. They compare linear and nonlinear models for out-of-sample prediction.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-13T15:34:30.514097
1d5944dc1f483123,"A constrained least square method for estimating a smooth, nonnegative forward rate sequence","We will develop an efficient method for estimating a smooth nonnegative forward rate sequence using the market price of riskless bonds. This method is an improvement of the classical Carleton-Cooper's method based on standard least square method, which often generates a non-smooth forward rate sequence and hence is not used in practice. The method to be proposed in this paper is intended to resolve this difficulty. We will impose a smoothness condition while maintaining the fitting error within an acceptable level. The resulting optimization problem is shown to be convex in the region of interest. Therefore, we can calculate a globally optimal solution very fast by standard nonlinear programming algorithms. We will demonstrate that this method generates a smooth forward rate sequence at the expense of a very small increase of fitting error. © World Scientific Publishing Company. © 2008 Elsevier B.V., All rights reserved.","Konno, H.; Ito, S.",2005,10.1142/s0219024905003293,https://www.scopus.com/inward/record.uri?eid=2-s2.0-27544457345&doi=10.1142%2FS0219024905003293&partnerID=40&md5=d4b267dbfd7198ed1b95a3f81c3e5184,scopus,"This paper proposes an improved method for estimating smooth, nonnegative forward rate sequences using market prices of riskless bonds. It addresses limitations of the classical Carleton-Cooper method by incorporating a smoothness condition into a constrained least squares approach, resulting in a convex optimization problem solvable with nonlinear programming algorithms. The method demonstrates the ability to produce smooth forward rate sequences with only a minor increase in fitting error.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:34:36.669462
bb18143d023a488b,A dynamic asset allocation approach under technical analysis and machine learning classification,"This research evaluates the effectiveness of machine learning models by comparing the performance of seven models in predicting the return movement of four asset types over different return periods. The assets include S&P 500 Futures, Gold Futures, US 10Y Treasury Bond Futures, and a risk-free investment. The study considers 1-week, 4-week, and 12-week forward return periods. The analysis assesses model accuracy and portfolio performance using data spanning two decades. ML-based portfolios modestly outperform an equal-weighted benchmark in terms of risk-adjusted metrics over longer horizons, even though the differences are generally insignificant. However, the study emphasizes the importance of considering diverse economic cycles to understand asset behaviour comprehensively. Nevertheless, the study did not find significant evidence that combining machine learning prediction with dynamic asset allocation can outperform passive portfolio investment. The study underscores the potential of non-linear models in short-term trading but also notes limitations in adaptiveness, threshold design, and model generalization under atypical conditions. © 2025 Elsevier B.V., All rights reserved.","Luc, G.B.; Wu, C.-C.",2025,10.1080/00036846.2025.2526177,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010230429&doi=10.1080%2F00036846.2025.2526177&partnerID=40&md5=1bfee009edd82f459adff74af01afe83,scopus,"This research evaluates seven machine learning models for predicting asset returns (S&P 500 Futures, Gold Futures, US 10Y Treasury Bond Futures, risk-free investment) over 1, 4, and 12-week periods. While ML-based portfolios showed modest outperformance over an equal-weighted benchmark in risk-adjusted metrics for longer horizons, the differences were generally insignificant. The study found no significant evidence that combining ML prediction with dynamic asset allocation outperforms passive investment, highlighting the potential of non-linear models for short-term trading but noting limitations in adaptiveness and generalization.",True,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T15:35:13.519668
3ab17db382610c1a,A dynamic target volatility strategy for asset allocation using artificial neural networks,"A challenge to developing data-driven approaches in finance and trading is the limited availability of data because periods of instability, such as during financial market crises, are relatively rare. This study applies a stability-oriented approach (SOA) based on statistical tests to compare data for the current period to a past set of data for a stable period, providing higher reliability due to a more abundant source of data. Based on an SOA, this study uses an artificial neural network (ANN), which is one of the commonly applied machine learning algorithms, for simultaneously forecasting the volatility and classifying the level of market stability. In addition, this study develops a dynamic target volatility strategy for asset allocation using an ANN to enhance the ability of a target volatility strategy that is established for automatically allocating capital between a risky asset and a risk-free cash position. In order to examine the impact of the proposed strategy, the results are compared to the buy-and-hold strategy, the static asset allocation strategy, and the conventional target volatility strategy using different volatility forecasting methodologies. An empirical case study of the proposed strategy is simulated in both the Korean and U.S. stock markets.",,2018,10.1080/0013791x.2018.1461287,,proquest,"This study proposes a dynamic target volatility strategy for asset allocation using artificial neural networks (ANNs) and a stability-oriented approach (SOA). The ANN forecasts volatility and classifies market stability, enhancing a target volatility strategy for capital allocation between risky assets and cash. The strategy's performance is compared to buy-and-hold, static allocation, and conventional target volatility strategies, with simulations in Korean and U.S. stock markets.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:35:22.458337
58faaf67a8d7f201,A framework to measure integrated risk,A framework underlying various models that measure the credit risk of a portfolio is extended in this paper to allow the integration of credit risk with a range of market risks using Monte Carlo simulation. A structural model is proposed that allows interest rates to be stochastic and provides closed-form expressions for the market value of a firm's equity and its probability of default. This model is embedded within the integrated framework and the general approach illustrated by measuring the risk of a foreign exchange forward when there is a significant probability of default by the counterparty. For this example moving from a market risk calculation to an integrated risk calculation reduces the expected future value of the instrument by an amount that Could not be calculated using the common pre-settlement exposure technique for estimating the credit risk of a derivative.,"Medova, EA; Smith, RG",2005,10.1080/14697680500117583,,wos,"This paper extends a framework for measuring credit risk to integrate it with market risks using Monte Carlo simulation. It proposes a structural model with stochastic interest rates, providing closed-form expressions for equity value and default probability. The approach is illustrated by measuring the risk of a foreign exchange forward with counterparty default risk, showing that integrated risk calculation significantly reduces the expected future value compared to standard pre-settlement exposure techniques.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:35:24.950443
8cbc033b8fc4ec78,A genetic algorithm estimation of the term structure of interest rates,"The term structure of interest rates is a key instrument for financial research. It provides relevant information for pricing deterministic financial cash flows, it measures economic market expectations and it is extremely useful when assessing the effectiveness of monetary policy decisions. However, it is not directly observable and needs to be estimated by smoothing asset pricing data through statistical techniques. The most popular techniques adjust parsimonious functional forms based on bond yields to maturity. Unfortunately, these functions, which need to be optimised, are highly non-linear which make them very sensitive to the initial conditions. In this context, this paper proposes the use of genetic algorithms to find the values for the initial conditions and to reduce the risk of false convergence, showing that stable parameters are obtained without imposing arbitrary restrictions.",,2009,10.1016/j.csda.2008.10.030,,proquest,"This paper proposes using genetic algorithms to estimate the term structure of interest rates, addressing the challenges of non-linear functional forms and sensitivity to initial conditions in traditional methods. The approach aims to find optimal initial conditions and reduce the risk of false convergence, leading to stable parameter estimates without arbitrary restrictions.",True,True,False,gemini-2.5-flash-lite,Trine,M,,2025-10-13T15:37:22.199453
b5b17374b22243de,A hybrid approach for portfolio construction: Combing two‐stage ensemble forecasting model with portfolio optimization,"Combining the stock prediction with portfolio optimization can improve the performance of the portfolio construction. In this article, we propose a novel portfolio construction approach by utilizing a two‐stage ensemble model to forecast stock prices and combining the forecasting results with the portfolio optimization. To be specific, there are two phases in the approach: stock prediction and portfolio optimization. The stock prediction has two stages. In the first stage, three neural networks, that is, multilayer perceptron (MLP), gated recurrent unit (GRU), and long short‐term memory (LSTM) are used to integrate the forecasting results of four individual models, that is, LSTM, GRU, deep multilayer perceptron (DMLP), and random forest (RF). In the second stage, the time‐varying weight ordinary least square model (OLS) is utilized to combine the first‐stage forecasting results to obtain the ultimate forecasting results, and then the stocks having a better potential return on investment are chosen. In the portfolio optimization, a diversified mean‐variance with forecasting model named DMVF is proposed, in which an average predictive error term is considered to obtain excess returns, and a 2‐norm cost function is introduced to diversify the portfolio. Using the historical data from the Shanghai stock exchange as the study sample, the results of the experiments indicate the DMVF model with two‐stage ensemble prediction outperforms benchmarks in terms of return and return‐risk characteristics.",,2024,10.1111/coin.12617,,proquest,"This paper proposes a hybrid approach for portfolio construction that combines a two-stage ensemble forecasting model with portfolio optimization. The forecasting model uses neural networks (MLP, GRU, LSTM) and other models (DMLP, RF) in the first stage, and time-varying weighted OLS in the second stage. The portfolio optimization uses a diversified mean-variance model (DMVF) that considers forecasting errors and a 2-norm cost function for diversification. Experiments on Shanghai stock exchange data show the proposed model outperforms benchmarks.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:37:31.935964
28c83a8bcd39edfd,A hybrid model integrating artificial neural network with multiple GARCH-type models and EWMA for performing the optimal volatility forecasting of market risk factors,"The 2008 financial crisis has highlighted the lack of precision in the market risk metrics that financial institutions must report to the regulator. The use of Machine Learning techniques in stock markets and the treasury (Front–Back Office) of financial institutions is a key tool for optimizing its own resources, internal processes and risk measures. We propose a hybrid methodology to better capture the volatility of market risk factors with Value-at-risk models in periods of stress, but also in periods of stability compared to traditional metrics. This hybrid model uses different types of artificial neural networks and traditional metrics to perform the optimal forecast of volatility applied to the main market risk nodes of the Spanish stock market. We use data from the following main market risk factors: returns on Santander Bank shares, the Spanish Stock Market Index, Euro/Dollar exchange rates and the index that measures the total return performance of a funded long credit position in the on-the-run iTraxx Crossover 5-Year-Index. Our contribution is a hybrid model that combines correct sequential pattern learnings with an improved prediction performance in the volatility of market risk factors. Our findings show that the Support Vector Machine and the Long-Short-Term Memory Model present better prediction results in all factors in the stability periods. Therefore, the proposed method is promising for application in risk management systems. © 2023 Elsevier B.V., All rights reserved.","Pérez-Hernández, F.; Arévalo-De-Pablos, A.; Camacho-Miñano, M.-D.-M.",2024,10.1016/j.eswa.2023.122896,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180444826&doi=10.1016%2Fj.eswa.2023.122896&partnerID=40&md5=35a9237d6bbd45c16c7e293c52c5f62e,scopus,"This paper proposes a hybrid model combining artificial neural networks (specifically Support Vector Machine and Long-Short-Term Memory) with traditional GARCH-type models and EWMA for improved volatility forecasting of market risk factors. The model aims to enhance Value-at-Risk calculations, particularly during market stress. Empirical results on Spanish stock market data (Santander Bank shares, IBEX 35 index, EUR/USD exchange rate, and iTraxx Crossover index) indicate that the hybrid model offers better prediction performance, especially in stable market periods.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:37:45.369018
876edb131cd8d459,A long-short dual-mode knowledge distillation framework for empirical asset pricing models in digital financial networks,"The continuous combination of digital network technology and traditional financial services has given birth to digital financial networks, which explore massive economic data under the AI-driven models to achieve intelligent connections among financial institutions, markets, transactions, and instruments. Empirical asset pricing is a challenging task in financial analysis, which has attracted research attention. However, existing studies only focus on tackling the challenges of equity risk premium in the single stock market. Considering multiple economic linkages between the two countries, the transaction history of the US stock market as empirical knowledge is a powerful supplement to improve the prediction of equity risk premium in the China market. In this paper, we aim to fully leverage the prior information in two stock markets for empirical asset pricing models. Due to the rich financial domain knowledge, there may be various characteristic signals that partially overlap in different periods. To address these issues, we propose a framework based on long-short dual-mode knowledge distillation, termed as LSDM-KD, which incorporates US and China stock market models, and a shared characteristic signals model. The method effectively understands the relationships between assets and market behaviour, reducing reliance on expensive correlation databases and professional knowledge. Extensive experiments conducted on US and China stock market datasets demonstrate that our LSDM-KD can significantly improve the performance of empirical asset pricing.",,2024,10.1080/09540091.2024.2306970,,proquest,"This paper proposes a long-short dual-mode knowledge distillation framework (LSDM-KD) to improve empirical asset pricing models by leveraging information from both the US and China stock markets. The framework aims to capture overlapping characteristic signals and understand asset-market relationships, reducing the need for external data and expertise. Experiments show LSDM-KD significantly enhances asset pricing performance.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:37:57.736783
7e93c19fba02abbf,A measure of Turkey's sovereign and banking sector credit risk: Asset swap spreads,"The existence of the credit derivatives written on the eurobonds such as credit default swaps or asset swaps allows policymakers and investors to monitor the evolvement of credit risk. However, these instruments are mostly available in advanced economies, whereas the market for credit derivatives in emerging market countries, including Turkey, is limited in terms of liquidity and maturity. In this regard, this study aims to construct a proxy for the credit risk of the Turkish Treasury and banking sector in international markets by calculating asset swap spread for US dollar-denominated fixed coupon eurobonds, which requires a robust estimation of the relevant yield curves. The study firstly presents the estimation of the sovereign and banking sector yield curves and then constructs a synthetic asset swap structure to obtain embedded credit risk premia in the eurobond curves. Our findings show that the proposed credit risk indicator is vastly correlated with credit default swap premium. In addition to this, estimated eurobond curves are also useful for monitoring borrowing cost dynamics of the Turkish Treasury and banking sector in international markets. (C) 2021 Production and hosting by Elsevier B.V. on behalf of Central Bank of The Republic of Turkey.","Kucuksarac, Doruk; Kazdal, Abdullah; Korkmaz, Halil Ibrahim; Onay, Yigit",2021,10.1016/j.cbrev.2021.05.001,,wos,This study constructs a proxy for the credit risk of the Turkish Treasury and banking sector by calculating asset swap spreads for US dollar-denominated eurobonds. It estimates sovereign and banking sector yield curves and uses a synthetic asset swap structure to derive credit risk premia. The findings indicate a strong correlation with credit default swap premiums and suggest the yield curves can monitor borrowing costs.,False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:38:55.757524
34e04b398fa30214,A new game-theoretical multi-objective evolutionary approach for cash-in-transit vehicle routing problem with time windows (A Real life Case),"Cash transfer from a central treasury to bank branches, which is with high security, is one of the crucial processes in the banking system. In this paper, a new multi-objective game theory-based model is developed to increase the security of cash-in-transit. For this purpose and in order to reduce the transportation costs, a bi-objective vehicle routing problem with time window is developed where the risk of transfers (including armed robbers attack and theft) and the distance traveled by vehicles are minimized. In order to better estimate the robber's performance, the probability of robber's ambush is calculated by the game theory approach, in such a way that a two-player, zero-sum game is played between the robber and the cash carrier. The probability of theft success is also estimated in the proposed approach through a multiple-criteria decision-making and in order to be further representative of real-life situations. A periodic review is also added to the proposed model to increase the cash transport security in which the previously used links would enjoy less chance of choosing in the current period. Moreover, a new multi-objective hybrid genetic algorithm incorporated with a number of new heuristics and operators is developed to tackle the proposed model. The efficiency and effectiveness of the algorithm are examined through several standard data sets, and the results indicate the effectiveness of the proposed solution algorithm. The wide applicability of our proposed approach in real-life situations is examined with a real case study as well. © 2020 Elsevier B.V., All rights reserved.","Ghannadpour, S.F.; Zandiyeh, F.",2020,10.1016/j.asoc.2020.106378,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084485975&doi=10.1016%2Fj.asoc.2020.106378&partnerID=40&md5=b4c39d98a905071477c2592dc16d0b26,scopus,"This paper introduces a novel game theory-based multi-objective evolutionary approach for the cash-in-transit vehicle routing problem with time windows. The model aims to minimize both the risk of attacks and transportation costs by incorporating game theory to estimate robber behavior and a multi-criteria decision-making approach for theft success probability. A hybrid genetic algorithm is used to solve the model, and its effectiveness is demonstrated through standard datasets and a real-life case study.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:39:06.369056
cb693ae22d0c21b2,A new optimisation approach to assess optimal asset allocation in European non-life insurance companies,"This study addresses the allocation of optimal assets in non-life insurance companies' environment with a new vision. Most previous studies are based on the maximisation of the utility function. However, in this paper, we focused on the maximisation of technical efficiency (TE). In order to validate our objective, we select a set of European non-life insurance companies (ENIC) over the period 2008-2014. In the first step, we estimate the production function characterised by the directional output distance function (DODF). In the second one, we use two metaheuristics (PSO and GA) to assess the optimal asset allocation (OAA). The empirical results show that the proportion allocated to the 'alternative investment with high-risk high-return' (AIhh) is on average lower than those found in previous studies. However, the percentage allocated to the 'risk-free assets' (RFA) is on average different from zero. This can be explained by the attention given to the competitiveness, survival and long-term profitability respecting the maximisation of TE. So, any insurance company must give more attention to the presence of different stakeholders and resolve the conflicts of interest between them.",,2021,10.1504/ijads.2021.116004,,proquest,"This study proposes a new approach to optimal asset allocation for European non-life insurance companies, focusing on maximizing technical efficiency rather than utility. It uses the directional output distance function and metaheuristics (PSO and GA) to assess asset allocation, finding a lower proportion of high-risk/high-return assets and a non-zero proportion of risk-free assets compared to previous studies. The authors emphasize the importance of considering stakeholder interests for competitiveness, survival, and long-term profitability.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:39:13.336401
2708d8ad18a0ab27,A non-Gaussian Ornstein-Uhlenbeck model for pricing wind power futures,"The recent introduction of wind power futures written on the German wind power production index has brought with it new interesting challenges in terms of modelling and pricing. Some particularities of this product are the strong seasonal component embedded in the underlying, the fact that the wind index is bounded from both above and below and also that the futures are settled against a synthetically generated spot index. Here, we consider the non-Gaussian Ornstein-Uhlenbeck type processes proposed by Barndorff-Nielsen and Shephard in the context of modelling the wind power production index. We discuss the properties of the model and estimation of the model parameters. Further, the model allows for an analytical formula for pricing wind power futures. We provide an empirical study, where the model is calibrated to 37 years of German wind power production index that is synthetically generated assuming a constant level of installed capacity. Also, based on 1 year of observed prices for wind power futures with different delivery periods, we study the market price of risk. Generally, we find a negative risk premium whose magnitude decreases as the length of the delivery period increases. To further demonstrate the benefits of our proposed model, we address the pricing of European options written on wind power futures, which can be achieved through Fourier techniques.",,2018,10.1080/1350486x.2018.1438904,,proquest,"This paper proposes a non-Gaussian Ornstein-Uhlenbeck model for pricing wind power futures, addressing the seasonality, boundedness, and synthetic settlement of the underlying index. The model's properties and parameter estimation are discussed, leading to an analytical pricing formula. An empirical study calibrates the model to German wind power production data and analyzes the market price of risk, finding a generally negative risk premium that decreases with delivery period length. The paper also demonstrates pricing European options on wind power futures using Fourier techniques.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:39:19.668843
c82f6df79c2745a6,A non-knotty inflation risk premium model,"In this article, I estimate the inflation risk premium (IRP) using a low-dimensional arbitrage-free dynamic model through a novel strategy. Instead of modelling the nominal and real yields jointly, I make assumptions about the short-term inflation rate. More specifically, I assume it follows a Gaussian process. This framework has a closed-form expression for IRP. Since inflation yields are not observed, to estimate the model parameters I approximate them by the break-even inflation rate. This approximation works well because the convexity correction is very small. I find that the estimated IRP is strongly correlated with those obtained using surveys or more complex models. Therefore, I provide an easier procedure to obtain IRP, avoiding the cumbersome estimation process of high-order models.",,2023,10.1080/00036846.2022.2111023,,proquest,"This article proposes a low-dimensional, arbitrage-free dynamic model to estimate the inflation risk premium (IRP). The model assumes a Gaussian process for the short-term inflation rate, leading to a closed-form expression for IRP. Inflation yields are approximated by break-even inflation rates for parameter estimation, a method validated by the small convexity correction. The estimated IRP shows strong correlation with survey-based and complex model results, offering a simpler alternative for IRP estimation.",False,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:39:57.387693
ead79b7f6088813e,A nonparametric data mining approach for risk prediction in car insurance: A case study from the Montenegrin market,"For prediction of risk in car insurance we used the nonparametric data mining techniques such as clustering, support vector regression (SVR) and kernel logistic regression (KLR). The goal of these techniques is to classify risk and predict claim size based on data, thus helping the insurer to assess the risk and calculate actual premiums. We proved that used data mining techniques can predict claim sizes and their occurrence, based on the case study data, with better accuracy than the standard methods. This represents the basis for calculation of net risk premium. Also, the article discusses advantages of data mining methods compared to standard methods for risk assessment in car insurance, as well as the specificities of the obtained results due to small insurance market, such as Montenegrin. © 2017 Elsevier B.V., All rights reserved.","Kašćelan, V.; Kašćelan, L.; Novovic Buric, M.N.",2016,10.1080/1331677x.2016.1175729,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009997041&doi=10.1080%2F1331677X.2016.1175729&partnerID=40&md5=7f3a8611352e0c53d21f363d9b290835,scopus,"This study applies nonparametric data mining techniques, including clustering, Support Vector Regression (SVR), and Kernel Logistic Regression (KLR), to predict risk and claim size in car insurance. The methods demonstrated higher accuracy than standard approaches, providing a basis for net risk premium calculation. The article also discusses the advantages of data mining over traditional methods and the impact of a small market like Montenegro on the results.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:40:17.157482
836eb9e2c451e5c5,A nonparametric estimator for the covariance function of functional data,"Many quantities of interest in economics and finance can be represented as partially observed functional data. Examples include structural business cycle estimation, implied volatility smile, the yield curve. Having embedded these quantities into continuous random curves, estimation of the covariance function is needed to extract factors, perform dimensionality reduction, and conduct inference on the factor scores. A series expansion for the covariance function is considered. Under summability restrictions on the absolute values of the coefficients in the series expansion, an estimation procedure that is resilient to overfitting is proposed. Under certain conditions, the rate of consistency for the resulting estimator achieves the minimax rate, allowing the observations to be weakly dependent. When the domain of the functional data is K(>1) dimensional, the absolute summability restriction of the coefficients avoids the so called curse of dimensionality. As an application, a Box-Pierce statistic to test independence of partially observed functional data is derived. Simulation results and an empirical investigation of the efficiency of the Eurodollar futures contracts on the Chicago Mercantile Exchange are included. © 2015 Elsevier B.V., All rights reserved.","Sancetta, A.",2014,10.1017/s0266466614000784,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949744301&doi=10.1017%2FS0266466614000784&partnerID=40&md5=235cb272bbf1152c694cf9b9858eb102,scopus,"This paper proposes a nonparametric estimator for the covariance function of functional data, applicable to economic and financial quantities like business cycles and implied volatility. The method uses a series expansion and is designed to prevent overfitting, achieving minimax consistency rates under certain conditions, even with weakly dependent data. It addresses the curse of dimensionality for multi-dimensional functional data and includes a Box-Pierce statistic for testing independence. The study is supported by simulations and an empirical analysis of Eurodollar futures contracts.",True,True,False,gemini-2.5-flash-lite,Trine,M,,2025-10-13T15:41:11.280755
c3bd093b2f2483fb,"A note on the impact of the United States federal budget deficit on the intermediate-term interest rate, 1960-94","There is an extensive literature investigating the impact of federal budget deficits in the US on interest rate yields. This literature focuses almost entirely on short-term rates (under one year to maturity) and long-term rates (ten years or more to maturity). However, almost no attention has been directed at intermediate-term interest rate yields, that is yields on bonds maturing more than one year and less than ten years in the future. Also, this literature essentially ignores the second half of the 1980s and the 1990s. Accordingly, this note empirically investigates the impact of budget deficits in the US on the interest rate yield on three-year US Treasury notes for the period 1960 through the end of 1994. Based on OLS and IV estimates, two of which include net international capital inflows, it is found that budget deficits do raise intermediate-term rates, which may strengthen arguments that budget deficits lead to crowding out. © 2017 Elsevier B.V., All rights reserved.","Cebula, R.J.",1997,10.1080/758530655,https://www.scopus.com/inward/record.uri?eid=2-s2.0-5844313200&doi=10.1080%2F758530655&partnerID=40&md5=c8a1fd58caaa68d76aef4209a45c7f92,scopus,"This note empirically investigates the impact of US federal budget deficits on intermediate-term interest rates (specifically, three-year US Treasury notes) from 1960 to 1994. Using OLS and IV estimates, the study finds that budget deficits tend to increase intermediate-term rates, potentially supporting the crowding-out hypothesis. The research highlights a gap in existing literature that primarily focused on short-term and long-term rates, and largely ignored the latter half of the 1980s and the 1990s.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:41:37.687646
904ecc8c3e03282b,A novel approach to Predict WTI crude spot oil price: LSTM-based feature extraction with Xgboost Regressor,"This paper presents a novel model based on LSTM to predict future prices of WTI crude oil. The WTI price forecasting utilizes data on spot gold price, US 10-year bond yield, global economic activity, and US dollar index from January 1986 to May 2023. The model's performance is assessed using measures such as MAE, MSE, RMSE, MAPE, and R2 metrics. The results generated by the proposed new model are compared to those of the existing machine and deep learning methods, and it is observed that the new model performs better than the existing models in all statistical tests. The study further examined the decision-making processes of the model using SHAP analysis and assessed the individual contribution of each feature to the model's predictions. The correlation between the US Dollar Index and Gold prices and WTI crude oil prices is evident. The SHAP research has demonstrated that the model effectively captures complicated economic linkages and enhances the accuracy of forecasts. The results of this study enhance the development of models that are capable of predicting results, even in times of significant instability, such as economic crises. Using sophisticated data analytics and AI methods would improve the efficiency of energy market oversight. © 2024 Elsevier B.V., All rights reserved.","Simsek, A.I.; Bulut, E.; Gur, Y.E.; Gültekin Tarla, E.",2024,10.1016/j.energy.2024.133102,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203530978&doi=10.1016%2Fj.energy.2024.133102&partnerID=40&md5=d9cf0148dabf51a718ea46ae4789d646,scopus,"This paper proposes a novel LSTM-based model for predicting WTI crude oil spot prices, incorporating features like gold price, US 10-year bond yield, global economic activity, and the US dollar index. The model outperforms existing methods and uses SHAP analysis to explain its predictions, highlighting the correlation between economic indicators and oil prices. The study aims to improve forecasting accuracy, especially during market instability.",True,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:42:54.366530
d63fadf05abeb819,A novel decomposition integration model for power coal price forecasting,"Accurate prediction of steam coal prices is important for stabilizing the coal trading market and formulating coal use strategies scientifically. In this paper, a new decomposition integration model (VADM) is proposed to predict coal prices by combining the variational modal decomposition (VMD), arithmetic optimization algorithm (AOA), deep temporal convolutional network (DeepTCN), and mean impact value algorithm (MIV). Firstly, the AOA optimization algorithm is used to improve the VMD, AOA-VMD was obtained. It is used to decompose the steam coal price series. Then, the decomposed subsequences are predicted for the prediction of steam coal prices by using DeepTCN. Finally, the MIV algorithm is applied to analyze the impact of different factors on the price of steam coal. It is found that: the steam coal price sub-series decomposed by AOA-VMD are smoother and more linear compared with the original series; the errors in forecasting steam coal prices are significantly reduced after considering newly proposed factors, interest rates, such as the overnight Shanghai interbank offered rate and the six-month treasury bond yield; the MAPE, MASE and SMAPE of the VADM model all show different degrees of decline compared with benchmark models. The forecasting effect of VADM model is better than the benchmark model in terms of stability and accuracy, and can be used for short-term forecasting of coal prices. © 2023 Elsevier B.V., All rights reserved.","Wu, S.; Xia, G.; Liu, L.",2023,10.1016/j.resourpol.2022.103259,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145655492&doi=10.1016%2Fj.resourpol.2022.103259&partnerID=40&md5=227fae8fd06a4f24cab6e80ac1324e5c,scopus,"This paper proposes a novel decomposition integration model (VADM) for power coal price forecasting, combining variational modal decomposition (VMD), arithmetic optimization algorithm (AOA), deep temporal convolutional network (DeepTCN), and mean impact value algorithm (MIV). The model decomposes coal price series using AOA-VMD, predicts subsequences with DeepTCN, and analyzes factor impacts with MIV. The VADM model demonstrates improved accuracy and stability in short-term coal price forecasting compared to benchmark models, incorporating factors like interest rates.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:43:01.121072
99ce25282168bdae,A novel term structure stochastic model with adaptive correlation for trend analysis,"The prediction of underlying price continues to draw extensive attention in academic research. Based on a review of the advantages and disadvantages of different volatility models, we find that the Heston stochastic volatility model with an adaptive correlation coefficient is most suitable for analysing the Hong Kong options market. We subsequently propose a model-free implied volatility term structure formulated using options with different strikes and different maturities. The implied volatility is calculated by integrating the option price and strike price from the current time to the expiry date. Discrete points of term structure data are used to fit a term structure curve. Finally, we use the model-free implied volatility term structure as the long-run mean level of the Heston model to fully exploit the information content contained in the implied volatility term structure. We simulate the distribution of the underlying asset price based on the Heston model and constant elasticity of variance (CEV) model. The adaptive correlation Heston model provides superior results in terms of one-day-ahead prediction performance and the 79-day distribution of the underlying asset price compared with the CEV model. © 2021 Elsevier B.V., All rights reserved.","Du, J.; Lai, S.; Lai, K.K.; Zhou, S.",2021,10.1002/ijfe.2076,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089457308&doi=10.1002%2Fijfe.2076&partnerID=40&md5=0bd330a48e1a5066903ad5d67db64590,scopus,"This paper proposes a novel term structure stochastic model incorporating adaptive correlation, building upon the Heston model and a model-free implied volatility term structure. The model is applied to analyze the Hong Kong options market and demonstrates superior prediction performance compared to the Constant Elasticity of Variance (CEV) model.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:43:25.291216
08245f9f6de44032,"A novel, rule-based technical pattern identification mechanism: Identifying and evaluating saucers and resistant levels in the US stock market","This paper has two main purposes. The first one is the development of a rigorous rule-based mechanism for identifying the rounding bottoms (also known as saucers) pattern and resistant levels. The design of this model is based solely on principles of technical analysis, and thus making it a proper system for evaluating the efficacy of the aforementioned technical trading patterns. The second aim of this paper is measuring the predictive power of buy-signals generated by these technical patterns. Empirical results obtained from seven US tech stocks indicate that simple resistant levels outperform saucers patterns. Furthermore, positive statistical significant excess returns are being generated only in first sub-periods of examination. These returns decline or even vanish as the experiment proceeds to recent years. Our findings are aligned with the results reported by various former studies. The proposed identification mechanism can be used as a component of an expert system to assist academic community in evaluating trading strategies where technical patterns are embedded. (C) 2011 Elsevier Ltd. All rights reserved.","Zapranis, Achilleas; Tsinaslanidis, Prodromos E.",2012,10.1016/j.eswa.2011.11.079,,wos,"This paper develops a rule-based mechanism to identify and evaluate 'saucers' and 'resistant levels' in the US stock market, based on technical analysis principles. It then measures the predictive power of buy-signals from these patterns. Empirical results on seven US tech stocks show that resistant levels outperform saucers, and significant excess returns are observed only in earlier periods, diminishing over time. The authors suggest their mechanism can aid in evaluating trading strategies involving technical patterns.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:43:35.003306
5e7b8c965b7c7487,A penalized two-pass regression to predict stock returns with time-varying risk premia,"We develop a penalized two-pass regression with time-varying factor loadings. The penalization in the first pass enforces sparsity for the time-variation drivers while also maintaining compatibility with the no-arbitrage restrictions by regularizing appropriate groups of coefficients. The second pass delivers risk premia estimates to predict equity excess returns. Our Monte Carlo results and our empirical results on a large cross-sectional data set of US individual stocks show that penalization without grouping can yield to nearly all estimated time-varying models violating the no-arbitrage restrictions. Moreover, our results demonstrate that the proposed method reduces the prediction errors compared to a penalized approach without appropriate grouping or a time-invariant factor model. © 2023 Elsevier B.V., All rights reserved.","Bakalli, G.; Guerrier, S.; Scaillet, O.",2023,10.1016/j.jeconom.2022.12.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146471323&doi=10.1016%2Fj.jeconom.2022.12.004&partnerID=40&md5=3da444594d98ac1c1f1f5ec4e044ee7b,scopus,This paper proposes a penalized two-pass regression method with time-varying factor loadings to predict stock returns. The method uses penalization to enforce sparsity in time-variation drivers and maintain no-arbitrage restrictions. Empirical results show that this approach reduces prediction errors compared to other methods.,True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:43:50.869849
5c030899a8ca9d9a,A prediction model for the secure issuance scale of Chinese local government bonds,"PurposeThe purpose of this paper is to calculate the local guaranteed fiscal revenue with the local fiscal revenue of 31 provinces, and predict their guaranteed fiscal revenue in 2018 with the artificial neural network (ANN).Design/methodology/approachThe principal components analysis (PCA), particle swarm optimization (PSO) and extreme learning machine (ELM) model was designed to produce the inputs of KMV model. Then the KMV model was used for obtaining the default probabilities under different issuance scales. Data were collected from Wind Database. MATLAB 2018b and SPSS 22 were used in the field of modeling and results analysis.FindingsThis study’s findings show that PCA–PSO–ELM proposed in this research has the highest accuracy in terms of the prediction compared with ELM, back propagation neural network and auto regression. And PCA–PSO–ELM–KMV model can calculate the secure issuance scale of local government bonds effectively.Practical implicationsThe sustainability forecast in this study can help local governments effectively control the scale of debt issuance, strengthen the budget management of local debt and establish the corresponding risk warning mechanism, which could make local governments maintain good credit ratings.Originality/valueThis study sheds new light on helping local governments avoid financial risks effectively, and it is conducive to establish a debt repayment reserve system for local governments and the proper arrangement for stock debt.",,2021,10.1108/k-10-2019-0699,,proquest,"This study develops a prediction model using PCA-PSO-ELM-KMV to determine the secure issuance scale of Chinese local government bonds. The model forecasts guaranteed fiscal revenue and default probabilities, offering insights for debt management and risk mitigation.",True,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T15:45:01.440699
5c800f91626cad4f,A production function analysis of commercial dairy farms in the Highlands of Eritrea using ridge regression,"This study presents a production function analysis of fresh milk production in the Highlands of Eritrea, where most dairy farmers in Eritrea are located. To ensure representative production functions, this region was divided into three relatively homogenous study areas, namely Central Zone, Mendefera and Dekemhare. Most data for the study were collected in a survey of 120 respondents using a structured questionnaire. To obviate the problem of multicollinearity among explanatory variables, ridge regression was used to estimate milk production functions for each study area. Production elasticities of variable inputs, marginal products (MPx), values of marginal products (VMPx), marginal rates of input substitution (MRS) and leastcost combinations of purchased concentrates and forage were estimated for the three regions. The VMPs of all inputs for Central Zone dairy farmer respondents were estimated to be greater than their input prices, implying that the resources were under-utilized from a profit-maximising perspective (i.e. where VMPx = Px). However, respondents in Mendefera and Dekemhare used concentrates in excess of optimum levels (i.e. VMPx < Px). Analysis of the least-cost combination of purchased concentrates and forage suggests that dairy farmer respondents were also not allocating these resources on a minimum-cost basis. However, the profit maximizing and least-cost criteria assume perfect knowledge, a risk-free environment and competitive markets. Improved information, farmer training and better infrastructure (roads and telecommunications) to promote competitive markets could help to enhance resource allocation decisions by dairy producers. © 2013 Elsevier B.V., All rights reserved.","Ghebremariam, W.K.; Ortmann, G.F.; Nsahlai, I.V.",2006,10.1080/03031853.2006.9523745,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887173439&doi=10.1080%2F03031853.2006.9523745&partnerID=40&md5=47b75f133719f2c7f00f4ae602b92772,scopus,"This study analyzes milk production on commercial dairy farms in Eritrea's Highlands using ridge regression to address multicollinearity. It estimates production elasticities, marginal products, and least-cost input combinations for three regions. Findings indicate resource under-utilization in the Central Zone and over-utilization of concentrates in Mendefera and Dekemhare, suggesting suboptimal resource allocation. The study highlights the need for improved information, training, and infrastructure to enhance decision-making.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:45:11.706072
9cd458c37bf8d94f,A quasi-maximum likelihood method for estimating the parameters of multivariate diffusions,"A quasi-maximum likelihood procedure for estimating the parameters of multi-dimensional diffusions is developed in which the transitional density is a multivariate Gaussian density with first and second moments approximating the true moments of the unknown density. For affine drift and diffusion functions, the moments are exactly those of the true transitional density and for nonlinear drift and diffusion functions the approximation is extremely good and is as effective as alternative methods based on likelihood approximations. The estimation procedure generalises to models with latent factors. A conditioning procedure is developed that allows parameter estimation in the absence of proxies. (C) 2012 Elsevier B.V. All rights reserved.","Hurn, A. S.; Lindsay, K. A.; McClelland, A. J.",2013,10.1016/j.jeconom.2012.09.002,,wos,"This paper proposes a quasi-maximum likelihood method for estimating parameters of multivariate diffusions, approximating the transitional density with a multivariate Gaussian. This method is exact for affine drift/diffusion functions and provides a good approximation for nonlinear cases, generalizing to models with latent factors and incorporating a conditioning procedure for parameter estimation without proxies.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:45:13.140528
6eebbf4ce06ab357,A static replication approach for callable interest rate derivatives: mathematical foundations and efficient estimation of SIMM–MVA,"The computation of credit risk measures such as exposure and Credit Value Adjustments (CVA) requires the simulation of future portfolio prices. Recent metrics, such as dynamic Initial Margin (IM) and Margin Value Adjustments (MVA) additionally require the simulation of future conditional sensitivities. For portfolios with non-linear instruments that do not admit closed-form valuation formulas, this poses a significant computational challenge. This problem is addressed by proposing a static replication algorithm for interest rate options with early-exercise features under an affine term-structure model. Under the appropriate conditions, we can find an equivalent portfolio of vanilla options that replicate these products. Specifically, we decompose the product into a portfolio of European swaptions. The weights and strikes of the portfolio are obtained by regressing the target option value with interpretable, feed-forward neural networks. Once an equivalent portfolio of European swaptions is determined, we can leverage on closed-form expressions to obtain the conditional prices and sensitivities, which serve as an input to exposure and SIMM-driven MVA quantification. For a consistent forward sensitivity estimation, this involves the differentiation of the portfolio-weights. The accuracy and convergence of the method is demonstrated through several representative numerical examples, benchmarked against the established least-square Monte Carlo method. © 2024 Elsevier B.V., All rights reserved.","Hoencamp, J.H.; Jain, S.; Kandhai, B.D.",2024,10.1080/14697688.2024.2312523,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185970359&doi=10.1080%2F14697688.2024.2312523&partnerID=40&md5=e658f86122a239c289b291ec3fe810ae,scopus,"This paper proposes a static replication method for valuing callable interest rate derivatives and estimating SIMM-MVA. It uses neural networks to replicate complex options with a portfolio of vanilla options, enabling efficient calculation of future prices and sensitivities, and is benchmarked against the least-square Monte Carlo method.",True,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T15:46:02.222584
232cb3a8af099a6f,A study of cross-industry return predictability in the Chinese stock market,"We investigate cross-industry return predictability for the Shanghai and Shenzhen stock exchanges, by constructing 6- and 26- industry portfolios. The dominance of retail investors in these markets, in conjunction with the gradual diffusion of information hypothesis provide the theoretical background that allows us to employ machine learning methods to test for cross-industry predictability. We find that Oil, Telecommunications and Finance industry portfolio returns are significant predictors of other industries. Our out-of-sample forecasting exercise shows that the OLS post-LASSO estimation outperforms a variety of benchmarks and a long–short trading strategy generates an average annual excess return of 13%. © 2022 Elsevier B.V., All rights reserved.","Ellington, M.; Stamatogiannis, M.P.; Zheng, Y.",2022,10.1016/j.irfa.2022.102249,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133314818&doi=10.1016%2Fj.irfa.2022.102249&partnerID=40&md5=f14048287a71b77d7730f86dffa447ab,scopus,"This study examines cross-industry return predictability in the Chinese stock market using machine learning on 6 and 26 industry portfolios from the Shanghai and Shenzhen stock exchanges. It finds that Oil, Telecommunications, and Finance industries predict returns in other sectors. An out-of-sample test shows OLS post-LASSO estimation is superior to benchmarks, and a long-short strategy yields a 13% annual excess return.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:46:21.275476
0c12365026f9ffcd,A tale of two coffees? Analysing interaction and futures market efficiency,"PurposeThe purpose of this paper is to assess the informational efficiency of Arabica (other milds) and Robusta coffee futures markets in terms of predicting future coffee spot prices.Design/methodology/approachFutures market efficiency is associated with the existence of a long-run equilibrium relationship between spot and future prices such that coffee futures prices are unbiased predictors of future spot prices. This study applies unit root testing to daily data for futures-spot price differentials. A range of maturities for futures contracts are considered, and the study also uses a recursive approach to consider time variation in futures market efficiency.FindingsThe other milds and Robusta futures prices tend to be unbiased predictors for their own respective spot prices. The paper further finds that other milds and Robusta futures prices are unbiased predictors of the respective Robusta and other milds spot prices. Recursive estimation suggests that the futures market efficiency associated with these cross cases has increased, though with no clear link to the implementation of the 2007 International Coffee Agreement.Originality/valueThe paper draws new insights into futures market efficiency by examining the two key types of coffee and analyses the potential interactions between them. Hitherto, no attention has been paid to futures contracts of the Robusta variety. The employment of unit root testing of spot futures coffee price differentials can be viewed as more stringent than an approach based on non-cointegration testing.",,2020,10.1108/sef-09-2019-0356,,proquest,"This paper assesses the informational efficiency of Arabica and Robusta coffee futures markets in predicting future coffee spot prices using unit root testing on futures-spot price differentials. It finds that both Arabica and Robusta futures prices are unbiased predictors of their respective spot prices, and also predict each other's spot prices. Recursive estimation indicates an increase in market efficiency over time, though not clearly linked to the 2007 International Coffee Agreement. The study offers new insights by examining both coffee types and their interactions, with a particular focus on Robusta futures.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:46:31.442322
8fb4c155f69de669,A transformer-based model for default prediction in mid-cap corporate markets,"In this paper, we study mid-cap companies, i.e. publicly traded companies with less than US$10 billion in market capitalisation. Using a large dataset of US mid-cap companies observed over 30 years, we look to predict the default probability term structure over the short to medium term and understand which data sources (i.e. fundamental, market or pricing data) contribute most to the default risk. Whereas existing methods typically require that data from different time periods are first aggregated and turned into cross-sectional features, we frame the problem as a multi-label panel data classification problem. To tackle it, we then employ transformer models, a state-of-the-art deep learning model emanating from the natural language processing domain. To make this approach suitable to the given credit risk setting, we use a loss function for multi-label classification, to deal with the term structure, and propose a multi-channel architecture with differential training that allows the model to use all input data efficiently. Our results show that the proposed deep learning architecture produces superior performance, resulting in a sizeable improvement in AUC (Area Under the receiver operating characteristic Curve) over traditional models. In order to interpret the model, we also demonstrate how to produce an importance ranking for the different data sources and their temporal relationships, using a Shapley approach for feature groups. © 2023 Elsevier B.V., All rights reserved.","Korangi, K.; Mues, C.; Bravo, C.",2023,10.1016/j.ejor.2022.10.032,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143550150&doi=10.1016%2Fj.ejor.2022.10.032&partnerID=40&md5=629f9a4355496d502e429e10a7d84773,scopus,"This paper introduces a transformer-based deep learning model for predicting default probabilities in mid-cap corporate markets. It utilizes a large dataset of US mid-cap companies over 30 years, framing the problem as a multi-label panel data classification task. The model demonstrates superior performance compared to traditional methods and includes an analysis of feature importance using a Shapley approach.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:46:37.991476
1d0805954066098a,ABS inflows to the United States and the global financial crisis,"Relative to the 'global savings glut' (GSG) hypothesis, we present a more complete picture of how capital flows contributed to the financial crisis, drawing attention to the sizable inflows from European investors into U.S. private-label asset-backed securities (ABS), including mortgage-backed securities and other structured investment products. The GSG hypothesis argues that the surge in capital inflows from emerging market economies to the United States led to significant declines in long-term interest rates in the United States and other industrial economies. In turn, these lower interest rates, when combined with both innovations and deficiencies of the U.S. credit market, are believed to have contributed to the U.S. housing bubble and to the buildup in financial vulnerabilities that led to the financial crisis. Because the GSG countries for the most part restricted their U.S. purchases to Treasuries and Agency debt, their provision of savings to ultimately risky subprime mortgage borrowers was necessarily indirect, pushing down yields on safe assets and increasing the appetite for alternative investments on the part of other investors. Foreign acquisitions of private-label ABS, primarily by Europeans, provided credit more directly and, by adding to domestic demand for these securities, contributed to the decline in their spreads over Treasury yields. Through a combination of empirical estimation and model simulation, we verify that both GSG inflows into Treasuries and Agencies, as well as European acquisitions of ABS, played a role in contributing to downward pressures on U.S. interest rates. All rights reserved, Elsevier",,2012,10.1016/j.jinteco.2012.04.001,,proquest,"This paper examines the role of capital inflows, specifically from European investors into U.S. asset-backed securities (ABS), in contributing to the global financial crisis. It contrasts this with the 'global savings glut' (GSG) hypothesis, which focuses on inflows from emerging market economies into U.S. Treasuries and Agency debt. The authors argue that European ABS inflows provided credit more directly and influenced interest rates and spreads, playing a significant role alongside GSG inflows in pressuring U.S. interest rates downwards.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:46:40.042095
09e7ecec6845d7fd,ADAPTIVE LASSO-TYPE ESTIMATION FOR MULTIVARIATE DIFFUSION PROCESSES,"The least absolute shrinkage and selection operator (LASSO) is a widely used statistical methodology for simultaneous estimation and variable selection. It is a shrinkage estimation method that allows one to select parsimonious models. In other words, this method estimates the redundant parameters as zero in the large samples and reduces variance of estimates. In recent years, many authors analyzed this technique from a theoretical and applied point of view. We introduce and study the adaptive LASSO problem for discretely observed multivariate diffusion processes. We prove oracle properties and also derive the asymptotic distribution of the LASSO estimator. This is a nontrivial extension of previous results by Wang and Leng (2007, Journal of the American Statistical Association, 102(479), 1039-1048) on LASSO estimation because of different rates of convergence of the estimators in the drift and diffusion coefficients. We perform simulations and real data analysis to provide some evidence on the applicability of this method.","De Gregorio, Alessandro; Iacus, Stefano M.",2012,10.1017/s0266466611000806,,wos,This paper introduces and analyzes an adaptive LASSO estimation method for discretely observed multivariate diffusion processes. It extends previous LASSO work by addressing different convergence rates for drift and diffusion coefficients and demonstrates the method's applicability through simulations and real data analysis.,True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:46:50.906364
af7ab673b17e0c3d,AI-Driven Dental Caries Management Strategies: From Clinical Practice to Professional Education and Public Self Care,"Dental caries is one of the most prevalent chronic diseases among both children and adults, despite being largely preventable. This condition has significant negative impacts on human health and imposes a substantial economic burden. In recent years, scientists and dentists have increasingly started to utilize artificial intelligence (AI), particularly machine learning, to improve the efficiency of dental caries management. This study aims to provide an overview of the current knowledge about the AI-enabled approaches for dental caries management within the framework of personalized patient care. Generally, AI works as a promising tool that can be used by both dental professionals and patients. For dental professionals, it predicts the risk of dental caries by analyzing dental caries risk and protective factors, enabling to formulate personalized preventive measures. AI, especially those based on machine learning and deep learning, can also analyze images to detect signs of dental caries, assist in developing treatment plans, and help to make a risk assessment for pulp exposure during treatment. AI-powered tools can also be used to train dental students through simulations and virtual case studies, allowing them to practice and refine their clinical skills in a risk-free environment. Additionally, AI tracks brushing patterns and provides feedback to improve oral hygiene practices of the patients and the general population, thereby improving their understanding and compliance. This capability of AI can inform future research and the development of new strategies for dental caries management and control.Dental caries is one of the most prevalent chronic diseases among both children and adults, despite being largely preventable. This condition has significant negative impacts on human health and imposes a substantial economic burden. In recent years, scientists and dentists have increasingly started to utilize artificial intelligence (AI), particularly machine learning, to improve the efficiency of dental caries management. This study aims to provide an overview of the current knowledge about the AI-enabled approaches for dental caries management within the framework of personalized patient care. Generally, AI works as a promising tool that can be used by both dental professionals and patients. For dental professionals, it predicts the risk of dental caries by analyzing dental caries risk and protective factors, enabling to formulate personalized preventive measures. AI, especially those based on machine learning and deep learning, can also analyze images to detect signs of dental caries, assist in developing treatment plans, and help to make a risk assessment for pulp exposure during treatment. AI-powered tools can also be used to train dental students through simulations and virtual case studies, allowing them to practice and refine their clinical skills in a risk-free environment. Additionally, AI tracks brushing patterns and provides feedback to improve oral hygiene practices of the patients and the general population, thereby improving their understanding and compliance. This capability of AI can inform future research and the development of new strategies for dental caries management and control.",,2025,10.1016/j.identj.2025.04.007,,proquest,"This study reviews the application of AI, particularly machine learning and deep learning, in dental caries management. AI tools are discussed for risk prediction, image analysis for detection and treatment planning, and training dental students. Furthermore, AI can monitor and provide feedback on patient oral hygiene practices, contributing to improved self-care and informing future research.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:47:08.847416
efd37e8cd45dfd0b,ASYMMETRICAL EFFECTS OF REAL EFFECTIVE EXCHANGE RATE OF KUNA ON FOREIGN TRADE BALANCE IN REPUBLIC OF CROATIA,"This paper strives on giving an answer to whether assymetrical effects of real effective ex-change rate of kuna, J-curve and Marshall-Lerner condition exist in the Republic of Croatia. Price rigidity, capacity constraints, and adjustment costs are just some reasons why depreciation and appreciation of the real effective exchange rate do not necessarily have equally strong effect of the opposite direction on net exports. Econometric estimation by nonlinear autoregressive distributed lags method (NARDL) enables separation of positive (appreciation) from negative (depreciation) changes in the real effective exchange rate, showing that its changes do not affect net exports sym-metrically and linearly. Existence of J-curve and positive impact of the real effective exchange rate depreciation on trade balance (Marshall-Lerner condition) are confirmed, while appreciation has no significant effect. Results obtained in this paper point out that Croatia `s foreign trade bal-ance can be improved by real effective exchange rate depreciation. However, this should be taken with caution given that nominal depreciation leads to growing external indebtedness of domestic economy, affects expectations, especially those related to inflation, import prices (therefore domes-tic prices), risk premium, returns on financial assets, and there are indications that depreciation of nominal and real exchange rate in Croatia have a contractional effect on domestic economic activity, so costs of such a policy may far outweigh the benefits of currency weakening. In addition, research limitations include low variability of the real effective exchange rate (which might make it difficult to obtain econometric relevant estimates), the problem of choosing representative exchange rate measures, and the choice of proxy variables for domestic and foreign income. In the observed period, appreciation and depreciation of the real effective exchange rate were relatively mild, so the conclusions are not necessarily valid in the case of a sudden and large fluctuations in it. Compared to previous research, asymmetric effect of the real effective exchange rate is confirmed on the bal-ance of goods and services (not only balance of goods), and possible causes of nonlinearity in the response of foreign trade balance to changes in the real effective exchange rate are discussed.","Novinc, Filip",2023,10.32910/ep.74.4.4,,wos,"This paper investigates the asymmetrical effects of the real effective exchange rate (REER) of the Croatian Kuna on the foreign trade balance. Using the nonlinear autoregressive distributed lags (NARDL) method, the study confirms the existence of the J-curve and the Marshall-Lerner condition, indicating that depreciation of the REER positively impacts the trade balance, while appreciation has no significant effect. The research highlights potential benefits of REER depreciation for improving the trade balance but cautions about its negative consequences on external debt, inflation, and economic activity. Limitations include low REER variability and challenges in selecting appropriate exchange rate measures and proxy variables. The study extends previous research by confirming asymmetric effects on the balance of goods and services and discussing potential causes of nonlinearity.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:47:19.674332
d0e42311667ba639,Adaptive investment strategies for periodic environments,"In this paper, an adaptive investment strategy for environments with periodic returns on investment is presented. In this approach, an investment model is considered where the agent decides at every time step the proportion of wealth to invest in a risky asset, keeping the rest of the budget in a risk-free asset. Every investment is evaluated in the market via stylized return on investment function (RoI), which is modeled by a stochastic process with unknown periodicities and levels of noise. For comparison, two reference strategies are presented which represent the case of agents with zero knowledge and complete knowledge of the dynamics of the returns. An investment strategy based on technical analysis to forecast the next return is also considered. To account for the performance of the different strategies, some computer experiments are performed to calculate the average budget that can be obtained with them over a certain number of time steps. To assure fair comparisons, the parameters of each strategy are first tuned for budget maximization. Afterward, the performance of these strategies is compared for RoI's with different periodicities and levels of noise. © 2008 World Scientific Publishing Company. © 2017 Elsevier B.V., All rights reserved.","Navarro-Barrientos, J.-E.",2008,10.1142/s0219525908001933,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57249084067&doi=10.1142%2FS0219525908001933&partnerID=40&md5=b1124d852118bf010bb60d870df16faa,scopus,"This paper presents an adaptive investment strategy for periodic return environments, where an agent allocates wealth between a risky and a risk-free asset. The return on investment is modeled as a stochastic process with unknown periodicities and noise. The proposed strategy is compared against reference strategies with zero and complete knowledge, as well as a technical analysis-based strategy. Performance is evaluated through computer experiments, with strategy parameters tuned for budget maximization before comparing performance across different periodicities and noise levels.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:47:36.058003
8527a7f34521c485,Adaptive online portfolio selection with transaction costs,"As an application of machine learning techniques in financial fields, online portfolio selection has been attracting great attention from practitioners and researchers, which makes timely sequential decision making available when market information is constantly updated. For online portfolio selection, transaction costs incurred by changes of investment proportions on risky assets have a significant impact on the investment strategy and the return in long-term investment horizon. However, in many online portfolio selection studies, transaction costs are usually neglected in the decision making process. In this paper, we consider an adaptive online portfolio selection problem with transaction costs. We first propose an adaptive online moving average method (AOLMA) to predict the future returns of risky assets by incorporating an adaptive decaying factor into the moving average method, which improves the accuracy of return prediction. The net profit maximization model (NPM) is then constructed where transaction costs are considered in each decision making process. The adaptive online net profit maximization algorithm (AOLNPM) is designed to maximize the cumulative return by integrating AOLMA and NPM together. Numerical experiments show that AOLNPM dominates several state-of-the-art online portfolio selection algorithms in terms of various performance metrics, i.e., cumulative return, mean excess return, Sharpe ratio, Information ratio and Calmar ratio. © 2021 Elsevier B.V., All rights reserved.","Guo, S.; Gu, J.-W.; Ching, W.-K.",2021,10.1016/j.ejor.2021.03.023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103966971&doi=10.1016%2Fj.ejor.2021.03.023&partnerID=40&md5=09d67e9398dc3fb20f5fd921477927f0,scopus,This paper proposes an adaptive online portfolio selection method (AOLNPM) that incorporates transaction costs and uses an adaptive moving average method (AOLMA) for return prediction. The method aims to maximize net profit and has shown superior performance compared to existing algorithms in numerical experiments.,True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:54:02.857282
302c792dbf247468,Algorithmic sign prediction and covariate selection across eleven international stock markets,"I investigate whether an expert system can be used for profitable long-term asset management. The trading strategy of the expert system needs to be based on market predictions. To this end, I generate binary predictions of the market returns by using statistical and machine-learning algorithms. The methods used include logistic regressions, regularized logistic regressions and similarity-based classification. I test the methods in a contemporary data set involving data from eleven developed markets. Both statistical and economic significance of the results are considered. As an ensemble, the results seem to indicate that there is some degree of mild predictability in the stock markets. Some of the results obtained are highly significant in the economic sense, featuring annualized excess returns of 3.1% (France), 2.9% (Netherlands) and 0.8% (United States). However, statistically significant results are seldom found. Consequently, the results do not completely invalidate the efficient-market hypothesis. © 2018 Elsevier B.V., All rights reserved.","Karhunen, M.",2019,10.1016/j.eswa.2018.07.061,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051398572&doi=10.1016%2Fj.eswa.2018.07.061&partnerID=40&md5=553aa1a74c3267f31622ea2e5a12741a,scopus,"This study explores the use of statistical and machine-learning algorithms (logistic regressions, regularized logistic regressions, similarity-based classification) to predict stock market returns across eleven international markets. The goal is to assess if an expert system can achieve profitable long-term asset management through these market predictions. While some economic significance was observed (e.g., annualized excess returns in France, Netherlands, and the US), statistically significant results were rare, leading to the conclusion that the efficient-market hypothesis is not entirely invalidated.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:54:10.410491
3dfd04e4fb4d9063,Alternative Maximum Likelihood Estimation of Structural Vector Autoregressive Models Partially Identified with Short-Run Restrictions,"This paper presents an alternative maximum likelihood estimation method for partially identified vector autoregressive models. This method might be especially useful to handle very large systems of variables by reducing the dimension of the likelihood space. As an application, we consider an open economy model to investigate the effects of monetary policy on exchange rates and term structures. We find that exchange rates tend to overshoot and term structures have hump-shaped responses to monetary policy shocks. © 2013 The Ohio State University. © 2013 Elsevier B.V., All rights reserved.","Jang, K.",2013,10.1111/jmcb.12010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875290853&doi=10.1111%2Fjmcb.12010&partnerID=40&md5=21a61ad4420d752980f482e339f0aa47,scopus,"This paper proposes an alternative maximum likelihood estimation method for partially identified structural vector autoregressive models, particularly beneficial for large systems. An application to an open economy model demonstrates its use in analyzing monetary policy effects on exchange rates and term structures, revealing overshooting exchange rates and hump-shaped term structure responses.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:54:18.876584
5be924db9dfca63e,An Evaluation of Machine Learning Models for Forecasting Short-Term U.S. Treasury Yields,"This study explores the historical evolution and short-term predictive modeling of the U.S. 10-year Treasury bond yield, a critical indicator in global financial markets. Recognizing its sensitivity to macroeconomic conditions, the research integrates economic variables, including the federal funds rate, core Consumer Price Index (CPI), real Gross Domestic Product (GDP) growth rate, and the U.S. federal debt growth rate, to assess their influence on yield movements. Four forecasting models are employed for comparative analysis: linear regression (LR), decision tree (DT), random forest (RF), and multilayer perceptron (MLP) neural networks. Using historical data from the Federal Reserve Economic Data (FRED), this study finds that the RF model offers the most accurate short-term predictions, achieving the lowest mean squared error (MSE) and mean absolute error (MAE), with an R2 value of 0.5760. The results highlight the superiority of ensemble-based nonlinear models in capturing complex interactions between economic indicators and yield dynamics. This research not only provides empirical support for using machine learning in economic forecasting but also offers practical implications for bond traders, system developers, and financial institutions aiming to enhance predictive accuracy and risk management.",,2025,10.3390/app15126903,,proquest,"This study evaluates four machine learning models (linear regression, decision tree, random forest, and multilayer perceptron) for forecasting the U.S. 10-year Treasury bond yield, incorporating economic variables. The random forest model demonstrated the highest accuracy, suggesting the effectiveness of ensemble-based nonlinear models for predicting yield movements and offering practical insights for financial markets.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-13T15:54:31.993345
08de1ecc4154ae9b,An analysis of nonlinearities in term premiums and forward rates,"Previous studies often assume a linear relation between term premiums on Treasury securities and forward interest rates even though a nonlinear relation is a theoretical and an empirical possibility. To examine the relation, this paper uses a nonparametric kernel approach that permits both linear and nonlinear associations. The linear specification yields conditional expectations of term premiums that are similar to those predicted by the kernel approach only at the mean forward premiums. Generally, kernel estimation shows that the responses of expected term premiums to changes in forward premiums are time-varying and are significantly different from the constant slope coefficients produced by linear estimations. The evidence also shows that forward premiums contain much more information content for predicting future term premiums than has been found with linear estimation procedures. © 2005 Elsevier Science B.V., Amsterdam. All rights reserved.","Huang, R.D.; Lin, C.S.Y.",1996,10.1016/s0927-5398(96)00008-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030527175&doi=10.1016%2FS0927-5398%2896%2900008-4&partnerID=40&md5=2762a85a3dc870c284cd425c9e590525,scopus,"This paper uses a nonparametric kernel approach to analyze nonlinear relationships between term premiums on Treasury securities and forward interest rates. The findings indicate that nonlinearities are significant, time-varying, and that forward premiums contain more predictive information for future term premiums than previously found with linear models.",True,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T15:54:56.678093
342960dee21cfd44,An application of comonotonicity theory in a stochastic life annuity framework,"A life annuity contract is an insurance instrument which pays pre-scheduled living benefits conditional on the survival of the annuitant. In order to manage the risk borne by annuity providers, one needs to take into account all sources of uncertainty that affect the value of future obligations under the contract. In this paper, we define the concept of annuity rate as the conditional expected present value random variable of future payments of the annuity, given the future dynamics of its risk factors. The annuity rate deals with the non-diversifiable systematic risk contained in the life annuity contract, and it involves mortality risk as well as investment risk. While it is plausible to assume that there is no correlation between the two risks, each affects the annuity rate through a combination of dependent random variables. In order to understand the probabilistic profile of the annuity rate, we apply comonotonicity theory to approximate its quantile function. We also derive accurate upper and lower bounds for prediction intervals for annuity rates. We use the Lee-Carter model for mortality risk and the Vasicek model for the term structure of interest rates with an annually renewable fixed-income investment policy. Different investment strategies can be handled using this framework.",,2011,10.1016/j.insmatheco.2010.11.008,,proquest,"This paper applies comonotonicity theory to a stochastic life annuity framework to manage risk. It defines the annuity rate as the conditional expected present value of future payments, considering mortality and investment risks. The Lee-Carter model for mortality and the Vasicek model for interest rates are used to approximate the quantile function of the annuity rate and derive bounds for prediction intervals.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:55:01.460292
516f97e519503054,An effective hybrid approach for forecasting currency exchange rates,"Accurately forecasting the movement of exchange rates is of interest in a variety of fields, such as international business, financial management, and monetary policy, though this is not an easy task due to dramatic fluctuations caused by political and economic events. In this study, we develop a new forecasting approach referred to as FSPSOSVR, which is able to accurately predict exchange rates by combining particle swarm optimization (PSO), random forest feature selection, and support vector regression (SVR). PSO is used to obtain the optimal SVR parameters for predicting exchange rates. Our analysis involves the monthly exchange rates from January 1971 to December 2017 of seven countries including Australia, Canada, China, the European Union, Japan, Taiwan, and the United Kingdom. The out-of-sample forecast performance of the FSPSOSVR algorithm is compared with six competing forecasting models using the mean absolute percentage error (MAPE) and root mean square error (RMSE), including random walk, exponential smoothing, autoregres-sive integrated moving average (ARIMA), seasonal ARIMA, SVR, and PSOSVR. Our empirical results show that the FSPSOSVR algorithm consistently yields excellent predictive accuracy, which compares favorably with competing models for all currencies. These findings suggest that the proposed algorithm is a promising method for the empirical forecasting of exchange rates. Finally, we show the empirical relevance of exchange rate forecasts arising from FSPSOSVR by use of foreign exchange carry trades and find that the proposed trading strategies can deliver positive excess returns of more than 3% per annum for most currencies, except for AUD and NTD. © 2021 Elsevier B.V., All rights reserved.","Shen, M.-L.; Lee, C.-F.; Liu, H.-H.; Chang, P.-Y.; Yang, C.-H.",2021,10.3390/su13052761,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102711580&doi=10.3390%2Fsu13052761&partnerID=40&md5=a7f81af1c0335702887643e103e0ea2c,scopus,"This study introduces FSPSOSVR, a hybrid forecasting approach combining particle swarm optimization (PSO), random forest feature selection, and support vector regression (SVR), to predict currency exchange rates. The model was tested on monthly exchange rates of seven countries from 1971 to 2017. FSPSOSVR demonstrated superior predictive accuracy compared to six other models (random walk, exponential smoothing, ARIMA, seasonal ARIMA, SVR, PSOSVR) based on MAPE and RMSE. The study also showed that FSPSOSVR-based trading strategies could generate positive excess returns, except for AUD and NTD.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:55:08.912202
9a0f00b75445a4f1,An end-to-end deep learning framework for the portfolio optimization with stop-loss orders,"Incorporating stop-loss orders into portfolio optimization is an effective strategy for mitigating tail risk. However, traditional methods often rely on static stop-loss rules paired with separate price prediction models, leading to amplified error propagation in the application of stop-loss orders. We propose a novel end-to-end (E2E) data-driven deep learning framework that simultaneously determines portfolio allocations and stop-loss orders for risky assets, with each asset triggering sales when its prices reach the stop-loss order. The E2E approach uses a two-branch neural network architecture that integrates historical transaction data with additional inputs such as predicted returns and price volatility. A recurrent neural network extracts feature representation vectors for each risky asset from its time series data, and two fully connected layers process the aggregated information to generate the optimal joint action of portfolio allocations and corresponding stop-loss orders. The effectiveness of the E2E solution is demonstrated by numerical experiments on four stock market datasets. The results show that the E2E strategy outperforms alternative benchmarks across multiple metrics, including cumulative returns, alpha returns, and risk-adjusted returns. The strategy exhibits superior ability to maintain robust returns and control tail risk, achieving an average excess return of 0.33% and an average Calmar ratio of 4.50 over four datasets. In addition, the introduction of stop-loss orders yields significantly positive excess returns, and the strategy maintains its advantage even when a transaction cost rate of 0.3% is included. © 2025 Elsevier B.V., All rights reserved.","Zhang, Y.; Liu, Y.; Liu, W.; Yang, X.",2025,10.1016/j.asoc.2025.113465,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009613641&doi=10.1016%2Fj.asoc.2025.113465&partnerID=40&md5=4a06f7897a7b12fe2e864719fa90d7f0,scopus,"This paper introduces an end-to-end deep learning framework for portfolio optimization that integrates stop-loss orders. The framework uses a two-branch neural network to simultaneously determine portfolio allocations and stop-loss levels, leveraging historical data, predicted returns, and volatility. Numerical experiments on stock market data show the framework outperforms benchmarks in terms of returns and risk management, even with transaction costs.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:55:21.273875
01f7b4f943c3d0b7,An estimation model for the term structure of yield spread,"An estimation model for term structure of yield spread has become an extremely important subject to evaluate securities with default risk. By Duffie and Singleton model, yield spread was explained by two factors, namely collection rate and default probability. An estimation of the collection rate is given from historical earnings data, but estimation of default probability is known to be a remaining problem. There are some approaches to express default probability. One of them is to describe it through hazard process, and the other is to represent it by risk neutral transition probability matrix of credit-rating class. Some models that use Gaussian type hazard process or Vasicek type hazard process have already constructed. An advantage of evaluation using a rating transition probability matrix is that it is easy to obtain an image of movement of the credit-rating class. We do not need to show the calculation basis of the threshold or an assumption for distribution of prospective yield spread. But the model that uses the risk neutral transition probability matrix has not established yet, because of the computational difficulty required to estimate large number of the parameters. At first, for the purposes of this article, we will estimate the term structure of credit spreads results from the possibility of future defaults. It is assumed that credit risk is specified as a discrete-state Markov chain. And we construct a model which can be used to estimate the baseline transition matrix of the credit-rating class, risk-adjusting factors, industrial drift factors, corporate drift factors and recovery ratio, from yield spreads for individual bond. This enables us to compute the implied term structure from market data. We are capable of computing the implied term structure from market date by this process. Next, we will provide a valuation model for the term structure of yield spread. © 2001 Kluwer Academic Publishers. © 2018 Elsevier B.V., All rights reserved.","Aonuma, K.; Tanabe, T.",2001,10.1023/a:1011967507050,https://www.scopus.com/inward/record.uri?eid=2-s2.0-52549092985&doi=10.1023%2FA%3A1011967507050&partnerID=40&md5=a0e6e9585c1ad15f528cb05bd29ea952,scopus,"This paper proposes a model to estimate the term structure of yield spreads, crucial for valuing securities with default risk. It builds upon the Duffie and Singleton model, incorporating factors like collection rate and default probability. The model assumes credit risk follows a discrete-state Markov chain and aims to estimate transition matrices, risk-adjusting factors, industrial and corporate drift factors, and recovery ratios from market yield spread data. This allows for the computation of implied term structures.",True,False,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T15:55:33.010512
b4caec0083b6f9ca,An infinite hidden Markov model with GARCH for short-term interest rates,"This paper introduces a novel Bayesian time series model that combines the nonparametric features of an infinite hidden Markov model with the volatility persistence captured by the GARCH framework, to effectively model and forecast short-term interest rates. When applied to US 3-month Treasury bill rates, the GARCH-IHMM reveals both structural and persistent changes in volatility, thereby enhancing the accuracy of density forecasts compared to existing benchmark models. Out-of-sample evaluations demonstrate the superior performance of our model in density forecasts and in capturing volatility dynamics due to its adaptivity to different macroeconomic environments. © 2025 Elsevier B.V., All rights reserved.","Li, C.; Yang, Q.",2025,10.1016/j.frl.2025.107294,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002424722&doi=10.1016%2Fj.frl.2025.107294&partnerID=40&md5=5636e92e21eac3c3d6ffe3b06d97955c,scopus,"This paper proposes a new Bayesian time series model integrating an infinite hidden Markov model (IHMM) with GARCH to model and forecast short-term interest rates. Applied to US 3-month Treasury bill rates, the GARCH-IHMM captures both structural and persistent volatility changes, outperforming benchmark models in density forecasts and volatility dynamics due to its adaptability to different macroeconomic conditions.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-13T15:55:46.794567
84d299eae50d47d6,An online estimation scheme for a Hull-White model with HMM-driven parameters,"This paper considers the implementation of a mean-reverting interest rate model with Markov-modulated parameters. Hidden Markov model filtering techniques in Elliott (1994, Automatica, 30:1399-1408) and Elliott et al. (1995, Hidden Markov Models: Estimation and Control. Springer, New York) are employed to obtain optimal estimates of the model parameters via recursive filters of auxiliary quantities of the observation process. Algorithms are developed and implemented on a financial dataset of 30-day Canadian Treasury bill yields. We also provide standard errors for the model parameter estimates. Our analysis shows that within the dataset and period studied, a model with two regimes is sufficient to describe the interest rate dynamics on the basis of very small prediction errors and the Akaike information criterion. © 2007 Springer-Verlag. © 2009 Elsevier B.V., All rights reserved.","Erlwein-Sayer, C.; Mamon, R.",2009,10.1007/s10260-007-0082-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-58849132965&doi=10.1007%2Fs10260-007-0082-4&partnerID=40&md5=7f659b37a7d8a0412dc1e1436f438fff,scopus,"This paper implements a Hull-White interest rate model with parameters driven by a Hidden Markov Model (HMM). It uses HMM filtering techniques to recursively estimate model parameters and applies these algorithms to Canadian Treasury bill yield data. The study finds that a two-regime model adequately describes the interest rate dynamics within the studied dataset, supported by small prediction errors and the Akaike information criterion.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-13T15:55:59.578369
4b4d931ddca887fb,Analyst Reports and Stock Performance: Evidence From the Chinese Market,"This article applies natural language processing (NLP) to extract and quantify textual information for predicting stock performance. Utilizing an extensive dataset of Chinese analyst reports and employing a customized BERT deep learning model for Chinese text, the study categorizes the sentiment of these reports as positive, neutral, or negative. The findings highlight the predictive power of this sentiment indicator for stock volatility, excess returns, and trading volume. Specifically, analyst reports with strong positive sentiment are associated with increased excess returns and intraday volatility. Conversely, reports with strong negative sentiment also heighten volatility and trading volume but lead to a decline in future excess returns. Notably, the magnitude of the effect is more pronounced for positive sentiment reports compared to negative ones. This article contributes to the empirical literature on sentiment analysis and the stock market’s response to news, particularly within the context of the Chinese stock market. © 2025 Elsevier B.V., All rights reserved.","Liu, R.; Liang, J.; Chen, H.; Hu, Y.",2025,10.1007/s10690-025-09522-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001866318&doi=10.1007%2Fs10690-025-09522-w&partnerID=40&md5=899a1a91422d5f7e03aad92258d8bf4b,scopus,"This study uses NLP and a BERT deep learning model to analyze Chinese analyst reports, categorizing sentiment (positive, neutral, negative). The sentiment is found to predict stock volatility, excess returns, and trading volume, with positive sentiment linked to higher returns and volatility, and negative sentiment linked to higher volatility and trading volume but lower future returns. The impact of positive sentiment is stronger than negative sentiment.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:56:02.734409
c2c26c1d93079f74,Anomalies and the Expected Market Return,"We provide the first systematic evidence on the link between long‐short anomaly portfolio returns—a cornerstone of the cross‐sectional literature—and the time‐series predictability of the aggregate market excess return. Using 100 representative anomalies from the literature, we employ a variety of shrinkage techniques (including machine learning, forecast combination, and dimension reduction) to efficiently extract predictive signals in a high‐dimensional setting. We find that long‐short anomaly portfolio returns evince statistically and economically significant out‐of‐sample predictive ability for the market excess return. The predictive ability of anomaly portfolio returns appears to stem from asymmetric limits of arbitrage and overpricing correction persistence.",,2022,10.1111/jofi.13099,,proquest,"This study investigates the predictive power of long-short anomaly portfolio returns for aggregate market excess returns. The authors utilize various shrinkage techniques, including machine learning, to extract predictive signals from a large set of anomalies. They find that anomaly portfolio returns significantly predict market excess returns, attributing this to asymmetric limits of arbitrage and persistent overpricing correction.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:56:09.210837
a1a6312920cca97a,Are bond returns predictable with real-time macro data?,"We investigate the predictability of bond returns using real-time macro variables and consider the possibility of a nonlinear predictive relationship and the presence of weak factors. To address these issues, we propose a scaled sufficient forecasting (sSUFF) method and analyze its asymptotic properties. Using both the existing and the new method, we find empirically that real-time macro variables have significant forecasting power both in-sample and out-of-sample. Moreover, they generate sizable economic values, and their predictability is not spanned by the yield curve. We also observe that the forecasted bond returns are countercyclical, and the magnitude of predictability is stronger during economic recessions, which lends empirical support to well-known macro finance theories.",,2023,10.1016/j.jeconom.2022.09.008,,proquest,"This study explores the predictability of bond returns using real-time macroeconomic data, considering both linear and nonlinear relationships. A novel scaled sufficient forecasting (sSUFF) method is introduced and analyzed. Empirical results demonstrate that real-time macro variables possess significant in-sample and out-of-sample forecasting power, generating substantial economic value and not being fully explained by the yield curve. The study also finds that forecasted bond returns are countercyclical and exhibit stronger predictability during economic recessions, aligning with macro-finance theories.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-13T15:56:21.910384
289cad8e7325e40a,Artificial Intelligence in Manufacturing Industry Worker Safety: A New Paradigm for Hazard Prevention and Mitigation,"The phenomenal rise of artificial intelligence (AI) in the last decade, and its evolution as a versatile addition to various fields, necessitates its usage for novel purposes in multidimensional fields like the manufacturing industry. Even though AI has been rigorously studied for process optimization, wastage reduction, and other quintessential aspects of the manufacturing industry, there has been limited focus on worker safety as a theme in the current literature. Safety standards contribute to worker safety, but there is no one-size-fits-all approach in these standards or policies, which warrants evaluation and integration of new ideas and technologies to reach the closest to ideal standards. This includes but is not limited to health, regulation of operations, predictive maintenance, and automation and control. The rise of Industry 4.0 and the migration towards Industry 5.0 facilitate easy integration of advanced technologies like AI into the manufacturing industry with real-time predictive capabilities, and this can help reduce human errors and mitigate hazards in processes where sensitivity is crucial or hazards are frequent. Keeping the future outlook in focus, AI can contribute to training workers in risk-free environments, promote engineering education for easy adaptation to new technology, and reduce resistance to changes in the industry. Furthermore, there is an urgent need for standards and regulations to govern and integrate AI technologies judiciously into the manufacturing industry, which holds AI models and their creators accountable for their decisions. This could further extend to preventing the adversarial use of new technology. This study exhaustively discusses the potential and ongoing contributions of this technology to the safety of workers in the manufacturing industry. © 2025 Elsevier B.V., All rights reserved.","Khurram, M.; Zhang, C.; Muhammad, S.; Kishnani, H.; An, K.; Abeywardena, K.; Chadha, U.; Behdinan, K.",2025,10.3390/pr13051312,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006749937&doi=10.3390%2Fpr13051312&partnerID=40&md5=62881fa545a36f4d695feaaad90cb4af,scopus,"This article explores the application of Artificial Intelligence (AI) in enhancing worker safety within the manufacturing industry. It highlights AI's potential in hazard prevention, mitigation, and creating safer work environments, especially with the advent of Industry 4.0 and 5.0. The study also emphasizes the need for standards and regulations to govern AI integration and ensure accountability.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:56:24.425982
7dddef039290f6a0,Ask BERT: How Regulatory Disclosure of Transition and Physical Climate Risks Affects the CDS Term Structure,"We use BERT, an AI-based algorithm for language understanding, to quantify regulatory climate risk disclosures and analyze their impact on the term structure in the credit default swap (CDS) market. Risk disclosures can either increase or decrease CDS spreads, depending on whether the disclosure reveals new risks or reduces uncertainty. Training BERT to differentiate between transition and physical climate risks, we find that disclosing transition risks increases CDS spreads after the Paris Climate Agreement of 2015, while disclosing physical risks decreases the spreads. In addition, we also find that the election of Trump had a negative impact on CDS spreads for firms exposed to transition risk. These impacts are consistent with theoretical predictions and economically and statistically significant. © 2024 Elsevier B.V., All rights reserved.","Kölbel, J.F.; Leippold, M.; Rillaerts, J.; Wang, Q.",2024,10.1093/jjfinec/nbac027,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182901564&doi=10.1093%2Fjjfinec%2Fnbac027&partnerID=40&md5=23e2756786ad479db12b6ac26122ffba,scopus,"This study employs BERT, an AI language model, to quantify regulatory climate risk disclosures and their effect on the credit default swap (CDS) market's term structure. The findings indicate that disclosing transition risks widens CDS spreads post-Paris Agreement, while disclosing physical risks narrows them. Trump's election also negatively impacted CDS spreads for firms exposed to transition risk. These effects align with theoretical expectations and are statistically significant.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:56:28.831663
1aa5adffb2e84661,Asset Pricing When 'This Time Is Different',"Recent evidence suggests that younger people update beliefs in response to aggregate shocks more than older people. We embed this generational learning bias in an equilibrium model in which agents have recursive preferences and are uncertain about exogenous aggregate dynamics. The departure from rational expectations is statistically modest, but generates high average risk premiums varying at generational frequencies, a positive relation between past returns and agents' future return forecasts, and substantial and persistent over-and undervaluation. Consistent with the model, the price-dividend ratio is empirically more sensitive to macroeconomic shocks when the fraction of young in the population is higher.","Collin-Dufresne, Pierre; Johannes, Michael; Lochstoer, Lars A.",2017,10.1093/rfs/hhw084,,wos,"This paper develops an equilibrium asset pricing model incorporating a generational learning bias, where younger individuals update beliefs more than older ones in response to aggregate shocks. This bias leads to high average risk premiums, a positive relationship between past returns and future forecasts, and significant over/undervaluation. Empirically, the price-dividend ratio is more sensitive to macroeconomic shocks when the young population fraction is higher.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:56:33.261099
668802cdc78345c9,Asset Returns: Reimagining Generative ESG Indexes and Market Interconnectedness,"Financial economists have long studied factors related to risk premiums, pricing biases, and diversification impediments. This study examines the relationship between a firm’s commitment to environmental, social, and governance principles (ESGs) and asset market returns. We incorporate an algorithmic protocol to identify three nonobservable but pervasive E, S, and G time-series factors to meet the study’s objectives. The novel factors were tested for information content by constructing a six-factor Fama and French model following the imposition of the isolation and disentanglement algorithm. Realizing that nonlinear relationships characterize models incorporating both observable and nonobservable factors, the Fama and French model statement was estimated using an enhanced shallow-learning neural network. Finally, as a post hoc measure, we integrated explainable AI (XAI) to simplify the machine learning outputs. Our study extends the literature on the disentanglement of investment factors across two dimensions. We first identify new time-series-based E, S, and G factors. Second, we demonstrate how machine learning can be used to model asset returns, considering the complex interconnectedness of sustainability factors. Our approach is further supported by comparing neural-network-estimated E, S, and G weights with London Stock Exchange ESG ratings.",,2024,10.3390/jrfm17100463,,proquest,"This study investigates the link between ESG commitments and asset returns by developing three novel ESG time-series factors using an algorithmic protocol. It constructs a six-factor Fama and French model, incorporating these factors and using an enhanced shallow-learning neural network to account for nonlinear relationships. Explainable AI (XAI) is used for post hoc analysis. The research extends factor disentanglement by identifying new ESG factors and demonstrating machine learning's utility in modeling asset returns with complex sustainability factor interconnections, validated against London Stock Exchange ESG ratings.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:56:51.610768
3c7234e3f11f883f,Asset pricing with neural networks: Significance tests,"This study proposes a novel hypothesis test for evaluating the statistical significance of input variables in multi-layer perceptron (MLP) regression models. Theoretical foundations are established through consistency results and estimation rate analysis using the sieves method. To validate the test's performance in complex and realistic settings, an extensive Monte Carlo simulation is conducted. Results of the simulation reveal that the test has a high power and low rate of false positives, making it a powerful tool for detecting true effects in data. The test is further applied to identify the most influential predictors of equity risk premiums, with results indicating that only a small number of characteristics have statistical significance and all macroeconomic predictors are insignificant at the 1% level. These findings are consistent across a variety of neural network architectures. © 2023 Elsevier B.V., All rights reserved.","Fallahgoul, H.; Franstianto, V.; Lin, X.",2024,10.1016/j.jeconom.2023.105574,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175615182&doi=10.1016%2Fj.jeconom.2023.105574&partnerID=40&md5=266d86eec03bf13321f13fad852108df,scopus,"This study introduces a new statistical significance test for input variables in MLP regression models, grounded in sieve method theory. Simulations show the test is powerful and accurate. Applied to equity risk premiums, it found few significant predictors and no significant macroeconomic predictors, consistent across various neural network architectures.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:56:54.509991
cc2d45a53ccb3e61,Asymmetry in the link between the yield spread and industrial production: Threshold effects and forecasting,"We analyse the nonlinear behaviour of the information content in the spread for future real economic activity. The spread linearly predicts one-year-ahead real growth in nine industrial production sectors of the USA and four of the UK over the last 40 years. However, recent investigations on the spread-real activity relation have questioned both its linear nature and its time-invariant framework. Our in-sample empirical evidence suggests that the spread-real activity relationship exhibits asymmetries that allow for different predictive power of the spread when past spread values were above or below some threshold value. We then measure the out-of-sample forecast performance of the nonlinear model using predictive accuracy tests. The results show that significant improvement in forecasting accuracy, at least for one-step-ahead forecasts, can be obtained over the linear model. Copyright (C) 2004 John Wiley Sons, Ltd.","Paya, I; Venetis, IA; Peel, DA",2004,10.1002/for.921,,wos,"This study investigates the nonlinear relationship between the yield spread and industrial production, finding that the spread's predictive power for future economic activity is asymmetric and depends on threshold values. A nonlinear model demonstrates improved forecasting accuracy over a linear model for one-year-ahead real growth in US and UK industrial sectors.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:57:10.157722
dd98079f619d0fee,Bayesian inference for the hazard term structure with functional predictors using Bayesian predictive information criteria,"A Bayesian method for estimation of a hazard term structure is presented in a functional data analysis framework. The hazard terms structure is designed to include the effects of changes in economic conditions, as well as trends in stock prices and accounting variables from financial statements. The hazard function contains time-varying parameters that are modelled using splines. To estimate the model parameters, a Markov-chain Monte Carlo sampling algorithm is developed. The Bayesian predictive information criterion is employed to assess the default predictive power of the estimated model. The method is then applied to a Japanese firm's default data listed on the Japanese Stock Exchange. The results demonstrate that the proposed method performs well. © 2007 Elsevier B.V. All rights reserved. © 2009 Elsevier B.V., All rights reserved.","Ando, T.",2009,10.1016/j.csda.2007.12.014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-61549125754&doi=10.1016%2Fj.csda.2007.12.014&partnerID=40&md5=c8e5911b96b6a03f4d653c0cb4b296cb,scopus,"This paper proposes a Bayesian method for estimating hazard term structures using functional data analysis, incorporating economic conditions, stock prices, and accounting variables. The model uses time-varying parameters estimated via splines and a Markov-chain Monte Carlo algorithm. The Bayesian predictive information criterion is used for model assessment, and the method is applied to Japanese firm default data.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:57:25.098951
4b816497adef29ec,Bayesian inference in a Stochastic Volatility Nelson-Siegel model,"Bayesian inference is developed and applied for an extended Nelson-Siegel term structure model capturing interest rate risk. The so-called Stochastic Volatility Nelson-Siegel (SVNS) model allows for stochastic volatility in the underlying yield factors. A Markov chain Monte Carlo (MCMC) algorithm is proposed to efficiently estimate the SVNS model using simulation-based inference. The SVNS model is applied to monthly US zero-coupon yields. Significant evidence for time-varying volatility in the yield factors is found. The inclusion of stochastic volatility improves the model's goodness-of-fit and clearly reduces the forecasting uncertainty, particularly in low-volatility periods. The proposed approach is shown to work efficiently and is easily adapted to alternative specifications of dynamic factor models revealing (multivariate) stochastic volatility. © 2010 Elsevier B.V. All rights reserved. © 2012 Elsevier B.V., All rights reserved.","Hautsch, N.; Yang, F.",2012,10.1016/j.csda.2010.07.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862010805&doi=10.1016%2Fj.csda.2010.07.003&partnerID=40&md5=2473ed923e34cceeeb4420f58f6ec03d,scopus,"This paper develops and applies Bayesian inference for a Stochastic Volatility Nelson-Siegel (SVNS) model, which extends the Nelson-Siegel term structure model to include stochastic volatility in interest rate risk factors. A Markov chain Monte Carlo (MCMC) algorithm is used for estimation. Applied to US zero-coupon yields, the SVNS model demonstrates time-varying volatility, improves goodness-of-fit, and reduces forecasting uncertainty, especially in low-volatility periods. The method is adaptable to other dynamic factor models with stochastic volatility.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-13T15:57:37.295679
fd425a361c8e4fbd,Belgian economic policy uncertainty index: Improvement through text mining,"Recently, the literature has measured economic policy uncertainty using news references, resulting in the frequently-mentioned ‘Economic Policy Uncertainty index’ (EPU). In the original setup, a news article is assumed to address policy uncertainty if it contains certain predefined keywords. We argue that the original setup is prone to measurement error, and propose an alternative methodology using text mining techniques. We compare the original method to modality annotation and support vector machines (SVM) classification in order to create an EPU index for Belgium. Validation on an out-of-sample test set speaks in favour of using an SVM classification model for constructing a news-based policy uncertainty indicator. The indicators are then used to forecast 10 macroeconomic and financial variables. The original method of measuring EPU does not have predictive power for any of these 10 variables. The SVM indicator has a higher predictive power and, notably, changes in the level of policy uncertainty during tumultuous periods of high uncertainty and risk can predict changes in the sovereign bond yield and spread, the credit default swap spread, and consumer confidence. © 2018 Elsevier B.V., All rights reserved.","Tobback, E.; Naudts, H.; Daelemans, W.; Junqué de Fortuny, E.; Martens, D.",2018,10.1016/j.ijforecast.2016.08.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85001799528&doi=10.1016%2Fj.ijforecast.2016.08.006&partnerID=40&md5=a3c133019eda083c81645c0cff02d8cb,scopus,"This paper proposes an improved method for measuring economic policy uncertainty (EPU) in Belgium using text mining and support vector machine (SVM) classification, outperforming the original keyword-based EPU index. The SVM-based EPU indicator demonstrates predictive power for macroeconomic and financial variables, particularly during periods of high uncertainty, and can predict changes in sovereign bond yield and spread, credit default swap spread, and consumer confidence.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-13T15:57:56.801147
6ce0745ed5df403e,Benchmark bonds interactions under regime shifts,"In the present paper we examine the interactions among five benchmark ten year government bonds, namely those of the USA, Germany, France, Italy and the Netherlands. Our aim is to illustrate empirically a net of interactions existing among the major bond markets of Europe and the US market taking into account shifts in the underlying stochastic processes. For this purpose, differing from the rest of the relevant empirical literature, after specifying the long run equilibrium relations, we estimate the linkages between the bond markets as subject to hidden Markov chains, by applying the Markov Switching Vector Error Correction framework (MS-VECM). This formulation is found to efficiently reflect the shifts brought about by significant economic events, such as the European monetary unification. As a result we illustrate different short-run relations referring to the periods before and after the monetary union. Overall, our empirical results indicate that stronger interactions among the markets of the system exist in the period after the EMU. Also, by means of a variance decomposition analysis we assess leader-follower relations which indicate that the benchmark status of bonds has changed since the introduction of the common monetary policy framework in Europe. Reprinted by permission of Blackwell Publishers",,2012,10.1111/j.1468-036x.2009.00535.x,,proquest,"This paper empirically examines the interactions among five benchmark ten-year government bonds (USA, Germany, France, Italy, and Netherlands) using a Markov Switching Vector Error Correction (MS-VECM) framework. The study accounts for regime shifts, particularly those related to the European monetary unification, and illustrates different short-run relations before and after the EMU. Results show stronger interactions post-EMU and changing leader-follower dynamics among the benchmark bonds.",True,True,False,gemini-2.5-flash-lite,Trine,M,,2025-10-13T15:58:15.711236
fe830c5db5e38b8f,Beyond Stochastic Volatility and Jumps in Returns and Volatility,"While a great deal of attention has been focused on stochastic volatility in stock returns, there is strong evidence suggesting that return distributions have time-varying skewness and kurtosis as well. Under the risk-neutral measure, for example, this can be observed from variation across time in the shape of Black Scholes implied volatility smiles. This article investigates model characteristics that are consistent with variation in the shape of return distributions using a stochastic volatility model with a regime-switching feature to allow for random changes in the parameters governing volatility of volatility, leverage effect, and jump intensity. The analysis consists of two steps. First, the models are estimated using only information from observed returns and option-implied volatility. Standard model assessment tools indicate a strong preference in favor of the proposed models. Since the information from option-implied skewness and kurtosis is not used in fitting the models, it is available for diagnostic purposes. In the second step of the analysis, regressions of option-implied skewness and kurtosis on the filtered state variables (and some controls) suggest that the models have strong explanatory power for these characteristics.","Durham, Garland; Park, Yang-Ho",2013,10.1080/07350015.2013.747800,,wos,"This article proposes a stochastic volatility model with a regime-switching feature to capture time-varying skewness and kurtosis in stock returns, beyond just stochastic volatility and jumps. The model is estimated using observed returns and option-implied volatility, and its explanatory power for option-implied skewness and kurtosis is assessed.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:58:22.460979
3cf97d4770aaca9e,Binomial Markov-Switching Multifractal model with Skewed t innovations and applications to Chinese SSEC Index,"This paper presents the Binomial Markov-switching Multifractal (BMSM) model of asset returns with Skewed t innovations (BMSM-Skewed t for short), which considers the fat tails, skewness and multifractality in asset returns simultaneously. The parameters of BMSM-Skewed t model can be estimated by Maximum Likelihood (ML) methods, and volatility forecasting can be accomplished via Bayesian updating. In order to evaluate the performance of BMSM-Skewed t model, BMSM model with Normal innovations (BMSM-N), BMSM model with Student-t innovations (BMSM-t) and GARCH(1,1) models (GARCH-N, GARCH-t and GARCH-Skewed t) are chosen for comparison. Through empirical studies on Shanghai Stock Exchange Composite Index (SSEC), we find that for sample estimation, BMSM models outperform the GARCH(1,1) models through BIC and AIC rules, and BMSM-Skewed t performs the best among all the models due to its fat tails, skewness and multifractality. In addition, BMSM-Skewed t model dominates other models at most forecasting horizons for out-of-sample volatility forecasts in terms of MSE, MAE and SPA test. (C) 2016 Elsevier B.V. All rights reserved.","Liu, Yufang; Zhang, Weiguo; Fu, Junhui",2016,10.1016/j.physa.2016.06.014,,wos,"This paper introduces the Binomial Markov-switching Multifractal (BMSM) model with Skewed t innovations to simultaneously capture fat tails, skewness, and multifractality in asset returns. The model is estimated using Maximum Likelihood methods and volatility forecasting is done via Bayesian updating. Empirical studies on the Shanghai Stock Exchange Composite Index (SSEC) show that the BMSM models outperform GARCH(1,1) models, with the BMSM-Skewed t model performing best. The BMSM-Skewed t model also demonstrates superior out-of-sample volatility forecasting performance.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:58:31.178719
22a2d2aa9f3427cd,Bond return predictability: Macro factors and machine learning methods,"We investigate the impact of macroeconomic variables on bond risk premia prediction via machine learning techniques. On the basis of Chinese treasury bonds from March 2006 to December 2022, we show that adding macroeconomic factors improves bond return forecasts and generates higher economic benefits to investors. This is achieved when the nonlinear relationship between macroeconomic variables and bond returns is modelled via machine learning methods. Furthermore, the importance of macroeconomic determinants changes along the yield curve. Our study sheds new light on the information contained in macroeconomic variables for treasury bond valuation and highlights the importance of utilizing appropriate machine learning methods.",,2024,10.1111/eufm.12483,,proquest,"This study explores the predictability of bond returns using macroeconomic variables and machine learning techniques on Chinese treasury bonds. It demonstrates that incorporating macroeconomic factors, especially through nonlinear modeling with machine learning, enhances return forecasts and investor benefits. The relevance of these factors varies across the yield curve.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-13T15:58:42.027457
46572bea2afbeb26,Bond risk premia in a small open economy with volatile capital flows: The case of Korea,"This paper investigates bond risk premia embedded in Korean government bonds. Unlike the U.S., Korea is a small open economy characterized by highly volatile capital flows and non-reserve currency country. My empirical findings show that among alternative predictive variables (including the macro and global liquidity factors) for one-year-ahead excess bond returns, the global liquidity factors, extracted from the panel data set of various global liquidity variables, are the only predictors that perform well across both in- and out-of-sample forecast analysis. In a similar vein, the regression analysis for the determinants of the estimated bond risk premia (with both monthly and quarterly frequencies) reveals that similar to the case of U.S. bond market, the risk premia in Korean government bonds are affected by domestic expected inflation, but more importantly, that they are affected heavily by the global liquidity variables, such as VIX, bank capital flows and the leverage of global banks. (C) 2019 Elsevier Ltd. All rights reserved.","Yun, Jaeho",2019,10.1016/j.jimonfin.2019.01.007,,wos,"This paper examines bond risk premia in Korean government bonds, highlighting Korea's status as a small open economy with volatile capital flows. Empirical findings indicate that global liquidity factors are significant predictors of excess bond returns, outperforming domestic macroeconomic variables. The study also reveals that while domestic expected inflation influences Korean bond risk premia, global liquidity variables like VIX, bank capital flows, and global bank leverage play a more substantial role.",False,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T15:59:03.804496
72088005c119975b,Bond risk premiums at the zero lower bound,"We document that the spread between long- and short-term government bond yields is a stronger predictor of excess bond returns when the U.S. economy is at the zero lower bound (ZLB) than away from this bound. The Gaussian shadow rate model with a linear or quadratic shadow rate is unable to explain this change in return predictability. The same holds for the quadratic term structure model and the autoregressive gamma-zero model that also enforce the ZLB. In contrast, the linear-rational square-root model explains our new empirical finding because the model allows for unspanned stochastic volatility as seen in bond yields.","Andreasen, Martin M.; Jorgensen, Kasper; Meldrum, Andrew",2025,10.1016/j.jeconom.2024.105939,,wos,"This paper investigates bond risk premiums at the zero lower bound (ZLB). It finds that the spread between long- and short-term government bond yields is a better predictor of excess bond returns at the ZLB. Existing models like the Gaussian shadow rate model and the quadratic term structure model fail to explain this, but a linear-rational square-root model succeeds by incorporating unspanned stochastic volatility.",False,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T15:59:32.928274
0fdd961fa4794d25,Building Risk into the Mitigation/Adaptation Decisions simulated by Integrated Assessment Models,"This paper proposes an operationally simple and easily generalizable methodology to incorporate climate change damage uncertainty into Integrated Assessment Models (IAMs). First uncertainty is transformed into a risk measure by extracting damage distribution means and variances from an ensemble of socio economic and climate change scenarios. Then a risk premium is computed under different degrees of risk aversion, quantifying what society would be willing to pay to insure against the uncertainty of the damages. Our estimates show that the premium for the risk is a potentially significant addition to the “standard average damage”, but highly sensitive to the attitudes toward risk. In the last research phase, the risk premium is incorporated into the climate change damage function of a widely used IAM which shows, consequently, a substantial increase in both mitigation and adaptation efforts, reflecting a more precautionary attitude by the social planner. Interestingly, adaptation is stimulated more than mitigation in the first half of this century, while the situation reverses afterwards.",,2019,10.1007/s10640-019-00384-1,,proquest,"This paper introduces a method to integrate climate change damage uncertainty into Integrated Assessment Models (IAMs) by transforming uncertainty into a risk measure. It calculates a risk premium based on societal risk aversion, which significantly increases mitigation and adaptation efforts within the IAM, leading to a more precautionary approach. The study also observes that adaptation is prioritized over mitigation in the early 21st century, with the trend reversing later.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T15:59:37.111615
5c1357ad22be14f6,Can Deep-Learning Models Predict Behavior of Treasury Bond Yields,"Treasury market dominates the fixed-income segment, especially in emerging economies such as India. The Treasury rates affect the investment decisions, portfolio management, capital structure, as well as corporate debt structure decisions of the firms. Even after this huge impact, the Treasury rate prediction problems are less explored. This paper investigates the efficacy of ensemble-based machine learning and deep learning models in predicting the behavior of Treasury bond yields. The work has employed six models on six Treasury index datasets with different maturity periods. The result suggests that the Recurrent Neural Network and its variants are best-suited models to work on Treasury bonds. The Gated Recurrent Unit gives the best result among all Recurrent Network variants. The deep learning models give higher accuracy for the Treasury yield with a greater maturity period. In contrast, the ensemble models perform better on the Treasury yield with a smaller maturity period. The findings call for incorporating machine learning-based prediction in the pricing and valuation of securities, investment plans, and debt structure decisions. © 2025 Elsevier B.V., All rights reserved.","Podder, D.; Mukherjee, R.; Hiremath, G.S.",2025,10.1007/s10614-025-10947-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007230849&doi=10.1007%2Fs10614-025-10947-8&partnerID=40&md5=0d918fe85ff3646830e1f74d6664d2bf,scopus,"This paper explores the use of ensemble-based machine learning and deep learning models, specifically Recurrent Neural Networks (RNNs) and their variants like Gated Recurrent Units (GRUs), for predicting Treasury bond yields in India. The study found that deep learning models, particularly GRUs, performed best for longer maturity periods, while ensemble models were more effective for shorter maturities. The findings suggest integrating machine learning for securities pricing, investment planning, and debt structure decisions.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-13T15:59:44.924693
e47a7f4118f99119,Can negative interest rates really affect option pricing? Empirical evidence from an explicitly solvable stochastic volatility model,"The profound financial crisis generated by the collapse of Lehman Brothers and the European sovereign debt crisis in 2011 have caused negative values of government bond yields both in the USA and in the EURO area. This paper investigates whether the use of models which allow for negative interest rates can improve option pricing and implied volatility forecasting. This is done with special attention to foreign exchange and index options. To this end, we carried out an empirical analysis on the prices of call and put options on the US S&P 500 index and Eurodollar futures using a generalization of the Heston model in the stochastic interest rate framework. Specifically, the dynamics of the option’s underlying asset is described by two factors: a stochastic variance and a stochastic interest rate. The volatility is not allowed to be negative, but the interest rate is. Explicit formulas for the transition probability density function and moments are derived. These formulas are used to estimate the model parameters efficiently. Three empirical analyses are illustrated. The first two show that the use of models which allow for negative interest rates can efficiently reproduce implied volatility and forecast option prices (i.e. S&P index and foreign exchange options). The last studies how the US three-month government bond yield affects the US S&P 500 index. © 2017 Elsevier B.V., All rights reserved.","Recchioni, M.C.; Sun, Y.; Tedeschi, G.",2017,10.1080/14697688.2016.1272763,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014563788&doi=10.1080%2F14697688.2016.1272763&partnerID=40&md5=fb231c8d70605cedda83b18b1ebc4c8b,scopus,"This paper investigates the impact of negative interest rates on option pricing and implied volatility forecasting, particularly for foreign exchange and index options. Using a generalized Heston model with stochastic interest rates, the study empirically analyzes call and put options on the US S&P 500 index and Eurodollar futures. The findings suggest that models accommodating negative interest rates can effectively reproduce implied volatility and forecast option prices.",True,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T15:59:55.645753
0171b0a089a6ffef,Can tree-structured classifiers add value to the investor?,"We analyse the investor welfare gain of including tree-structured classifiers’ predictions about the relative performance of stock vs. cash. The CART, bagging, and random forest methods select the VIX level and momentum, the earning bond yield level and momentum, and the detrended risk-free rate as the most important state variables to predict the outperformance of the S&P 500 vs. cash out-of-sample. These tree-structured classifiers’ predictions are used as a binary state variable to estimate optimal investor portfolios that also deliver out-of-sample higher Sharpe ratios and certainty equivalent return gains than competing portfolio strategies that exclude them. © 2019 Elsevier B.V., All rights reserved.","Laborda, R.; Laborda, J.",2017,10.1016/j.frl.2017.06.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020478167&doi=10.1016%2Fj.frl.2017.06.002&partnerID=40&md5=c5cc5c546ee15a2cf5366bb636d8e4ef,scopus,"This study investigates the benefits of using tree-structured classifiers (CART, bagging, random forest) for investor decision-making regarding stock vs. cash performance. The models identify key state variables like VIX level, momentum, earning bond yield, and detrended risk-free rate to predict S&P 500 outperformance. Incorporating these predictions into portfolio strategies results in improved Sharpe ratios and certainty equivalent returns compared to strategies that do not use them.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:00:13.814014
56598c57394f26e9,Can we measure inflation expectations using Twitter?,"Drawing on Italian tweets, we employ textual data and machine learning techniques to build new real-time measures of consumers' inflation expectations. First, we select keywords to identify tweets related to prices and expectations thereof. Second, we build a set of daily measures of inflation expectations around the selected tweets, combining the Latent Dirichlet Allocation (LDA) with a dictionary-based approach, using manually labeled bi-grams and tri-grams. Finally, we show that Twitter-based indicators are highly correlated with both monthly survey-based and daily market-based inflation expectations. Our new indicators anticipate consumers' expectations, proving to be a good real-time proxy, and provide additional information beyond market based expectations, professional forecasts, and realized inflation. The results suggest that Twitter can be a new timely source for eliciting beliefs. (C) 2022 Elsevier B.V. All rights reserved.","Angelico, Cristina; Marcucci, Juri; Miccoli, Marcello; Quarta, Filippo",2022,10.1016/j.jeconom.2021.12.008,,wos,"This study explores the use of Twitter data and machine learning (LDA and dictionary-based approach) to create real-time measures of consumer inflation expectations in Italy. The developed indicators show a strong correlation with survey-based and market-based expectations, suggesting Twitter as a timely source for eliciting beliefs.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:00:17.865346
925a15bd08c4a54b,Changes in predictive ability with mixed frequency data,"When assessing the predictive power of financial variables for economic activity, researchers usually aggregate higher-frequency data before estimating a forecasting model that assumes the relationship between the financial variable and the dependent variable to be linear. This paper proposes a model called smooth transition mixed data sampling (STMIDAS) regression, which relaxes both of these assumptions. Simulation exercises indicate that the improvements in forecasting accuracy from the use of mixed data sampling are larger in nonlinear than in linear specifications. When forecasting output growth with financial variables in real time, statistically significant improvements over a linear regression are more likely to arise from forecasting with STMIDAS than with MIDAS regressions. (C) 2012 International Institute of Forecasters. Published by Elsevier B.V. All rights reserved.","Galvao, Ana Beatriz",2013,10.1016/j.ijforecast.2012.10.006,,wos,"This paper introduces the smooth transition mixed data sampling (STMIDAS) regression model, which improves forecasting accuracy by relaxing assumptions of linear relationships and data aggregation. Simulation exercises and real-time forecasting of output growth with financial variables show that STMIDAS outperforms traditional MIDAS regressions, especially in nonlinear specifications.",True,False,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T16:00:47.450434
e093f0a24b02d3b8,Chasing the deal with the money: Measuring the required risk premium and expected abnormal returns of private equity funds to maximize their internal rate of return,"A number of scholars of private equity (“PE”) have attempted to assess the ex-post returns, or performance, of PEs by adopting an ex-post perspective of asset pricing. In doing so a set of phenomena has been recognized that is thought to be specific to the PE sector, such as “money-chasing deal phenomenon” (Gompers and Lerner, 2000) and “performance persistence” (Lerner and Schoar, 2005). However, based on their continuing use of an ex-post perspective, few scholars have paid attention to the possible extent to which these and other PE phenomena may affect expected returns from PE investments. To address this problem this article draws on an ex-ante perspective of investment decision-making in suggesting how a number of drivers and factors of PE phenomena may produce “abnormal returns”, and that each of those drivers and factors should therefore be considered in accurately assessing the required risk premium and expected abnormal returns of PE investments. In making these contributions we examined a private equity investment of a regional PE in Italy and administered a telephone questionnaire to 40 PEs in Italy and the UK and found principally that while size is the most important driver in producing abnormal returns illiquidity alone cannot explain the expected returns of PE investments (cf. Franzoni et al., 2012). Based on our findings we developed a predictive model of PE decision-making that draws on an ex-ante perspective of asset pricing and takes into account PE phenomena and abnormal returns. This model extends the work of Franzoni et al. (2012), Jegadeesh et al. (2009), and Korteweg and Sorensen (2010) who did not consider the possible influence of PE phenomena in decision-making and will also help PE managers in making better-informed decisions. © 2018 Elsevier B.V., All rights reserved.","Scarpati, F.; Ng, W.",2013,10.22495/rgcv3i3art6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939419937&doi=10.22495%2Frgcv3i3art6&partnerID=40&md5=4a3b001b9d54090bf9a7d53849498f2c,scopus,"This article uses an ex-ante perspective to examine how private equity (PE) phenomena, such as the ""money-chasing deal phenomenon"" and ""performance persistence,"" influence expected returns and risk premiums. The study surveyed 40 PEs in Italy and the UK, finding that while size is a key driver of abnormal returns, illiquidity alone does not explain expected returns. A predictive model is developed to aid PE managers in decision-making.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:00:52.562924
1b1a8cfbbcbec1d6,Climate Change and ESG: Focused on Green Bond Design,"We investigate about climate change, which is now emerging as a hot potato, and realize its seriousness. To respond to climate change, carbon reduction strategies are vitally important. We examine the effect of carbon reduction on ESG score improvement and show the importance of ESG management by using the concept of green spread which is a financial environmental cleanliness measure. An aim of this study is to predict numerically the impact of carbon reduction on national cleanliness and economic benefits using the methodology of CO2 emission-backed securities. In order to get attention from many people to the reduction of GHG gas, we compute risk premiums of the securities and did securitization of CO2 emissions. Then, issuing the securities will stimulate investors about the national CO2 reduction activities. Also, we expect that this study gives countries an incentive to reduce their CO2 emissions and prepare for climate change.","DONGHOON, SHIN; 최윤민; 김창기",2023,10.14251/jscm.2023.3.31,,wos,"This study investigates the impact of carbon reduction strategies on ESG score improvement and national economic benefits, using the concept of green spread and CO2 emission-backed securities. It aims to numerically predict the effects of carbon reduction and incentivize countries to reduce CO2 emissions and prepare for climate change.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:00:59.830198
ae63f674b58debe4,Closed-form likelihood expansions for multivariate time-inhomogeneous diffusions,"The aim of this paper is to find approximate log-transition density functions for multivariate time-nhomogeneous diffusions in closed form. There are many empirical evidences supporting that the data generating process governing dynamics of many economics variables might vary over time because of economic climate changes or time effects. One possible way to explain the time-dependent dynamics of state variables is to model the drift or volatility terms as functions of time t as well as state variables. A way to find closed-form likelihood expansion for a multivariate time-homogeneous diffusion has been developed by Ait-Sahalia (2008). This research is built on his work and extends his results to time-inhomogeneous cases. We conduct Monte Carlo simulation studies to examine performance of the approximate transition density function when it is used to obtain ML estimates. The results reveal that our method yields a very accurate approximate likelihood function, which can be a good candidate when the true likelihood function is unavailable as is often the case. (C) 2013 Elsevier B.V. All rights reserved.","Choi, Seungmoon",2013,10.1016/j.jeconom.2011.12.007,,wos,"This paper develops closed-form approximate log-transition density functions for multivariate time-inhomogeneous diffusions, extending previous work on time-homogeneous diffusions. The authors use Monte Carlo simulations to demonstrate the accuracy of their method for Maximum Likelihood estimation, suggesting it's a viable alternative when the true likelihood function is unknown.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:01:05.294362
4d1b62d1b15ae5dc,Cointegration analysis of hazard rates and CDSs: Applications to pairs trading strategy,"This study examines the cointegration relationship between multiple credit default swap (CDS) spreads by constructing the cointegrated hazard rate model, which assumes the structure of the vector error correction model (VECM) in the drift term of hazard rate processes. We merge the cointegration nature into the framework of arbitrage-free pricing and thereby derive the theoretical spread formula of multiple cointegrated CDSs. For the estimation of hazard rate dynamics, we develop a Bayesian statistical inference method combined with the numerical ordinary differential equation solver because the theoretical CDS spread cannot be expressed in closed form. In the empirical study of Japanese corporate CDSs, we find that the overall term structures of cointegrated CDSs can be explained by a simple two-dimensional VECM of cointegrated hazard rates. Furthermore, we study the pairs trading strategy of CDSs. © 2023 Elsevier B.V., All rights reserved.","Kato, K.; Nakamura, N.",2023,10.1016/j.physa.2023.128489,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147196257&doi=10.1016%2Fj.physa.2023.128489&partnerID=40&md5=6700306a9980b3c616b999d06fa79817,scopus,"This study investigates the cointegration relationship between credit default swap (CDS) spreads using a cointegrated hazard rate model, which incorporates a vector error correction model (VECM) into hazard rate processes. The authors derive a theoretical spread formula for multiple cointegrated CDSs within an arbitrage-free pricing framework. A Bayesian inference method is used to estimate hazard rate dynamics. Empirical analysis of Japanese corporate CDSs suggests a two-dimensional VECM can explain the term structures. The study also explores a pairs trading strategy for CDSs.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:01:18.845469
45a2cb3e694beb29,Cointegration and detectable linear and nonlinear causality: Analysis using the London Metal Exchange lead contract,"This study applies linear and nonlinear Granger causality tests to examine the dynamic relation between London Metal Exchange (LME) cash prices and three possible predictors. The analysis uses matched quarterly inventory, UK Treasury bill interest rates, futures prices and cash prices for the commodity lead traded on the LME. The effects of cointegration on both linear and nonlinear Granger causality tests is also examined. When cointegration is not modelled, evidence is found of both linear and nonlinear causality between cash prices and analysed predictor variables. However, after controlling for cointegration, evidence of significant nonlinear causality is no longer found. These results contribute to the empirical literature on commodity price forecasting by highlighting the relationship between cointegration and detectable linear and nonlinear causality. The importance of interest rate and inventory as well as futures price in forecasting cash prices is also illustrated. Failure to detect significant nonlinearity after controlling for cointegration may also go some way to explaining the reason for the disappointing forecasting performances of many nonlinear models in the general finance literature. It may be that the variables are correct, but the functional form is overly complex and a standard VAR or VECM may often apply. © 2004 Taylor and Francis Ltd. © 2008 Elsevier B.V., All rights reserved.","Chen, A.-S.; Lin, J.W.",2004,10.1080/0003684042000247352,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3543067056&doi=10.1080%2F0003684042000247352&partnerID=40&md5=4a669c06e8048b8dafca13e8c66096a8,scopus,"This study investigates the relationship between London Metal Exchange (LME) cash prices and predictor variables (inventory, interest rates, futures prices) using linear and nonlinear Granger causality tests. It examines the impact of cointegration on these tests. While initial analysis shows both linear and nonlinear causality, controlling for cointegration eliminates significant nonlinear causality. The findings suggest that cointegration is crucial for understanding causality and that simpler models like VAR or VECM might be more appropriate than complex nonlinear models for forecasting commodity prices.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:02:03.644802
06de698e287ecd19,"Collateralizable wealth, asset returns, and systemic risk: International evidence","Purpose - The purpose of this chapter is to assess the role of collateralizable wealth and systemic risk in explaining future asset returns. Methodology/approach - To test this hypothesis, the chapter uses the residuals of the trend relationship among housing wealth and labor income to predict both stock returns and government bond yields. Specifically, it shows that nonlinear deviations of housing wealth from its cointegrating relationship with labor income, hwy, forecast expected future returns. Findings - Using data for a set of industrialized countries, the chapter finds that when the housing wealth-to-income ratio falls, investors demand a higher risk premium for stocks. As for government bond returns: (i) when they are seen as a component of asset wealth, investors react in the same manner and (ii) if, however, investors perceive the increase in government bond returns as signaling a future rise in taxes or a deterioration of public finances, then they interpret the fall in the housing wealth-to-income ratio as a fall in future bond premia. Finally, this work shows that the occurrence of crisis episodes amplifies the transmission of housing market shocks to financial markets. Originality/value of chapter - These findings are novel. They also open new and challenging avenues for understanding the dynamics of the relationship between the housing sector, stock market and government bond developments, and the banking system. Copyright © 2013 by Emerald Group Publishing Limited. © 2014 Elsevier B.V., All rights reserved.","Sousa, R.M.",2010,10.1108/s1571-0386(2010)0000020006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897585761&doi=10.1108%2FS1571-0386%282010%290000020006&partnerID=40&md5=369313bd78587f6aa5380ba6033b921d,scopus,"This chapter investigates the relationship between collateralizable wealth, systemic risk, and future asset returns, using housing wealth and labor income data from industrialized countries. It finds that deviations in the housing wealth-to-income ratio predict stock returns and government bond yields, with housing market shocks amplifying transmission to financial markets during crises. The study utilizes nonlinear deviations to forecast returns.",False,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T16:02:34.898664
81fa20cb81e0b280,Combining the wisdom of crowds and technical analysis for financial market prediction using deep random subspace ensembles,"Many researchers and practitioners have attempted to predict financial market trends for excess returns using multiple information sources including social media. Recent studies have investigated the relation between public sentiment and stock price movements and demonstrated that investment decisions are affected by public opinion. In this paper, we design a novel framework that combines the wisdom of crowds and technical analysis for financial market prediction using a new fusion strategy. A machine learning technique called deep random subspace ensembles (DRSE), which integrates deep learning algorithms and ensemble learning methods, is proposed according to the characteristics of the prediction task. Based on collected real-world datasets, the experimental results show that our proposed method outperforms the baseline models in predicting stock market by at least 14.2% in terms of AUC value, indicating the efficacy of DRSE as a viable mechanism for financial market prediction. © 2019 Elsevier B.V., All rights reserved.","Wang, Q.; Xu, W.; Zheng, H.",2018,10.1016/j.neucom.2018.02.095,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045111057&doi=10.1016%2Fj.neucom.2018.02.095&partnerID=40&md5=f9d301ef6362de9274f7f73bb4fbd365,scopus,"This paper proposes a novel framework combining crowd wisdom and technical analysis for financial market prediction using a deep random subspace ensemble (DRSE) method. Experiments on real-world datasets show DRSE outperforms baseline models in stock market prediction, achieving at least a 14.2% improvement in AUC.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:02:53.380336
8ac5d4f4177d3e3a,Common Risk Factors in Cryptocurrency,"We find that three factors-cryptocurrency market, size, and momentum-capture the cross-sectional expected cryptocurrency returns. We consider a comprehensive list of price- and market-related return predictors in the stock market and construct their cryptocurrency counterparts. Ten cryptocurrency characteristics form successful long-short strategies that generate sizable and statistically significant excess returns, and we show that all of these strategies are accounted for by the cryptocurrency three-factor model. Lastly, we examine potential underlying mechanisms of the cryptocurrency size and momentum effects.","Liu, Yukun; Tsyvinski, Aleh; Wu, X., I",2022,10.1111/jofi.13119,,wos,"This study identifies three key factors (cryptocurrency market, size, and momentum) that explain expected returns in the cryptocurrency market. It tests ten cryptocurrency characteristics for their predictive power, finding that all are captured by the three-factor model, and explores the mechanisms behind the size and momentum effects.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:02:55.960844
c75986c18e735da3,Communicating the promise for ocular gene therapies: Challenges and recommendations,"Purpose To identify challenges and pose solutions for communications about ocular gene therapy between patients and clinicians as clinical research progresses. Design Literature review with recommendations. Methods Literature review of science communication best practices to inform recommendations for patient-clinician discussions about ocular gene therapy. Results Clinicians need to employ communications about ocular gene therapy that are both attentive to patient priorities and concerns and responsive to other sources of information, including overly positive news media and the Internet. Coverage often conflates research with therapy - clinical trials are experimental and are not risk free. If proven safe and efficacious, gene therapy may present a treatment but not a cure for patients who have already experienced vision loss. Clinicians can assist patients by providing realistic estimates for lengthy clinical development timelines and positioning current research within models of clinical translation. This enables patients to weigh future therapeutic options when making current disease management decisions. Conclusions Ocular gene therapy clinical trials are raising hopes for treating a myriad of hereditary retinopathies, but most such therapies are many years in the future. Clinicians should be prepared to counter overly positive messaging, found in news media and on the Internet, with optimism tempered by evidence to support the ethical translation of gene therapy and other novel biotherapeutics. © 2018 Elsevier B.V., All rights reserved.","Benjaminy, S.; Kowal, S.P.; MacDonald, I.M.; Bubela, T.",2015,10.1016/j.ajo.2015.05.026,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939258801&doi=10.1016%2Fj.ajo.2015.05.026&partnerID=40&md5=fdaf1a7223d511416c7b84feed4cf280,scopus,"This literature review identifies challenges and proposes solutions for communication between patients and clinicians regarding ocular gene therapy. It highlights the need for clinicians to provide realistic expectations, address misinformation from media and the internet, and differentiate between research and actual therapy, emphasizing that gene therapies are often years away and may not be cures.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:02:58.326086
12a420d5f76f8b7d,Comparing Machine Learning Models for Short-Term U.S. Treasury Yield Forecasting,"This study examines historical trends in the U.S. 10-year Treasury yield and evaluates the effectiveness of four machine learning models, linear regression, decision tree, random forest, and multi-layer perceptron (MLP) neural networks, for short-term yield forecasting. As a key benchmark in global financial markets, the 10-year Treasury yield is influenced by multiple economic factors, including core inflation, the federal funds rate, GDP growth, and the U.S. Federal government's debt growth rate. Leveraging historical data from the Federal Reserve Economic Database (FRED), this study develops predictive models to assess the impact of these factors on yield fluctuations. Empirical results indicate that the random forest model outperforms the other approaches, achieving the lowest mean squared error (MSE) and mean absolute error (MAE), alongside an R2 of 0.6073. This suggests its superior ability to capture nonlinear relationships in yield movements. The decision tree model also demonstrates competitive accuracy but is more susceptible to overfitting. Conversely, linear regression provides useful interpretability but struggles to capture complex economic interactions, leading to lower predictive accuracy. Despite its potential for handling nonlinear dependencies, the MLP model underperforms compared to the random forest, yielding an R2 of 0.5058. The findings underscore the advantages of machine learning, particularly ensemble-based methods, in short-term Treasury yield forecasting. © 2025 Elsevier B.V., All rights reserved.","Wang, M.Y.-F.; Wang, Y.-F.",2025,10.1002/cpe.70265,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105015141371&doi=10.1002%2Fcpe.70265&partnerID=40&md5=47ea3ed053791be37d1bc13f1326b377,scopus,"This study compares four machine learning models (linear regression, decision tree, random forest, MLP) for short-term U.S. 10-year Treasury yield forecasting, using historical data and economic factors. The random forest model showed the best performance, indicating the effectiveness of ensemble methods for this task.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-13T16:03:04.576712
502c599f9f5a60b2,Comparison procedure of predicting the time to default in behavioural scoring,"The paper deals with the problem of predicting the time to default in credit behavioural scoring. This area opens a possibility of including a dynamic component in behavioural scoring modelling which enables making decisions related to limit, collection and recovery strategies, retention and attrition, as well as providing an insight into the profitability, pricing or term structure of the loan. In this paper, we compare survival analysis and neural networks in terms of modelling and results. The neural network architecture is designed such that its output is comparable to the survival analysis output. Six neural network models were created, one for each period of default. A radial basis neural network algorithm was used to test all six models. The survival model used a Cox modelling procedure. Further, different performance measures of all models were discussed since even in highly accurate scoring models, misclassification patterns appear. A systematic comparison '3 + 2 + 2' procedure is suggested to find the most effective model for a bank. Additionally, the survival analysis model is compared to neural network models according to the relative importance of different variables in predicting the time to default. Although different models can have very similar performance measures they may consist of different variables. The dataset used for the research was collected from a Croatian bank and credit customers were observed during a 12-month period. The paper emphasizes the importance of conducting a detailed comparison procedure while selecting the best model that satisfies the users' interest. © 2008 Elsevier Ltd. All rights reserved. © 2009 Elsevier B.V., All rights reserved.","Šarlija, N.; Benšíc, M.; Zekić-Sušac, M.",2009,10.1016/j.eswa.2008.11.042,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60849137198&doi=10.1016%2Fj.eswa.2008.11.042&partnerID=40&md5=c735efcf3fe984ca06b4c187f150ef0d,scopus,"This paper compares survival analysis and neural networks for predicting time to default in credit behavioral scoring. It details the methodology, including the design of neural network architectures and the use of Cox modeling for survival analysis. The study evaluates model performance using various measures and suggests a systematic comparison procedure. It also examines the relative importance of variables in prediction and uses data from a Croatian bank with a 12-month observation period.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:03:15.376805
980a360dd8f36999,Conditioning information and variance bounds on pricing kernels,"Gallant, Hansen, and Tauchen (1990) show how to use conditioning information optimally to construct a sharper unconditional variance bound (the GHT bound) on pricing kernels. The literature predominantly resorts to a simple but suboptimal procedure that scales returns with predictive instruments and computes standard bounds using the original and scaled returns. This article provides a formal bridge between the two approaches. We propose an optimally scaled bound that coincides with the GHT bound when the first and second conditional moments are known. When these moments are misspecified, our optimally scaled bound yields a valid lower bound for the standard deviation of pricing kernels, whereas the GHT bound does not. We illustrate the behavior of the bounds using a number of linear and nonlinear models for consumption growth and bond and stock returns. We also illustrate how the optimally scaled bound can be used as a diagnostic for the specification of the first two conditional moments of asset returns.","Bekaert, G; Liu, J",2004,10.1093/rfs/hhg052,,wos,"This article bridges the gap between the optimal conditioning information approach (GHT bound) and a common suboptimal procedure for bounding pricing kernels. It proposes an ""optimally scaled bound"" that matches the GHT bound when conditional moments are known and provides a valid lower bound for the standard deviation of pricing kernels even when these moments are misspecified. The authors demonstrate the bounds' behavior with linear and nonlinear models for consumption growth and asset returns, and suggest the optimally scaled bound as a diagnostic tool for moment specification.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:03:25.961518
eed3af2446ec0a3a,"Contemporaneous threshold autoregressive models: Estimation, testing and forecasting","This paper proposes a contemporaneous smooth transition threshold autoregressive model (C-STAR) as a modification of the smooth transition threshold autoregressive model surveyed in Terasvirta [1998. Modelling economic relationships with smooth transition regressions. In: Ullah, A., Giles, D.E.A. (Eds.), Handbook of Applied Economic Statistics. Marcel Dekker, New York, pp. 507-552.], in which the regime weights depend on the ex ante probability that a latent regime-specific variable will exceed a threshold value. We argue that the contemporaneous model is well suited to rational expectations applications (and pricing exercises), in that it does not require the initial regimes to be predetermined. We investigate the properties of the model and evaluate its finite-sample maximum likelihood performance. We also propose a method to determine the number of regimes based on a modified Hansen [1992. The likelihood ratio test under nonstandard conditions: testing the Markov switching model of GNP. Journal of Applied Econometrics 7, S61-S82.] procedure. Furthermore, we construct multiple-step ahead forecasts and evaluate the forecasting performance ofthe model. Finally, an empirical application of the short term interest rate yield is presented and discussed. (c) 2006 Elsevier B.V. All rights reserved.","Dueker, Michael J.; Sola, Martin; Spagnolo, Fabio",2007,10.1016/j.jeconom.2006.10.022,,wos,"This paper introduces the Contemporaneous Smooth Transition Threshold Autoregressive (C-STAR) model, an extension of existing STAR models. The C-STAR model's regime weights are influenced by the ex ante probability of a latent regime-specific variable exceeding a threshold. This model is particularly suitable for rational expectations and pricing applications as it doesn't necessitate predetermined initial regimes. The paper examines the model's properties, evaluates its maximum likelihood performance in finite samples, and proposes a method for determining the number of regimes using a modified Hansen procedure. It also covers multi-step ahead forecasting and presents an empirical application to short-term interest rate yields.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-13T16:04:01.531056
1914bca37c18fad2,Corporate bond pricing and the effects of endogenous default and call options,"Acharya and Carpenter [2002] model the value of a defaultable and callable bond. Focusing on the interaction between the call and default options, they note that the existence of one option delays the exercise of the other. This has strong empirical implications for duration and for the determinants of yield spread changes. For duration, when considered separately, both default and call shorten duration. However, when the interaction between the two risks is also accounted for, it is shown to lengthen duration. Since duration is equivalent to the interest-rate elasticity of the bond, the above analysis is also important for the interest-rate sensitivity of the bond yield spread. Sarkar and Hong [2004] also derive a structural model for pricing a fixed-price callable and defaultable bond, and their model has similar implications to that of Acharya and Carpenter [2002]. In this article we test the main implications of these theoretical models. In particular, we test the interest-rate elasticity of the call spread and that of the default spread, allowing for interaction between both spreads. We also examine the impact of both risks and their interaction on the effective duration of corporate bonds. In our tests we examine both bonds carrying a fixed-price call option and those carrying the newer and more popular makewhole option. We find evidence supporting the predictions of the two models. First, our data show a statistically significant interaction between default spreads and call spreads. Second, as the theory predicts, we show that, when considered separately, both default and call risks shorten duration for fixed-price callable bonds. When both risks are considered together, in general we find that the interaction term lengthens the effective duration of callable defaultable bonds. This finding is in agreement with the theory. The implication of our findings is that portfolio managers of callable corporate bonds, who use duration as an immunization tool or practice active rate anticipation strategies, must pay attention to the maturity of the bond. For example, it is clear that no risk adjustment is necessary when one manages A rated make-whole callable bonds with short and medium maturities. However, it is important to consider not only the impacts of default and call risks on estimating duration but also the interaction between these variables when one manages callable debt of all other maturities. We repeat our tests for bond portfolios stratified by ratings. In general, the results for the portfolios are also in agreement with the implications of the Acharya and Carpenter [2002] model. Another prediction made by Acharya and Carpenter is that both the noncallable and callable bonds' sensitivity to firm value decrease. For noncallable bonds, we find that the default spread's sensitivity to equity return is much greater for BBB rated bonds compared with AA and A rated bonds. Acharya and Carpenter [2002] also argue that callable bond's sensitivity to firm value is lower than that of a noncallable bond. In general, our results support this theoretical prediction, especially for make-whole bonds. The models of Acharya and Carpenter [2002] and Sarkar and Hong [2004] focus on pricing fixed-price callable bonds. However, the make-whole call provision became the standard for callable bonds in recent years, both in the U.S. and Canada. Powers and Tsyplakov [2004] derive a structural model for pricing make-whole callable bonds. Their model does not allow the study of duration similar to that of Acharya and Carpenter [2002] and Sarkar and Hong [2004]. Given the popularity of make-whole bonds, a possible direction for future research is the introduction of a structural bond-pricing model that allows studying the duration and yield-spread elasticity of these bonds. © 2010 Elsevier B.V., All rights reserved.","Jacoby, G.; Shiller, I.",2010,10.3905/jfi.2010.20.2.080,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957848360&doi=10.3905%2Fjfi.2010.20.2.080&partnerID=40&md5=93545e73cc89f0bfc89b6c60f801f529,scopus,"This study empirically tests the implications of theoretical models by Acharya and Carpenter (2002) and Sarkar and Hong (2004) on corporate bond pricing, focusing on the interaction between default and call options. The research examines how these risks affect bond duration and yield spreads, considering both fixed-price and make-whole call provisions. Findings support the models, indicating that the interaction between default and call risks generally lengthens effective duration, contrary to their individual effects. The study also investigates the sensitivity of bond prices to firm value and equity returns across different credit ratings, finding support for theoretical predictions.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:04:05.641023
078ff19bcec16a24,Cost-benefit analysis in a climate of change: setting social discount rates in the case of Ireland,"The global practice of Cost-Benefit Analysis (CBA), to analyse the welfare impacts of public investments, has undergone profound changes in recent years. The reforms in general practice have primarily been driven by the discussions of the implications of climate change and environmental degradation. Central to the discussion has been the social discount rate, used to value future costs and benefits in the present, and also the dual discount rates for ""environmental goods"", as goods that are of no, or of risky substitution. Official rates, in many nations, are calculated using the ""Ramsey"" formula. The literature has explored the relevant factors in this formula, but with less attention paid to the selection of the rate of future growth in consumption, or to the setting of dual discount rates in national practice guidance. Through considering the case of Ireland, this study demonstrates that the selection of growth rates in consumption, in the context of future uncertainty, requires the use of plausible scenarios, rather than historical trends or forecasts. By employing economic scenarios, alongside established values for the other factors, the main discount rate for Ireland is calculated in a range of 1.7 to 2.8 per cent. Seperately, a dual discount rate, for capital that cannot be replaced, is estimated at ≤1.3 per cent. The main discount rate is validated by comparison against discount rates found in the literature, applied in other comparable nations, and by the rate estimated from the real yield on government bonds. All four independent lines of evidence support the range estimated. This demonstrates that the Irish government's estimated discount rate, of 4.0 per cent, is not credible, and needs reduction, alongside introduction of dual discounting.",,2021,10.3934/gf.2021010,,proquest,"This study analyzes the social discount rate for Ireland, considering climate change implications. It argues for the use of plausible scenarios for future consumption growth and proposes a dual discount rate for non-replaceable capital. The calculated main discount rate ranges from 1.7% to 2.8%, and the dual rate is estimated at 1.3%. These findings challenge the Irish government's current 4.0% rate and advocate for its reduction and the adoption of dual discounting.",False,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:04:18.044873
446b6410f6523443,Counting the investor vote: Political business cycle effects on sovereign bond spreads in developing countries,"International business research has paid scant attention to whether and how electoral politics and economic policies affect foreign investment risk assessment, particularly in developing countries, where the last decade has seen both considerable foreign investment and domestic progress toward democratization and electoral competitiveness. We respond with development and testing of a framework using partisan and opportunistic political business cycle (PBC) theory to predict the investment risk perceived by investors holding sovereign bonds during 19 presidential elections in 12 developing countries from 1994 to 2000. Consistent with our framework, we find that bondholders perceive higher (lower) investment risk in the form of higher (lower) credit spreads on their sovereign bonds as right-wing (left-wing) political incumbents appear more likely to be replaced by left-wing (right-wing) challengers. For international business research, our findings illustrate the promise of PBC theory in explaining the election-period behavior of sovereign bondholders and, perhaps, other investors who also 'vote' in developing country elections and can substantially influence the price and availability of capital there. For developing country investors and states, our findings highlight the financial effects of democracy in action, and underscore the importance of state communication with investors during election periods. © 2005 Palgrave Macmillan Ltd. All rights reserved. © 2024 Elsevier B.V., All rights reserved.","Vaaler, P.M.; Schrage, B.N.; Block, S.A.",2005,10.1057/palgrave.jibs.8400111,https://www.scopus.com/inward/record.uri?eid=2-s2.0-14944342935&doi=10.1057%2Fpalgrave.jibs.8400111&partnerID=40&md5=682574fa0a86b86aad256960bf9d9a1c,scopus,"This study investigates the impact of electoral politics on sovereign bond spreads in developing countries, using partisan and opportunistic political business cycle theory. It finds that bondholders perceive higher investment risk (wider credit spreads) when left-wing challengers are likely to replace right-wing incumbents, and vice versa. The research suggests that political business cycle theory can explain investor behavior during elections and highlights the financial implications of democracy in developing nations.",False,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T16:04:41.311744
983a23dd8bacff46,Credit Debt Default Risk Assessment Based on the XGBoost Algorithm: An Empirical Study from China,"The bond market is an important part of China’s capital market. However, defaults have become frequent in the bond market in recent years, and consequently, the default risk of Chinese credit bonds has become increasingly prominent. Therefore, the assessment of default risk is particularly important. In this paper, we utilize 31 indicators at the macroeconomic level and the corporate microlevel for the prediction of bond defaults, and we conduct principal component analysis to extract 10 principal components from them. We use the XGBoost algorithm to analyze the importance of variables and assess the credit debt default risk based on the XGBoost prediction model through the calculation of evaluation indicators such as the area under the ROC curve (AUC), accuracy, precision, recall, and F1-score, in order to evaluate the classification prediction effect of the model. Finally, the grid search algorithm and k-fold cross-validation are used to optimize the parameters of the XGBoost model and determine the final classification prediction model. Existing research has focused on the selection of bond default risk prediction indicators and the application of XGBoost algorithm in default risk prediction. After optimization of the parameters, the optimized XGBoost algorithm is found to be more accurate than the original algorithm. The grid search and k-fold cross-validation algorithms are used to optimize the XGBoost model for predicting the default risk of credit bonds, resulting in higher accuracy of the proposed model. Our research results demonstrate that the optimized XGBoost model has a significantly improved prediction accuracy, compared to the original model, which is beneficial to improving the prediction effect for practical applications.",,2022,10.1155/2022/8005493,,proquest,"This study uses the XGBoost algorithm to assess credit debt default risk in China's bond market, employing 31 indicators reduced to 10 principal components. The model's performance is evaluated using AUC, accuracy, precision, recall, and F1-score, with parameters optimized via grid search and k-fold cross-validation. The optimized XGBoost model shows improved prediction accuracy compared to the original.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:04:54.748314
fe9ab6866fa2b491,"Credit Spreads, Leverage and Volatility: A Cointegration Approach","This work documents the existence of a cointegration relationship between credit spreads, leverage and equity volatility for a large set of US companies. It is shown that accounting for the long-run equilibrium dynamic between these variables is essential to correctly explain credit spread changes. Using a novel structural model in which equity is modeled as a compound option on the firm's assets, a new methodology for estimating the unobservable market value of the firm's assets and volatility is developed. The proposed model allows to significantly reduce the pricing errors in predicting credit spreads when compared with several structural models. In terms of correlation analysis, it is shown that not accounting for the long-run equilibrium equation embedded in an error correction mechanism (ECM) results into a misspecification problem when regressing a set of explanatory variables onto the spread changes. Once credit spreads, leverage and volatility are correctly modeled, thus allowing for a long-run equilibrium, the fit of the regressions sensibly increases if compared to the results of previous research. It is further shown that most of the cross-sectional variation of the spreads appears to be more driven by firm-specific characteristics rather than systematic factors.","Maglione, Federico",2022,10.3390/computation10090155,,wos,"This study investigates the cointegration relationship between credit spreads, leverage, and equity volatility for US companies. It proposes a structural model treating equity as a compound option on firm assets, enabling estimation of unobservable market values and volatility. The model improves credit spread prediction accuracy compared to existing structural models and highlights the importance of accounting for long-run equilibrium dynamics via an error correction mechanism. The findings suggest firm-specific characteristics are primary drivers of credit spread variation.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:05:06.746043
c93e69f76880bb85,"Credit growth, the yield curve and financial crisis prediction: Evidence from a machine learning approach","We develop early warning models for financial crisis prediction applying machine learning techniques on macrofinancial data for 17 countries over 1870–2016. Most nonlinear machine learning models outperform logistic regression in out-of-sample predictions and forecasting. We identify economic drivers of our machine learning models by applying a novel framework based on Shapley values, uncovering nonlinear relationships between the predictors and crisis risk. Throughout, the most important predictors are credit growth and the slope of the yield curve, both domestically and globally. A flat or inverted yield curve is of most concern when nominal interest rates are low and credit growth is high. © 2023 Elsevier B.V., All rights reserved.","Bluwstein, K.; Buckmann, M.; Joseph, A.; Kapadia, S.; Şimşek, Ö.",2023,10.1016/j.jinteco.2023.103773,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153873224&doi=10.1016%2Fj.jinteco.2023.103773&partnerID=40&md5=abb66d3fa385d12f8957f0e53e487141,scopus,"This study develops early warning models for financial crisis prediction using machine learning techniques on macrofinancial data from 17 countries spanning 1870-2016. The models, particularly nonlinear ones, demonstrate superior out-of-sample predictive performance compared to logistic regression. Key predictors identified through Shapley values include credit growth and the yield curve slope, with a flat or inverted yield curve being particularly concerning when combined with low nominal interest rates and high credit growth.",True,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T16:05:26.949778
a8284129b0b72d35,Credit rating algorithm of corporate bonds based on Gaussian process mixture model and improved K-means,"The primary challenge in credit analysis revolves around uncovering the correlation between repayment terms and yield to maturity, constituting the interest rate term structure-an essential model for corporate credit term evaluation. Presently, interest rate term structures are predominantly examined through economic theoretical models and quantitative models. However, predicting treasury bond yields remains a challenging task for both approaches. Leveraging the clustering analysis algorithm theory and the attributes of an insurance company’s customer database, this paper enhances the K-means clustering algorithm, specifically addressing the selection of initial cluster centers in extensive sample environments. Utilizing the robust data fitting and analytical capabilities of the Gaussian process mixture model, the study applies this methodology to model and forecast Treasury yields. Additionally, the research incorporates customer credit data from a property insurance company to investigate the application of clustering algorithms in the analysis of insurance customer credit. © 2024 Elsevier B.V., All rights reserved.","Xia, W.",2023,10.61091/jcmcc117-14,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184140053&doi=10.61091%2Fjcmcc117-14&partnerID=40&md5=db4b7e03f54cd2d82f22b94ad0140e56,scopus,"This paper proposes an enhanced K-means clustering algorithm for selecting initial cluster centers in large datasets, applied to a Gaussian process mixture model for forecasting Treasury yields. It also explores the use of clustering algorithms for analyzing insurance customer credit data.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-13T16:05:58.513103
17468938c29ef9d9,Currency Value,"We assess the properties of currency value strategies based on real exchange rates. We find that real exchange rates have predictive power for the cross-section of currency excess returns. However, adjusting real exchange rates for key country-specific fundamentals (productivity, the quality of export goods, net foreign assets, and output gaps) better isolates information related to the currency risk premium. In turn, the resultant measure of currency value displays considerably stronger predictive power for currency excess returns. Finally, the predictive information content in our currency value measure is distinct from that embedded in popular currency strategies, such as carry and momentum.","Menkhoff, Lukas; Sarno, Lucio; Schmeling, Maik; Schrimpf, Andreas",2017,10.1093/rfs/hhw067,,wos,This study investigates currency value strategies using real exchange rates and country-specific fundamentals. It finds that adjusting real exchange rates for factors like productivity and net foreign assets enhances their predictive power for currency excess returns. The proposed currency value measure is distinct from existing strategies like carry and momentum.,False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:06:01.595921
45985ece0e559f1f,Data-Driven and Mechanistic Soil Modeling for Precision Fertilization Management in Cotton,"This study introduces a novel methodology for predicting cotton yield by integrating machine learning (ML) with mechanistic soil modeling. This hybrid approach enhances yield prediction by combining data-driven ML techniques with soil process modeling. Using the developed yield model, yield curves for various nitrogen (N) levels can be constructed to identify the optimal N dose that maximizes yield. Estimating cotton N requirements is crucial, as growers often apply excessive N, exceeding the amount needed for maximum yield. By comparing the Mean Absolute Error (MAE) between predicted and observed cotton yield values across three ML algorithms, i.e., Random Forest (RF), XGBoost, and LightGBM, the RF model achieved the lowest error (422.6 kg/ha), outperforming XGBoost (446 kg/ha) and LightGBM (449 kg/ha). Additionally, the RF model exhibited high sensitivity to N fertilization, ranking N as the most influential variable in feature importance analysis. Furthermore, phosphorus (P) availability in the soil model was found to be a significant factor influencing the RF yield model, highlighting P’s crucial role in cotton growth and productivity. © 2025 Elsevier B.V., All rights reserved.","Iatrou, M.; Tziachris, P.; Bilias, F.; Kekelis, P.; Pavlakis, C.; Theofilidou, A.; Papadopoulos, I.; Strouthopoulos, G.; Giannopoulos, G.; Arampatzis, D.; Vergos, E.; Karydas, C.; Beslemes, D.; Aschonitis, V.",2025,10.3390/nitrogen6020029,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009251145&doi=10.3390%2Fnitrogen6020029&partnerID=40&md5=b1c1d36c3eaba2b309b31ebf8aaad2ca,scopus,"This study presents a hybrid approach combining machine learning (ML) with mechanistic soil modeling to predict cotton yield and optimize nitrogen (N) fertilization. The Random Forest (RF) model demonstrated the best performance in yield prediction, identifying N as the most influential variable and highlighting the importance of phosphorus (P) availability. This method aims to reduce excessive N application by growers.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:06:15.011892
e83cfb6b1bda5312,Decision-making during the credit crisis: Did the Treasury let commercial banks fail?,"Limited attention has been paid to the comparative fate of banks benefiting from Capital Purchase Program (CPP) funding and less fortunate banks subject to FDIC resolution. We address this omission by investigating two core issues. One is whether commercial banks that ended up being subject to FDIC resolution received CPP funds. The other is whether the non-allocation of CPP funds made FDIC receivership more likely for viable commercial banks. Our findings show almost no overlap between CPP-funded and FDIC-resolved commercial banks, but we provide evidence that a significant number of FDIC-resolved banks could have avoided receivership if they had been allocated CPP funding. By comparing estimated funding and resolution costs we also show that bailing out more banks would have been cost-efficient. While our results do not allow for any policy suggestion on the optimality of bailouts per se, they suggest that once a bailout program is already on the table, it is better to err on the side of rescuing too many rather than too few banks. © 2016 Elsevier B.V., All rights reserved.","Croci, E.; Hertig, G.; Nowak, E.",2016,10.1016/j.jempfin.2016.01.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962173589&doi=10.1016%2Fj.jempfin.2016.01.001&partnerID=40&md5=85c4a94fa1e40a43f82f5e97c5786c32,scopus,"This study investigates whether commercial banks that received Capital Purchase Program (CPP) funding were treated differently from those subject to FDIC resolution during the credit crisis. The findings indicate minimal overlap between CPP-funded and FDIC-resolved banks, but suggest that some FDIC-resolved banks might have avoided resolution with CPP funding. The analysis also implies that rescuing more banks would have been cost-efficient, advocating for a more inclusive bailout approach.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:06:22.427175
3237c5fa565c89c2,Decomposing the yield curve with linear regressions and survey information,"The decomposition of bond yields into term premiums and average expected future short rates is impaired by the limited availability of information about the dynamics of the expectations component. Therefore, many studies require the model-implied average expected future short rates to be close to short rate expectations from surveys. In this paper, I restrict the variance of changes in model-implied average expected future short rates to match the variance of changes in short rate expectations from surveys. The variance of changes in survey expectations is relatively similar across markets and thus provides a reliable source of additional information about the expectation formation of investors. Technically, I impose a nonlinear restriction to the term structure model of Adrian, Crump, and Moench (2013). I show that typical small sample problems of term structure estimations can be mitigated if the restriction on the variance of changes is imposed. However, the analysis also makes a case for unrestricted estimations if they are based on a dataset with a typical sample length in macro finance. © 2023 Elsevier B.V., All rights reserved.","Halberstadt, A.",2023,10.1016/j.qref.2023.07.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166646520&doi=10.1016%2Fj.qref.2023.07.002&partnerID=40&md5=dfd4e947091892f6484d399c6125a8f4,scopus,This paper proposes a method to decompose bond yields into term premiums and expected future short rates by incorporating survey information to constrain the variance of changes in expected future short rates. This approach aims to mitigate small sample issues in term structure estimations and improve the accuracy of yield curve models.,False,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T16:06:52.182236
4e00c304d3b4af2d,Deep Learning Model for Stock Excess Return Prediction Based on Nonlinear Random Matrix and Esg Factor,"Aiming at the problem that the traditional model has low accuracy in describing stock excess return, in order to further analyze the change law of stock excess, based on the nonlinear random matrix and esg factor theory, the traditional learning model is analyzed, and the corresponding optimized deep learning model is obtained by introducing the single ring theorem and statistical data. Through the analysis and research of related indexes, the change rules of different indexes are obtained, and the optimization model is used to calculate and forecast the excess return of stocks. The results show that the statistics and spectral radius show typical local linear variation with the increase of eigenvalue. The corresponding statistics show a trend of gradual increase. The corresponding spectral radius has a decreasing variation law, and the two curves have obvious symmetry at some eigenvalues. It can be seen from the change curves under different factors that the change trend of the yield curve is mainly affected by the investment factor, while the change rule of the specific value of the yield curve is controlled by the profit factor. This shows that the two factors have the same influence on the stock excess return. The influence of optimized deep learning model on stock excess index has typical linear characteristics, which can be divided into linear increase and linear decline according to different change rules. The basic type has the greatest influence, while the corresponding pattern analysis type has the least influence. Finally, the method of experimental verification is used to verify the stock excess data, and the results show that the optimized deep learning model can better characterize the experimental results. Therefore, the optimized deep learning model based on nonlinear random matrix and esg factor can carry out targeted analysis of different types of stock returns, thus improving research ideas and calculation methods for the application of deep learning model in different fields.",,2022,10.1155/2022/5239493,,proquest,"This study proposes an optimized deep learning model for stock excess return prediction, incorporating nonlinear random matrix theory and ESG factors. The model aims to improve accuracy over traditional methods by analyzing index changes and utilizing the single ring theorem. Results indicate that investment and profit factors influence yield curve trends and specific values, respectively. The deep learning model exhibits linear characteristics in its influence on stock excess returns, with the basic type having the most significant impact. Experimental verification confirms the model's effectiveness in characterizing stock excess data.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:06:58.647096
29abf8e325a84dfb,Deep treasury management for banks,"Retail banks use Asset Liability Management (ALM) to hedge interest rate risk associated with differences in maturity and predictability of their loan and deposit portfolios. The opposing goals of profiting from maturity transformation and hedging interest rate risk while adhering to numerous regulatory constraints make ALM a challenging problem. We formulate ALM as a high-dimensional stochastic control problem in which monthly investment and financing decisions drive the evolution of the bank's balance sheet. To find strategies that maximize long-term utility in the presence of constraints and stochastic interest rates, we train neural networks that parametrize the decision process. Our experiments provide practical insights and demonstrate that the approach of Deep ALM deduces dynamic strategies that outperform static benchmarks. © 2023 Elsevier B.V., All rights reserved.","Englisch, H.; Krabichler, T.; Müller, K.J.; Schwarz, M.",2023,10.3389/frai.2023.1120297,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152561995&doi=10.3389%2Ffrai.2023.1120297&partnerID=40&md5=b02754657e30e9df825a73854a19d290,scopus,"This paper presents a novel approach called Deep ALM, which uses neural networks to solve the complex Asset Liability Management (ALM) problem for retail banks. ALM aims to hedge interest rate risk while managing the balance sheet and adhering to regulations. The Deep ALM method formulates ALM as a high-dimensional stochastic control problem and trains neural networks to derive dynamic investment and financing strategies that outperform traditional methods.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:07:00.959740
827ebc1f4234f963,Deep-learning models for forecasting financial risk premia and their interpretations,"The measurement of financial risk premia, the amount that a risky asset will outperform a risk-free one, is an important problem in asset pricing. The noisiness and non-stationarity of asset returns makes the estimation of risk premia using machine learning (ML) techniques challenging. In this work, we develop ML models that solve the problems associated with risk premia forecasting by separating risk premia prediction into two independent tasks, a time series model and a cross-sectional model, and using neural networks with skip connections to enable their deep neural network training. These models are tested robustly with different metrics, and we observe that our models outperform several existing standard ML models. A known issue with ML models is their ‘black box’ nature, i.e. their opaqueness to interpretability. We interpret these deep neural networks using local approximation-based techniques that provide explanations for our model's predictions. © 2023 Elsevier B.V., All rights reserved.","Lo, A.W.; Singh, M.",2023,10.1080/14697688.2023.2203844,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163601916&doi=10.1080%2F14697688.2023.2203844&partnerID=40&md5=95327ef8a4d77eea1380fa58a81d947c,scopus,"This paper proposes deep learning models for forecasting financial risk premia, addressing challenges of noise and non-stationarity by separating prediction into time series and cross-sectional tasks. The models utilize neural networks with skip connections and are shown to outperform existing ML models. Interpretability of these 'black box' models is achieved through local approximation techniques.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:07:28.916916
bf5e5a0f65403ae9,DeepClue: Visual Interpretation of Text-Based Deep Stock Prediction,"The recent advance of deep learning has enabled trading algorithms to predict stock price movements more accurately. Unfortunately, there is a significant gap in the real-world deployment of this breakthrough. For example, professional traders in their long-term careers have accumulated numerous trading rules, the myth of which they can understand quite well. On the other hand, deep learning models have been hardly interpretable. This paper presents DeepClue, a system built to bridge text-based deep learning models and end users through visually interpreting the key factors learned in the stock price prediction model. We make three contributions in DeepClue. First, by designing the deep neural network architecture for interpretation and applying an algorithm to extract relevant predictive factors, we provide a useful case on what can be interpreted out of the prediction model for end users. Second, by exploring hierarchies over the extracted factors and displaying these factors in an interactive, hierarchical visualization interface, we shed light on how to effectively communicate the interpreted model to end users. Specially, the interpretation separates the predictables from the unpredictables for stock prediction through the use of intercept model parameters and a risk visualization design. Third, we evaluate the integrated visualization system through two case studies in predicting the stock price with financial news and company-related tweets from social media. Quantitative experiments comparing the proposed neural network architecture with state-of-the-art models and the human baseline are conducted and reported. Feedbacks from an informal user study with domain experts are summarized and discussed in details. The study results demonstrate the effectiveness of DeepClue in helping to complete stock market investment and analysis tasks.",L. Shi; Z. Teng; L. Wang; Y. Zhang; A. Binder,2019,10.1109/tkde.2018.2854193,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8408524,ieeexplore,"This paper introduces DeepClue, a system designed to visually interpret text-based deep learning models for stock price prediction. It addresses the interpretability gap between complex models and end-users by extracting and visualizing key predictive factors in an interactive, hierarchical interface. The system separates predictable from unpredictable elements and is evaluated through case studies and user feedback, demonstrating its effectiveness in investment and analysis tasks.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:07:51.946324
fbf80399751a0d02,"Default, liquidity, and crises: An econometric framework","This article presents a general discrete-time affine framework aimed at jointly modeling yield curves associated with different debtors. The underlying fixed-income securities may differ in terms of credit quality and/or in terms of liquidity. The risk factors follow conditionally Gaussian processes, with drifts and covariance matrices that are subject to regime shifts described by a Markov chain with (historical) non-homogenous transition probabilities. Bond prices are given by quasi-explicit formulas. The tractability of the framework is illustrated by the estimation of a term-structure model of the spreads between U.S. BBB-rated corporate bonds and Treasuries. Alternative applications are proposed, including a sector-contagion model as well as the explicit modeling of credit-rating transitions. © The Author, 2012. © 2013 Elsevier B.V., All rights reserved.","Monfort, A.; Renne, J.-P.",2013,10.1093/jjfinec/nbs020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875255758&doi=10.1093%2Fjjfinec%2Fnbs020&partnerID=40&md5=4142c53787af3e844a2eb0e5d250da50,scopus,"This article introduces a discrete-time affine framework to model yield curves for different debtors, considering credit quality and liquidity. It uses conditionally Gaussian risk factors with Markov-switching regime shifts and provides quasi-explicit bond pricing formulas. The framework is demonstrated by estimating a term-structure model for U.S. BBB-rated corporate bond spreads against Treasuries, with potential applications in sector contagion and credit-rating transitions.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:08:16.395993
0f6fe1c92923c038,"Demand for Information, Uncertainty, and the Response of US Treasury Securities to News","We use clickstream data to show that investors' demand for information about macroeconomic factors affecting the path of future interest rates is a measure of their uncertainty about this path. In particular, an increase in information demand ahead of influential economic announcements affecting investors' beliefs about future interest rates predicts a stronger reaction of U.S. Treasury note yields to these announcements, as it should if information demand positively covaries with uncertainty. This relationship does not vanish after using standard measures of uncertainty as predictors, suggesting that clickstream data contain unique information about investors' uncertainty.","Benamar, Hedi; Foucault, Thierry; Vega, Clara",2021,10.1093/rfs/hhaa072,,wos,"This study utilizes clickstream data to demonstrate that increased investor demand for information regarding macroeconomic factors influencing future interest rates serves as a proxy for uncertainty about those rates. Higher information demand preceding significant economic announcements correlates with a more pronounced reaction in U.S. Treasury note yields, consistent with information demand reflecting uncertainty. This finding persists even when controlling for conventional uncertainty measures, indicating that clickstream data offers distinct insights into investor uncertainty.",False,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:08:56.540506
96b3bcf64aff81eb,Density estimation for nonlinear parametric models with conditional heteroscedasticity,"This article studies density and parameter estimation problems for nonlinear parametric models with conditional heteroscedasticity. We propose a simple density estimate that is particularly useful for studying the stationary density of nonlinear time series models. Under a general dependence structure, we establish the root it consistency of the proposed density estimate. For parameter estimation, a Bahadur type representation is obtained for the conditional maximum likelihood estimate. The parameter estimate is shown to be asymptotically efficient in the sense that its limiting variance attains the Cramer-Rao lower bound. The performance of our density estimate is studied by simulations. (C) 2009 Elsevier B.V. All rights reserved.","Zhao, Zhibiao",2010,10.1016/j.jeconom.2009.09.013,,wos,"This article proposes a density estimation method for nonlinear parametric models with conditional heteroscedasticity, useful for analyzing stationary densities of nonlinear time series. It establishes root-n consistency for the density estimate and obtains a Bahadur-type representation for the conditional maximum likelihood estimate, proving its asymptotic efficiency. Simulation studies are included.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:09:07.232162
e1c46f7c4c1f6375,Developing a Composite Measure to Represent Information Flows in Networks: Evidence from a Stock Market,"There is increasing interest in information systems research to model information flows from different sources (e.g., social media, news) associated with a network of assets (e.g., stocks, products) and to study the economic impact of such information flows. This paper employs a design science approach and proposes a new composite metric, eigen attention centrality (EAC), as a proxy for information flows associated with a node that considers both attention to a node and coattention with other nodes in a network. We apply the EAC metric in the context of financial market where nodes are individual stocks and edges are based on coattention relationships among stocks. Composite information from different channels is used to measure attention and coattention. To evaluate the effectiveness of the EAC metric on predicting outcomes, we conduct an in-depth performance evaluation of the EAC metric by (1) using multiple linear and nonlinear prediction methods and (2) comparing EAC with a benchmark model without EAC and models with a set of alternative network metrics. Our analysis shows that EAC significantly outperforms other measures in predicting the direction and magnitude of abnormal returns of stocks. Besides, our EAC specification has better predictive performance than alternative specifications, and EAC outperforms direct attention in predicting abnormal returns. Using the EAC metric, we derive a stock portfolio and develop a trading strategy that provides significant and positive excess returns. Lastly, we find that composite information has significantly better predictive performance than separate information sources, and such superior performance owes to information from social media instead of traditional media.",,2022,10.1287/isre.2021.1066,,proquest,"This paper introduces a new composite metric, eigen attention centrality (EAC), to represent information flows in networks, specifically applied to stock markets. EAC considers both attention to and coattention with other nodes. The metric's effectiveness is evaluated using various prediction methods, showing it outperforms benchmark and alternative network metrics in predicting stock returns. A trading strategy based on EAC yields significant excess returns, and composite information, particularly from social media, proves more predictive than individual sources.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:09:12.396671
03db3ea92636934e,Development of hybrid weighted networks of RNN and DBN for facilitating the secure information system in cyber security using meta-heuristic improvement,"As communication and information technologies are integral to everyone’s daily activities, the significance of cybersecurity has become more pronounced due to the growing vulnerability of these technologies to cyber threats. Traditional cyber security systems use various preventive measures to secure the information and trust authentication methods are used to provide the essential security measure against cyber attacks. These methods are efficient and are also equipped to perform in real-world scenarios. However, the conventional cyber security system does not provide essential security against all types of cyber attacks as they are nature-distributed for controlling the systems. Securing these Distributed Control Systems is highly significant for providing a secure and risk-free operation of the connected systems from cyber attacks and other threats. Therefore, a novel method of risk prediction and risk mitigation is developed using the heuristic-based Hybrid Deep Weighted Networks for protecting the data in the information system. The recommended work is based on risk analysis and a cyber security framework built around the information technology security system. This model aimed to design the cyber security system by mitigating all the threats in that particular information system. The main aim is to predict the risk level and mitigate the security threats completely from the system. To achieve this, initially, the data are gathered from different sources and given to the HDWN. The HDWN is developed by combining the Deep Belief Network and a Recurrent Neural Network. These two networks help to predict the risk values of the threat. To attain the enhanced results, the parameters in this model are optimized by using a hybrid algorithm known as African Vultures with Water Wave Optimization, which is developed by combining the Water Wave Optimization algorithm with the African Vulture Optimization Algorithm. Another intention of this model is to mitigate the threat present in the system. Based on the predicted risk value, the system generates warning signals to alert the admin to block the communication. Thus, the threat and risk from the system are predicted and mitigated without interrupting the system’s performance. Finally, the performance validation is performed on the developed model by comparing it with diverse approaches, and the results demonstrate that the proposed model provides impressive outcomes in ensuring data security. © 2025 Elsevier B.V., All rights reserved.","Lakshman Naik, R.L.; Jain, S.; Bairam, M.",2025,10.1007/s11276-025-03955-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003692475&doi=10.1007%2Fs11276-025-03955-x&partnerID=40&md5=d4fbb63b13dd15cc3ea6db266c72f329,scopus,"This paper proposes a novel risk prediction and mitigation method for cybersecurity using a Hybrid Deep Weighted Network (HDWN), which combines Recurrent Neural Networks (RNN) and Deep Belief Networks (DBN). The model is optimized using a hybrid meta-heuristic algorithm (African Vultures with Water Wave Optimization) to predict risk levels and mitigate threats in information systems without interrupting performance. The authors claim their model achieves impressive results in ensuring data security compared to other approaches.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:09:18.229473
0866f9d53f9c17c0,Digital transformation: statistical evaluation of success factors of an ICO-campaign,"High rates of growth of the ICO market and its excess returns stipulate a significant interest of investors to projects which use initial token allocation (ICO) for attracting investments. This work takes into account the fact that even a potentially profitable project may fail to collect the required amount of money and to start placing tokens on the stock exchange. We are speaking about success of an ICO-campaign for fund raising. In order to estimate the influence of factors and check the suggested research hypotheses, logistic regression was used. The selection included 672 projects. As a dependent variable, the proportion of the amount collected in the ICO process from the required value is selected. Depending on the tested hypothesis the influencing variables took into account the presence of a pre-sale stage and the bounty program and also the price of the token, the upper limit of fund raising, the duration of the ICO-campaign and the number of team members. The work results allow token emitters to substantiate managing the success of the ICO-campaign of the project and the investors to see whether it deserves their attention. Besides, the obtained materials can be useful for specialists in forming the legal framework of token transactions.",,2019,10.1088/1757-899x/497/1/012087,,proquest,"This study uses logistic regression to evaluate success factors for Initial Coin Offering (ICO) campaigns, analyzing 672 projects. It considers variables like pre-sale stages, bounty programs, token price, fundraising limits, campaign duration, and team size to predict the proportion of funds raised relative to the target. The findings aim to help token issuers manage campaign success, investors assess project viability, and policymakers understand token transactions.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:12:25.442287
01ee1f71747e58fc,Discrete sine transform for multi-scale realized volatility measures,"In this study we present a new realized volatility estimator based on a combination of the multi-scale regression and discrete sine transform (DST) approaches. Multi-scale estimators similar to that recently proposed by Zhang (2006) can, in fact, be constructed within a simple regression-based approach by exploiting the linear relation existing between the market microstructure bias and the realized volatilities computed at different frequencies. We show how such a powerful multi-scale regression approach can also be applied in the context of the Zhou [Nonlinear Modelling of High Frequency Financial Time Series, pp. 109-123, 1998] or DST orthogonalization of the observed tick-by-tick returns. Providing a natural orthonormal basis decomposition of observed returns, the DST permits the optimal disentanglement of the volatility signal of the underlying price process from the market microstructure noise. The robustness of the DST approach with respect to the more general dependent structure of the microstructure noise is also shown analytically. The combination of the multi-scale regression approach with DST gives a multi-scale DST realized volatility estimator similar in efficiency to the optimal Cramer-Rao bounds and robust against a wide class of noise contamination and model misspecification. Monte Carlo simulations based on realistic models for price dynamics and market microstructure effects show the superiority of DST estimators over alternative volatility proxies for a wide range of noise-to-signal ratios and different types of noise contamination. Empirical analysis based on six years of tick-by-tick data for the S&P 500 index future, FIB 30, and 30 year U.S. Treasury Bond future confirms the accuracy and robustness of DST estimators for different types of real data. © 2012 Taylor and Francis Group, LLC. © 2012 Elsevier B.V., All rights reserved.","Curci, G.; Corsi, F.",2012,10.1080/14697688.2010.490561,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857188308&doi=10.1080%2F14697688.2010.490561&partnerID=40&md5=2c4e6b57d7f22d1af8754ae1cd5f005c,scopus,"This study introduces a novel realized volatility estimator by integrating multi-scale regression with the discrete sine transform (DST). The DST provides an orthonormal basis for decomposing returns, effectively separating volatility signals from microstructure noise. The proposed multi-scale DST estimator demonstrates high efficiency, robustness against noise and model misspecification, and outperforms alternative volatility proxies in Monte Carlo simulations. Empirical analysis on S&P 500 index futures, FIB 30, and U.S. Treasury Bond futures confirms its accuracy and robustness.",True,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:13:41.018188
8d2944af2307c048,Distressed debt prices and recovery rate estimation,"This paper has two purposes. First, it uses distressed debt prices to estimate recovery rates at default. In this regard, estimates are obtained for three recovery rate models: recovery of face value, recovery of Treasury, and recovery of market value. We show that identifying the economic default date, as distinct from the recorded default date, is crucial to obtaining unbiased estimates. The economic default date is defined to be the first date when the market prices the firm's debt as if it has defaulted. An implication is that the standard industry practice of using 30-day post default prices to compute recovery rate yields biased estimates. Second, we construct and estimate a distressed debt pricing model. We use this model to implicitly estimate the parameters of the embedded recovery rate process and to price distressed debt. Our distressed debt pricing model fits market prices well, with an average pricing error of less than one basis point.","Guo, Xin; Jarrow, Robert A.; Lin, Haizhi",2008,10.1007/s11147-009-9029-2,,wos,"This paper estimates recovery rates at default using distressed debt prices and proposes three models for recovery rates: recovery of face value, Treasury, and market value. It highlights the importance of identifying the economic default date over the recorded one for unbiased estimates and suggests that standard industry practices using post-default prices are biased. The study also develops and estimates a distressed debt pricing model, which implicitly estimates recovery rate parameters and prices distressed debt, showing a high degree of accuracy.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:13:45.432572
dfa20513b40f2c55,Diving into recession: the collective knowledge of online users as an early warning system for recessionary expectations,"As concerns about economic downturns manifest in online discussions, we investigate whether sentiment extracted from social media can serve as an early warning signal for recessionary pressures. Using a dataset of Twitter (X) posts related to economic prospects, we apply a range of sentiment analysis techniques, including a lexicon and rule-based method (VADER) and deep learning approaches (GPT and BERT). We assess the relationship between online sentiment and key recession indicators, such as the yield curve and GDPNow forecasts, using a combination of econometric and machine learning methods. In addition, we perform a comparative evaluation of sentiment classification techniques, incorporating both traditional models and deep learning architectures. Our results confirm that Twitter discussions precede changes in recessionary indicators and can thus provide forward-looking insights into economic sentiment. Furthermore, the comparative analysis reveals variations in sentiment detection across different methodologies, emphasizing the importance of selecting appropriate approaches in economic forecasting. © 2025 Elsevier B.V., All rights reserved.","Hayawi, K.; Shahriar, S.; Samuel Mathew, S.S.; Polyzos, E.; Ganguli, K.K.",2026,10.1016/j.im.2025.104252,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105015563853&doi=10.1016%2Fj.im.2025.104252&partnerID=40&md5=a7e2b0bfc6dee3e9b87a6eddcf573d18,scopus,"This study explores the use of online user sentiment, specifically from Twitter (X) posts, as an early warning system for recessionary expectations. It employs various sentiment analysis techniques (VADER, GPT, BERT) and econometric/machine learning methods to correlate online sentiment with recession indicators like the yield curve and GDPNow forecasts. The findings suggest that Twitter discussions can precede changes in these indicators, offering forward-looking economic insights, and highlight the importance of choosing appropriate sentiment analysis methodologies.",True,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T16:14:04.297104
bb9ac1ae6fbd153b,Do Post-Corona European Economic Policies Lift Growth Prospects? Exploring an ML-Methodology,"This article explores the determinants of people’s growth prospects in survey data as well as the impact of the European recovery fund to future growth. The focus is on the aftermath of the Corona pandemic, which is a natural limit to the sample size. We use Eurobarometer survey data and macroeconomic variables, such as GDP, unemployment, public deficit, inflation, bond yields, and fiscal spending data. We estimate a variety of panel regression models and develop a new simulation-regression methodology due to limitation of the sample size. We find the major determinant of people’s growth prospect is domestic GDP per capita, while European fiscal aid does not significantly matter. In addition, we exhibit with the simulation-regression method novel scientific insights, significant outcomes, and a policy conclusion alike.",,2022,10.3390/jrfm15030120,,proquest,"This article investigates the determinants of individual growth prospects and the impact of the European recovery fund on future growth in the post-Corona era. Using Eurobarometer survey data and macroeconomic variables, the study employs panel regression models and a novel simulation-regression methodology due to sample size limitations. Key findings indicate that domestic GDP per capita is the primary driver of growth prospects, while European fiscal aid shows no significant impact. The simulation-regression method yields novel scientific insights and policy conclusions.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:14:19.013798
a913bfc8378c3d14,Do asymmetric and nonlinear adjustments explain the forward premium anomaly?,"This paper explores some of the asymmetries and nonlinearities in an attempt to throw light on the forward premium anomaly, where spot exchange rate returns are typically found to be negatively correlated with the lagged interest rate differential and lead to an apparent rejection of Uncovered Interest Parity (UIP). The approach in this paper is motivated by some recent theoretical literature on the limits to speculation and hysteresis. The paper estimates Logistic Smooth Transition Dynamic Regression (LSTR) models with a variety of transition variables, including the lagged forward premium, monetary and income fundamentals and also variables associated with time varying risk premium, including the conditional variances of some fundamentals. Results are reported for nine different currencies. Many of the estimated LSTR models provide evidence for the existence of an outer regime that is consistent with UIP. Estimation of the standard forward premium regression on observations falling in these regimes across the sample is moderately supportive of UIP holding in the outer regime. A simulation experiment also suggests that an LSTR dgp can produce data consistent with the anomaly. However, parameter estimation issues leads to considerable uncertainty with the estimated transition functions and hence imprecise definitions of regimes. The results are an interesting step in the direction of understanding the nonlinear dimension of the problem without fully resolving the anomaly.",,2006,10.1016/j.jimonfin.2005.10.002,,proquest,"This paper investigates the forward premium anomaly, where spot exchange rate returns are negatively correlated with lagged interest rate differentials, potentially rejecting Uncovered Interest Parity (UIP). Using Logistic Smooth Transition Dynamic Regression (LSTR) models, the study explores asymmetries and nonlinearities, incorporating variables like lagged forward premium, monetary fundamentals, and risk premium indicators. While some results suggest UIP holds in specific 'outer regimes,' the anomaly is not fully resolved due to estimation uncertainties. The findings indicate that nonlinear dynamics might contribute to the anomaly but require further investigation.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:14:42.430428
066db9df3dc96c98,Does extreme climate concern drive equity premiums? Evidence from China,"We construct an extreme climate concern indicator (ECC) on the basis of the coverage of the extreme climate news reports. First, ECC significantly negatively forecasts stock market returns in subsequent months. The predictability of ECC returns outperforms alternative confidence indicators and economic predictors over both in-sample and out-of-sample periods. Second, relative to before the Paris Agreement entered into force, extreme climate concerns prominently enhanced the forecasting capabilities after the signing of the Paris Agreement. Third, the return prediction accuracy of ECC in periods of low climate concern is significantly greater than that in periods of high climate concern, which is also consistent with the limited attention of investors. Finally, ECC substantially brings appreciable economic gains to investors, and the relevant empirical results pass a series of robustness tests.",,2024,10.1057/s41599-024-03705-y,,proquest,"This study develops an ""extreme climate concern"" (ECC) indicator using news reports to predict stock market returns. The ECC negatively forecasts returns, outperforming other predictors. Its predictive power increased after the Paris Agreement and is stronger during periods of low climate concern, suggesting investor attention limits. The ECC also generates economic gains for investors.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:14:47.445380
cfa12aa6640fe808,Dynamic semiparametric factor models in risk neutral density estimation,"Dynamic semiparametric factor models (DSFM) simultaneously smooth in space and are parametric in time, approximating complex dynamic structures by time invariant basis functions and low dimensional time series. In contrast to traditional dimension reduction techniques, DSFM allows the access of the dynamics embedded in high dimensional data through the lower dimensional time series. In this paper, we study the time behavior of risk assessments from investors facing random financial payoffs. We use DSFM to estimate risk neutral densities from a dataset of option prices on the German stock index DAX. The dynamics and term structure of risk neutral densities are investigated by Vector Autoregressive (VAR) methods applied on the estimated lower dimensional time series.","Giacomini, Enzo; Haerdle, Wolfgang; Kraetschmer, Volker",2009,10.1007/s10182-009-0115-4,,wos,"This paper introduces Dynamic Semiparametric Factor Models (DSFM) for estimating risk-neutral densities from option prices. DSFM models complex dynamics using time-invariant basis functions and low-dimensional time series, allowing insights into high-dimensional data. The study applies DSFM to DAX index option prices to analyze the dynamics and term structure of risk-neutral densities using Vector Autoregressive (VAR) methods on the estimated time series.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:15:14.960920
4e1455a476507192,ESG and corporate credit spreads,"Purpose: The authors investigate the implications of environmental, social and governance (ESG) practices of firms for the pricing of their credit default swaps (CDS). In doing so, the authors compare European and US firms and consider nonlinear and indirect effects. This complements the previous literature focusing on linear and direct effects using bond yields and credit ratings of US firms. Design/methodology/approach: For this purpose, the authors apply fixed effects regressions on a comprehensive panel data set of US and European firms. Further, nonlinear and indirect effects are investigated utilizing quantile regressions and a path analysis. Findings: The evidence indicates that higher ESG ratings mitigate credit risks of US and European firms from 2007 to 2019. The risk mitigation effect is U-shaped across ESG quantiles, which is consistent with opposing effects of growing stakeholder influence capacity and diminishing marginal returns on ESG investments. The authors further reveal a mediating indirect volatility channel that substantially amplifies the direct effect of ESG on credit risk. A one-standard-deviation improvement in ESG ratings is estimated to reduce CDS spreads of low, medium and high ESG firms by approximately 4%, 8% and 3%, respectively. Originality/value: This is the first study to examine whether credit markets reflect regional differences between Europe and the US with regard to the ESG-CDS-relationship. In addition, this paper contributes to the existing literature by investigating differences in the response of CDS spreads across ESG quantiles and to study potential indirect channels connecting ESG and CDS spreads using structural credit risk variables. © 2022 Elsevier B.V., All rights reserved.","Barth, F.; Hübel, B.; Scholz, H.",2022,10.1108/jrf-03-2021-0045,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124343222&doi=10.1108%2FJRF-03-2021-0045&partnerID=40&md5=17a558b0787d2116f8797fc6df2d9467,scopus,"This study investigates the impact of ESG practices on credit default swap (CDS) spreads for US and European firms from 2007 to 2019. It finds that higher ESG ratings mitigate credit risks, with a U-shaped effect across ESG quantiles, suggesting a balance between stakeholder influence and diminishing marginal returns on ESG investments. The research also identifies an indirect volatility channel that amplifies the ESG-credit risk relationship. The study utilizes fixed effects regressions, quantile regressions, and path analysis to explore these effects, differentiating itself by examining regional differences between Europe and the US and investigating nonlinear and indirect channels.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:15:17.531097
5d75205f29fc9e07,ESTIMATING WEIGHTED BANK BETA INDEX UNDER MACRO EFFECTS IN VIETNAM IN INDUSTRY 4.0 AND ROLES OF DIGITAL TRANSFORMATION FOR BETTER RISK MANAGEMENT INFORMATION SYSTEM,"The applications of mathematics in finance have been developed widely in recent years. In our study, authors aim to propose weighted beta index formula in banking industry and then, factors that affect bank sustainable development, as well as risk management policies and strategies for commercial banks in Vietnam financial market. This study mainly use combination of quantitative methods (statistics, calculation formulas) and qualitative methods including synthesis, inductive and explanatory methods. Our study results show that first, because mean value of weighted beta in period 2011-2020 higher than beta in 2011-2016 time, we need to pay attention to risk management solution in bank system. Second, as CPI has negative impact while G, IM and R has positive impact on weighted beta: and Risk free rate have higher effects on market risks of banks, Ministry of Finance, State bank of Vietnam and relevant agencies need to control GDP growth as well as rates of Treasury bonds and lending rate (not increase so much) toward benefits for managing risk. And do not need to reduce CPI too much. Then we also mention the roles of digital data and transformation to help us to build better risk model and management information system at banks. © 2025 Elsevier B.V., All rights reserved.","Xuan Que, H.; Tran Ngoc Huy, D.; Duc Thang, T.; Van Thanh, T.; Hoang, H.",2022,10.31407/ijees12.338,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172739866&doi=10.31407%2Fijees12.338&partnerID=40&md5=9077207f47188466c73640dbdb313952,scopus,"This study proposes a weighted beta index formula for the Vietnamese banking industry, analyzing factors affecting bank development and risk management. It uses quantitative and qualitative methods, finding that the weighted beta increased from 2011-2016 to 2011-2020, necessitating risk management attention. CPI negatively impacts weighted beta, while GDP, imports, and interest rates positively impact it. The study also highlights the role of digital transformation in improving risk management information systems.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:15:20.396191
7b3ea689637650bb,Early warning strategies for corporate operational risk: A study by an improved random forest algorithm using FCM clustering,"To enhance the accuracy and response speed of the risk early warning system, this study develops a novel early warning system that combines the Fuzzy C-Means (FCM) clustering algorithm and the Random Forest (RF) model. Firstly, based on operational risk theory, market risk, research and development risk, financial risk, and human resource risk are selected as the primary indicators for enterprise risk assessment. Secondly, the Criteria Importance Through Intercriteria Correlation (CRITIC) weight method is employed to determine the importance of these risk indicators, thereby enhancing the model's prediction ability and stability. Following this, the FCM clustering algorithm is utilized for pre-processing sample data to improve the efficiency and accuracy of data classification. Finally, an improved RF model is constructed by optimizing the parameters of the RF algorithm. The data selected is mainly from RESSET/DB, covering the issuance, trading, and rating data of fixed-income products such as bonds, government bonds, and corporate bonds, and provides basic information, net value, position, and performance data of funds. The experimental results show that the model achieves an F1 score of 87.26%, an accuracy of 87.95%, an Area under the Curve (AUC) of 91.20%, a precision of 89.29%, and a recall of 87.48%. They are respectively 6.45%, 4.45%, 5.09%, 4.81%, and 3.83% higher than the traditional RF model. In this study, an improved RF model based on FCM clustering is successfully constructed, and the accuracy of risk early warning models and their ability to handle complex data are significantly improved.To enhance the accuracy and response speed of the risk early warning system, this study develops a novel early warning system that combines the Fuzzy C-Means (FCM) clustering algorithm and the Random Forest (RF) model. Firstly, based on operational risk theory, market risk, research and development risk, financial risk, and human resource risk are selected as the primary indicators for enterprise risk assessment. Secondly, the Criteria Importance Through Intercriteria Correlation (CRITIC) weight method is employed to determine the importance of these risk indicators, thereby enhancing the model's prediction ability and stability. Following this, the FCM clustering algorithm is utilized for pre-processing sample data to improve the efficiency and accuracy of data classification. Finally, an improved RF model is constructed by optimizing the parameters of the RF algorithm. The data selected is mainly from RESSET/DB, covering the issuance, trading, and rating data of fixed-income products such as bonds, government bonds, and corporate bonds, and provides basic information, net value, position, and performance data of funds. The experimental results show that the model achieves an F1 score of 87.26%, an accuracy of 87.95%, an Area under the Curve (AUC) of 91.20%, a precision of 89.29%, and a recall of 87.48%. They are respectively 6.45%, 4.45%, 5.09%, 4.81%, and 3.83% higher than the traditional RF model. In this study, an improved RF model based on FCM clustering is successfully constructed, and the accuracy of risk early warning models and their ability to handle complex data are significantly improved.",,2025,10.1371/journal.pone.0318491,,proquest,"This study proposes an improved early warning system for corporate operational risk by integrating Fuzzy C-Means (FCM) clustering with a Random Forest (RF) model. It identifies key risk indicators (market, R&D, financial, human resource), uses the CRITIC method for weighting, and employs FCM for data pre-processing. The improved RF model demonstrates enhanced accuracy and performance compared to the traditional RF model, with reported metrics including an F1 score of 87.26% and AUC of 91.20%. Data sources include RESSET/DB for fixed-income products.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:15:40.667373
7b6e0566cefac712,Econometric Model Using Arbitrage Pricing Theory and Quantile Regression to Estimate the Risk Factors Driving Crude Oil Returns,"This work presents a novel approach to determining the risk and return of crude oil stocks by employing Arbitrage Pricing Theory and Quantile Regression. Arbitrage Pricing Theory identifies the risk factors likely to impact crude oil returns. Subsequently, Quantile Regression estimates the relationship between the selected factors and the returns across different distribution quantiles. The West Texas Intermediate (WTI) crude oil price is used in this study as a benchmark for crude oil prices. WTI’s price fluctuations can significantly impact the performance of global crude oil stocks and, subsequently, the global economy. Various statistical measures are used in this study to determine the proposed model's stability. The results show that changes in WTI returns can have varying effects depending on market conditions and levels of volatility. This study emphasizes the influence of structural discontinuities on returns. These are likely generated by changes in the global economy and the unpredictable demand for crude oil during the pandemic. The inclusion of pandemic, geopolitical, and inflation-related explanatory variables adds uniqueness to the study as it considers current global events that can affect crude oil returns. Findings show that the key factors that pose significant risks to returns are industrial production, inflation, the global price of energy, the shape of the yield curve, and global economic policy uncertainty. This implies that while making investment decisions in WTI futures, investors should pay particular attention to these elements. © 2024 Elsevier B.V., All rights reserved.","Maitra, S.; Mishra, V.; Kundu, S.; Chopra, M.",2024,10.62527/joiv.8.1.2268,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189629613&doi=10.62527%2Fjoiv.8.1.2268&partnerID=40&md5=91a6c326b4237fac4db0eac49a9b571c,scopus,"This study uses Arbitrage Pricing Theory and Quantile Regression to analyze crude oil stock returns, focusing on the West Texas Intermediate (WTI) benchmark. It identifies risk factors like industrial production, inflation, energy prices, yield curve shape, and economic policy uncertainty, highlighting their varying impact across different market conditions and the influence of structural discontinuities such as the pandemic. The model incorporates pandemic, geopolitical, and inflation variables.",True,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:15:53.767819
593bb4ca69c5d955,Empirical Asset Pricing via Machine Learning,"We perform a comparative analysis of machine learning methods for the canonical problem of empirical asset pricing: measuring asset risk premiums. We demonstrate large economic gains to investors using machine learning forecasts, in some cases doubling the performance of leading regression-based strategies from the literature. We identify the best-performing methods (trees and neural networks) and trace their predictive gains to allowing nonlinear predictor interactions missed by other methods. All methods agree on the same set of dominant predictive signals, a set that includes variations on momentum, liquidity, and volatility.Authors have furnished an Internet Appendix, which is available on the Oxford University Press Web site next to the link to the final published paper online.",,2020,10.1093/rfs/hhaa009,,proquest,"This paper compares machine learning methods for empirical asset pricing, specifically measuring asset risk premiums. It shows significant economic gains for investors using machine learning forecasts, outperforming traditional regression-based strategies. The study identifies trees and neural networks as top performers, attributing their success to capturing nonlinear predictor interactions. Key predictive signals identified include momentum, liquidity, and volatility.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:15:56.898466
e504e29bd4408ea9,Empirical asset pricing with nonlinear risk premia,"We introduce a new model for the joint dynamics of the S&P 100 index and the VXO implied volatility index. The nonlinear specification of the variance process is designed to simultaneously accommodate extreme persistence and strong mean reversion. This grants superior forecasting power over the standard (linear) specifications for implied variance forecasting.We obtain statistically significant predictions in an out-of-sample exercise spanning several market crashes starting 1986 and including the recent subprime crisis. The model specification is possible through a simple continuous-time no-arbitrage asset pricing framework that combines semi-analytic pricing with a nonlinear specification for the market price of risk. © The Author, 2013. Published by Oxford University Press. All rights reserved. © 2014 Elsevier B.V., All rights reserved.","Mijatović, A.; Schneider, P.",2014,10.1093/jjfinec/nbt018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902659724&doi=10.1093%2Fjjfinec%2Fnbt018&partnerID=40&md5=1106d2afa715a821243ef162a47eabd7,scopus,"This paper proposes a new model for the joint dynamics of the S&P 100 index and the VXO implied volatility index. The model features a nonlinear variance process that captures both extreme persistence and mean reversion, leading to improved out-of-sample forecasts for implied variance, particularly during market crashes. The framework is based on a no-arbitrage asset pricing model with a nonlinear market price of risk.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:15:59.263382
de032d8bca7f4579,Empirical evidence on the Euler equation for consumption in the US,"Recently developed econometric methods, that are robust to weak instruments and exploit information in possible structural changes, are applied to study the Euler equation for consumption using aggregate US post-war data. Several extensions to the baseline Euler equation model are investigated. The results are insensitive to using linear versus nonlinear specifications, different instruments or different consumption data, but they are very sensitive to asset returns. With risk-free returns, the elasticity of intertemporal substitution is tightly estimated around zero, while with stock market returns, it is significantly positive but very imprecisely estimated. There is no evidence of parameter instability. © 2021 Elsevier B.V., All rights reserved.","Ascari, G.; Magnusson, L.M.; Mavroeidis, S.",2021,10.1016/j.jmoneco.2019.12.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076864764&doi=10.1016%2Fj.jmoneco.2019.12.004&partnerID=40&md5=977ffefb7cfe3b8791af31afbd50e27f,scopus,This study applies advanced econometric methods to US post-war aggregate data to analyze the Euler equation for consumption. It explores various model extensions and finds that results are robust to specification choices but sensitive to asset returns. The elasticity of intertemporal substitution is estimated near zero with risk-free returns and imprecisely positive with stock returns. No parameter instability was detected.,True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:16:03.047576
d1f11092e4cf4435,Enhancing stock timing predictions based on multimodal architecture: Leveraging large language models (LLMs) for text quality improvement,"This study aims to enhance stock timing predictions by leveraging large language models (LLMs), specifically GPT-4, to filter and analyze online investor comment data. Recognizing challenges such as variable comment quality, redundancy, and authenticity issues, we propose a multimodal architecture that integrates filtered comment data with stock price dynamics and technical indicators. Using data from nine Chinese banks, we compare four filtering models and demonstrate that employing GPT-4 significantly improves financial metrics like profit-loss ratio, win rate, and excess return rate. The multimodal architecture outperforms baseline models by effectively preprocessing comment data and combining it with quantitative financial data. While focused on Chinese banks, the approach can be adapted to broader markets by modifying the prompts of large language models. Our findings highlight the potential of LLMs in financial forecasting and provide more reliable decision support for investors. © 2025 Elsevier B.V., All rights reserved.","Chen, M.; Tang, Y.; Qi, Q.; Dai, H.; Lin, Y.; Ling, C.; Li, T.",2025,10.1371/journal.pone.0326034,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008471640&doi=10.1371%2Fjournal.pone.0326034&partnerID=40&md5=98a5d95e4c7ae2a55e9367ab9130232b,scopus,"This study enhances stock timing predictions by using GPT-4 to filter and analyze online investor comments, integrating this data with stock prices and technical indicators. The multimodal architecture, tested on Chinese banks, significantly improved financial metrics compared to baseline models, demonstrating the value of LLMs in financial forecasting.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:16:07.348433
02d7fb5a3d6611e6,Equilibrium forward curves for commodities,"We develop an equilibrium model of the term structure of forward prices for storable commodities. As a consequence of a nonnegativity constraint on inventory, the spot commodity has an embedded timing option that is absent in forward contracts. This option's value changes over time due to both endogenous inventory and exogenous transitory shocks to supply and demand. Our model makes predictions about Volatilities of forward prices at different horizons and shows how conditional violations of the Samuelson effect occur. We extend the model to incorporate a permanent second factor and calibrate the model to crude oil futures data.","Routledge, BR; Seppi, DJ; Spatt, CS",2000,10.1111/0022-1082.00248,,wos,"This paper presents an equilibrium model for commodity forward prices, considering inventory non-negativity and its impact on timing options. It analyzes volatility and the Samuelson effect, extends the model with a second factor, and calibrates it to crude oil futures data.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:16:10.697726
e533d86ac723242f,Equilibrium modeling of asset prices: Rationality versus rules of thumb,"General equilibrium models with representative agents have proved to be inadequate descriptions of U.S. financial data. I present a model with heterogeneous agents, optimizers, and nonoptimizers that exhibits high stock-price volatility and mimics empirical regularities found in U.S. consumption, stock return, and three-month treasury-bill return data. The simulation and estimation of the model are performed using a new technique called “backsolving,” which is of independent interest to researchers attempting to solve nonlinear, stochastic models. © 1990 American Statistical Association. © 2016 Elsevier B.V., All rights reserved.","Ingram, B.F.",1990,10.1080/07350015.1990.10509781,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910958135&doi=10.1080%2F07350015.1990.10509781&partnerID=40&md5=c2de1a714026f6f119f9fc2871c0c81a,scopus,"This paper presents a heterogeneous agent model that better explains U.S. financial data than traditional representative agent models. It incorporates both optimizing and non-optimizing agents to capture high stock-price volatility and empirical regularities in consumption and returns. The study also introduces a novel 'backsolving' technique for solving nonlinear, stochastic models.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:16:13.830363
f3a9a0a79144a932,"Equity market valuation, systematic risk and monetary policy","This study examines the relationship between equity market valuation and risk indicators that portend economic downswings. The indicators are implied options volatility, Treasury-Eurodollar (TED) spread and exchange rate. While implied volatility captures market risk in that it reflects the fear factor embedded in the price of an option, TED spread reflects the default risk premium that is priced into a key short-term credit instrument. Equity markets often show a tendency to reflect the incidence of these risk factors. And because they provide valuable information about the health of the economy, many have argued that equity market valuation be taken into account in the formulation of monetary policy. Results of this study not only show a statistically significant inverse relationship between the stock market and these risk factors, but also evidence of a cointegration. In a variance decomposition of the series, we find that equity valuation is a major contributor to the forecast error variances of each of the risk indicators, a finding that lends tacit support to the argument that risk indicators associated with the equity market be considered in monetary policy decisions.","Obi, Pat; Dubihlela, Job; Choi, Jeong-Gil",2012,10.1080/00036846.2011.579065,,wos,"This study investigates the link between equity market valuation and risk indicators (implied options volatility, TED spread, exchange rate) that signal economic downturns. It finds a significant inverse relationship and cointegration between stock market and these risk factors. The findings suggest that equity market valuation significantly contributes to the forecast error variances of risk indicators, supporting its consideration in monetary policy.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:16:16.514752
b0057b5ec8318381,Equity returns of financial institutions and the pricing of interest rate risk,"This study investigates the issue of whether financial intermediaries' common stock returns incorporate a risk premium for their inherent exposure to unexpected changes in interest rates. A wide range of financial institutions is employed to test the hypothesis that the interest rate risk is priced by capital markets. In addition, the above sample is extended by incorporating firms from the non-financial sector. A two-factor model with the market portfolio and the changes in market yields, as exogenously specified risk variables, is employed. The model is estimated via a seemingly unrelated regression estimation (SURE) framework with both cross-equation restrictions and within equation nonlinear constraints on the parameters. The findings indicate that financial institutions' equity returns incorporate a risk premium for their exposure to market yields' surprises. The return generating function of the insurance business could be further explained by an additional factor such as currency movements. It is also empirically supported that the market premium drops out from the estimation process. When commercial and industrial firms are included in the estimation process, the findings unveil a reduction in the magnitude of the interest rate risk premium. © 2005 Taylor & Francis Group Ltd. © 2008 Elsevier B.V., All rights reserved.","Staikouras, S.K.",2005,10.1080/09603100500039557,https://www.scopus.com/inward/record.uri?eid=2-s2.0-17744384695&doi=10.1080%2F09603100500039557&partnerID=40&md5=081d75e8784219025c52e20bc5c362a1,scopus,"This study examines if financial intermediaries' stock returns reflect a risk premium for unexpected interest rate changes. Using a diverse sample of financial institutions and extending it to non-financial firms, the research employs a two-factor model (market portfolio and market yield changes) estimated with SURE. Results show that financial institutions' equity returns do include a risk premium for market yield surprises, with insurance firms potentially influenced by currency movements. The market premium is found to be insignificant. Including commercial and industrial firms reduces the magnitude of the interest rate risk premium.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:16:18.798090
b186ed887412ad85,"Equity risk premiums (ERP): Determinants, estimation and implications - A post-crisis update","The risk premium is a fundamental and critical component in portfolio management, corporate finance and valuation. Given its importance, it is surprising that more attention has not been paid in practical terms to estimation issues. In this paper, we began by looking at the determinants of equity risk premiums including macro economic volatility, investor risk aversion and behavioral components. We then looked at the three basic approaches used to estimate equity risk premiums - the survey approach, where investors or managers are asked to provide estimates of the equity risk premium for the future, the historical return approach, where the premium is based upon how well equities have done in the past and the implied approach, where we use future cash flows or observed bond default spreads to estimate the current equity risk premium. The premiums we estimate can vary widely across approaches, and we considered two questions towards the end of the paper. The first is why the numbers vary across approaches and the second is how to choose the right number to use in analysis. For the latter question, we argued that the choice of a premium will depend upon the forecast period, whether your believe markets are efficient and whether you are required to be market neutral in your analysis. © 2009 New York University Salomon Center and Wiley Periodicals, Inc. © 2012 Elsevier B.V., All rights reserved.","Damodaran, A.",2009,10.1111/j.1468-0416.2009.00151.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72149127876&doi=10.1111%2Fj.1468-0416.2009.00151.x&partnerID=40&md5=c449d6a2cfde5f6af31db937ef4a7b0b,scopus,"This paper examines the determinants, estimation methods (survey, historical, implied), and implications of equity risk premiums (ERP). It discusses why estimates vary across methods and how to choose an appropriate ERP based on the forecast period, market efficiency beliefs, and neutrality requirements.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:16:21.729680
06fcd3a9f651e0dc,Estimating Latent Factors Based on Statistical Data Analysis,"In recent years, statistical methods have been widely used to estimate latent risk factors that affect the prices of financial assets. This paper develops new estimators for asset pricing factors by introducing dependence measure--distance covariance, that can identify nonlinear dependence. We combined distance covariance with Principal Component Analysis (PCA) and Risk-Premium PCA (RPPCA) and made contrast analysis based on Chinese market data. RPPCA, as a new method, shows strong applicability and detects factors with high Sharpe-ratio efficiently. Moreover, distance covariance produces better performance than covariance in PCA as a factor estimator, which illustrates the superiority of the distance covariance. Finally, the most striking results revealed by the study is that RPPCA including distance covariance of residuals outperforms others with a smaller pricing error and a significantly large Sharpe-ratio.",,2021,10.1088/1742-6596/1995/1/012065,,proquest,"This paper proposes new methods for estimating latent risk factors in asset pricing by incorporating distance covariance, a measure of nonlinear dependence, into Principal Component Analysis (PCA) and Risk-Premium PCA (RPPCA). Empirical analysis on Chinese market data demonstrates that RPPCA with distance covariance is effective in identifying factors with high Sharpe ratios and achieves smaller pricing errors compared to other methods.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:16:50.463688
85e46487d751331e,Estimating and testing non-affine option pricing models with a large unbalanced panel of options,"In this paper, we considerjoint estimation of objective and risk-neutral parameters for stochastic volatility option pricing models using both stock and option prices. A common strategy simplifies the task by limiting the analysis to just one option per date. We first discuss its drawbacks on the basis of model interpretation, estimation results and pricing exercises. We then turn the attention to a more flexible approach, that successfully exploits the wealth of information contained in large heterogeneous panels of options, and we apply it to actual S&P 500 index and index call options data. Our approach breaks the stochastic singularity between contemporaneous option prices by assuming that every observation is affected by measurement error, essentially recasting the problem as a non-linear filtering one. The resulting likelihood function is evaluated using a Monte Carlo Importance Sampling (MC-IS) strategy, combined with a Particle Filter algorithm. The results provide useful intuitions on the directions that should be followed to extend the model, in particular by allowing jumps or regime switching in the volatility process.","Ferriani, Fabrizio; Pastorello, Sergio",2012,10.1111/j.1368-423x.2012.00372.x,,wos,"This paper proposes a method for jointly estimating parameters for stochastic volatility option pricing models using both stock and option prices. It addresses the limitations of simpler approaches that use only one option per date and instead utilizes large, heterogeneous panels of options. The method treats option price observations as having measurement errors, reframing the problem as a non-linear filtering task. A Particle Filter and Monte Carlo Importance Sampling are used to evaluate the likelihood function. The results offer insights for extending the model, such as incorporating jumps or regime switching in volatility.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:16:54.048909
da93425cec293619,Estimating global bank network connectedness,"We use LASSO methods to shrink, select, and estimate the high‐dimensional network linking the publicly traded subset of the world's top 150 banks, 2003–2014. We characterize static network connectedness using full‐sample estimation and dynamic network connectedness using rolling‐window estimation. Statically, we find that global bank equity connectedness has a strong geographic component, whereas country sovereign bond connectedness does not. Dynamically, we find that equity connectedness increases during crises, with clear peaks during the Great Financial Crisis and each wave of the subsequent European Debt Crisis, and with movements coming mostly from changes in cross‐country as opposed to within‐country bank linkages.",,2018,10.1002/jae.2585,,proquest,"This study estimates global bank network connectedness from 2003-2014 using LASSO methods on publicly traded banks. It analyzes both static and dynamic network structures, finding that equity connectedness is geographically concentrated and increases during crises, particularly the Great Financial Crisis and European Debt Crisis, driven by cross-country linkages.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:16:56.646931
cade2431f60ef1f8,Estimating market liquidity from daily data: Marrying microstructure models and machine learning,"We apply machine learning to estimate daily measures of market liquidity by combining microstructure models with low-frequency daily data only, in stock markets in the United States and China. Boosting trees and neural networks significantly improve the performance across different liquidity measures. Our machine learning models are interpretable and improvements are due to (a) more information from raw data that microstructure models do not capture; and (b) better use of information from learned nonlinear and non-monotonic relationships. We further demonstrate two applications of our trained machine learning models in estimating the illiquidity risk premium and systematic liquidity risk. © 2025 Elsevier B.V., All rights reserved.","Dai, Y.; Shi, C.; Zhang, R.",2025,10.1016/j.finmar.2025.101019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017326098&doi=10.1016%2Fj.finmar.2025.101019&partnerID=40&md5=3e7a6b83920c50e3f9a006efbbac3e9e,scopus,"This study uses machine learning techniques, specifically boosting trees and neural networks, to estimate daily market liquidity measures using only low-frequency daily data from US and Chinese stock markets. The models improve performance by capturing additional information from raw data and learning complex relationships, offering interpretability and applications in estimating illiquidity risk premium and systematic liquidity risk.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:16:58.847328
49367fe67a852c41,Estimating the Equity Premium,"Existing empirical research investigating the size of the equity premium has largely consisted of a series of innovations around a common theme: producing a better estimate of the equity premium by using better data or a better estimation technique. The equity premium estimate that emerges from most of this work matches one moment of the data alone: the mean difference between an estimate of the return to holding equity and a risk-free rate. We instead match multiple moments of U.S. market data, exploiting the joint distribution of the dividend yield, return volatility, and realized excess returns, and find that the equity premium lies within 50 basis points of 3.5%, a range much narrower than was achieved in previous studies. Additionally, statistical tests based on the joint distribution of these moments reveal that only those models of the conditional equity premium that embed time variation, breaks, and/or trends are supported by the data. In order to develop the joint distribution of the dividend yield, return volatility, and excess returns, we need a model of price and return fundamentals. We document that even recently developed analytically tractable models that permit autocorrelated dividend growth rates and discount rates impose restrictions that are rejected by the data. We therefore turn to a wider range of models, requiring numerical solution methods and parameter estimation by the simulated method of moments.","Donaldson, R. Glen; Kamstra, Mark J.; Kramer, Lisa A.",2010,10.1017/s0022109010000347,,wos,"This study estimates the equity premium by matching multiple moments of U.S. market data, including the dividend yield, return volatility, and realized excess returns. The authors find a narrower range for the equity premium (within 50 basis points of 3.5%) than previous studies and suggest that models supporting time variation, breaks, and/or trends are supported by the data. They also explore various models, including those requiring numerical solution methods and simulated method of moments estimation, due to limitations of existing analytically tractable models.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:17:01.347448
42d8ee05b390a701,Estimating the risk-return profile of new venture investments using a risk-neutral framework and 'thick' models,"This study proposes cascade neural networks to estimate the model parameters of the Cox-Ross-Rubinstein risk-neutral approach, which, in turn, explain the risk-return profile of firms at venture capital and initial public offering (IPO)financing rounds. Combining the two methods provides better estimation accuracy than risk-adjusted valuation approaches, conventional neural networks, and linear benchmark models. The findings are persistent across in-sample and out-of-sample tests using 3926 venture capital and 1360 US IPO financing rounds between January 1989 and December 2008. More accurate estimates of the risk-return profile are due to less heterogeneous risk-free rates of return from the risk-neutral framework. Cascade neural networks nest both the linear and nonlinear functional estimation form in addition to taking account of variable interaction effects. Better estimation accuracy of the risk-return profile is desirable for investors so they can make a more informed judgement before committing capital at different stages of development and various financing rounds. Reprinted by permission of Routledge, Taylor and Francis Ltd.",,2014,10.1080/1351847x.2012.708471,,proquest,"This study uses cascade neural networks within a risk-neutral framework to estimate the risk-return profile of new venture investments at venture capital and IPO stages. The proposed method shows improved estimation accuracy compared to traditional risk-adjusted valuation, conventional neural networks, and linear models, based on analysis of US venture capital and IPO financing rounds from 1989-2008. The authors suggest this enhanced accuracy aids investor decision-making.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:17:04.813923
817d1aea113ecade,Estimating the spot rate curve using the Nelson-Siegel model. A ridge regression approach,"The Nelson-Siegel model is widely used in practice for fitting the term structure of interest rates. Due to the ease in linearizing the model, a grid search or an OLS approach using a fixed shape parameter are popular estimation procedures. The estimated grid search parameters, however, have been reported (1) to behave erratically over time, and (2) to have relatively large variances. On the other hand, parameter estimates based on a fixed shape parameter, while avoiding multicollinearity, turn out to be too smooth. We show that the Nelson-Siegel model can become heavily collinear depending on the estimated/fixed shape parameter. A simple procedure based on ridge regression can remedy the reported problems significantly. © 2013 Elsevier Inc. © 2013 Elsevier B.V., All rights reserved.","Annaert, J.; Claes, A.G.P.; Ceuster, M.J.K.; Zhang, H.",2013,10.1016/j.iref.2013.01.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876326729&doi=10.1016%2Fj.iref.2013.01.005&partnerID=40&md5=4bbf46f64bd8757be4fa5c077b100dfb,scopus,"This paper proposes a ridge regression approach to estimate the Nelson-Siegel model for the term structure of interest rates, addressing issues of erratic parameter behavior and large variances associated with grid search methods, and excessive smoothness from fixed shape parameter methods. The authors demonstrate that the Nelson-Siegel model can suffer from multicollinearity and that ridge regression offers a significant improvement.",True,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:17:07.646060
b07e605b9567222c,European Union Allowance price forecasting with Multidimensional Uncertainties: A TCN-iTransformer Approach for Interval Estimation,"In response to the research demand for forecasting European Union Allowance (EUA) prices, this paper proposes a probabilistic forecasting framework based on a spatiotemporal convolutional neural network. This framework innovatively integrates multidimensional external uncertainty indicators, captures the long-term dependencies of carbon prices through a spatiotemporal convolutional structure, and combines quantile regression with conformal prediction to effectively estimate prediction intervals. Empirical studies demonstrate that the proposed TCN-iTransformer model outperforms existing methods in both point prediction and interval prediction, exhibiting excellent prediction interval coverage probability and normalized average width at different confidence intervals. The Diebold–Mariano (DM) test and ordinary least squares (OLS) regression analysis further validate the predictive advantages of the proposed model. Furthermore, SHAP analysis reveals that the U.S. Treasury yield spread has the most significant impact on EUA price forecasting, while geopolitical risks predominantly exert negative effects. The research findings provide important references for constructing risk mitigation strategies in the European Union carbon emissions market under complex market environments. © 2025 Elsevier B.V., All rights reserved.","Wu, R.; Abedin, M.; Zeng, H.; Lucey, B.",2025,10.1002/for.70024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105016613972&doi=10.1002%2Ffor.70024&partnerID=40&md5=9305158e398c871b8bd5e60c86d2c3c3,scopus,"This paper proposes a probabilistic forecasting framework using a spatiotemporal convolutional neural network (TCN-iTransformer) to forecast European Union Allowance (EUA) prices. It integrates external uncertainty indicators and uses quantile regression with conformal prediction for interval estimation. The model outperforms existing methods in point and interval prediction, with SHAP analysis identifying the U.S. Treasury yield spread as a significant factor. The findings offer insights for risk mitigation in the EU carbon emissions market.",True,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T16:17:21.413837
7612e5acd7dc712d,Evaluating the 'Fed Model' of Stock Price Valuation: An out-of-sample forecasting perspective,"The ""Fed Model"" postulates a cointegrating relationship between the equity yield on the S&P 500 and the bond yield. We evaluate the Fed Model as a vector error correction forecasting model for stock prices and for bond yields. We compare out-of-sample forecasts of each of these two variables from a univariate model and various versions of the Fed Model including both linear and nonlinear vector error correction models. We find that for stock prices the Fed Model improves on the univariate model for longer-horizon forecasts, and the nonlinear vector error correction model performs even better than its linear version. © 2006 Elsevier Ltd. All rights reserved. © 2008 Elsevier B.V., All rights reserved.","Jansen, D.W.; Wang, Z.",2006,10.1016/s0731-9053(05)20026-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645856784&doi=10.1016%2FS0731-9053%2805%2920026-9&partnerID=40&md5=6ec05711fd88e6ea0dcf87377d48d09f,scopus,"This study evaluates the 'Fed Model' for stock price valuation using a vector error correction forecasting approach. It compares the Fed Model's out-of-sample forecasts for stock prices and bond yields against a univariate model, considering both linear and nonlinear versions. The findings suggest that the Fed Model enhances longer-horizon stock price forecasts, with nonlinear models showing superior performance.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:17:28.162461
6753cb49c5eeba27,Evolutionary-based return forecasting with nonlinear STAR models: evidence from the Eurozone peripheral stock markets,"Traditional linear regression and time-series models often fail to produce accurate forecasts due to inherent nonlinearities and structural instabilities, which characterize financial markets and challenge the Efficient Market Hypothesis. Machine learning techniques are becoming widespread tools for return forecasting as they are capable of dealing efficiently with nonlinear modeling. An evolutionary programming approach based on genetic algorithms is introduced in order to estimate and fine-tune the parameters of the STAR-class models, as opposed to conventional techniques. Using a hybrid method we employ trading rules that generate excess returns for the Eurozone southern periphery stock markets, over a long out-of-sample period after the introduction of the Euro common currency. Our results may have important implications for market efficiency and predictability. Investment or trading strategies based on the proposed approach may allow market agents to earn higher returns.",,2018,10.1007/s10479-015-2078-z,,proquest,"This paper proposes an evolutionary programming approach using genetic algorithms to estimate nonlinear STAR models for stock market return forecasting. The authors demonstrate that this hybrid method generates excess returns in Eurozone peripheral stock markets, challenging the Efficient Market Hypothesis and suggesting potential for higher investment returns.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:17:40.814360
04e2d697edac7893,Examination and Modification of Multi-Factor Model in Explaining Stock Excess Return with Hybrid Approach in Empirical Study of Chinese Stock Market,"To search significant variables which can illustrate the abnormal return of stock price, this research is generally based on the Fama-French five-factor model to develop a multi-factor model. We evaluated the existing factors in the empirical study of Chinese stock market and examined for new factors to extend the model by OLS and ridge regression model. With data from 2007 to 2018, the regression analysis was conducted on 1097 stocks separately in the market with computer simulation based on Python. Moreover, we conducted research on factor cyclical pattern via chi-square test and developed a corresponding trading strategy with trend analysis. For the results, we found that except market risk premium, each industry corresponds differently to the rest of six risk factors. The factor cyclical pattern can be used to predict the direction of seven risk factors and a simple moving average approach based on the relationships between risk factors and each industry was conducted in back-test which suggested that SMB (size premium), CMA (investment growth premium), CRMHL (momentum premium), and AMLH (asset turnover premium) can gain positive return.",,2019,10.3390/jrfm12020091,,proquest,"This study extends the Fama-French five-factor model to explain stock excess returns in the Chinese stock market using OLS and ridge regression. It identifies significant factors, analyzes their cyclical patterns, and develops a trading strategy based on trend analysis, finding that SMB, CMA, CRMHL, and AMLH can yield positive returns.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:17:55.817617
30458d1834570aa3,Exchange rate parities and Taylor rule deviations,"This paper investigates the PPP and UIP conditions by taking into account possible nonlinearities as well as the role of Taylor rule deviations under alternative monetary policy frameworks. The analysis is conducted using monthly data from January 1993 to December 2020 for five inflation-targeting countries (the UK, Canada, Australia, New Zealand and Sweden) and three non-targeting ones (the USA, the Euro Area and Switzerland). Both a benchmark linear VECM and a nonlinear Threshold VECM are estimated; the latter includes Taylor rule deviations as the threshold variable. The results can be summarized as follows. First, the nonlinear specification provides much stronger evidence for the PPP and UIP conditions, the estimated adjustment speed towards equilibrium being twice as fast. Second, Taylor rule deviations play an important role: the adjustment speed is twice as fast when deviations are small and the credibility of the central bank is higher. Third, inflation targeting tends to generate a higher degree of credibility for the monetary authorities, thereby reducing deviations of the exchange rate from the PPP- and UIP-implied equilibrium.",,2022,10.1007/s00181-021-02192-3,,proquest,"This paper examines Purchasing Power Parity (PPP) and Uncovered Interest Parity (UIP) conditions, considering nonlinearities and deviations from the Taylor rule under different monetary policy regimes. Using monthly data from 1993-2020 for eight countries, the study employs both linear and nonlinear Vector Error Correction Models (VECM). The findings indicate that nonlinear models offer stronger support for PPP and UIP, with faster adjustment speeds. Taylor rule deviations significantly impact adjustment speed, which is faster when deviations are smaller and central bank credibility is higher. Inflation targeting appears to enhance central bank credibility, leading to reduced exchange rate deviations from PPP and UIP.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:17:58.080926
17dfde09cbd8f47e,Exploring External Influences on Cryptocurrency Prices: Using A Multi-Analytical Approach,"Cryptocurrencies have experienced exponential growth within the last decade, with market capitalization hovering above the one-trillion-dollar mark since 2022. One area of concern for current and potential crypto users and investors is their unprecedented price volatility. As cryptos become interlinked with the regulated financial system, questions emerge regarding the possibility of linkages of their prices to the external environments. Financial and macroeconomic factors of inflation, economic growth, interest rates, currency exchange rates, equity market returns, corporate bond yields, gold and oil prices are examined against the cryptocurrency returns. This study encompasses a multi-analytical approach, firstly with the empirical tests of Spearman’s correlational analysis to discover the most pertinent relationships, followed by the PCA analysis to reduce redundancy. The predictive regression model of the Granger Causality test, a vector autoregression (VAR) time series forecasting method, is applied to examine whether the highly effective factors Granger cause the crypto price movements. The Machine Learning Random Forest Regression is also applied where a nuanced understanding of the external factors affecting cryptos prices is gained. The findings of this study pertain to more recent times when the pandemic crisis has subsided and stable economies are in place. The results examined four major cryptos of Bitcoin, Binance Coin, Ripple and Tether, where most behaviours suggest that users and investors are willing to take on riskier assets during periods of economic growth, a strong equity market complements crypto demands and gold and oil are good substitutes for cryptos. Tether, a stablecoin, was the least impacted by external factors and behaved similarly to a fiat currency. This investigation into external factors will empower cryptocurrency users and investors with valuable insights into the crypto price mechanisms, enabling them to refine their investing and portfolio diversification strategies.",,2025,10.32479/ijefi.19455,,proquest,"This study investigates the influence of financial and macroeconomic factors (inflation, economic growth, interest rates, currency exchange rates, equity market returns, corporate bond yields, gold and oil prices) on the prices of four major cryptocurrencies (Bitcoin, Binance Coin, Ripple, and Tether). It employs a multi-analytical approach including Spearman's correlational analysis, PCA, Granger Causality tests (VAR), and Random Forest Regression. Findings suggest that during economic growth and strong equity markets, investors are more willing to invest in riskier assets like cryptocurrencies, and gold/oil act as substitutes. Tether, a stablecoin, showed behavior similar to fiat currency, being less impacted by external factors. The study aims to provide insights for cryptocurrency users and investors to refine their strategies.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:18:01.930257
3da66af54c1d1abc,Exploring the role of artificial intelligence in orthopedic medical education: A narrative review,"Artificial intelligence (AI) is transforming orthopedic medical education by enhancing diagnostic accuracy, surgical training, and personalized learning. This narrative review explores AI's applications, including machine learning (ML) and computer vision for interpreting imaging studies, virtual reality (VR) and augmented reality (AR) for immersive surgical simulations, and natural language processing (NLP) for streamlining clinical workflows. AI-powered tools offer objective feedback, adaptive learning modules, and risk-free environments for skill acquisition, bridging gaps in traditional training methods. However, challenges such as data privacy, algorithmic bias, and the need for robust validation remain. Ethical considerations, including patient trust and trainee over-reliance on AI, must also be addressed. Despite these barriers, AI democratizes access to high-quality education, particularly in resource-limited settings, through cloud-based platforms and mobile applications. The future of AI in orthopedics is promising, with advancements in predictive analytics, robotic-assisted surgery, and haptic feedback technologies poised to further revolutionize training. Collaborative efforts among educators, clinicians, and developers are essential to ensure responsible integration. This review highlights AI's potential to reshape orthopedic education while emphasizing the importance of preserving the mentor-trainee relationship and fostering evidence-based adoption.Artificial intelligence (AI) is transforming orthopedic medical education by enhancing diagnostic accuracy, surgical training, and personalized learning. This narrative review explores AI's applications, including machine learning (ML) and computer vision for interpreting imaging studies, virtual reality (VR) and augmented reality (AR) for immersive surgical simulations, and natural language processing (NLP) for streamlining clinical workflows. AI-powered tools offer objective feedback, adaptive learning modules, and risk-free environments for skill acquisition, bridging gaps in traditional training methods. However, challenges such as data privacy, algorithmic bias, and the need for robust validation remain. Ethical considerations, including patient trust and trainee over-reliance on AI, must also be addressed. Despite these barriers, AI democratizes access to high-quality education, particularly in resource-limited settings, through cloud-based platforms and mobile applications. The future of AI in orthopedics is promising, with advancements in predictive analytics, robotic-assisted surgery, and haptic feedback technologies poised to further revolutionize training. Collaborative efforts among educators, clinicians, and developers are essential to ensure responsible integration. This review highlights AI's potential to reshape orthopedic education while emphasizing the importance of preserving the mentor-trainee relationship and fostering evidence-based adoption.",,2025,10.1016/j.jcot.2025.103100,,proquest,"This narrative review examines the impact of artificial intelligence (AI) on orthopedic medical education, detailing applications in diagnostics, surgical training, and personalized learning through technologies like machine learning, computer vision, VR, AR, and NLP. It discusses benefits such as objective feedback and accessible training, alongside challenges like data privacy and ethical concerns. The review emphasizes AI's potential to democratize education and revolutionize training, advocating for responsible integration and collaboration.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:18:05.731905
6e8a4c15b74abc41,"External shocks, cross-border flows and macroeconomic risks in emerging market economies","We study the relationship between cross-border flows and risks to macroeconomic stability for a sample of ten major emerging market economies (EMEs) from 2000 to 2017 in the presence of external shocks. We examine this relationship with a focus on two key channels of cross-border flows, namely external debt securities (EDS) and cross-border loans (CBLs). Our analysis focuses on the transition in cross-border flows post-global financial crisis 2008 (GFC) termed as the second phase of global liquidity (Shin in Keynote address at Federal Reserve Bank of San Francisco Asia economic policy conference, 2013). Panel vector autoregression estimations show that volatility in global risk perception affects cross-border flows to EMEs more as compared to the effect of the US monetary policy stance. Post-GFC, EDS flows rise with shocks in global risk perception, while CBL flows register a decline. CBL flows are also associated with larger risks post-GFC compared to the pre-GFC period, which is in contrast to the result for EDS flows. Second, a panel threshold estimation confirms a nonlinear association between EDS/CBL flows and macroeconomic risks largely dependent upon global uncertainty. US GDP growth also affects the nonlinearity, but US federal funds rate have insignificant threshold effects. Our results conclude that global uncertainty is a significant driver of cross-border flows to EMEs post-GFC and that it is a strong signal in determining riskiness of EDS flows and CBL flows for EMEs.","Goyal, Ashima; Verma, Akhilesh K.; Sengupta, Rajeswari",2022,10.1007/s00181-021-02099-z,,wos,"This study investigates the link between cross-border flows (external debt securities and cross-border loans) and macroeconomic stability risks in emerging market economies (EMEs) from 2000-2017, considering external shocks. The analysis reveals that global risk perception volatility impacts EME cross-border flows more than US monetary policy. Post-2008 financial crisis, external debt securities flows increased with global risk perception shocks, while cross-border loans declined and became riskier. Panel threshold estimations confirm a nonlinear relationship between these flows and macroeconomic risks, influenced by global uncertainty and US GDP growth.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:18:09.514635
f62c257dbca918f1,FINITE-SAMPLE PROPERTIES OF THE GENERALIZED-METHOD OF MOMENTS IN TESTS OF CONDITIONAL ASSET PRICING-MODELS,"We develop evidence on the finite sample properties of the Generalized Method of Moments (GMM) in an asset pricing context. The models imply nonlinear, cross-equation restrictions on predictive regressions for security returns. We find that a two-stage GMM approach produces goodness-of-fit statistics that reject the restrictions too often. An iterated GMM approach has superior finite sample properties. The coefficient estimates are approximately unbiased in simpler models, but their asymptotic standard errors are understated. Simple adjustments for the standard errors are partially successful in correcting the bias. In more complex models the coefficients and their standard errors can be highly unreliable. The power of the tests to reject a single-premium model is higher against a two-premium, fixed-beta alternative than against a conditional Capital Asset Pricing Model with time-varying betas.","FERSON, WE; FOERSTER, SR",1994,10.1016/0304-405x(94)90029-9,,wos,"This paper investigates the finite sample properties of the Generalized Method of Moments (GMM) when applied to conditional asset pricing models. It finds that a two-stage GMM approach tends to reject restrictions too frequently, while an iterated GMM approach shows better finite sample performance. Coefficient estimates can be biased in simpler models, and their standard errors are often understated, though adjustments can help. In more complex models, both coefficients and standard errors can be unreliable. The study also compares the power of tests against different asset pricing model alternatives.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:18:11.581040
99d45debc2b13a50,FORECASTING SERIES CONTAINING OFFSETTING BREAKS: OLD SCHOOL AND NEW SCHOOL METHODS OF FORECASTING TRANSNATIONAL TERRORISM,"Transnational terrorism data are difficult to forecast because they contain an unknown number of structural breaks of unknown functional form. The rise of religious fundamentalism, the demise of the Soviet Union, and the rise of al Qaeda have changed the nature of transnational terrorism. 'Old School' forecasting methods simply smooth or difference the data. 'New School' methods use estimated break dates to control for regime shifts when forecasting. We compare the various forecasting methods using a Monte Carlo study with data containing different types of breaks. The study's results are used to forecast various types of transnational terrorist incidents.","Enders, Walter; Liu, Yu; Prodan, Ruxandra",2009,10.1080/10242690802425772,,wos,"This paper compares 'old school' (smoothing/differencing) and 'new school' (estimated break dates) forecasting methods for transnational terrorism data, which are characterized by structural breaks. A Monte Carlo study evaluates these methods, and the results are applied to forecast terrorist incidents.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:18:16.164855
05792d551d45f496,Factor Investing Based on Musharakah Principle,"Shariah stock investing has become a widely discussed topic in financial industry as part of today's investment strategy. The strategy primarily applies market capitalization allocations. However, some researchers have argued that market capitalization weighting is inherently flawed and have advocated replacing market capitalization allocations with factor allocations. In this paper, we discuss the rationale for factor investing based on Musharakah principle. The essential elements or factors of Musharakah principle such as business sector, management capability, profitability growth and capital efficiency are embedded in the Shariah-compliant stock. We then transform these factors into indexation for better analysis and performance measurement. Investment universe for this research covers Malaysian stocks for the period of January 2009 to December 2013. We found out that these factor indexes have historically earned excess returns over market capitalization weighted indexes and experienced higher Sharpe Ratios.","Simon, Shahril; Omar, Mohd; Lazam, Norazliani Md; Amin, Mohd Nazrul Mohd",2015,10.1063/1.4932468,,wos,"This paper proposes a factor investing strategy based on the Musharakah principle for Shariah-compliant stocks. It identifies key factors like business sector, management capability, profitability growth, and capital efficiency, transforms them into indexation, and empirically tests their performance against market capitalization-weighted indexes using Malaysian stocks from 2009-2013. The findings suggest that factor indexes historically outperformed market capitalization indexes and had higher Sharpe Ratios.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:18:18.132054
23c2359624d6bbdc,Factor Investment or Feature Selection Analysis?,"This study has made significant findings in A-share market data processing and portfolio management. Firstly, by adopting the Lasso method and CPCA framework, we effectively addressed the problem of multicollinearity among feature indicators, with the Lasso method demonstrating superior performance in handling this issue, thus providing a new method for financial data processing. Secondly, Deep Feedforward Neural Networks (DFN) exhibited exceptional performance in portfolio management, significantly outperforming other evaluated machine learning methods, and achieving high levels of out-of-sample performance and Sharpe ratios. Additionally, we consistently identified price changes, earnings per share, net assets per share, and excess returns as key factors influencing predictive signals. Finally, this study combined the Lasso method with DFN, providing a new perspective and methodological support for asset pricing measurement in the financial field.",,2025,10.3390/math13010009,,proquest,"This study applies the Lasso method and CPCA framework to address multicollinearity in A-share market data, with Lasso showing superior performance. Deep Feedforward Neural Networks (DFN) were found to be highly effective for portfolio management, outperforming other machine learning methods. Key influencing factors identified include price changes, earnings per share, net assets per share, and excess returns. The research combines Lasso and DFN for asset pricing measurement.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:18:46.164401
4b1a93d999b867fb,Filtering of a Discrete-Time HMM-Driven Multivariate Ornstein-Uhlenbeck Model With Application to Forecasting Market Liquidity Regimes,"This paper investigates the modeling of risk due to market and funding liquidity by capturing the joint dynamics of three time series: the treasury-Eurodollar spread, the VIX, and a metric derived from the S&P 500 spread. We propose a two-regime mean-reverting model for explaining the behaviour of three time series, which mirror liquidity levels for financial markets. An expectation-maximisation algorithm in conjunction with multivariate filters is employed to construct optimal parameter estimates of the proposed model. The selection of the modeling set-up is justified by balancing the best-fit criterion and model complexity. The model performance is demonstrated on historical market data, and a descriptive analysis of the different liquidity measures shows the presence of clear high and low states.",A. Tenyakov; R. Mamon; M. Davison,2016,10.1109/jstsp.2016.2549499,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7445144,ieeexplore,"This paper proposes a two-regime mean-reverting model to capture the joint dynamics of the treasury-Eurodollar spread, VIX, and S&P 500 spread, reflecting market and funding liquidity. An expectation-maximization algorithm and multivariate filters are used for parameter estimation. The model's performance is demonstrated on historical data, showing distinct high and low liquidity states.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:18:50.926130
aae98e7d411abd8a,Financial Stock Investment Management Using Deep Learning Algorithm in the Internet of Things,"This paper aims to explore a new model to study financial stock investment management (SIM) and obtain excess returns. Consequently, it proposes a financial SIM model using deep Q network (DQN) as reinforcement earning (RL) algorithm and Long Short-Term Memory (LSTM) as deep neural network (DNN). Then, after training and optimization, the proposed model is back-tested. The research findings are as follows: the LSTM neural network (NN)-based model will import the observation of the market at each time and the change of transaction information over time. The LSTM network can find and learn the potential relationship between time series data. There are two hidden layers and one output layer in the model. The hidden layer is an LSTM structure and the output layer is the fully connected NN. DQN algorithm first stores the experience sample data of the agent-environment interaction into the experience pool. It then randomly selects a small batch of data from the experience pool to train the network. Doing so removes the correlation and dependence between samples so that the DNN model can better learn the value function in the RL task. The model can predict the future state according to historical information and decide which actions to take in the next step. Meanwhile, five stocks of Chinese A-shares are selected to form an asset pool. The initial 500,000 amount of the account is divided into five equal shares, which are invested and traded. Overall, the model account’s rate of return (RoR) during the back-test is 32.12%. The Shanghai Stock Exchange (SSI) has risen by 19.157% in the same period. Thus, the model’s performance has exceeded the SSI’s in the same period. E stock has the maximum RoR of 78.984%. The RoR of A, B, and C stocks is 54.129%, 11.594%, and 9.815%, respectively. B stock presents a minimum RoR of 6.084%. All these stocks have got positive returns. Therefore, the proposed financial SIM based on the DL algorithm is scientific and feasible. The research content has certain significant reference for the DL-based financial SIM.",,2022,10.1155/2022/4514300,,proquest,"This paper proposes a financial stock investment management model using Deep Q Network (DQN) and Long Short-Term Memory (LSTM) deep learning algorithms. The model, trained and back-tested on five Chinese A-shares, achieved a 32.12% rate of return, outperforming the Shanghai Stock Exchange's 19.157% increase over the same period. The research suggests this deep learning-based approach is scientific, feasible, and offers significant reference for financial investment management.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:18:57.931207
a7bce1b51fda5973,"Financial asset returns, direction-of-change forecasting, and volatility dynamics","We consider three sets of phenomena that feature prominently in the financial economics literature: (1) conditional mean dependence (or lack thereof) in asset returns, (2) dependence (and hence forecastability) in asset return signs, and (3) dependence (and hence forecastability) in asset return volatilities. We show that they are very much interrelated and explore the relationships in detail. Among other things, we show that (1) volatility dependence produces sign dependence, so long as expected returns are nonzero, so that one should expect sign dependence, given the overwhelming evidence of volatility dependence; (2) it is statistically possible to have sign dependence without conditional mean dependence; (3) sign dependence is not likely to be found via analysis of sign autocorrelations, runs tests, or traditional market timing tests because of the special nonlinear nature of sign dependence, so that traditional market timing tests are best viewed as tests for sign dependence arising from variation in expected returns rather than from variation in volatility or higher moments; (4) sign dependence is not likely to be found in very high-frequency (e.g., daily) or very low-frequency (e.g., annual) returns; instead, it is more likely to be found at intermediate return horizons; and (5) the link between volatility dependence and sign dependence remains intact in conditionally non-Gaussian environments, for example, with time-varying conditional skewness and/or kurtosis.","Christoffersen, Peter F.; Diebold, Francis X.",2006,10.1287/mnsc.1060.0520,,wos,"This paper investigates the interrelationships between conditional mean dependence in asset returns, dependence in asset return signs (direction-of-change forecasting), and dependence in asset return volatilities. It demonstrates that volatility dependence can lead to sign dependence, especially when expected returns are non-zero. The study also highlights that sign dependence might not be easily detected using traditional methods like sign autocorrelations or market timing tests due to its nonlinear nature, and it is more likely to be found at intermediate return horizons. The link between volatility and sign dependence persists even in non-Gaussian environments.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:19:00.548577
50461e7a8d89df57,Financial attention and the demand for information,"This study tracks the daily traffic on the leading financial websites in the US, and uses the attention paid to these websites as a proxy for retailers’ aggregate demand for information. We determine that attention to financial websites is positively correlated with uncertainty and negatively associated with investor sentiment. Furthermore, market shocks drive attention to financial websites, and heightened attention predicts an increase in the following trading day's volatility. Consistent with the information arrival hypothesis, the search for information is higher on Mondays and Tuesdays and lower on weekends. As some retail investors are noise traders, attention to financial websites has a positive effect on volatility and increases trading volume. Finally, using 5-min intraday data, we construct a daily-implied risk aversion proxy and provide evidence supporting the theoretical contention that risk-averse agents gather information as a hedge against uncertainty. However, our findings do not support the avoidance of information theories. © 2019 Elsevier B.V., All rights reserved.","Qadan, M.; Zoua'bi, M.",2019,10.1016/j.socec.2019.101450,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069670415&doi=10.1016%2Fj.socec.2019.101450&partnerID=40&md5=a6e61eecd3f0a101c774cad8ea7d88e8,scopus,"This study uses daily traffic on US financial websites as a proxy for retailer demand for information. It finds that attention to these sites correlates positively with uncertainty and negatively with investor sentiment, and is driven by market shocks. Increased attention predicts higher subsequent volatility. Information seeking is higher early in the week, consistent with the information arrival hypothesis. Noise traders' attention increases volatility and trading volume. The study also constructs a daily-implied risk aversion proxy, supporting the idea that risk-averse agents seek information as a hedge against uncertainty, but not theories of information avoidance.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:19:02.397878
5842551f94fabb30,Firm-level productivity and stock return: New evidence from China,"This study provides novel insights into the stock return predictability related to the firm-level total factor productivity (TFP) in the China's stock market, covering the period from 1999 to 2020. Contrary to studies on other countries, our research identifies a positive correlation between the productivity of Chinese firms and their future stock returns. We suggest that this correlation is driven by mispricing rather than risk premium. Specifically, we argue that investors' difficulties in accurately assessing firm-level productivity and limited attention result in the inadequate incorporation of this information into stock prices. Furthermore, our analysis explores additional factors contributing to the observed productivity-related market anomaly, including investor overconfidence, positive feedback trading, lottery preference, information uncertainty, and limits to arbitrage, indicating investors' irrationality and behavioral biases. Importantly, our study also demonstrates that firm-level productivity serves as a reliable predictor of a firm's future profitability. © 2024 Elsevier B.V., All rights reserved.","Tang, N.; Gao, M.; Zhou, Y.; Zhou, F.; Zhu, J.",2024,10.1016/j.iref.2024.103557,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203455085&doi=10.1016%2Fj.iref.2024.103557&partnerID=40&md5=766099b5385b7c429057e195b3e80ea1,scopus,"This study investigates the relationship between firm-level total factor productivity (TFP) and stock returns in China from 1999 to 2020. It finds a positive correlation, suggesting mispricing due to investor difficulties in assessing TFP, limited attention, investor overconfidence, and other behavioral biases. The study also confirms TFP as a predictor of future profitability.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:19:04.146301
897cf60508a18c31,Fiscal multipliers in South Africa after the global financial crisis,"Background: South Africa’s fiscal position has deteriorated considerably over the last 10 years, with debt levels reaching historical highs in the post-apartheid period. National Treasury’s intentions for fiscal consolidation have again drawn attention to the fiscal multiplier literature.Aim: The aim in the study is to calculate the size of fiscal expenditure multipliers over the period 2009 to 2019, taking into account the specific economic conditions and the funding choices of government.Setting: In the study fiscal policy is considered at a time when the debt to gross domestic product (GDP) ratio was rising rapidly.Methods: We use an econometric model to calculate the fiscal multipliers over the past decade. Our estimates take account of the specific fiscal conditions for each year, in particular the changing relationship between debt and the sovereign risk premia as well as the impact of tax increases.Results: The model suggests that the fiscal multiplier declined from 1.5 in 2010 to around zero in 2019 as the debt levels became progressively more unsustainable and large tax increases muted the aggregate demand effects from higher government expenditure.Conclusion: The low fiscal multipliers suggest that fiscal consolidation will be less costly in terms of growth forgone than generally perceived.JEL classification: C50, E62, H62, H63",,2022,10.4102/sajems.v25i1.4191,,proquest,"This study calculates fiscal expenditure multipliers in South Africa from 2009 to 2019 using an econometric model. It finds that multipliers declined from 1.5 in 2010 to near zero in 2019 due to rising debt levels and tax increases, suggesting fiscal consolidation may be less costly than anticipated.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:19:08.231515
8ae5fb614411eb93,Forecasting CDS Term Structure Based on Nelson–Siegel Model and Machine Learning,"In this study, we analyze the term structure of credit default swaps (CDSs) and predict future term structures using the Nelson–Siegel model, recurrent neural network (RNN), support vector regression (SVR), long short-term memory (LSTM), and group method of data handling (GMDH) using CDS term structure data from 2008 to 2019. Furthermore, we evaluate the change in the forecasting performance of the models through a subperiod analysis. According to the empirical results, we confirm that the Nelson–Siegel model can be used to predict not only the interest rate term structure but also the CDS term structure. Additionally, we demonstrate that machine-learning models, namely, SVR, RNN, LSTM, and GMDH, outperform the model-driven methods (in this case, the Nelson–Siegel model). Among the machine learning approaches, GMDH demonstrates the best performance in forecasting the CDS term structure. According to the subperiod analysis, the performance of all models was inconsistent with the data period. All the models were less predictable in highly volatile data periods than in less volatile periods. This study will enable traders and policymakers to invest efficiently and make policy decisions based on the current and future risk factors of a company or country.",,2020,10.1155/2020/2518283,,proquest,"This study forecasts the term structure of credit default swaps (CDSs) using the Nelson-Siegel model and various machine learning techniques (RNN, SVR, LSTM, GMDH). Empirical results show that machine learning models, particularly GMDH, outperform the Nelson-Siegel model. Performance varies with data volatility, with less predictability during highly volatile periods. The findings aim to aid traders and policymakers in investment and policy decisions.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:19:36.531584
14951138294d4fff,Forecasting Chinese Stock Market Volatility With Volatilities in Bond Markets,"In this paper, we investigate whether the bond markets contain important information that can improve the accuracy of stock market volatility forecasts in China. We use realized volatility (RV) implemented by different maturity treasury bond futures contracts to predict the Chinese stock market volatility. Our work is based on the heterogeneous autoregressive (HAR) framework. Empirical results show that the volatility of treasury bond contracts with longer maturities (especially 10 years) has the best effect on predicting the Chinese stock market volatility, both in sample and out of sample. Two machine learning methods, the scaled principal component analysis (SPCA) and the least absolute shrinkage and selection operator (lasso), are also more effective than the HAR benchmark model's prediction. Finally, mean–variance investors can achieve substantial economic gains by allocating their investment portfolios based on volatility forecasts after introducing treasury bond futures volatility.",,2025,10.1002/for.3215,,proquest,"This study explores the predictive power of Chinese bond market volatilities for the Chinese stock market volatility. Using a heterogeneous autoregressive (HAR) framework and machine learning methods (SPCA, lasso), it finds that longer-maturity treasury bond futures volatility (especially 10-year) significantly improves stock market volatility forecasts. The findings suggest that mean-variance investors can benefit economically from incorporating these bond volatility forecasts into their portfolio allocation strategies.",True,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T16:19:47.698506
08fc23dd1562d6e4,Forecasting ETF Performance: A Comparative Study of Deep Learning Models and the Fama-French Three-Factor Model,"The global financial landscape has witnessed a significant shift towards Exchange-Traded Funds (ETFs), with their market capitalization surpassing USD 10 trillion in 2023, due to advantages such as low management fees, high liquidity, and broad market exposure. As ETFs become increasingly central to investment strategies, accurately forecasting their performance has become crucial. This study addresses this need by comparing the efficacy of deep learning models against the traditional Fama-French three-factor model in predicting daily ETF returns. The methodology employs eight artificial neural network architectures, including ANN, LSTM, GRU, CNN, and their variants, implemented in Python and applied to data ranging from 2010 to 2020, while also exploring the impact of additional factors on forecast accuracy. Empirical results reveal that LSTM and the Fama-French three-factor model exhibit a superior performance in ETF return prediction. This study contributes to the literature on financial forecasting and offers practical insights into investment decision making. By leveraging advanced artificial intelligence techniques, this study aims to enhance the toolkit available for ETF performance analysis, potentially improving investment strategies in this dynamic market segment.",,2024,10.3390/math12193158,,proquest,"This study compares deep learning models (ANN, LSTM, GRU, CNN, and variants) with the Fama-French three-factor model for forecasting daily ETF returns from 2010-2020. LSTM and the Fama-French model showed superior performance. The research aims to improve ETF performance analysis and investment strategies.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:20:09.165780
f924407706d3a0c2,Forecasting benchmarks of long-term stock returns via machine learning,"Recent advances in pension product development seem to favour alternatives to the risk free asset often used in the financial theory as a performance standard for measuring the value generated by an investment or a reference point for determining the value of a financial instrument. To this end, in this paper, we apply the simplest machine learning technique, namely, a fully nonparametric smoother with the covariates and the smoothing parameter chosen by cross-validation to forecast stock returns in excess of different benchmarks, including the short-term interest rate, long-term interest rate, earnings-by-price ratio, and the inflation. We find that, net-of-inflation, the combined earnings-by-price and long-short rate spread form our best-performing two-dimensional set of predictors for future annual stock returns. This is a crucial conclusion for actuarial applications that aim to provide real-income forecasts for pensioners.",,2019,10.1007/s10479-019-03338-4,,proquest,"This paper uses a nonparametric smoother with cross-validation to forecast stock returns in excess of various benchmarks, including interest rates and inflation. The best predictors for future annual stock returns were found to be the earnings-by-price ratio and the long-short rate spread, net of inflation. This finding is relevant for actuarial applications providing real-income forecasts.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:20:19.144782
cb1a4b16413ff22b,"Forecasting oil prices with penalized regressions, variance risk premia and Google data","This paper investigates whether augmenting models with the variance risk premium (VRP) and Google search data improves the quality of the forecasts for real oil prices. We considered a time sample of monthly data from 2007 to 2019 that includes several episodes of high volatility in the oil market. Our evidence shows that penalized regressions provided the best forecasting performances across most of the forecasting horizons. Moreover, we found that models using the VRP as an additional predictor performed best for forecasts up to 6–12 months ahead forecasts, while models using Google data as an additional predictor performed better for longer-term forecasts up to 12–24 months ahead. However, we found that the differences in forecasting performances were not statistically different for most models, and only the Principal Component Regression (PCR) and the Partial least squares (PLS) regression were consistently excluded from the set of best forecasting models. These results also held after a set of robustness checks that considered model specifications using a wider set of influential variables, a Hierarchical Vector Auto-Regression model estimated with the LASSO, and a set of forecasting models using a simplified specification for Google Trends data. © 2024 Elsevier B.V., All rights reserved.","Lycheva, M.; Mironenkov, A.; Kurbatskii, A.; Fantazzini, D.",2022,10.22394/1993-7601-2022-68-28-49,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148285877&doi=10.22394%2F1993-7601-2022-68-28-49&partnerID=40&md5=96233f97d9b1ad6c3495359610f59ea2,scopus,"This study explores the forecasting accuracy of real oil prices using penalized regressions, incorporating the variance risk premium (VRP) and Google search data. The findings suggest that penalized regressions generally offer the best performance, with VRP being beneficial for short-to-medium term forecasts (6-12 months) and Google data for longer-term forecasts (12-24 months). However, the performance differences were often not statistically significant, with Principal Component Regression (PCR) and Partial Least Squares (PLS) regression consistently underperforming.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:20:22.613227
c973994bdef4a8d4,Forecasting sovereign risk in the Euro area via machine learning,"We test the usefulness of machine learning (ML) for the valuation and pricing of sovereign risk in the Euro area along two important dimensions: i) its predictive accuracy compared with traditional econometric methods, and ii) its assessment of the main economic factors underlying market perceptions of sovereign risk.We find that ML techniques can capture the dynamics inherent in the market valuation of country risk far more efficiently than traditional econometric models, both in the cross‐section and in the time series. Moreover, we show that public sentiment about financial news, redenomination fears and the degree of hawkishness/dovishness expressed in the ECB president's speeches are major contributors to sovereign bond spreads. We also confirm that macroeconomic and global financial factors affect sovereign risk assessment and the corresponding formation of sovereign spreads.",,2023,10.1002/for.2938,,proquest,"This study evaluates the effectiveness of machine learning (ML) for valuing and pricing sovereign risk in the Euro area. It compares ML's predictive accuracy to traditional econometric methods and identifies key economic factors influencing market perceptions of sovereign risk. The findings indicate that ML models outperform traditional methods in capturing the dynamics of country risk valuation and highlight the significant impact of public sentiment, redenomination fears, ECB president's speeches, macroeconomic factors, and global financial conditions on sovereign bond spreads.",True,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T16:20:37.500085
13bf1e3982d4d0a1,Forecasting stock prices changes using long-short term memory neural network with symbolic genetic programming,"This study introduces an augmented Long-Short Term Memory (LSTM) neural network architecture, integrating Symbolic Genetic Programming (SGP), with the objective of forecasting cross-sectional price returns across a comprehensive dataset comprising 4500 listed stocks in the Chinese market over the period from 2014 to 2022. Using the S&P Alpha Pool Dataset for China as basic input, this architecture incorporates data augmentation and feature extraction techniques. The result of this study demonstrates significant improvements in Rank Information coefficient (Rank IC) and IC information ratio (ICIR) by 1128% and 5360% respectively when it is applied to fundamental indicators. For technical indicators, the hybrid model achieves a 206% increase in Rank IC and an impressive surge of 2752% in ICIR. Furthermore, the proposed hybrid SGP-LSTM model outperforms major Chinese stock indexes, generating average annualized excess returns of 31.00%, 24.48%, and 16.38% compared to the CSI 300 index, CSI 500 index, and the average portfolio, respectively. These findings highlight the effectiveness of SGP-LSTM model in improving the accuracy of cross-sectional stock return predictions and provide valuable insights for fund managers, traders, and financial analysts.",,2024,10.1038/s41598-023-50783-0,,proquest,"This study proposes a hybrid Symbolic Genetic Programming-Long-Short Term Memory (SGP-LSTM) neural network model for forecasting stock price returns in the Chinese market. The model integrates data augmentation and feature extraction, demonstrating significant improvements in prediction accuracy (Rank IC and ICIR) for both fundamental and technical indicators. The SGP-LSTM model also outperformed major Chinese stock indexes, generating substantial annualized excess returns.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:20:49.615600
04e2f27a592dcab7,Forecasting the term structure of government bond yields,"Despite powerful advances in yield curve modeling in the last 20 years, comparatively little attention has been paid to the key practical problem of forecasting the yield curve. In this paper we do so. We use neither the no-arbitrage approach nor the equilibrium approach. Instead, we use variations on the Nelson-Siegel exponential components framework to model the entire yield curve, period-by-period, as a three-dimensional parameter evolving dynamically. We show that the three time-varying parameters may be interpreted as factors corresponding to level, slope and curvature, and that they may be estimated with high efficiency. We propose and estimate autoregressive models for the factors, and we show that our models are consistent with a variety of stylized facts regarding the yield curve. We use our models to produce term-structure forecasts at both short and long horizons, with encouraging results. In particular, our forecasts appear much more accurate at long horizons than various standard benchmark forecasts. © 2008 Elsevier B.V., All rights reserved.","Diebold, F.X.; Li, C.",2006,10.1016/j.jeconom.2005.03.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-31344448314&doi=10.1016%2Fj.jeconom.2005.03.005&partnerID=40&md5=c1160704fad68807ebd914e62fa33324,scopus,"This paper forecasts the term structure of government bond yields using a dynamic three-dimensional parameter model based on the Nelson-Siegel framework. The time-varying parameters, interpreted as level, slope, and curvature, are estimated efficiently and modeled using autoregressive processes. The proposed models produce encouraging term-structure forecasts, particularly at long horizons, outperforming standard benchmarks.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-13T16:20:54.599620
d9e813cb1f5afe0d,Forecasting the term structure of interest rates using integrated nested laplace approximations,"This article discusses the use of Bayesian methods for inference and forecasting in dynamic term structure models through integrated nested Laplace approximations (INLA). This method of analytical approximation allows accurate inferences for latent factors, parameters and forecasts in dynamic models with reduced computational cost. In the estimation of dynamic term structure models it also avoids some simplifications in the inference procedures, such as the inefficient two-step ordinary least squares (OLS) estimation. The results obtained in the estimation of the dynamic Nelson-Siegel model indicate that this method performs more accurate out-of-sample forecasts compared to the methods of two-stage estimation by OLS and also Bayesian estimation methods using Markov chain Monte Carlo (MCMC). These analytical approaches also allow efficient calculation of measures of model selection such as generalized cross-validation and marginal likelihood, which may be computationally prohibitive in MCMC estimations. Copyright © 2014 John Wiley & Sons, Ltd. Copyright © 2014 John Wiley & Sons, Ltd. © 2022 Elsevier B.V., All rights reserved.","Laurini, M.P.; Hotta, L.K.",2014,10.1002/for.2288,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897012994&doi=10.1002%2Ffor.2288&partnerID=40&md5=a252763e08afa540fa05096fdebc11ac,scopus,"This article explores the use of Integrated Nested Laplace Approximations (INLA), a Bayesian method, for forecasting the term structure of interest rates within dynamic term structure models. The study demonstrates that INLA provides more accurate out-of-sample forecasts compared to traditional OLS and MCMC methods, while also offering computational advantages for model selection metrics.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-13T16:21:00.128822
3341cd39f6436146,Forecasting time series subject to multiple structural breaks,"This paper provides a new approach to forecasting time series that are subject to discrete structural breaks. We propose a Bayesian estimation and prediction procedure that allows for the possibility of new breaks occurring over the forecast horizon, taking account of the size and duration of past breaks (if any) by means of a hierarchical hidden Markov chain model. Predictions are formed by integrating over the parameters from the meta-distribution that characterizes the stochastic break-point process. In an application to U.S. Treasury bill rates, we find that the method leads to better out-of-sample forecasts than a range of alternative methods. © 2006 The Review of Economic Studies Limited. © 2008 Elsevier B.V., All rights reserved.","Pesaran, M.; Pettenuzzo, D.; Timmermann, A.",2006,10.1111/j.1467-937x.2006.00408.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749045516&doi=10.1111%2Fj.1467-937X.2006.00408.x&partnerID=40&md5=5ea9b46ce031cdf0af74247798ef4184,scopus,This paper introduces a Bayesian approach using a hierarchical hidden Markov chain model to forecast time series with multiple structural breaks. The method accounts for potential new breaks and uses past break information. An application to U.S. Treasury bill rates shows improved out-of-sample forecasts compared to other methods.,True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-13T16:21:12.116082
a2c19207a6e258b2,Foreign exchange risk and risk exposure in the Japanese stock market,"Purpose - Whether stock returns are linked to exchange rate changes and whether foreign exchange risk is priced in a domestic context are less conclusive and thus still subject to a great debate. The purpose of this paper is to provide new empirical evidence on these two inter-related issues, which are critical to investors and corporate risk management. Design/methodology/approach - This paper applies two different econometric approaches: Nonlinear Seemingly Unrelated Regression (NLSUR) via Hansen's Generalized Method of Moment (GMM) and multivariate GARCH in mean (MGARCH-M) to examine the exchange rate exposure and its pricing. Findings - Using industry data for Japan, similar to previous studies, foreign exchange risk is not priced based on the test of an unconditional two-factor asset pricing model. However, strong evidence of time-varying foreign exchange risk premium and significant exchange rate betas are obtained based on the tests of conditional asset pricing models using MGARCH-M approach where both conditional first and second moments of industry returns and risk factors are estimated simultaneously. Research limitations/implications - The strong empirical evidence found in this study implies that corporate currency hedging not only results in more stable cash flows for a firm, but also reduces its cost of capital, and hence is justifiable. Originality/value - This paper conducts an in-depth investigation regarding the exchange rate exposure and its pricing by utilizing two different econometric approaches: NLSUR via Hansen's GMM and MGARCH-M. In doing so, a more reliable conclusion about the exchange rate exposure and its pricing can be drawn.",,2010,10.1108/03074351011042991,,proquest,"This paper investigates the relationship between stock returns and exchange rate changes in the Japanese market using two econometric approaches: Nonlinear Seemingly Unrelated Regression (NLSUR) via Hansen's Generalized Method of Moment (GMM) and multivariate GARCH in mean (MGARCH-M). While unconditional models show no pricing of foreign exchange risk, conditional models reveal time-varying risk premiums and significant exchange rate betas. The findings suggest that corporate currency hedging can stabilize cash flows and reduce the cost of capital.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:21:16.079197
8413a03397041cf2,From Man vs. Machine to Man plus Machine: The art and AI of stock analyses,"An AI analyst trained to digest corporate disclosures, industry trends, and macroeconomic indicators surpasses most analysts in stock return predictions. Nevertheless, humans win Man vs. Machinewhen institutional knowledge is crucial, e.g., involving intangible assets and financial distress. AI wins when information is transparent but voluminous. Humans provide significant incremental value in Man + Machine, which also substantially reduces extreme errors. Analysts catch up with machines after alternative databecome available if their employers build AI capabilities. Documented synergies between humans and machines inform how humans can leverage their advantage for better adaptation to the growing AI prowess.","Cao, Sean; Jiang, Wei; Wang, Junbo; Yang, Baozhong",2024,10.1016/j.jfineco.2024.103910,,wos,"This study explores the synergy between human analysts and AI in stock market analysis. While AI excels at processing large volumes of transparent data for stock return predictions, human analysts remain crucial for understanding intangible assets and financial distress. The combination of human and machine intelligence (Man + Machine) significantly improves predictions and reduces extreme errors, with humans catching up to AI when alternative data becomes available and AI capabilities are integrated.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:21:20.847758
5c50a6dc234cc7e9,Front-Page News: The Effect of News Positioning on Financial Markets,"This paper estimates the effect of news positioning on the speed of price discovery, using exogenous variation in prominent (front-page) positioning of news articles on the Bloomberg terminal. Front-page articles see 240% higher trading volume and 176% larger absolute excess returns during the first 10 minutes after publication than equally important non-front-page articles. Overall, the information in front-page articles is fully incorporated into prices within an hour of publication. The response to non-front-page information of similar importance eventually converges but takes more than two days to be fully reflected in prices.","Fedyk, Anastassia",2024,10.1111/jofi.13287,,wos,"This study investigates how the placement of news (front-page vs. non-front-page) on the Bloomberg terminal impacts the speed of price discovery in financial markets. Front-page news leads to significantly higher trading volume and immediate price changes, with information fully incorporated into prices within an hour. Non-front-page news of similar importance takes over two days to be fully reflected.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:21:22.332768
e7deb740efeb41c6,Gaussian Weighting Reversion Strategy for Accurate Online Portfolio Selection,"In this paper, we design and implement a new on-line portfolio selection strategy based on reversion mechanism and weighted on-line learning. Our strategy, called “Gaussian Weighting Reversion” (GWR), improves the reversion estimator to form optimal portfolios and effectively overcomes the shortcomings of existing on-line portfolio selection strategies. Firstly, GWR uses Gaussian function to weight data in a sliding window to exploit the “time validity” of historical market data. It means that the more recent data are more valuable for market prediction than the earlier. Secondly, the self-learning for various sliding windows is created to make our strategy adaptive to different markets. In addition, double estimations are first proposed to be made at each time point, and the average of double estimations is obtained to alleviate the influence of noise and outliers. Extensive evaluation on six public datasets shows the advantages of our strategy compared with other nine competing strategies, including the state-of-the-art ones. Finally, the complexity analysis of GWR shows its availability in large-scale real-life online trading.",X. Cai; Z. Ye,2019,10.1109/tsp.2019.2941067,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8834832,ieeexplore,"This paper introduces the Gaussian Weighting Reversion (GWR) strategy for online portfolio selection. GWR utilizes a weighted online learning approach with a Gaussian function to prioritize recent market data. It also incorporates self-learning for adaptive window sizes and double estimations to reduce noise. Empirical evaluations on six datasets demonstrate GWR's superiority over nine other strategies, and complexity analysis confirms its suitability for real-time trading.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:34:13.236807
7ff2a7cb36ec3f22,Generating currency trading rules from the term structure of forward foreign exchange premia,"The quality of an exchange rate forecasting model has typically been judged relative to a random-walk in terms of out-of-sample forecast errors. The difficulty of outperforming this benchmark is well documented, although Clarida and Taylor have demonstrated how the random walk can be beaten in this metric by exploiting information embedded within the term structure of forward exchange rate premia. But this achievement does not guarantee success within an investment context. We therefore assess whether the Clarida-Taylor framework can be used to generate significant trading profits in combination with an acceptable degree of risk in a realistic investment portfolio context. (C) 2013 Published by Elsevier Ltd.","Sager, Michael; Taylor, Mark P.",2014,10.1016/j.jimonfin.2013.03.005,,wos,"This paper evaluates whether the Clarida-Taylor framework, which uses information from the term structure of forward exchange rate premia to forecast exchange rates, can generate significant trading profits in a realistic investment portfolio context, going beyond simply outperforming a random-walk benchmark.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:34:19.066800
77d74bafaf54a565,Gold Price Prediction Using Two-layer Decomposition and XGboost Optimized by the Whale Optimization Algorithm,"Gold price prediction is of great importance in big data computing and economic sphere. This paper aims to contribute to the study of hybrid models that can be used to forecast the price of gold. In this study, The Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN) is employed to decompose a residual term containing complex information following the variational modal decomposition (VMD) and an extreme gradient boosting tree (XGBoost) optimized by the Whale Optimization Algorithm (WOA) is combined to construct the VMD-RES.-CEEMDAN-WOA-XGBoost model. The closing price data of COMEX gold futures from 1 October 2018 to 20 November 2023 were selected as examples of gold futures price. A variety of factors that can affect the price of gold are considered in the research. This study indicates that the combined forecasting model proposed in this paper has superior performance when compared to the other comparison forecasting models evaluated. Furthermore, it has been found through SHAP analysis that the Nasdaq index, silver price, and the yield of US 10-year Treasury bonds are most closely related to the prediction of gold price.",,2025,10.1007/s10614-024-10736-9,,proquest,"This paper proposes a hybrid model (VMD-RES.-CEEMDAN-WOA-XGBoost) for gold price prediction, combining Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN) and an extreme gradient boosting tree (XGBoost) optimized by the Whale Optimization Algorithm (WOA). The model was tested on COMEX gold futures data from 2018-2023 and showed superior performance compared to other models. SHAP analysis identified the Nasdaq index, silver price, and US 10-year Treasury bond yield as key factors influencing gold price.",True,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:34:31.001042
d88500794b020572,Governmental Investment Impacts on the Construction Sector Considering the Liquidity Trap,"Considering the liquidity trap is critical as a primary step for a complete understanding of public investment's impacts on the financial supply and demand within the construction industry during deflationary periods. However, minimal research has been conducted to formulate efficient models that can quantify optimal governmental investments. To bridge the gap, an integrated model of the investment savings-liquidity preference money supply (IS-LM) curve and the dynamic stochastic general equilibrium (DSGE) analysis was developed to investigate the balance of supply and demand during deflation status in addition to the associated spending adjustment mechanisms. The most recent data were analyzed, and the deep parameters were obtained using Bayesian estimation via the Markov chain Monte Carlo (MCMC) technique. The analysis result showed that public investment within economies in a deflationary state, which is in a liquidity trap, are expected to crowd out private investment. Also, due to the issuance of government bonds during deflation, the effect of public investment in this situation is more significant than that during inflation. Therefore, decision makers can use the proposed model to manage and quantify the highway construction and maintenance sector's governmental annual optimal investment. © 2021 Elsevier B.V., All rights reserved.","Alshboul, O.; Shehadeh, A.; Hamedat, O.",2022,10.1061/(asce)me.1943-5479.0001003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122233790&doi=10.1061%2F%28ASCE%29ME.1943-5479.0001003&partnerID=40&md5=f82449215e4cf8df3ce0d0b1678d527e,scopus,This study develops an integrated IS-LM and DSGE model to analyze the impact of governmental investment on the construction sector during a liquidity trap. It found that public investment can crowd out private investment and has a more significant effect during deflation than inflation. The model can be used to quantify optimal annual governmental investment in highway construction and maintenance.,True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:34:39.651112
82eadd829f8e6a8b,Habit formation and macroeconomic models of the term structure of interest rates,"This paper introduces a new class of nonaffine models of the term structure of interest rates that is supported by an economy with habit formation. Distinguishing features of the model are that the interest rate dynamics are nonlinear, interest rates depend on lagged monetary and consumption shocks, and the price of risk is not a constant multiple of interest rate volatility. We find that habit persistence can help reproduce the nonlinearity of the spot rate process, the documented deviations from the expectations hypothesis, the persistence of the conditional volatility of interest rates, and the lead-lag relationship between interest rates and monetary aggregates.","Buraschi, Andrea; Jiltsov, Alexei",2007,10.1111/j.1540-6261.2007.01299.x,,wos,"This paper presents a new class of non-affine models for the term structure of interest rates, incorporating habit formation in the economy. Key features include nonlinear interest rate dynamics, dependence on lagged monetary and consumption shocks, and a price of risk not directly proportional to interest rate volatility. The model successfully explains the nonlinearity of the spot rate process, deviations from the expectations hypothesis, persistent conditional volatility, and the lead-lag relationship between interest rates and monetary aggregates.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:35:09.078878
1ffdcdc331372c74,Have trend-following signals in commodity futures markets become less reliable in recent years?,"Various trend-following trading rules have been shown to be valuable for predicting market directions and thus the formulation of investment strategies. However, recent equity market research has provided striking evidence that the predictive power of such rules appears to diminish over time due to increased investor attention and lowered arbitrage barriers. Given that trend-following rules are also very successful and have been widely used in futures markets, we analyze whether a similar effect can be observed for commodity futures contracts. Using a trend regression approach based on time-varying success ratios, we detect significantly higher predictive accuracy for cross-sectional than for time-series strategies. In addition, with the exception of a few commodities, we find no significant trending behavior in trading rule reliability. These results, which are robust in a variety of settings, indicate strong momentum stability in futures markets and justify the application of this class of trading rules in commodity futures investing.","Auer, Benjamin R.",2021,10.1007/s11408-021-00385-5,,wos,"This study investigates whether trend-following signals in commodity futures markets have become less reliable recently, similar to observations in equity markets. The authors analyze trend-following rules using a time-varying success ratio approach. They find that cross-sectional strategies are more accurate than time-series strategies, but overall, there is no significant decline in the reliability of trading rules for most commodities, suggesting stable momentum in futures markets.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:35:14.951095
03efc18d2b3b0920,Health and Quality Risk Assessment of Bottled Water,"The risk and quality assessment paper is dedicated to estimation of impact (for human health) of bottled drinking-water package (especially PET one). The investigation is concentrated on using one integral method for different risk types (factors connected with potential carcinogenicity: concentrations of antimony, formaldehyde, diethylhexylphthalate; and organoleptic factors: turbidity, colour, and pH). We imply the nature of organoleptic (quality assessment) factors close to risk ones because their indirect influence on polluting power of chemicals can be amplifying. The acceptable risk levels for these types are fixed as 10-1 and 10-5 respectively. The research is based on Russian and International (principally, American) scientific researches and standards. The calculation of risk metric is proposed to be estimated in dimensionless number (hazard quotient – HQ). HQ can be transformed in probabilistic numbers in conversion to events per million (Risk Index – RI and risk of olfactory-reflectory impact factors, Integral Index of Water Risk). In the article we used the idea of “chronic daily intake” (CDI) as an acceptable risk-free measure of factors of potential carcinogenicity, which is an adequate evaluation of permissible concentration. 5 brands of Russian bottled water were analyzed, it turned out that one of them had an exceeded acceptable level of risk.",,2019,10.1088/1755-1315/272/2/022142,,proquest,"This paper assesses the health and quality risks associated with bottled water packaging, particularly PET. It uses an integrated method to evaluate carcinogenicity factors (antimony, formaldehyde, diethylhexylphthalate) and organoleptic factors (turbidity, color, pH), proposing a hazard quotient (HQ) for risk calculation. Analysis of five Russian bottled water brands revealed one exceeded acceptable risk levels.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:35:17.084018
2c4a4637c32b86cb,High frequency online inflation and term structure of interest rates: Evidence from China,"In the digital era, the information value of online prices, characterized by weak price stickiness and high sensitivity to economic shocks, deserves more attention. This paper integrates the high-frequency online inflation rate into the dynamic Nelson-Siegel (DNS) model to explore its relationship with the term structure of interest rates. The empirical results show that the weekly online inflation significantly predicts the yield curve, especially the slope factor, whereas the monthly official inflation cannot predict the yield curve and is instead predicted by the yield curve factors. The mechanism analysis reveals that, due to low price stickiness, online inflation is more sensitive to short-term economic fluctuations and better reflects money market liquidity, thereby having significant predictive power for short-term interest rates and the slope factor. Specifically, online inflation for non-durable goods and on weekdays shows stronger predictive power for the slope factor. The heterogeneity in price stickiness across these categories explains the varying impacts on the yield curve. © 2025 Elsevier B.V., All rights reserved.","Zhang, T.; Tang, K.; Liu, T.; Jiang, T.",2025,10.1016/j.jempfin.2025.101626,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006742365&doi=10.1016%2Fj.jempfin.2025.101626&partnerID=40&md5=3f0e5cbafd7d26ef438cc757b09558ad,scopus,"This paper integrates high-frequency online inflation into the dynamic Nelson-Siegel model to analyze its impact on China's interest rate term structure. Empirical findings indicate that weekly online inflation significantly predicts the yield curve, particularly its slope, while monthly official inflation does not. The study suggests that online inflation's sensitivity to economic fluctuations and its reflection of money market liquidity contribute to its predictive power for short-term interest rates and the slope factor, with non-durable goods and weekday online inflation showing stronger effects.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-13T16:35:48.148715
7263c110b7556c6d,"How Are Interbank and Sovereign Debt Markets Linked? Evidence from 14 OECD Countries, the Euro Area and Russia","The paper explores causal linkages between interbank and sovereign bond markets in 14 OECD countries, the Euro area and Russia during the 2008-2009 crisis and post-crisis period. The analysis has been carried out for individual countries and in a multivariate framework. It enables to identify systemically important countries in both markets. The USA, Switzerland, Australia, South Korea and Russia are of particular significance in the interbank lending market. Switzerland, the UK, Poland, Australia and Canada play a pivotal role in the public debt market. The analysis under the multivariate framework reveals substantial heterogeneity in the network structure of both markets. Only 12% of causal relationships coincide, which may fuel financial contagion. Volatility spillovers underlie the causal linkages. They are estimated by means of dynamic volatility indices based on rolling correlation matrices and help identify the transformation of the international banking turmoil into the sovereign debt crisis.","Stolbov, Mikhail",2014,10.2298/pan1403331s,,wos,"This paper investigates the causal links between interbank and sovereign bond markets in 14 OECD countries, the Euro area, and Russia, focusing on the 2008-2009 crisis and post-crisis periods. It identifies systemically important countries in both markets and reveals significant heterogeneity in network structures, with only 12% of causal relationships coinciding. Volatility spillovers are identified as the basis for these causal linkages, explaining the transmission of international banking turmoil to sovereign debt crises.",True,True,False,gemini-2.5-flash-lite,Trine,M,,2025-10-13T16:36:08.536217
e43070a782da8f80,How good are analyst forecasts of oil prices?,"Even though there is a wide consensus that having good oil price forecasts is very valuable for many agents in the economy, results have not been fully satisfactory and there is an ongoing effort to improve their accuracy. Research has explored many different modeling approaches including time series, regressions, and artificial intelligence, among others. Also, many different sources of input data have been used like spot and futures prices, product spreads, and micro and macro variables. This paper explores how useful analyst expected price data are for forecasting when appropriate measures are taken to account for their sparse nature and high volatility. It proposes a multifactor stochastic pricing model, with time-varying risk premiums calibrated with filtered futures and analyst forecasts using a Kalman Filter. The forecasting model is applied to ten years of oil prices and analyst forecasts, from NYMEX and Bloomberg, respectively. Results are very encouraging showing that the model forecasts are much better than the no-change forecasts, commonly used as a benchmark, and better than those from the widely used Bloomberg's Consensus Expected Price Model. We conclude that analyst forecasts are a valuable source of input data that should be considered in future forecasting models.",,2021,10.1016/j.eneco.2021.105500,,proquest,"This paper investigates the usefulness of analyst forecasts for predicting oil prices. It proposes a multifactor stochastic pricing model incorporating a Kalman Filter to handle sparse and volatile analyst data. The model, tested on ten years of oil price data, demonstrates superior forecasting accuracy compared to benchmark models, suggesting analyst forecasts are a valuable input for future models.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:36:12.999791
339904381e96023a,Hybrid LSTM–Transformer Architecture with Multi-Scale Feature Fusion for High-Accuracy Gold Futures Price Forecasting,"Amidst global economic fluctuations and escalating geopolitical risks, gold futures, as a pivotal safe-haven asset, demonstrate price dynamics that directly impact investor decision-making and risk mitigation effectiveness. Traditional forecasting models face significant limitations in capturing long-term trends, addressing abrupt volatility, and mitigating multi-source noise within complex market environments characterized by nonlinear interactions and extreme events. Current research predominantly focuses on single-model approaches (e.g., ARIMA or standalone neural networks), inadequately addressing the synergistic effects of multimodal market signals (e.g., cross-market index linkages, exchange rate fluctuations, and policy shifts) and lacking the systematic validation of model robustness under extreme events. Furthermore, feature selection often relies on empirical assumptions, failing to uncover non-explicit correlations between market factors and gold futures prices. A review of the global literature reveals three critical gaps: (1) the insufficient integration of temporal dependency and global attention mechanisms, leading to imbalanced predictions of long-term trends and short-term volatility; (2) the neglect of dynamic coupling effects among cross-market risk factors, such as energy ETF-metal market spillovers; and (3) the absence of hybrid architectures tailored for high-frequency noise environments, limiting predictive utility for decision support. This study proposes a three-stage LSTM–Transformer–XGBoost fusion framework. Firstly, XGBoost-based feature importance ranking identifies six key drivers from thirty-six candidate indicators: the NASDAQ Index, S&P 500 closing price, silver futures, USD/CNY exchange rate, China’s 1-year Treasury yield, and Guotai Zhongzheng Coal ETF. Second, a dual-channel deep learning architecture integrates LSTM for long-term temporal memory and Transformer with multi-head self-attention to decode implicit relationships in unstructured signals (e.g., market sentiment and climate policies). Third, rolling-window forecasting is conducted using daily gold futures prices from the Shanghai Futures Exchange (2015–2025). Key innovations include the following: (1) a bidirectional LSTM–Transformer interaction architecture employing cross-attention mechanisms to dynamically couple global market context with local temporal features, surpassing traditional linear combinations; (2) a Dynamic Hierarchical Partition Framework (DHPF) that stratifies data into four dimensions (price trends, volatility, external correlations, and event shocks) to address multi-driver complexity; (3) a dual-loop adaptive mechanism enabling endogenous parameter updates and exogenous environmental perception to minimize prediction error volatility. This research proposes innovative cross-modal fusion frameworks for gold futures forecasting, providing financial institutions with robust quantitative tools to enhance asset allocation optimization and strengthen risk hedging strategies. It also provides an interpretable hybrid framework for derivative pricing intelligence. Future applications could leverage high-frequency data sharing and cross-market risk contagion models to enhance China’s influence in global gold pricing governance.",,2025,10.3390/math13101551,,proquest,"This study proposes a hybrid LSTM–Transformer–XGBoost fusion framework for gold futures price forecasting, addressing limitations of traditional models in capturing long-term trends, volatility, and multi-source noise. The framework integrates XGBoost for feature importance, a dual-channel deep learning architecture (LSTM and Transformer) for temporal memory and implicit relationship decoding, and rolling-window forecasting. Key innovations include a bidirectional LSTM–Transformer interaction architecture, a Dynamic Hierarchical Partition Framework (DHPF), and a dual-loop adaptive mechanism. The research aims to provide financial institutions with robust quantitative tools for asset allocation and risk hedging.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:36:17.737075
f85dca55e6510587,"Identifying and Exploiting Alpha in Linear Asset Pricing Models with Strong, Semi-Strong, and Latent Factors","The risk premia of traded factors are the sum of factor means and a parameter vector, we denote by ϕ, which is identified from the cross-sectional regression of α<inf>i</inf> on the vector of factor loadings, β<inf>i</inf>. If ϕ is non-zero, then α<inf>i</inf> are non-zero and one can construct “phi-portfolios” which exploit the systematic components of non-zero alpha. We show that for known values of β<inf>i</inf> and when ϕ is non-zero, there exist phi-portfolios that dominate mean–variance (MV) portfolios. This article then proposes a two-step bias corrected estimator of ϕ and derives its asymptotic distribution allowing for idiosyncratic pricing errors, weak missing factors, and weak error cross-sectional dependence. Small sample results from extensive Monte Carlo experiments show that the proposed estimator has the correct size with good power properties. This article also provides an empirical application to a large number of U.S. securities with risk factors selected from a large number of potential risk factors according to their strength and constructs phi-portfolios and compares their Sharpe ratios to MV and S&P portfolios. © 2025 Elsevier B.V., All rights reserved.","Pesaran, M.H.; Smith, R.P.",2025,10.1093/jjfinec/nbae029,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003408796&doi=10.1093%2Fjjfinec%2Fnbae029&partnerID=40&md5=f1f288926de6bb2574a79c557b5caa70,scopus,"This article proposes a bias-corrected estimator for factor risk premia (phi) in linear asset pricing models, allowing for idiosyncratic pricing errors and weak dependencies. It demonstrates that 'phi-portfolios' can outperform mean-variance portfolios when phi is non-zero. An empirical application to U.S. securities is presented, comparing the performance of these portfolios.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:36:30.069873
b6709facf4548a31,Identifying and measuring the contagion channels at work in the European financial crises,"We investigate the phenomenon of contagion with a special focus on the recent financial crisis, distinguishing four alternative channels, namely the flight-to-quality, flight-to-liquidity, risk premium, and correlated information channels. Specifically, we employ the differences among estimates and impulse response functions across linear and nonlinear models to identify and measure cross-asset contagion. An application to weekly Eurozone data for a 2007–2014 sample reveals that a two-state Markov switching model shows economically weak, though accurately estimated, contagion effects in a crisis regime. These findings are mainly explained by a flight-to-quality channel. Furthermore, we extend our analysis to explore whether European markets may or may not have been subject to contagion when exposed to external shocks, such as those originated from the US subprime crisis. © 2017 Elsevier B.V., All rights reserved.","Guidolin, M.; Pedio, M.",2017,10.1016/j.intfin.2017.01.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009812010&doi=10.1016%2Fj.intfin.2017.01.001&partnerID=40&md5=28b192b2ae5c94f358f7816372c12690,scopus,"This paper investigates contagion channels during the European financial crises, distinguishing between flight-to-quality, flight-to-liquidity, risk premium, and correlated information channels. Using linear and nonlinear models applied to Eurozone data from 2007-2014, the study finds weak contagion effects in a crisis regime, primarily attributed to a flight-to-quality channel. The analysis also explores contagion from external shocks, such as the US subprime crisis.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:36:32.453297
1ff0c33a84a29bac,Implied volatility sentiment: a tale of two tails,"We propose a sentiment measure jointly derived from out-of-the-money index puts and single stock calls: implied volatility (IV-) sentiment. In contrast to implied correlations, our measure uses information from the tails of the risk-neutral densities from these two markets rather than across their entire moneyness structures. We find that IV-sentiment measure adds value over and above traditional factors in predicting the equity risk premium out-of-sample. Forecasting results are superior when constrained ensemble models are used vis-à-vis unregularized machine learning techniques. In a mean-reversion strategy, our IV-sentiment measure delivers economically significant results, with limited exposure to a set of cross-sectional equity factors, including Fama and French's five factors, the momentum factor and the low-volatility factor, and seems valuable in preventing momentum crashes. Our novel measure reflects overweight of tail events, which we interpret as a behavioral bias. However, we cannot rule out a risk-compensation rationale. © 2020 Elsevier B.V., All rights reserved.","Félix, L.; Kräussl, R.; Stork, P.",2020,10.1080/14697688.2019.1696018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078861813&doi=10.1080%2F14697688.2019.1696018&partnerID=40&md5=411cec64c69eb2f534d55dd481c69d0b,scopus,"This paper introduces a novel sentiment measure, IV-sentiment, derived from out-of-the-money index puts and single stock calls. This measure utilizes information from the tails of risk-neutral densities. The study demonstrates that IV-sentiment enhances out-of-sample predictions of the equity risk premium, particularly when using constrained ensemble models. It also proves effective in a mean-reversion strategy, showing economic significance and limited exposure to common equity factors, while helping to prevent momentum crashes. The authors interpret the measure as reflecting a behavioral bias related to tail events, though a risk-compensation explanation is also considered.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:36:49.816889
ffa96bdfcb8e2559,Improving the Accuracy of Forecasting the TSA Daily Budgetary Fund Balance Based on Wavelet Packet Transforms,"Improving the accuracy of cash flow forecasting in the TSA is the key to fulfilling government payment obligations, minimizing the cost of maintaining the cash reserve, providing the absence of outstanding debt accumulation, and ensuring investment in various financial instruments to obtain additional income. The article describes a method for improving the accuracy of forecasting a time series composed of daily budgetary fund balances in the TSA, based on its preliminary decomposition using a discrete wavelet packet transform of the Daubechies family. This makes it possible to increase the accuracy of traditional forecasting methods from 80% to more than 96%. The decomposition level varied from one to eight to minimize the mean absolute error and improve the forecasting accuracy. Calculations of statistical tests for adequacy confirm the effectiveness of the proposed method for improving forecasting accuracy. The scientific novelty of the proposed method for improving the forecasting accuracy of time series from daily budgetary fund balances in the TSA lies in proving the need for preliminary timeseries decomposition and subsequent construction of forecasts for the obtained parts, resulting in high forecasting accuracy. The result differs significantly from traditional econometric methods (ARIMA/SARIMA), characterized by a much lower accuracy (50–80%) and a decrease in forecasting accuracy with an increase in the forecast horizon. This article is novel, as it forms a new approach to solving the problem of increasing the efficiency of using budgetary funds, associated with improving the accuracy of forecasting daily budgetary fund balance in the TSA.",,2022,10.3390/joitmc8030107,,proquest,This article proposes a method to improve the accuracy of forecasting the TSA daily budgetary fund balance by using wavelet packet transforms for time series decomposition before applying traditional forecasting methods. This approach significantly increases accuracy compared to standard econometric models like ARIMA/SARIMA.,True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:37:01.321618
5355b131ba74d9f1,Incorporating Research Reports and Market Sentiment for Stock Excess Return Prediction: A Case of Mainland China,"The prediction of stock excess returns is an important research topic for quantitative trading, and stock price prediction based on machine learning is receiving more and more attention. This article takes the data of Chinese A-shares from July 2014 to September 2017 as the research object, and proposes a method of stock excess return forecasting that combines research reports and investor sentiment. The proposed method measures individual stocks released by analysts, separates the two indicators of research report attention and rating sentiment, calculates investor sentiment based on external market factors, and uses the LSTM model to represent the time series characteristics of stocks. The results show that (1) the accuracy and F1 evaluation indicators are used, and the proposed algorithm is better than the benchmark algorithm. (2) The performance of deep learning LSTM algorithm is better than traditional machine learning algorithm SVM. (3) Investor sentiment as the initial hidden state of the model can improve the accuracy of the algorithm. (4) The attention of the split research report takes the two indicators of investor sentiment and price as the input of the model, which can effectively improve the performance of the model.",,2020,10.1155/2020/8894757,,proquest,"This study proposes a method for forecasting stock excess returns in Mainland China by combining analyst research reports and investor sentiment, using an LSTM model. The results indicate that this combined approach outperforms benchmark algorithms and traditional machine learning models like SVM, with specific components like investor sentiment as the initial hidden state and split research report attention improving model performance.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:37:05.617592
f02729ef603cf0e9,Inference on co-integration parameters in heteroskedastic vector autoregressions,"We consider estimation and hypothesis testing on the coefficients of the co-integrating relations and the adjustment coefficients in vector autoregressions driven by shocks which display both conditional and unconditional heteroskedasticity of a quite general and unknown form. We show that the conventional results in Johansen (1996) for the maximum likelihood estimators and associated likelihood ratio tests derived under homoskedasticity do not in general hold under heteroskedasticity. As a result, standard confidence intervals and hypothesis tests on these coefficients are potentially unreliable. Solutions based on Wald tests (using a sandwich estimator of the variance matrix) and on the use of the wild bootstrap are discussed. These do not require the practitioner to specify a parametric model for volatility. We establish the conditions under which these methods are asymptotically valid. A Monte Carlo simulation study demonstrates that significant improvements in finite sample size can be obtained by the bootstrap over the corresponding asymptotic tests in both heteroskedastic and homoskedastic environments. An application to the term structure of interest rates in the US illustrates the difference between standard and bootstrap inferences regarding hypotheses on the co-integrating vectors and adjustment coefficients. (C) 2015 Elsevier B.V. All rights reserved.","Boswijk, H. Peter; Cavaliere, Giuseppe; Rahbek, Anders; Taylor, A. M. Robert",2016,10.1016/j.jeconom.2015.07.005,,wos,"This paper investigates the estimation and hypothesis testing of co-integration parameters in vector autoregressions with heteroskedasticity. It demonstrates that standard methods assuming homoskedasticity are unreliable under heteroskedasticity and proposes solutions using Wald tests and wild bootstrap, which do not require specifying a parametric volatility model. A simulation study shows bootstrap improvements, and an application to the US term structure of interest rates illustrates differences in inference.",False,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T16:37:19.836836
65827861f45c0f78,Inflation Prediction Method Based on Deep Learning,"Forward-looking forecasting of the inflation rate could help the central bank and other government departments to better use monetary policy to stabilize prices and prevent the impact of inflation on market entities, especially for low- and middle-income groups. It can also help financial institutions and investors better make investment decisions. In this sense, the forecast of inflation rate is of great significance. The existing literature mainly uses linear models such as autoregressive (AR) and vector autoregressive (VAR) models to predict the inflation rate. The nonlinear relationship between variables and the mining of historical data information are relatively lacking. Therefore, the prediction strategies and accuracy of the existing literature need to be improved. The predictive model designed in deep learning can fully mine the nonlinear relationship between variables and process complex long-term time series dynamic information, thereby making up for the deficiencies of existing research. Therefore, this paper employs the recurrent neural networks with gated recurrent unit (GRU-RNN) model to train and analyze the Consumer Price Index (CPI) indicators to obtain inflation-related prediction results. The experimental results on historical data show that the GRU-RNN model has good performance in predicting China's inflation rate. In comparison, the performance of the proposed method is significantly better than some traditional models, showing its superior effectiveness.","Yang, Cheng; Guo, Shuhua",2021,10.1155/2021/1071145,,wos,"This paper proposes a deep learning model, specifically a Gated Recurrent Unit Recurrent Neural Network (GRU-RNN), for predicting China's inflation rate using Consumer Price Index (CPI) data. The authors argue that deep learning models can better capture nonlinear relationships and long-term dynamics compared to traditional linear models like AR and VAR, leading to improved prediction accuracy. Experimental results demonstrate that the GRU-RNN model outperforms traditional methods.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:37:24.303108
3de04c8e7553d522,Interest Rate Based on The Lie Group SO(3) in the Evidence of Chaos,"This paper aims to test the structure of interest rates during the period from 1 September 1981 to 28 December 2020 by using Lie algebras and groups. The selected period experienced substantial events impacting interest rates, such as the economic crisis, the military intervention of the USA in Iraq, and the COVID-19 pandemic, in which economies were in lockdown. These conditions caused the interest rate to have a nonlinear structure, chaotic behavior, and outliers. Under these conditions, an alternative method is proposed to test the random and nonlinear structure of interest rates to be evolved by a stochastic differential equation captured on a curved state space based on Lie algebras and group. Then, parameter estimates of this equation were obtained by OLS, NLS, and GMM estimators (hereafter, LieNLS, LieOLS, and LieGMM, respectively). Therefore, the interest rates that possess nonlinear structures and/or chaotic behaviors or outliers were tested with LieNLS, LieOLS, and LieGMM. We compared our LieNLS, LieOLS, and LieGMM results with the traditional OLS, NLS, and GMM methods, and the results favor the improvement achieved by the proposed LieNLS, LieOLS, and LieGMM in terms of the RMSE and MAE in the out-of-sample forecasts. Lastly, the Lie algebras with NLS estimators exhibited the lowest RMSE and MAE followed by the Lie algebras with GMM, and the Lie algebras with OLS, respectively.",,2022,10.3390/math10213998,,proquest,"This paper investigates the nonlinear and chaotic structure of interest rates from 1981 to 2020 using Lie algebras and groups. It proposes an alternative method to test random and nonlinear structures, evolving interest rates with a stochastic differential equation on a curved state space. The study compares parameter estimates from LieNLS, LieOLS, and LieGMM with traditional OLS, NLS, and GMM, finding improved out-of-sample forecast accuracy with the Lie-based methods, particularly LieNLS.",True,False,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T16:38:12.568852
b5ff09ccbbe072af,Interest rate fluctuations and the UK financial services industry,"The article explores the relationship between short-term interest rates and the equity returns of the UK financial services industry. Based on the arbitrage pricing theory, the present study seeks to answer the sensitivity and pricing questions. The former is tested with a linear two-index model attempting to identify any interest rate risk exposure of these stock returns. The latter, however, is examined using a nonlinear multivariate analysis based on the Seemingly Unrelated Regression Equations (SURE) model by imposing cross- and within-equation constraints on the estimated parameters. The econometric analysis unveils a significant negative interest rate effect and the existence of a risk premium incorporated in the expected returns of portfolios consisting of these stocks. © 2007 Elsevier B.V., All rights reserved.","Artikis, P.; Kalotychou, E.; Staikouras, S.K.",2007,10.1080/17446540601118319,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548570513&doi=10.1080%2F17446540601118319&partnerID=40&md5=519a8765fc7ddc63aca544b961ed56b8,scopus,"This study investigates the impact of short-term interest rate fluctuations on the equity returns of the UK financial services industry, utilizing arbitrage pricing theory. It employs a linear two-index model to assess interest rate risk exposure and a nonlinear multivariate SURE model to examine pricing questions, revealing a significant negative interest rate effect and a risk premium in expected returns.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:38:30.020617
7a23cbb917fcc583,"Interest rate next-day variation prediction based on hybrid feedforward neural network, particle swarm optimization, and multiresolution techniques","Multiresolution analysis techniques including continuous wavelet transform, empirical mode decomposition, and variational mode decomposition are tested in the context of interest rate next-day variation prediction. In particular, multiresolution analysis techniques are used to decompose interest rate actual variation and feedforward neural network for training and prediction. Particle swarm optimization technique is adopted to optimize its initial weights. For comparison purpose, autoregressive moving average model, random walk process and the naive model are used as main reference models. In order to show the feasibility of the presented hybrid models that combine multiresolution analysis techniques and feedforward neural network optimized by particle swarm optimization, we used a set of six illustrative interest rates; including Moody's seasoned Aaa corporate bond yield, Moody's seasoned Baa corporate bond yield, 3-Month, 6-Month and 1-Year treasury bills, and effective federal fund rate. The forecasting results show that all multiresolution-based prediction systems outperform the conventional reference models on the criteria of mean absolute error, mean absolute deviation, and root mean-squared error. Therefore, it is advantageous to adopt hybrid multiresolution techniques and soft computing models to forecast interest rate daily variations as they provide good forecasting performance. © 2021 Elsevier B.V., All rights reserved.","Lahmiri, S.",2016,10.1016/j.physa.2015.09.061,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945945078&doi=10.1016%2Fj.physa.2015.09.061&partnerID=40&md5=68bb4b64f208c7540f7135a221279046,scopus,"This study proposes a hybrid model combining multiresolution analysis techniques (continuous wavelet transform, empirical mode decomposition, variational mode decomposition) with a feedforward neural network optimized by particle swarm optimization for predicting next-day interest rate variations. The model was tested on six different interest rates and demonstrated superior performance compared to conventional models like ARMA, random walk, and naive models, based on MAE, MAD, and RMSE metrics.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-13T16:39:01.353895
e1ac05598dcf5552,Interest rates mapping,"The present study deals with the analysis and mapping of Swiss franc interest rates. Interest rates depend on time and maturity, defining term structure of the interest rate curves (IRC). In the present study IRC are considered in a two-dimensional feature space-time and maturity. Exploratory data analysis includes a variety of tools widely used in econophysics and geostatistics. Geostatistical models and machine learning algorithms (multilayer perceptron and Support Vector Machines) were applied to produce interest rate maps. IR maps can be used for the visualisation and pattern perception purposes, to develop and to explore economical hypotheses, to produce dynamic asset-liability simulations and for financial risk assessments. The feasibility of an application of interest rates mapping approach for the IRC forecasting is considered as well. © 2008 Elsevier Ltd. All rights reserved. © 2008 Elsevier B.V., All rights reserved.","Kanevski, M.; Maignan, M.; Pozdnoukhov, A.; Timonin, V.",2008,10.1016/j.physa.2008.02.069,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42649101653&doi=10.1016%2Fj.physa.2008.02.069&partnerID=40&md5=3d37fefbc596802a878a5487263c92be,scopus,"This study analyzes and maps Swiss franc interest rates, considering their dependence on time and maturity to define the term structure of interest rate curves (IRC). It applies geostatistical models and machine learning algorithms (multilayer perceptron and Support Vector Machines) to create interest rate maps. These maps can be used for visualization, hypothesis exploration, financial simulations, and risk assessment. The study also explores the feasibility of using this mapping approach for IRC forecasting.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:39:12.537377
84ac3579499de299,Interval forecasting. An analysis based upon ARCH-quantile estimators,"In this paper we explore techniques for obtaining interval forecasts based on estimated time-series models for processes which may exhibit autoregressive conditional heteroskedasticity (ARCH). To deal with the available variety of possible interval forecasts, we propose a method for combining these forecasts based on quantile regression techniques. Our approach is practical rather than theoretical, with attention focused directly on obtaining interval forecasts for two U.S. time series: a measure of unemployment and a Treasury bill rate. We evaluate the performance of our procedures using a variety of diagnostics. We find interval estimates which perform reasonably well, judged by both in-sample and out-of-sample criteria. Our experience suggests that a certain amount of care is required in order to obtain useful forecasts. © 1989. © 2014 Elsevier B.V., All rights reserved.","Granger, C.W.J.; White, H.; Kamstra, M.",1989,10.1016/0304-4076(89)90031-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38249026075&doi=10.1016%2F0304-4076%2889%2990031-6&partnerID=40&md5=38e979d6b16ecea739e87e183cf50b40,scopus,"This paper proposes a practical method for combining interval forecasts from ARCH-quantile estimators for time-series models, focusing on applications to U.S. unemployment and Treasury bill rates. The performance is evaluated using in-sample and out-of-sample criteria, suggesting careful implementation is needed for useful forecasts.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-13T16:39:47.037190
9172933e44b166cd,Investigation of the Financial Stability of S&P 500 Using Realized Volatility and Stock Returns Distribution,"In this work, the financial data of 377 stocks of Standard & Poor’s 500 Index (S&P 500) from the years 1998–2012 with a 250-day time window were investigated by measuring realized stock returns and realized volatility. We examined the normal distribution and frequency distribution for both daily stock returns and volatility. We also determined the beta-coefficient and correlation among the stocks for 15 years and found that, during the crisis period, the beta-coefficient between the market index and stock’s prices and correlation among stock’s prices increased remarkably and decreased during the non-crisis period. We compared the stock volatility and stock returns for specific time periods i.e., non-crisis, before crisis and during crisis year in detail and found that the distribution behaviors of stock return prices has a better long-term effect that allows predictions of near-future market behavior than realized volatility of stock returns. Our detailed statistical analysis provides a valuable guideline for both researchers and market participants because it provides a significantly clearer comparison of the strengths and weaknesses of the two methods.",,2018,10.3390/jrfm11020022,,proquest,"This study investigates the financial stability of the S&P 500 by analyzing realized stock returns and volatility for 377 stocks from 1998-2012. It examines stock return and volatility distributions, beta-coefficients, and correlations, finding that beta and correlation increase during crisis periods. The research suggests stock return price distributions are better predictors of near-future market behavior than realized volatility.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:39:49.187102
57b422147c25c2e6,Investor attention and FX market volatility,"We study the relationship between investors' active attention, measured by a Google search volume index (SVI), and the dynamics of currency prices. Investor attention is correlated with the trading activities of large FX market participants. Investor attention comoves with contemporaneous FX market volatility and predicts subsequent FX market volatility, after controlling for macroeconomic fundamentals. In addition, investor attention is related to the currency risk premium. Our results suggest that investor attention is a priced source of risk in FX markets. © 2015 Elsevier B.V., All rights reserved.","Goddard, J.; Kita, A.; Wang, Q.",2015,10.1016/j.intfin.2015.05.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931270574&doi=10.1016%2Fj.intfin.2015.05.001&partnerID=40&md5=74e9e7e91322306d1e91a40836ecb27b,scopus,"This study investigates the link between investor attention, proxied by Google search volume, and foreign exchange (FX) market volatility. The findings indicate that investor attention correlates with trading by large FX participants, moves with current FX volatility, and can predict future volatility, even when accounting for macroeconomic factors. Furthermore, attention is associated with currency risk premiums, suggesting it's a priced risk factor in FX markets.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:39:50.508716
95a3a3ccfd16d50a,Investor attention and cryptocurrency: Evidence from the Bitcoin market,"[...]investor attention had been applied in traditional financial markets, i.e., stock market and FX market, and proved to be an influential factor in certain markets.The empirical results may shed lights on investors in Bitcoin market to focus more on the variations in behavioral variable; Second, existing studies mainly focused on the linear connections between Bitcoin market and investor attention, failing to comprehensively explore the non-linear connections between the two.[...]current research may be incomplete in explaining the relationships between investor attention and Bitcoin market.The results for out-of-sample predictions further illustrate the importance of investor attention in Bitcoin market and will surely guide the investors to forecast the Bitcoin return with the investor attention.[...]the empirical results add evidence on the in-sample and out-of-sample analysis; Fourth, based on the empirical results of out-of-sample predictions for Bitcoin return, we construct several simple portfolios including Bitcoin asset and risk-free asset to further explore the usefulness of investor attention in Bitcoin portfolio management based on the framework of asset allocation.[...]Neves [42] suggested that investment attractiveness had a prominent role in Bitcoin price formation, while other researchers [3, 34, 43, 44] argued the stock market, exchange rate, gold, oil, Economic Policy Uncertainty (EPU), and the Geopolitical Risk Index, etc.",,2021,10.1371/journal.pone.0246331,,proquest,"This study investigates the impact of investor attention on the Bitcoin market, extending previous research by exploring non-linear relationships and utilizing investor attention for out-of-sample predictions and portfolio management. The findings suggest that investor attention is a significant factor in forecasting Bitcoin returns and can guide asset allocation strategies.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:39:54.603359
d7eb87f6f4ac7e58,Investor attention and stock market volatility,"We investigate, in a theoretical framework, the joint role played by investors' attention to news and learning uncertainty in determining asset prices. The model provides two main predictions. First, stock return variance and risk premia increase with both attention and uncertainty. Second, this increasing relationship is quadratic. We empirically test these two predictions, and we show that the data lend support to the increasing relationship. The evidence for a quadratic relationship is mixed. Overall, our study shows theoretically and empirically that both attention and uncertainty are key determinants of asset prices. © 2019 Elsevier B.V., All rights reserved.","Andrei, D.; Hasler, M.",2015,10.1093/rfs/hhu059,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939258280&doi=10.1093%2Frfs%2Fhhu059&partnerID=40&md5=6ea0f026b737f00de5f512be2b375874,scopus,"This study theoretically and empirically investigates how investor attention to news and learning uncertainty jointly influence asset prices. It predicts that stock return variance and risk premia increase with both attention and uncertainty, with a potentially quadratic relationship. The empirical tests support the increasing relationship but provide mixed evidence for the quadratic aspect.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:39:57.035784
54f9c8bc1b19bffb,Investor attention using the Google search volume index – impact on stock returns,"PurposeThe purpose of this paper is to investigate whether the investor attention using the Google search volume index (GSVI) can be used to forecast stock returns. The authors also find the answer to whether the “price pressure hypothesis” would hold true for the Indian stock market.Design/methodology/approachThe authors employ a more recent fully balanced panel data for the period from July 2012 to Jun 2017 (260 weeks) of observations for companies of NIFTY 50 of the National Stock Exchange in the Indian stock market. The authors are motivated by Tetlock (2007) and Bijl et al. (2016) to employ regression approach of econometric estimation.FindingsThe authors find that high Google search volumes lead to positive returns. More precisely, the high Google search volumes predict positive and significant returns in the subsequent fourth and fifth weeks. The GSVI performs as an useful predictor of the direction as well as the magnitude of the excess returns. The higher quantiles of the GSVI have corresponding higher excess returns. The authors notice that the domestic investor searches are correlated with higher excess returns than the worldwide investor searches. The findings imply that the signals from the search volume data could be of help in the construction of profitable trading strategies.Originality/valueTo the best of the authors knowledge, no paper has examined the relationship between Google search intensity and stock-trading behavior in the Indian stock market. The authors use a more recent data for the period from 2012 to 2017 to investigate whether search query data on company names can be used to predict weekly stock returns for individual firms. This study complements the prior studies by investigating the relationship between search intensity and stock-trading behavior in the Indian stock market.",,2019,10.1108/rbf-04-2018-0033,,proquest,"This study investigates the predictive power of Google Search Volume Index (GSVI) for stock returns in the Indian stock market. Using a balanced panel data from July 2012 to June 2017 for NIFTY 50 companies, the authors found that higher GSVI predicts positive and significant stock returns in the subsequent fourth and fifth weeks. The GSVI is shown to be a useful predictor of both the direction and magnitude of excess returns, with higher quantiles of GSVI corresponding to higher excess returns. Domestic investor searches were found to be more correlated with higher excess returns than worldwide searches. The findings suggest that search volume data can aid in constructing profitable trading strategies.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:40:00.137176
cc24ac6a38116c55,Is nonlinear drift implied by the short end of the term structure?,"Nonlinear drift models of the short rate are estimated using data on the short end of the term structure, where the cross-sectional relation is obtained by an analytical approximation. The findings reveal that (i) nonlinear physical drift is not implied unless it is strongly affected by cross-sectional dimensions of the data; (ii) nonlinear risk-neutral drift that allows for fast mean reversion for high rates is desirable to explain and predict observed patterns of yield spreads; and (iii) for higher frequency data from which transitory shocks are removed, (ii) still remains valid although the nonlinearity is somewhat reduced. Reprinted by permission of Oxford University Press",,2008,10.1093/rfs/hhm072,,proquest,"This study estimates nonlinear drift models for the short rate using data from the short end of the term structure. It finds that nonlinear risk-neutral drift is necessary to explain yield spread patterns, especially when allowing for fast mean reversion at high rates. While nonlinearity is somewhat reduced with higher frequency data, the model remains valid.",True,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T16:40:12.371516
465719be2fd4d237,Is the short rate drift actually nonlinear?,"Ait-Sahalia (1996) and Stanton (1997) use nonparametric estimators applied to short-term interest rate data to conclude that the drift function contains important nonlinearities. We study the finite-sample properties of their estimators by applying them to simulated sample paths of a square-root diffusion. Although the drift function is linear, both estimators suggest nonlinearities of the type and magnitude reported in Ait-Sahalia (1996) and Stanton (1997). Combined with the results of a weighted least squares estimator, this evidence implies that nonlinearity of the short rate drift is not a robust stylized fact.","Chapman, DA; Pearson, ND",2000,10.1111/0022-1082.00208,,wos,"This paper investigates the robustness of nonlinearities in the short rate drift, as suggested by previous nonparametric estimators. By applying these estimators to simulated data from a linear drift model, the authors demonstrate that the estimators can falsely detect nonlinearities. This suggests that the nonlinearity of the short rate drift may not be a reliable characteristic.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:40:30.268022
ea00aca5b04457b7,Is the term structure nonlinear? A semiparametric investigation,A semiparametric error correction model (ECM) is estimated using US term structure data. We use 5 and 10 year interest rates to predict short-term (1 month to 12 month) interest rates. It is found that the semiparametric ECM model predicts better than the popular linear ECM. These results provide further evidence of nonlinearity in the term structure.,"Bachmeier, L; Li, Q",2002,10.1080/13504850110053275,,wos,"This study estimates a semiparametric error correction model (ECM) using US term structure data, predicting short-term interest rates with 5 and 10-year rates. The semiparametric ECM outperforms the linear ECM, suggesting nonlinearity in the term structure.",True,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T16:40:39.920762
1931a2bd3a13f6eb,Joint Estimation of Factor Sensitivities and Risk Premia for the Arbitrage Pricing Theory,"The APT is represented as a multivariate regression model with across‐equations restrictions. Both observed and unobserved (latent) macroeconomic factors are included, thus generalizing and unifying two previous strands of literature. Large portfolios representing unobserved factors are treated as endogenous, and nonlinear 3SLS estimates are shown to differ sharply from estimates that ignore this endogeneity. Using monthly stock returns and six factors, we cannot reject January effects. The following results are invariant with respect to the inclusion of January effects: we reject the CAPM in favor of the APT; however, we cannot reject the APT restrictions on the linear factor model. 1988 The American Finance Association © 2016 Elsevier B.V., All rights reserved.","Burmeister, E.; McElroy, M.B.",1988,10.1111/j.1540-6261.1988.tb04603.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977711619&doi=10.1111%2Fj.1540-6261.1988.tb04603.x&partnerID=40&md5=e622a3c5536f6ad1f7416c41a68d2843,scopus,"This paper proposes a multivariate regression model for the Arbitrage Pricing Theory (APT) that incorporates both observed and unobserved macroeconomic factors. It addresses the endogeneity of factors representing large portfolios and compares nonlinear 3SLS estimates with those that ignore endogeneity. The study finds no significant January effects in monthly stock returns and rejects the Capital Asset Pricing Model (CAPM) in favor of the APT, while not rejecting the APT's linear factor model restrictions.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:40:42.021274
c35e1a92cfdc2c3d,Jump and variance risk premia in the S&P 500,"We analyze the risk premia embedded in the S&P 500 spot index and option markets. We use a long time series of spot prices and a large panel of option prices to jointly estimate the diffusive stock risk premium, the price jump risk premium, the diffusive variance risk premium and the variance jump risk premium. The risk premia are statistically and economically significant and move over time. Investigating the economic drivers of the risk premia, we are able to explain up to 63% of these variations. (C) 2016 Elsevier B.V. All rights reserved.","Neumann, Maximilian; Prokopczuk, Marcel; Simen, Chardin Wese",2016,10.1016/j.jbankfin.2016.03.013,,wos,"This study analyzes risk premia in the S&P 500 index and its options market. It jointly estimates diffusive and jump risk premia for both stock and variance, finding them to be significant and time-varying. The research also identifies economic drivers that explain a substantial portion of these variations.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:40:43.221568
84a95360c6c72c18,Kernel-Based Aggregating Learning System for Online Portfolio Optimization,"Recently, various machine learning techniques have been applied to solve online portfolio optimization (OLPO) problems. These approaches typically explore aggressive strategies to gain excess returns due to the existence of irrational phenomena in financial markets. However, existing aggressive OLPO strategies rarely consider the downside risk and lack effective trend representation, which leads to poor prediction performance and large investment losses in certain market environments. Besides, prediction with a single model is often unstable and sensitive to the noises and outliers, and the subsequent selection of optimal parameters also become obstacles to accurate estimation. To overcome these drawbacks, this paper proposes a novel kernel-based aggregating learning (KAL) system for OLPO. It includes a two-step price prediction scheme to improve the accuracy and robustness of the estimation. Specifically, a component price estimator is built by exploiting additional indicator information and the nonstationary nature of financial time series, and then an aggregating learning method is presented to combine multiple component estimators following different principles. Next, this paper conducts an enhanced tracking system by introducing a kernel-based increasing factor to maximize the future wealth of next period. At last, an online learning algorithm is designed to solve the system objective, which is suitable for large-scale and time-limited situations. Experimental results on several benchmark datasets from diverse real markets show that KAL outperforms other state-of-the-art systems in cumulative wealth and some risk-adjusted metrics. Meanwhile, it can withstand certain transaction costs.","Wang, Xin; Sun, Tao; Liu, Zhi",2020,10.1155/2020/6595329,,wos,"This paper introduces a novel kernel-based aggregating learning (KAL) system for online portfolio optimization (OLPO) to address limitations of existing aggressive strategies, such as neglecting downside risk and poor trend representation. The KAL system features a two-step price prediction scheme using component estimators and an aggregating learning method, enhanced by a kernel-based increasing factor for wealth maximization. An online learning algorithm is designed for efficiency. Experiments show KAL outperforms state-of-the-art systems in cumulative wealth and risk-adjusted metrics, and can handle transaction costs.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:40:47.271283
6379bcb62a5fb903,Kriging of financial term-structures,"Due to the lack of reliable market information, building financial term-structures may be associated with a significant degree of uncertainty. In this paper, we propose a new term-structure interpolation method that extends classical spline techniques by additionally allowing for quantification of uncertainty. The proposed method is based on a generalization of kriging models with linear equality constraints (market-fit conditions) and shape-preserving conditions such as monotonicity or positivity (no-arbitrage conditions). We define the most likely curve and show how to build confidence bands. The Gaussian process covariance hyper-parameters under the construction constraints are estimated using cross-validation techniques. Based on observed market quotes at different dates, we demonstrate the efficiency of the method by building curves together with confidence intervals for term-structures of OIS discount rates, of zero-coupon swaps rates and of CDS implied default probabilities. We also show how to construct interest-rate surfaces or default probability surfaces by considering time (quotation dates) as an additional dimension.",,2016,10.1016/j.ejor.2016.05.057,,proquest,"This paper introduces a novel method for interpolating financial term-structures, extending spline techniques with kriging models to quantify uncertainty. It incorporates market-fit, monotonicity, and positivity constraints, estimates Gaussian process hyperparameters via cross-validation, and demonstrates its efficiency in building confidence intervals for OIS discount rates, swap rates, and CDS implied default probabilities. The method can also construct interest-rate and default probability surfaces by including time as an additional dimension.",True,False,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T16:41:17.491200
fa741ee39590c1c0,Large data sets and machine learning: Applications to statistical arbitrage,"Machine learning algorithms and big data are transforming all industries including the finance and portfolio management sectors. While these techniques, such as Deep Belief Networks or Random Forests, are becoming more and more popular on the market, the academic literature is relatively sparse. Through a series of applications involving hundreds of variables/predictors and stocks, this article presents some of the state-of-the-art techniques and how they can be implemented to manage a long-short portfolio. Numerous practical and empirical issues are developed. One of the main questions beyond big data use is the value of information. Does an increase in the number of predictors improve the portfolio performance? Which features are the most important? A large number of predictors means, potentially, a high level of noise. How do the algorithms manage this? This article develops an application using a 22-year trading period, up to 300 U.S. large caps and around 600 predictors. The empirical results underline the ability of these techniques to generate useful trading signals for portfolios with important turnovers and short holding periods (one or five days). Positive excess returns are reported between 1993 and 2008. They are strongly reduced after accounting for transaction costs and traditional risk factors. When these machine learning tools were readily available in the market, excess returns turned into the negative in most recent times. Results also show that adding features is far from being a guarantee to boost the alpha of the portfolio. © 2019 Elsevier B.V., All rights reserved.","Huck, N.",2019,10.1016/j.ejor.2019.04.013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064846164&doi=10.1016%2Fj.ejor.2019.04.013&partnerID=40&md5=352bd849257f78318868f7b1b38825dd,scopus,"This article explores the application of machine learning (ML) and big data techniques to statistical arbitrage in portfolio management. It demonstrates the implementation of advanced ML methods like Deep Belief Networks and Random Forests using a large dataset spanning 22 years, hundreds of variables, and numerous stocks. The study investigates the impact of predictor quantity on portfolio performance and addresses issues of noise and feature importance. Empirical results show that while ML can generate trading signals, excess returns are significantly reduced by transaction costs and risk factors, and even turn negative in recent periods. The study also highlights that increasing the number of features does not necessarily improve portfolio alpha.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:41:34.304338
0da0f4c7d8aadf7f,Learning Forecast-Efficient Yield Curve Factor Decompositions with Neural Networks,"Most factor-based forecasting models for the term structure of interest rates depend on a fixed number of factor loading functions that have to be specified in advance. In this study, we relax this assumption by building a yield curve forecasting model that learns new factor decompositions directly from data for an arbitrary number of factors, combining a Gaussian linear state-space model with a neural network that generates smooth yield curve factor loadings. In order to control the model complexity, we define prior distributions with a shrinkage effect over the model parameters, and we present how to obtain computationally efficient maximum a posteriori numerical estimates using the Kalman filter and automatic differentiation. An evaluation of the model’s performance on 14 years of historical data of the Brazilian yield curve shows that the proposed technique was able to obtain better overall out-of-sample forecasts than traditional approaches, such as the dynamic Nelson and Siegel model and its extensions. © 2022 Elsevier B.V., All rights reserved.","Kauffmann, P.C.; Takada, H.H.; Terada, A.T.; Stern, J.M.",2022,10.3390/econometrics10020015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128238593&doi=10.3390%2Feconometrics10020015&partnerID=40&md5=065080cdb62c05c804b818ccfed43282,scopus,This study proposes a novel yield curve forecasting model that learns factor decompositions directly from data using a combination of a Gaussian linear state-space model and a neural network. It addresses the limitation of fixed factor loading functions in traditional models. The model incorporates prior distributions for complexity control and uses efficient estimation techniques. Empirical evaluation on Brazilian yield curve data demonstrates superior out-of-sample forecasting performance compared to traditional methods.,True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-13T16:41:56.422337
d29dd67ecedbed99,Learning and forecasts about option returns through the volatility risk premium,"We use learning in an equilibrium model to explain the puzzling predictive power of the volatility risk premium (VRP) for option returns. In the model, a representative agent follows a rational Bayesian learning process in an economy under incomplete information with the objective of pricing options. We show that learning induces dynamic differences between probability measures P and Q, which produces predictability patterns from the VRP for option returns. The forecasting features of the VRP for option returns, obtained through our model, exhibit the same behaviour as those observed in an empirical analysis with S&P 500 index options. (C) 2017 Elsevier B.V. All rights reserved.","Bernales, Alejandro; Chen, Louisa; Valenzuela, Marcela",2017,10.1016/j.jedc.2017.06.007,,wos,"This paper develops an equilibrium model where rational Bayesian learning explains the predictive power of the volatility risk premium (VRP) for option returns. The model demonstrates how learning creates dynamic differences between probability measures P and Q, leading to VRP-based predictability in option returns, consistent with empirical findings from S&P 500 index options.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:41:58.670159
b86b59095cc0f715,Long memory affine term structure models,"We develop a Gaussian discrete time essentially affine term structure model with long, memory state variables. This feature reconciles the strong persistence observed in nominal yields and inflation with the theoretical implications of affine models, especially for long maturities. We characterize in closed form the dynamic and cross-sectional implications of long memory for our model. We explain how long memory can naturally arise within the term structure of interest rates, providing a theoretical underpinning for our model. Despite the infinite-dimensional structure that long memory implies, we show how to cast the model in state space and estimate it by maximum likelihood. An empirical application of our model is presented. (C) 2015 Elsevier B.V. All rights reserved.","Golinski, Adam; Zaffaroni, Paolo",2016,10.1016/j.jeconom.2015.09.006,,wos,"This paper presents a Gaussian discrete time essentially affine term structure model incorporating long memory state variables. This model addresses the strong persistence in yields and inflation, aligning with affine model theory for long maturities. It provides closed-form solutions for the dynamic and cross-sectional impacts of long memory, explaining its natural emergence in interest rates. The model is cast in state space for maximum likelihood estimation, despite its infinite-dimensional nature, and includes an empirical application.",True,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T16:42:21.857797
478dbbeb2d89f44e,Long/Short Equity Risk Premia Parity Portfolios via Implicit Factors in Regularized Covariance Regression,"A robust time series basis decomposition and non-stationary trend extraction technique, known as Empirical Mode Decomposition (EMD), will be combined with Regularised Covariance Regression (RCR) to produce a novel covariance forecasting technique. EMD is designed for multiscale and adaptive time-frequency decomposition in nonstationary time series. EMD-RCR generates multi-time-frequency resolution adaptive forecasting models of predictive covariance forecasts for a universe of selected asset returns. This provides a unique method to obtain predictive covariance regression structures for the short-, mid-, and long-time-scale portfolio dynamics. EMD isolates structures in a frequency-hierarchical fashion (with automated sorting of structures through EMD-MDLP available) which allows this multi-time-frequency covariance forecasting framework that uses the structures isolated using EMD (referred to as IMFs: Intrinsic Mode Functions) as the explanatory variables in the RCR framework. Having developed these techniques, a case study is used for exposition for active portfolio asset management. The case study is based on a dynamic long/short equity and risk premia parity (or risk parity) portfolio-of-portfolios investment strategy using the 11 sectors dividing the 505 stocks of the S&P 500. Each of the 11 sector indices is constructed using a market capitalisation ratio of the companies within the respective sector. The portfolio will be reweighted monthly based on the covariance structure forecast using covariance regression, in which covariance regression factors will be obtained at multiple time-frequency scales endogenously from the ETF asset price returns from each sector. At the end of each month, the covariance is forecast for the next month or investment horizon. This is done using low-, mid-, and high-frequency IMFs isolated using EMD from the 11 sector indices over the previous year. The IMFs isolated from the 11 sector indices over the previous year are fitted against the daily logarithmic returns in the RCR model to make multi-frequency covariance forecasts. We construct long/short equity and risk premia parity portfolios using each different covariance forecast and review the results. The performance of the portfolios will be measured using multiple performance measures (the most relevant being risk-related measures with risk premia parity in focus) and contrasted against multiple benchmark portfolios using several well-known portfolio optimisation techniques such as PCA and multivariate GARCH extensions. This paper promotes what we term “implicit factor” extraction, empirical market factors, and RCR in portfolio optimisation, horizon-specific active portfolio optimisation, long/short equity portfolios, and risk parity portfolios.",C. Van Jaarsveldt; G. W. Peters; M. Ames; M. Chantler,2024,10.1109/access.2024.3444479,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10637332,ieeexplore,"This paper introduces a novel covariance forecasting technique by combining Empirical Mode Decomposition (EMD) with Regularised Covariance Regression (RCR). The EMD-RCR method is used to generate adaptive forecasting models for asset returns across different time scales. A case study demonstrates this technique for a dynamic long/short equity and risk premia parity portfolio strategy using S&P 500 sector indices. The portfolio is rebalanced monthly based on covariance forecasts derived from EMD-isolated intrinsic mode functions (IMFs) and RCR. The performance of these portfolios is evaluated against benchmarks and other optimization techniques, highlighting the use of ""implicit factors"" and RCR in portfolio optimization.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:42:40.690109
d6f098222fbb7a8d,Machine learning for US cross-industry return predictability under information uncertainty,"This paper investigates the association between industry information uncertainty and cross-industry return predictability using machine learning in a general predictive regression framework. We show that controlling for post-selection inference and performing multiple tests improves the in-sample predictive performance of cross-industry return predictability in industries characterized by high uncertainty. Ordinary least squares post-least absolute shrinkage and selection operator models incorporating lagged industry information uncertainty for the financial and commodity industries are critical to improving prediction performance. Furthermore, in-sample industry return forecasts establish heterogeneous predictability over US industries, in which excess returns are more predictable in sectors with medium or low uncertainty. © 2023 Elsevier B.V., All rights reserved.","Awijen, H.; Ben-Zaied, Y.; Ben Lahouel, B.; Khlifi, F.",2023,10.1016/j.ribaf.2023.101893,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147577512&doi=10.1016%2Fj.ribaf.2023.101893&partnerID=40&md5=982b6c1f6cf5e817b5b4628e63b106a5,scopus,"This paper explores how industry information uncertainty affects cross-industry return predictability in the US using machine learning. It finds that controlling for statistical inference issues and using models like OLS-LASSO with lagged uncertainty improves in-sample prediction, especially in financial and commodity industries. Predictability varies across industries, with medium or low uncertainty sectors showing more predictable excess returns.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:42:52.905919
fa3ff66abeee79a5,Machine learning portfolio allocation,"We find economically and statistically significant gains when using machine learning for portfolio allocation between the market index and risk-free asset. Optimal portfolio rules for time-varying expected returns and volatility are implemented with two Random Forest models. One model is employed in forecasting monthly excess returns with macroeconomic factors including payout yields. The second is used to estimate the prevailing volatility. Reward-risk timing with machine learning provides substantial improvements over the buy-and-hold in utility, risk-adjusted returns, and maximum drawdowns. This paper presents a unifying framework for machine learning applied to both return- and volatility-timing. © 2021 Elsevier B.V., All rights reserved.","Pinelis, M.; Ruppert, D.",2022,10.1016/j.jfds.2021.12.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122398619&doi=10.1016%2Fj.jfds.2021.12.001&partnerID=40&md5=da9f23a6b807a580ea95847d19e96657,scopus,"This paper explores the use of machine learning, specifically Random Forest models, for portfolio allocation between a market index and a risk-free asset. It demonstrates significant improvements over traditional buy-and-hold strategies by forecasting time-varying expected returns and volatility using macroeconomic factors. The study presents a framework for applying machine learning to both return and volatility timing.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:42:54.738215
135c3081d1b818b4,Macroeconomic Attention and Announcement Risk Premia,"We construct macroeconomic attention indexes (MAI), which are new measures of attention to different macroeconomic risks, including unemployment and monetary policy. Individual MAI tend to increase around related announcements and following changes in related fundamentals. Further, bad news raises attention more than good news. For unemployment and FOMC, attention predicts announcement risk premiums and implied volatility changes with large economic magnitudes. Our findings support theories of endogenous attention and announcement risk premiums, while demonstrating future research directions, including that announcements can raise new concerns. Macroeconomic announcements are important not only for contents and timing but also for attention.Authors have furnished an Internet Appendix, which is available on the Oxford University Press Web site next to the link to the final published paper online.",,2022,10.1093/rfs/hhac011,,proquest,"This paper introduces macroeconomic attention indexes (MAI) to measure attention to macroeconomic risks like unemployment and monetary policy. These indexes increase around relevant announcements and fundamental changes, with bad news having a greater impact than good news. The study finds that attention predicts announcement risk premiums and implied volatility, supporting theories of endogenous attention and announcement risk premiums. The authors suggest that macroeconomic announcements influence not only content and timing but also attention, potentially raising new concerns.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:42:56.888538
aa8f3a5c48986db6,Macroeconomic factors and emerging market equity returns: A Bayesian model selection approach,"Macroeconomics figures prominently in analyses of emerging markets, both as an asset class and for allocations within emerging markets. However, the literature on the drivers of emerging markets equity returns generally pays little attention to macroeconomic factors. This paper investigates the predictive power of several candidate macroeconomic factors for emerging market equity returns using the Bayesian model selection approach developed in Cremers [Cremers, K.J.M., 2002. Stock return predictability: a Bayesian model selection perspective. The Review of Financial Studies 15, 1223-1249]. The results provide strong evidence against all of the macro factors considered with the exception of exchange rate changes and, consistent with the existing literature, provide strong support for several financial factors, but not beta, as significant predictors of excess returns. © 2004 Published by Elsevier B.V. © 2018 Elsevier B.V., All rights reserved.","Hooker, M.A.",2004,10.1016/j.ememar.2004.09.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-10644293253&doi=10.1016%2Fj.ememar.2004.09.001&partnerID=40&md5=a4db5f2ad33213b6bf3c83efe1967361,scopus,"This paper examines the predictive power of macroeconomic factors for emerging market equity returns using a Bayesian model selection approach. It finds that exchange rate changes and certain financial factors are significant predictors, while most other macro factors are not.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:43:43.274282
61421c153a5054b3,Macroeconomic impact on the risk management of offshore wind farms,"The present study aims to develop a risk model to analyse offshore wind projects based on operational and macroeconomic data. The study investigates the underlying parameters defining the project-specific risk premium attached to an offshore wind project. These parameters are modelled as stochastic variables, and a probabilistic financial analysis is conducted using Monte Carlo Simulation. To calculate an interest rate based on operational characteristics, the present study assumes that two net present value equations yielding the same result can be written where certainty equivalent cash flows discounted at the risk-free rate and b) expected cash flows discounted at the cost of capital. The project-related risk is then estimated by solving the resulting equation for the unknown cost of capital. The macroeconomic factors are also considered as they impact the uncertainty associated with revenue and operating expenditure. The model developed to calculate the cost of capital is validated by comparing it with the data obtained for publicly traded renewable energy companies worldwide. Finally, the developed model is demonstrated for a fictitious ageing offshore wind farm under different economic circumstances. The parametric study is conducted on the effect of critical project-specific parameters such as the number of offshore wind turbines, the life extension duration, and the degree of uncertainty related to cash flows. © 2023 Elsevier B.V., All rights reserved.","Yeter, B.; Garbatov, Y.; Brennan, F.; Kolios, A.",2023,10.1016/j.oceaneng.2023.115224,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163853313&doi=10.1016%2Fj.oceaneng.2023.115224&partnerID=40&md5=e3ba1273ef2d4c7cb232dbc4702c5d8a,scopus,"This study develops a risk model for offshore wind projects using operational and macroeconomic data. It models project-specific risk parameters as stochastic variables and uses Monte Carlo Simulation for financial analysis. The model calculates the cost of capital by equating certainty equivalent cash flows discounted at the risk-free rate with expected cash flows discounted at the cost of capital. Macroeconomic factors influencing revenue and operating expenditure uncertainty are considered. The model is validated against data from global renewable energy companies and demonstrated on a hypothetical offshore wind farm under various economic scenarios, analyzing the impact of parameters like turbine count, life extension, and cash flow uncertainty.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:43:46.223546
b92002fe79b6fc1b,Making cost-benefit analysis a practical tool for evaluation,"A cost-benefit evaluation requires precise data on program out-comes. However, such data are unavailable when the analysis is prospective, and expensive and time-consuming to collect when the analysis is retrospective. This problem of uncertain data is partly solved by the revised version of the Treasury Board Benefit-Cost Analysis Guide (Watson & Mallory, 1997), which allows probabilistic estimates of program results to be used in the analysis. There are not yet many examples of this technique in practice. One is Transport Canada's evaluation of alternative requirements for small commercial vessels to carry emergency signaling equipment. This article describes that evaluation and assesses how well the methodology worked. Copyright © 2006 Canadian Evaluation Society. © 2024 Elsevier B.V., All rights reserved.","Watson, K.",2006,10.3138/cjpe.021.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250161650&doi=10.3138%2Fcjpe.021.003&partnerID=40&md5=0a455db4987c1230ae4a280f1d2da0f9,scopus,"This article discusses the challenges of data uncertainty in cost-benefit analysis, particularly for prospective evaluations. It introduces a revised methodology from the Treasury Board Benefit-Cost Analysis Guide that allows for probabilistic estimates of program results. The article then presents a case study of Transport Canada's evaluation of emergency signaling equipment requirements for small commercial vessels to assess the practical application of this methodology.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:43:53.127777
44be03948484da4f,Man versus Machine Learning Revisited,"Binsbergen, Han, and Lopez-Lira (2023) predict analysts' forecast errors using a random forest model. A strategy that trades against this model's predictions earns a monthly alpha of 1.54% ($ t $-value = 5.84). This estimate represents a large improvement over studies using classical statistical methods. We attribute the difference to a look-ahead bias. Removing the bias erases the alpha. Linear models yield as accurate forecasts and superior trading profits. Neither alternative machine learning models nor combinations thereof resurrect the predictability. We discuss the state of research into the term structure of analysts' forecasts and its causal relationship with returns.","Zhang, Yingguang; Zhu, Yandi; Linnainmaa, Juhani T.",2025,10.1093/rfs/hhaf066,,wos,"This study re-examines the use of machine learning (ML) in predicting analysts' forecast errors, finding that a random forest model initially suggested a profitable trading strategy. However, upon closer inspection, this profitability was attributed to a look-ahead bias. After correcting for this bias, the alpha disappeared. The study concludes that linear models are equally accurate for forecasting and yield superior trading profits, and that other ML models do not revive the predictability. The research also touches upon the term structure of analysts' forecasts and its link to returns.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:44:05.089298
5b3ffa4249af3ca0,Market Efficiency and Equity Risk Premium Predictability,"This work examines equity risk premium predictability in periods of market efficiency and market inefficiency. Efficiency is measured by the return's degree of multifractality, calculated from the multifractal detrended fluctuation analysis method. For the S&P 500 index during the 1951–2022 period, the results show that market efficiency varies over time, with recurrent periods of statistically significant inefficiency (multifractality). Moments of inefficiency are associated with (i) a higher level of financial uncertainty—financial uncertainty Granger causes the degree of multifractality (inefficiency), (ii) a greater variability in the patterns of dependence of returns and also (iii) with periods of more relevant volatility clusters. In times of market inefficiency (efficiency), the use of financial (technical) indicators shows statistically significant in-sample and out-of-sample accuracy for equity risk premium prediction. When the market is efficient (inefficient), the use of financial (technical) indicators should be avoided due to the degradation of their predictive capacity. To build accurate and statistically significant predictions of the risk premium, thus enhancing decision-making processes, investors should monitor the informational efficiency status of the market before selecting financial and technical indicators as predictive variables. Finally, during market inefficiency periods, there is a greater polarisation of investors' opinions, with increased attention to fundamental variables for risk premium prediction, leading to a breakdown in price trend patterns, explaining the worst predictive capacity of technical indicators. © 2025 Elsevier B.V., All rights reserved.","Maciel, L.; da Silva, R.F.",2025,10.1002/ijfe.3058,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207528014&doi=10.1002%2Fijfe.3058&partnerID=40&md5=1ffdd4485e71ae8d511b671b1fd70f5e,scopus,"This study investigates the predictability of the equity risk premium by analyzing market efficiency, measured through multifractality using the multifractal detrended fluctuation analysis method on S&P 500 index data from 1951-2022. The findings indicate that market efficiency fluctuates over time, with periods of significant inefficiency characterized by higher financial uncertainty, greater return dependency variability, and increased volatility clustering. During inefficient markets, financial indicators are more accurate for predicting the equity risk premium, while technical indicators are more effective during efficient markets. The study suggests investors monitor market efficiency to optimize their selection of predictive indicators and improve decision-making. Investor opinion polarization and a focus on fundamental variables during inefficiency periods contribute to the breakdown of price trend patterns and reduced predictive capacity of technical indicators.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:44:09.307369
cda1c91e62a7ddb3,Market Returns and a Tale of Two Types of Attention,"We provide novel evidence that aggregate investor attention to stocks predicts marketwide returns, but with a striking difference across investor clienteles. Daily aggregate retail attention (ARA) negatively predicts one-week-ahead market returns, is associated with aggregate retail order imbalance and flows to equity mutual funds, and exhibits a stronger predictability during periods of high marketwide uncertainty, poor liquidity, or more costly short selling. In contrast, aggregate institutional attention (AIA), when observed before major news announcements, positively predict future marketwide returns. In cross-sectional analysis, we show that the predictability is stronger for ARA among illiquid stocks and for AIA among high-beta stocks. The predictability results are robust out-of-sample and correspond to meaningful expected utility gains even for diversified investors. The findings are consistent with the idea that attention-driven retail buying can generate an aggregate price pressure on the stock market, whereas institutional attention precedes the resolution of marketwide uncertainty and the accrual of risk premiums.","Da, Zhi; Hua, Tian; Hung, Tim Chih-Ching; Peng, Lin",2025,10.1287/mnsc.2023.01294,,wos,"This study investigates how aggregate investor attention, categorized into retail (ARA) and institutional (AIA), influences market returns. ARA negatively predicts near-term returns and is linked to retail trading behavior, especially during uncertain or illiquid market conditions. AIA, particularly around news events, positively predicts returns. The findings suggest retail attention can cause price pressure, while institutional attention anticipates market uncertainty resolution and risk premium accrual.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:44:20.540613
53a26b3792f39f00,Maximum Likelihood Estimation in Markov Regime-Switching Models With Covariate-Dependent Transition Probabilities,"This paper considers maximum likelihood (ML) estimation in a large class of models with hidden Markov regimes. We investigate consistency of the ML estimator and local asymptotic normality for the models under general conditions, which allow for autoregressive dynamics in the observable process, Markov regime sequences with covariate-dependent transition matrices, and possible model misspecification. A Monte Carlo study examines the finite-sample properties of the ML estimator in correctly specified and misspecified models. An empirical application is also discussed.","Pouzo, Demian; Psaradakis, Zacharias; Sola, Martin",2022,10.3982/ecta17249,,wos,"This paper focuses on maximum likelihood estimation for Markov regime-switching models, particularly when transition probabilities depend on covariates. It examines the theoretical properties of the estimator, including consistency and local asymptotic normality, under various conditions such as autoregressive dynamics and potential model misspecification. The study is supported by a Monte Carlo simulation to assess finite-sample performance and includes an empirical application.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:44:39.206607
2ca7dfdb8bc7cff3,Measuring the Macroeconomic Impact of Monetary Policy at the Zero Lower Bound,"This paper employs an approximation that makes a nonlinear term structure model extremely tractable for analysis of an economy operating near the zero lower bound for interest rates. We show that such a model offers an excellent description of the data compared to the benchmark model and can be used to summarize the macroeconomic effects of unconventional monetary policy. Our estimates imply that the efforts by the Federal Reserve to stimulate the economy since July 2009 succeeded in making the unemployment rate in December 2013 1% lower, which is 0.13% more compared to the historical behavior of the Fed. © 2016 Elsevier B.V., All rights reserved.","Wu, J.C.; Xia, F.D.",2016,10.1111/jmcb.12300,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962845380&doi=10.1111%2Fjmcb.12300&partnerID=40&md5=a52aebcac9facacc69f11196ac83a70d,scopus,"This paper uses a tractable nonlinear term structure model to analyze the macroeconomic impact of monetary policy at the zero lower bound. The model provides a good fit to the data and suggests that Federal Reserve stimulus since July 2009 lowered unemployment by 1% by December 2013, exceeding historical Fed behavior by 0.13%.",True,True,False,gemini-2.5-flash-lite,Trine,M,,2025-10-13T16:45:45.689157
4d23cc600c835687,Measuring the natural rate of interest,"The natural rate of interest-the real interest rate consistent with output equaling its natural rate and stable inflation-plays a central role in macroeconomic theory and monetary policy. Estimation of the natural rate of interest, however, has received little attention. We apply the Kalman filter to estimate jointly time-varying natural rates of interest and output and trend growth. We find a close link between the natural rate of interest and the trend growth rate, as predicted by theory. Estimates of the natural rate of interest, however, are very imprecise and subject to considerable real-time measurement error.","Laubach, T; Williams, JC",2003,10.1162/003465303772815934,,wos,"This paper estimates the natural rate of interest, defined as the real interest rate consistent with output at its natural rate and stable inflation, using the Kalman filter to jointly estimate time-varying natural rates of interest and output and trend growth. The study finds a link between the natural rate of interest and trend growth, but notes that estimates are imprecise and subject to measurement error.",True,False,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T16:46:21.358616
014d516a2e0c45b7,"Measuring the performance of government bond portfolios with index-based level, slope, and curvature factors","This paper introduces a three-factor interest rate risk model to improve the measurement of active bond fund performance. Traditional models assume a linear relationship between risk exposure and expected returns, leading to biases. By incorporating level, slope, and curvature factors derived from Treasury index returns, the proposed model better captures the nonlinear nature of bond returns. Empirical tests on passive and active US government bond portfolios confirm its accuracy in estimating passive style returns and active alpha. The study also provides the first performance analysis of fixed-income separate accounts, revealing their economic significance and superior value-added performance over mutual funds. © 2025 Elsevier B.V., All rights reserved.","Rohleder, M.",2025,10.1002/rfe.70024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014890422&doi=10.1002%2Frfe.70024&partnerID=40&md5=6803ce0a2bde3094c0a0d12da1441a18,scopus,"This paper proposes a three-factor interest rate risk model (level, slope, curvature) using Treasury index returns to enhance the measurement of active bond fund performance, addressing limitations of traditional linear models. Empirical tests on US government bond portfolios validate its accuracy in estimating passive returns and active alpha, and reveal superior performance of fixed-income separate accounts over mutual funds.",True,True,False,gemini-2.5-flash-lite,Trine,M,,2025-10-13T16:46:38.374889
8991d3ec333a8394,Measuring the risk of a non-linear portfolio with fat-tailed risk factors through a probability conserving transformation,"This paper presents a new heuristic for fast approximation of VaR (Value-at-Risk) and CVaR (conditional Value-at-Risk) for financial portfolios, where the net worth of a portfolio is a non-linear function of possibly non-Gaussian risk factors. The proposed method is based on mapping non-normal marginal distributions into normal distributions via a probability conserving transformation and then using a quadratic, i.e. Delta–Gamma, approximation for the portfolio value. The method is very general and can deal with a wide range of marginal distributions of risk factors, including non-parametric distributions. Its computational load is comparable with the Delta–Gamma–Normal method based on Fourier inversion. However, unlike the Delta–Gamma–Normal method, the proposed heuristic preserves the tail behaviour of the individual risk factors, which may be seen as a significant advantage. We demonstrate the utility of the new method with comprehensive numerical experiments on simulated as well as real financial data.",P. Date; R. Bustreo,2016,10.1093/imaman/dpu015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8146531,ieeexplore,"This paper introduces a novel heuristic for approximating Value-at-Risk (VaR) and conditional Value-at-Risk (CVaR) in financial portfolios with non-linear risk factor dependencies and non-Gaussian distributions. It employs a probability-conserving transformation to map non-normal distributions to normal ones, followed by a Delta-Gamma approximation. This method is versatile, handling various marginal distributions, including non-parametric ones, and is computationally efficient, comparable to the Delta-Gamma-Normal method. A key advantage is its preservation of tail behavior in risk factors, demonstrated through experiments with simulated and real financial data.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:46:44.210561
b883d647a8f20108,"Model misspecification, the equilibrium natural interest rate, and the equity premium","This paper analyzes the natural rate of interest and the equity premium in a nonlinear model where agents are uncertain over both future technology growth and the future course of monetary policy. I show that model uncertainty, and notably uncertainty on the future course of monetary policy, can give rise to a sizable precautionary savings motive. This result is potentially problematic for both the estimation of the natural rate and its use as a policy indicator. Monetary uncertainty can also contribute to amplify the equity premium, and to account for its apparent, positive link with inflation. © 2009 The Ohio State University. © 2012 Elsevier B.V., All rights reserved.","Tristani, O.",2009,10.1111/j.1538-4616.2009.00263.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349421148&doi=10.1111%2Fj.1538-4616.2009.00263.x&partnerID=40&md5=e79ed1d77fb7696770f3d1293d03fc42,scopus,"This paper examines the natural rate of interest and the equity premium within a nonlinear model that incorporates uncertainty about future technological growth and monetary policy. The study highlights how model uncertainty, particularly concerning monetary policy, can stimulate precautionary savings, potentially complicating the estimation and policy relevance of the natural rate. Additionally, monetary uncertainty is shown to amplify the equity premium and explain its correlation with inflation.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:46:46.006682
0ae15ca252dbc816,Modeling Investor Responses to Green Bond Issuance: Multidimensional Perspectives and Evidence From China,"Although green bonds are rapidly growing to be a mature financing tool, the debate over whether there are benefits to be gained by issuers’ stocks has yet to be resolved, especially in the emerging market context. Issuing green bonds, as a financing procedure targeted to green engineering projects and demonstrating the issuers’ environmentally friendly attitude, does and how does it affect the issuers’ stock prices, liquidity, and risk? We address this issue by paying attention to the Chinese green bond issuance events. Utilizing the event study method, research shows that investors react positively to green bond issuance events. However, this reaction is only sensitive to green bond listing events, but not to announcements. Investor responses can be reflected in the abnormal changes in stock prices and liquidity. Both the stock systematic risk and idiosyncratic risk show little change after firms issue green bonds, which illustrates that green bond issuance cannot shape the inherent investors’ value judgments on issuer companies, thereby only producing temporary impacts. This study suggests that the green premium of corporate stocks induced by green bond issuance events may be sourced from investors’ optimistic predictions about green transformation, rather than investors’ subjective willingness to promote environmental sustainability.",T. Su; B. Lin,2025,10.1109/tem.2025.3538945,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10874181,ieeexplore,"This study investigates investor responses to green bond issuance in China using an event study method. It finds that investors react positively to green bond listing events, leading to abnormal changes in stock prices and liquidity, but not to announcements. The issuance of green bonds does not significantly alter stock systematic or idiosyncratic risk, suggesting temporary impacts. The authors propose that the observed 'green premium' stems from optimistic predictions about green transformation rather than a direct willingness to promote environmental sustainability.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:46:47.826181
4c71769a0ce9a6e9,Modeling Multi-horizon Electricity Demand Forecasts in Australia: A Term Structure Approach,"The Australian Electricity Market Operator generates one-day ahead electricity demand forecasts for the National Electricity Market in Australia and updates these forecasts over time until the time of dispatch. Despite the fact that these forecasts play a crucial role in the decision-making process of market participants, little attention has been paid to their evaluation and interpretation. Using half-hourly data from 2011 to 2015 for New South Wales and Queensland, it is shown that the official half-hourly demand forecasts do not satisfy the econometric properties required of rational forecasts. Instead there is a relationship between forecasts and forecast horizon similar to a term structure model of interest rates. To study the term structure of demand forecasts, a factor analysis that uses a small set of latent factors to explain the common variation among multiple observables is implemented. A three-factor model is identified with the factors admitting interpretation as the level, slope and curvature of the term structure of forecasts. The validity of the model is reinforced by assessing the economic value of demand forecasts. It is demonstrated that simple adjustments to long-horizon electricity demand forecasts based on the three estimated factors can enhance the informational content of the official forecasts.",,2023,10.5547/01956574.44.2.shur,,proquest,"This paper analyzes electricity demand forecasts in Australia, finding they do not meet rational forecast criteria. It proposes a term structure model using factor analysis to capture forecast horizon dynamics, identifying level, slope, and curvature factors. Adjusting long-horizon forecasts based on these factors can improve their informational content.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:46:54.708552
68e704365ecd74f1,Modeling the term structure from the on-the-run treasury yield curve,"We propose a new model to estimate the term structure of interest rates using observed on-the-run Treasury yields. The new model is an improvement over models that require a priori knowledge of the shape of the yield curve to estimate the term structure. The general form of the model is an exponential function that depends on the estimation of four parameters fit by nonlinear least squares and has straightforward interpretations. In comparing the proposed model with current yield-curve-smoothing models, we find that, for the data used, the proposed model does best overall in terms of pricing accuracy both in sample and out of sample. JEL classification: E43, G12. © The Southern Finance Association and the Southwestern Finance Association. © 2018 Elsevier B.V., All rights reserved.","Mansi, S.A.; Phillips, J.H.",2001,10.1111/j.1475-6803.2001.tb00830.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037790039&doi=10.1111%2Fj.1475-6803.2001.tb00830.x&partnerID=40&md5=b4245734dc46cc70b7f7dc5a6c901db8,scopus,"This paper introduces a novel model for estimating the term structure of interest rates using on-the-run Treasury yields. The model, an exponential function with four parameters, improves upon existing methods by not requiring prior assumptions about the yield curve's shape. It demonstrates superior pricing accuracy compared to current yield-curve-smoothing models, both in-sample and out-of-sample.",True,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T16:47:23.957998
4af1fb5d81fb748c,Modeling the time-varying volatility of the paper-bill spread,"The spread between the rates on commercial paper and Treasury bills has received considerable attention in the literature for its role as an indicator of real economic activity. In this paper we empirically examine what happens when the volatility of the spread changes over time. We estimate a nonlinear model that enables us to discern the asymmetric impact of negative and positive shocks to the spread. We find that a positive shock has a larger impact on the volatility of the spread than does a negative shock. © 2009 Elsevier Inc. All rights reserved. © 2024 Elsevier B.V., All rights reserved.","Malik, F.; Ewing, B.T.; Kruse, J.B.; Lynch, G.J.",2009,10.1016/j.jeconbus.2009.03.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650577177&doi=10.1016%2Fj.jeconbus.2009.03.002&partnerID=40&md5=2a88f73bc55ca9f949c5fbd33d3186de,scopus,"This paper empirically investigates the time-varying volatility of the paper-bill spread, a known indicator of economic activity. It employs a nonlinear model to analyze the asymmetric impact of positive and negative shocks on this volatility, finding that positive shocks have a greater effect.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:47:46.618367
b1e9b249051157d6,Modelling nonlinearities in equity returns: the mean impact curve analysis,"A time-varying model of equity returns consisting of a volatility factor with time-varying loading, is specified to investigate the dynamical effects of shocks on expected returns. The proposed specification yields a nonlinear relationship between the conditional mean and the news, referred to as the mean impact curve (MIC). Applying this framework to the AORD, Hang Seng and Straits Times equity indexes yields estimated MICs with qualitatively similar nonlinear characteristics for each equity market. An important implication of the empirical results is that the relationship between the conditional mean and the news is found to be dependent upon the size of the shock, a result which is consistent with equity markets displaying mean aversion over short horizons and mean reversion over long horizons.","Martin, Vance L.; Sarkar, Saikat; Kanto, Antti Jaakko",2014,10.1515/snde-2012-0030,,wos,"This paper proposes a time-varying model for equity returns that incorporates a volatility factor with time-varying loading to study the dynamic effects of shocks on expected returns. The model results in a nonlinear relationship between the conditional mean and news, termed the mean impact curve (MIC). Empirical analysis of the AORD, Hang Seng, and Straits Times equity indexes reveals similar nonlinear MIC characteristics across these markets. The findings suggest that the relationship between the conditional mean and news depends on shock size, aligning with equity markets exhibiting mean aversion in the short term and mean reversion in the long term.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:47:50.694333
49b43eda4b92602b,Modelling of Chinese corporate bond default - A machine learning approach,"We apply machine learning techniques to construct a series of models of corporate bond defaults. By combining Chinese accounting information and corporate bond data from January 2012 to December 2019, we construct an out-of-sample forecast that significantly improves the identification rate of corporate bond defaults, with an area under the receiver operating characteristics curve greater than 90 percent. Our models are robust to different machine learning models, including stacking, boosting, and bagging ensembling models. Our models consider cross-sectional heterogeneity, such as different ownership structures, accessibility to external finance, industry heterogeneity, different sample periods, and government policy impact.","Lu, Zhou; Zhuo, Zhuyao",2021,10.1111/acfi.12846,,wos,"This study uses machine learning to model Chinese corporate bond defaults, combining accounting and bond data from 2012-2019. The models, including ensembling techniques like stacking, boosting, and bagging, significantly improve default identification with an AUC over 90%. They account for factors like ownership, finance accessibility, industry, time periods, and government policy.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:47:54.109684
2c9c05575068c7f9,Modelling portfolio defaults using hidden Markov models with covariates,"We extend the hidden Markov Model for defaults of Crowder et al. (2005, Quantitative Finance 5, 27-34) to include covariates. The covariates enhance the prediction of transition probabilities from high to low default regimes. To estimate the model, we extend the EM estimating equations to account for the time varying nature of the conditional likelihoods due to sample attrition and extension. Using empirical U.S. default data, we find that GDP growth, the term structure of interest rates and stock market returns impact the state transition probabilities. The impact, however, is not uniform across industries. We only find a weak correspondence between industry credit cycle dynamics and general business cycles. Reprinted by permission of Blackwell Publishing",,2008,10.1111/j.1368-423x.2008.00232.x,,proquest,"This paper extends the Hidden Markov Model (HMM) for defaults by incorporating covariates to improve the prediction of transitions between high and low default regimes. The authors adapt EM estimating equations to handle time-varying conditional likelihoods due to sample attrition and extension. Empirical analysis using U.S. default data reveals that GDP growth, interest rate term structure, and stock market returns influence state transition probabilities, with varying effects across industries. A weak link is observed between industry credit cycles and general business cycles.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:47:56.724894
052c16a20382c1b6,Modelling the dependence structures of Australian iTraxx CDS index,"In contrast to market expectations, the correlation between credit default swap (CDS) spreads and their respective stock prices in Australia was found to be positive. The global financial crisis (GFC) affected the nonlinear association between the two asset classes with firms experiencing financial distress and stock prices plummeting. CDSs issuers reacted to such exogenous shocks by increasing their risk premiums on their spreads, reflecting the increased inherent risk. By splitting the data into pre- and post-GFC contexts and by employing the use of Archimedean copulas, we observe a negative co-movement in the post-GFC period. This finding is robust to several equity indices. Overall, such result is critical for investors engaging in arbitrageur activities. © 2013 Taylor & Francis. © 2013 Elsevier B.V., All rights reserved.","Fenech, J.-P.; Vosgha, H.; Shafik, S.",2014,10.1080/00036846.2013.849378,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887918664&doi=10.1080%2F00036846.2013.849378&partnerID=40&md5=e1bbd0d1c5a92b23deea51cdab6fda29,scopus,"This study models the dependence structures of Australian iTraxx CDS index spreads and stock prices, finding a positive correlation that was affected by the GFC. Using Archimedean copulas, a negative co-movement was observed in the post-GFC period, which is robust across several equity indices and has implications for arbitrage activities.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:48:01.376510
8fd13386d94ce678,Modelling the risk premium in the black-market zloty-dollar exchange rate,"This paper tests for the presence of nonlinear dependence in the black-market Polish zloty-dollar exchange rate. Using the GARCH-M model, we illustrate use of the Marquardt (Journal of the Society of Industrial and Applied Mathematics, 2, 1963) alternative to the Berndt (Annals of Economical Social Measurement, 4, 1974) iterative nonlinear algorithm for the estimation of such models, and discrimination between estimated models on the basis of the Brock and Potter (Handbook of Statistics, 11, 1993) test for so conditional variance misspecification. We find evidence of a time-varying risk premium such that foreign speculators are compensated for increased exchange rate risk by appreciation which increases the dollar value of zloty holdings, and which is able to account for all of the apparent nonlinearity in the zloty. © 2017 Elsevier B.V., All rights reserved.","McMillan, D.G.; Speight, A.E.H.",1999,10.1080/135048599353357,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040964268&doi=10.1080%2F135048599353357&partnerID=40&md5=cee866fd77f492916ad145b02de8d1d1,scopus,"This paper investigates the black-market zloty-dollar exchange rate using a GARCH-M model to test for nonlinear dependence and a time-varying risk premium. It employs specific iterative nonlinear algorithms for model estimation and discrimination based on conditional variance misspecification tests. The findings suggest that foreign speculators are compensated for exchange rate risk through appreciation, which explains the observed nonlinearity.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:48:14.759480
2c7bb54560d01c08,Moment Risk Premia and Stock Return Predictability,"We study the predictive power of option-implied moment risk premia embedded in the conventional variance risk premium. We find that although the second-moment risk premium predicts market returns in short horizons with positive coefficients, the third-moment (fourth-moment) risk premium predicts market returns in medium horizons with negative (positive) coefficients. Combining the higher-moment risk premia with the second-moment risk premium improves the stock return predictability over multiple horizons, both in sample and out of sample. The finding is economically significant in an asset-allocation exercise and survives a series of robustness checks.","Fan, Zhenzhen; Xiao, Xiao; Zhou, Hao",2022,10.1017/s002210902000085x,,wos,"This study investigates the predictive power of option-implied moment risk premia for stock returns. It finds that second-moment risk premia predict returns at short horizons, while third- and fourth-moment risk premia predict at medium horizons. Combining these higher-moment risk premia enhances stock return predictability across multiple horizons, with significant economic implications and robustness.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:48:20.562044
8a16f3a80a9ddf11,"Moments, shocks and spillovers in Markov-switching VAR models","To investigate how economies, financial markets or institutions can deal with stress, we often analyze the effects of shocks conditional on being in a recession or a bear market. MSVAR models are perfectly suited for such analyses because they combine gradual movements with sudden regime switches. In this paper, we develop a comprehensive methodology to conduct these analyses. We derive first and second moments conditional only on the regime distribution and propose impulse response functions for both moments. By formulating the MSVAR as an extended linear non-Gaussian VAR, all results are available in closed-form. We illustrate our methods with an application to stock and bond return predictability. We show how forecasts of means, volatilities and (auto-)correlations depend on the regimes. The effect of shocks becomes highly nonlinear, and they propagate via different channels. During bear markets, shocks have stronger effects on means and volatilities and die out more slowly. & COPY; 2023 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).","Kole, Erik; van Dijk, Dick",2023,10.1016/j.jeconom.2023.105474,,wos,"This paper develops a methodology for analyzing the effects of shocks in Markov-switching VAR (MSVAR) models, which are suitable for capturing both gradual movements and sudden regime switches. The authors derive conditional moments and impulse response functions in closed-form by formulating the MSVAR as an extended linear non-Gaussian VAR. An application to stock and bond returns demonstrates how forecasts of means, volatilities, and autocorrelations are regime-dependent, with shocks having nonlinear and slower-dying effects during bear markets.",True,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T16:48:49.058409
fdfb42067bfce0e2,Monetary base and federal government debt in the long‐run: A non‐linear analysis,"Government bonds are usually traded between the financial institutions and the Fed during the open market operations. These operations impact the bank reserves, subsequently influencing the monetary base. The monetary base and government bonds may portray a common trend and government debt could potentially bind the central bank to debt monetization. This paper, using monthly data on federal government debt and the monetary base from 1947:1 to 2018:10, investigates the presence of a long‐run equilibrium relationship between the two variables and as to how the long‐run equilibrium relationship vary in the short‐run. Threshold cointegration tests find evidence of a long‐run equilibrium relationship. Estimates of the threshold vector error‐correction model find statistically significant evidence of contraction in the monetary base growth in the short‐run in regime 1. In regime 2, the growth in the monetary base does not adjust to accommodate faster government debt growth. These estimates find no evidence of debt monetization or otherwise in either of the regimes in the United States. The Fed, by reducing the monetary base, perhaps focuses more on the inflation target. The findings also suggest a potential scenario where the Fed and the fiscal authority are not conjoined with each other in their operations.",,2020,10.1111/boer.12216,,proquest,This paper investigates the long-run equilibrium relationship between the monetary base and federal government debt in the US from 1947 to 2018 using threshold cointegration and a threshold vector error-correction model. The study finds evidence of a long-run relationship but no evidence of debt monetization. It suggests the Fed may prioritize inflation targets and operate independently of fiscal authority.,True,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:49:54.847824
872b565b23f27078,Monetary policy document analysis for prediction of monetary policy board decision,"In terms of market capitalization, the bond market is larger than the stock market, and the bond market is affected by macroeconomic indicators. Despite this, there has been relatively little research, making it a good candidate for the use of data mining techniques. In this paper, a novel approach designed to predict the vote results of the Korean Monetary Policy Committee regarding the base interest rate was proposed. To predict sentence sentiment, prior monetary policy decision text was used as input for classification models. The sentence sentiment prediction model showed 83.7% performance when using a support vector machine. In addition, it was observed that the bigrams extracted from documents provided important descriptions of the Korean economy at the time. Finally, the document sentiment of monetary policy decision was calculated using aggregating sentence sentiment, and the vote results were predicted using this sentiment. As a result, when using the support vector machine to predict the Monetary Policy Committee vote results, the performance improved by 29.5% over the baseline model. Statistical tests confirmed whether there is a difference in document sentiments between unanimous and non-unanimous, and the null hypothesis was rejected at a significance level of 5%.In terms of market capitalization, the bond market is larger than the stock market, and the bond market is affected by macroeconomic indicators. Despite this, there has been relatively little research, making it a good candidate for the use of data mining techniques. In this paper, a novel approach designed to predict the vote results of the Korean Monetary Policy Committee regarding the base interest rate was proposed. To predict sentence sentiment, prior monetary policy decision text was used as input for classification models. The sentence sentiment prediction model showed 83.7% performance when using a support vector machine. In addition, it was observed that the bigrams extracted from documents provided important descriptions of the Korean economy at the time. Finally, the document sentiment of monetary policy decision was calculated using aggregating sentence sentiment, and the vote results were predicted using this sentiment. As a result, when using the support vector machine to predict the Monetary Policy Committee vote results, the performance improved by 29.5% over the baseline model. Statistical tests confirmed whether there is a difference in document sentiments between unanimous and non-unanimous, and the null hypothesis was rejected at a significance level of 5%.",,2023,10.1016/j.heliyon.2023.e20696,,proquest,"This paper proposes a novel approach to predict the vote results of the Korean Monetary Policy Committee regarding the base interest rate by analyzing monetary policy documents. The study uses sentence sentiment analysis, with a support vector machine achieving 83.7% performance in predicting sentence sentiment. Bigrams from documents were found to provide important descriptions of the Korean economy. Aggregating sentence sentiments to calculate document sentiment allowed for the prediction of vote results, improving performance by 29.5% over the baseline model. Statistical tests also indicated a significant difference in document sentiments between unanimous and non-unanimous decisions.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:50:00.522821
1bbb2fffd29655e3,Multi-Country and Multi-Horizon GDP Forecasting Using Temporal Fusion Transformers,"This paper applies a new artificial intelligence architecture, the temporal fusion transformer (TFT), for the joint GDP forecasting of 25 OECD countries at different time horizons. This new attention-based architecture offers significant advantages over other deep learning methods. First, results are interpretable since the impact of each explanatory variable on each forecast can be calculated. Second, it allows for visualizing persistent temporal patterns and identifying significant events and different regimes. Third, it provides quantile regressions and permits training the model on multiple time series from different distributions. Results suggest that TFTs outperform regression models, especially in periods of turbulence such as the COVID-19 shock. Interesting economic interpretations are obtained depending on whether the country has domestic demand-led or export-led growth. In essence, TFT is revealed as a new tool that artificial intelligence provides to economists and policy makers, with enormous prospects for the future.",,2023,10.3390/math11122625,,proquest,"This paper utilizes the Temporal Fusion Transformer (TFT), an advanced AI architecture, for joint GDP forecasting across 25 OECD countries and multiple time horizons. The TFT model demonstrates interpretability, identifies temporal patterns and significant events, and handles multiple time series distributions. It outperforms traditional regression models, particularly during volatile periods like the COVID-19 pandemic, and offers economic insights into growth drivers.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:50:04.325062
893895957d5ecc26,Multifractality and the economic value of equity risk premium forecasts,"This paper presents novel evidence on the economic value of equity risk premium forecasts across varying degrees of market multifractality. The degree of multifractality in returns is measured to capture relevant nonlinear long-range autocorrelations. Risk premium forecasts are generated using economic indicators as predictors. Their economic value is assessed through a portfolio strategy that allocates between equities and risk-free assets based on these forecasts. The results show that higher levels of nonlinear long-range autocorrelations are associated with statistically significant out-of-sample risk premium forecasts. These periods also yield greater economic gains, underscoring the importance for investors of monitoring nonlinear dependencies when incorporating risk premium forecasts into portfolio decisions. © 2025 Elsevier B.V., All rights reserved.","Maciel, L.S.",2025,10.1080/13504851.2025.2498060,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003882998&doi=10.1080%2F13504851.2025.2498060&partnerID=40&md5=af289c71f5315456eb43d7eb26633d15,scopus,"This paper investigates the economic value of equity risk premium forecasts by examining the relationship between market multifractality and forecast accuracy. It uses economic indicators to generate forecasts and assesses their value through a portfolio strategy. The study finds that higher levels of nonlinear long-range autocorrelations in returns are linked to statistically significant out-of-sample forecasts and greater economic gains, suggesting investors should consider these nonlinear dependencies.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:50:07.711593
7b86fe43643e030c,Multiple Structural Breaks in Vector Error Correction Models,"The analysis of structural breaks in vector error correction models is often confined to possible level shifts and trend breaks. In contrast, only rudimentary tools are available to deal with breaks in the cointegration matrix and the adjustment towards equilibrium. Particularly, the possibility of multiple structural breaks during long sampling periods is often ignored which can lead to inconsistently estimated coefficients. In this paper, we study a two-step estimator based on a penalized regression to determine the number of structural breaks, their timing, and estimate the model's coefficients for each regime. We focus on two important cases, namely, (i) constant dynamics but changing long-run equilibria, and (ii) convergence to new long-run equilibria with new adjustment dynamics. We use simulations to investigate the finite sample performance of the two-step estimator and provide an empirical illustration using data on the term structure of interest rates. © 2025 Elsevier B.V., All rights reserved.","Franjic, D.; Möβler, M.; Schweikert, K.",2025,10.1515/snde-2025-0009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012983996&doi=10.1515%2Fsnde-2025-0009&partnerID=40&md5=9a0223f5b81c98fa7a4ed698689f1332,scopus,"This paper proposes a two-step penalized regression estimator to identify multiple structural breaks in vector error correction models, focusing on changes in cointegration matrices and adjustment dynamics. It considers scenarios with constant dynamics but shifting long-run equilibria, and new equilibria with altered adjustment dynamics. The method's performance is evaluated through simulations and an empirical application to interest rate term structure data.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:50:14.611779
a2acf588c5e07225,Multiplicative parameters and estimators: applications in economics and finance,"In this paper, we pay our attention to multiplicative parameters of random variables and their estimators. We study multiplicative properties of the multiplicative expectation and multiplicative variation as well as their estimators. For distributions having applications in finance and insurance we provide their multiplicative parameters and their properties. We consider, among others, heavy-tailed distributions such as lognormal and Pareto distributions, applied to the modelling of large losses. We discuss multiplicative models, in which the geometric mean and the geometric standard deviation are more natural than their arithmetic counterparts. We provide two examples from the Warsaw Stock Exchange in 1995-2009 and from a bid of 52-week treasury bills in 1992-2009 in Poland as an illustrative example.",,2016,10.1007/s10479-015-2035-x,,proquest,"This paper investigates multiplicative parameters and their estimators, focusing on their applications in economics and finance. It examines multiplicative expectation and variation, particularly for heavy-tailed distributions like lognormal and Pareto, which are relevant for modeling large losses in finance and insurance. The study highlights the advantages of multiplicative models, using geometric mean and standard deviation, and provides examples from the Warsaw Stock Exchange and Polish treasury bills.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:50:45.005176
c6a858641ba903a5,Multivariate CDS risk premium prediction with SOTA RNNs on MI[N]T countries,"In this study, CDS risk premiums of Mexico, Indonesia and Turkey were predicted by applying state-of-the-art forecasters in deep learning recurrent neural networks architectures which are the most recent ground-breaking predictors in the time series setting. The predictive power of each sota forecaster is compared, and the results are differentiated by country and type of sota predictors. While the long short-term memory model is better to predict Mexico's CDS risk premiums, the nonlinear autoregressive network with exogenous inputs model is found to be more suitable for Indonesia and Turkey. The results of Turkey model reached the highest forecast accuracy. © 2022 Elsevier B.V., All rights reserved.","Kutuk, Y.; Barokas, L.",2022,10.1016/j.frl.2021.102198,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107964009&doi=10.1016%2Fj.frl.2021.102198&partnerID=40&md5=308c6bc5add7af268f4e99f0c972824d,scopus,"This study predicts Credit Default Swap (CDS) risk premiums for Mexico, Indonesia, and Turkey using state-of-the-art deep learning recurrent neural network (RNN) models. It compares the predictive power of different RNN architectures, finding that Long Short-Term Memory (LSTM) is best for Mexico, while Nonlinear Autoregressive Network with Exogenous Inputs (NARX) is more suitable for Indonesia and Turkey. The NARX model achieved the highest forecast accuracy for Turkey.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:50:47.027265
9cf15666ac064bd7,Multivariate time-series modeling with generative neural networks,"Generative moment matching networks (GMMNs) are introduced as dependence models for the joint innovation distribution of multivariate time series (MTS). Following the popular copula–GARCH approach for modeling dependent MTS data, a framework based on a GMMN–GARCH approach is presented. First, ARMA–GARCH models are utilized to capture the serial dependence within each univariate marginal time series. Second, if the number of marginal time series is large, principal component analysis (PCA) is used as a dimension-reduction step. Last, the remaining cross-sectional dependence is modeled via a GMMN, the main contribution of this work. GMMNs are highly flexible and easy to simulate from, which is a major advantage over the copula–GARCH approach. Applications involving yield curve modeling and the analysis of foreign exchange-rate returns demonstrate the utility of the GMMN–GARCH approach, especially in terms of producing better empirical predictive distributions and making better probabilistic forecasts. © 2022 Elsevier B.V., All rights reserved.","Hofert, M.; Prasad, A.; Zhu, M.",2022,10.1016/j.ecosta.2021.10.011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119899540&doi=10.1016%2Fj.ecosta.2021.10.011&partnerID=40&md5=4c2db93c152de7fc06453d4c54c6c943,scopus,"This paper introduces Generative Moment Matching Networks (GMMNs) as a method for modeling the joint innovation distribution of multivariate time series (MTS). It proposes a GMMN-GARCH framework, which first uses ARMA-GARCH models for univariate series, then PCA for dimension reduction if needed, and finally GMMNs to model the remaining cross-sectional dependence. The approach is demonstrated with applications in yield curve modeling and foreign exchange rate returns, showing improved empirical predictive distributions and probabilistic forecasts.",True,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T16:51:18.266057
2cf4672afef2a0ae,Navigating the Complexity of Money Laundering: Anti–money Laundering Advancements with AI/ML Insights,"This study explores the fusion of artificial intelligence (AI) and machine learning (ML) methods within anti–money laundering (AML) frameworks using data from the US Treasury’s Financial Crimes Enforcement Network (FinCEN). ML and deep learning (DL) algorithms—such as random forest classifier, elastic net regressor, least absolute shrinkage and selection operator (LASSO) regression, gradient boosting regressor, linear regression, multilayer perceptron (MLP) classifier, convolutional neural network (CNN), random forest regressor, and K-nearest neighbor (KNN)—were used to forecast variables such as state, year, and transaction types (credit card and debit card). Hyperparameter tuning through grid search and randomized search was used to optimize model performance. The results demonstrated the efficacy of AI/ML algorithms in predicting temporal, spatial, and industry-specific money-laundering patterns. The random forest classifier achieved 99.99% average accuracy in state prediction, while the gradient boosting regressor and random forest classifier excelled in predicting year and state simultaneously, and credit card transactions, respectively. MLP and CNN showed promise in the context of debit card transactions. The gradient boosting regressor performed competitively with low mean squared error (MSE) (2.9) and the highest R-squared (R2) value of 0.24, showcasing its pattern-capturing proficiency. Logistic regression and random forest classifier performed well in predicting credit card transactions, with area under the receiver operating characteristic curve (ROC_AUC) scores of 0.55 and 0.53, respectively. For debit card prediction, MLP achieved a precision of 0.55 and recall of 0.42, while CNN showed a precision of 0.6 and recall of 0.54, highlighting their effectiveness. The study recommends interpretability, hyperparameter optimization, specialized models, ensemble methods, data augmentation, and real-time monitoring for improved adaptability to evolving financial crime patterns. Future improvements could include exploring the integration of blockchain technology in AML.",,2024,10.2478/ijssis-2024-0024,,proquest,"This study investigates the application of various AI/ML algorithms (including random forest, elastic net, LASSO, gradient boosting, linear regression, MLP, CNN, and KNN) to anti-money laundering (AML) frameworks using FinCEN data. The models were used to predict transaction types, state, and year, with hyperparameter tuning employed for optimization. The results indicate strong predictive performance for several algorithms in identifying money laundering patterns, with recommendations for further improvements such as interpretability and real-time monitoring.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:51:27.094138
0e8c12740794a640,"Neural Networks, the Treasury Yield Curve, and Recession Forecasting","The authors use neural networks to examine the power of Treasury term spreads and other macro-financial variables to forecast US recessions and compare them with probit regression. They propose a novel three-step econometric method for cross-validating and conducting statistical inference on machine learning classifiers and explaining forecasts. They find that probit regression does not underperform a neural network classifier in the present application, which stands in contrast to a growing body of literature demonstrating that machine learning methods outperform alternative classification algorithms. That said, neural network classifiers do identify important features of the joint distribution of recession over term spreads and other macro-financial variables that probit regression cannot. The authors discuss some possible reasons for their results and use their procedure to study US recessions over the post-Volcker period, analyzing feature importance across business cycles. © 2022 Elsevier B.V., All rights reserved.","Puglia, M.; Tucker, A.",2021,10.3905/jfds.2021.1.061,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127450588&doi=10.3905%2Fjfds.2021.1.061&partnerID=40&md5=abf79b5fec58c545a98983f579b5062c,scopus,"This study employs neural networks and probit regression to forecast US recessions using Treasury term spreads and other macro-financial variables. The authors introduce a novel econometric method for validating and explaining machine learning forecasts. They find that probit regression performs comparably to neural networks in this application, but neural networks reveal important distributional features missed by probit regression. The analysis covers the post-Volcker period, examining feature importance across business cycles.",True,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T16:52:09.809879
6b5d1dd465367a4c,Neural network prediction of crude oil futures using B-splines,"We propose two ways to improve the forecasting accuracy of a focused time-delay neural network (FTDNN) that forecasts the term structure of crude oil futures. Our results show that a convergence based FTDNN makes consistently more accurate predictions than the fixed-epoch FTDNN in Barunik and Malinska (2016). Further, we suggest using basis splines (B-splines), instead of Nelson-Siegel functions, to fit the term structure curves. The empirical results show that the B-spline expansions lead to consistently better 1 and 3 months ahead predictions compared to the convergence based FTDNN. We also explore conditions under which the B-spline based approach may be better for longer-term predictions.",,2021,10.1016/j.eneco.2020.105080,,proquest,"This paper proposes two methods to enhance the accuracy of a focused time-delay neural network (FTDNN) for forecasting crude oil futures term structures. The first method involves a convergence-based FTDNN, which outperforms the fixed-epoch FTDNN. The second method suggests using B-splines instead of Nelson-Siegel functions to fit term structure curves, leading to improved 1 and 3-month ahead predictions. The study also investigates the potential for B-splines in longer-term predictions.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:52:14.616153
b28e94345d9b2a0a,Neural networks in financial engineering: a study in methodology,"Neural networks have shown considerable successes in modeling financial data series. However, a major weakness of neural modeling is the lack of established procedures for performing tests for misspecified models, and tests of statistical significance for the various parameters that have been estimated. This is a serious disadvantage in applications where there is a strong culture for testing not only the predictive power of a model or the sensitivity of the dependent variable to changes in the inputs but also the statistical significance of the finding at a specified level of confidence. Rarely is this more important than in the case of financial engineering, where the data generating processes are dominantly stochastic and only partially deterministic. Partly a tutorial, partly a review, this paper describes a collection of typical applications in options pricing, cointegration, the term structure of interest rates and models of investor behavior which highlight these weaknesses and propose and evaluate a number of solutions. We describe a number of alternative ways to deal with the problem of variable selection, show how to use model misspecification tests, we deploy a novel way based on cointegration to deal with the problem of nonstationarity, and generally describe approaches to predictive neural modeling which are more in tune with the requirements for modeling financial data series.",A. . -P. N. Refenes; A. N. Burgess; Y. Bentz,1997,10.1109/72.641449,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=641449,ieeexplore,"This paper reviews and proposes solutions for weaknesses in neural network modeling for financial data, specifically addressing the lack of established procedures for model misspecification tests and statistical significance tests. It covers applications in options pricing, cointegration, interest rate term structure, and investor behavior, offering approaches for variable selection, model misspecification testing, and handling nonstationarity using cointegration, aiming for predictive neural modeling suitable for financial data.",True,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T16:53:02.061669
2576c128066ee3ce,New frontiers for arch models,"In the 20 years following the publication of the ARCH model, there has been a vast quantity of research uncovering the properties of competing volatility models. Wide-ranging applications to financial data have discovered important stylized facts and illustrated both the strengths and weaknesses of the models. There are now many surveys of this literature. This paper looks forward to identify promising areas of new research. The paper lists five new frontiers. It briefly discusses three-high-frequency volatility models, large-scale multivariate ARCH models, and derivatives pricing models. Two further frontiers are examined in more detail-application of ARCH models to the broad class of non-negative processes, and use of Least Squares Monte Carlo to examine non-linear properties of any model that can be simulated. Using this methodology, the paper analyses more general types of ARCH models, stochastic volatility models, long-memory models and breaking volatility models. The volatility of volatility is defined, estimated and compared with option-implied volatilities. Copyright (C) 2002 John Wiley Sons, Ltd.","Engle, R",2002,10.1002/jae.683,,wos,"This paper reviews the ARCH model literature over the past 20 years, highlighting its applications to financial data and identifying five promising areas for future research. It discusses high-frequency volatility models, large-scale multivariate ARCH models, and derivatives pricing models. Two areas are explored in depth: applying ARCH models to non-negative processes and using Least Squares Monte Carlo for non-linear properties of simulated models. The paper also defines, estimates, and compares the volatility of volatility with option-implied volatilities.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:53:08.695332
7c3de2cb255ed663,News implied volatility and aggregate economic activity: evidence from the Japanese government bond market,"Because options on 10-year Japanese government bond (JGB) futures are relatively new in the market, their implied volatility, JGB-VIX, is not available before 2007. For the period when JGB-VIX is available, we conduct supervised learning by using the daily newspaper articles as input and JGB-VIX as output. We then construct a new JGB market uncertainty measure, which we called JGB-NU, based on the predicted values of JGB-VIX from the estimated model and contents of the newspaper articles from 1981 to 2021. In the VAR analysis with JGB-NU, we confirm that JGB market uncertainty shocks have a negative impact on real economic activities in Japan. © 2024 Elsevier B.V., All rights reserved.","Goshima, K.; Ishijima, H.; Shintani, M.",2024,10.1080/13504851.2022.2140751,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141618967&doi=10.1080%2F13504851.2022.2140751&partnerID=40&md5=b1683f478f389f1a0e766902a8fed6b6,scopus,"This study constructs a new measure of Japanese government bond (JGB) market uncertainty, JGB-NU, by combining implied volatility (JGB-VIX) with newspaper article content. Using supervised learning with daily newspaper articles as input and JGB-VIX as output, they predict JGB-VIX and then create JGB-NU. The analysis shows that shocks to JGB market uncertainty negatively impact real economic activities in Japan.",True,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T16:54:05.958097
3304d99c523318c7,Nonlinear Kalman Filtering in Affine Term Structure Models,"The extended Kalman filter, which linearizes the relationship between security prices and state variables, is widely used in fixed-income applications. We investigate whether the unscented Kalman filter should be used to capture nonlinearities and compare the performance of the Kalman filter with that of the particle filter. We analyze the cross section of swap rates, which are mildly nonlinear in the states, and cap prices, which are highly nonlinear. When caps are used to filter the states, the unscented Kalman filter significantly outperforms its extended counterpart. The unscented Kalman filter also performs well when compared with the much more computationally intensive particle filter. These findings suggest that the unscented Kalman filter may be a good approach for a variety of problems in fixed-income pricing.","Christoffersen, Peter; Dorion, Christian; Jacobs, Kris; Karoui, Lotfi",2014,10.1287/mnsc.2013.1870,,wos,"This paper compares the performance of the extended Kalman filter, unscented Kalman filter, and particle filter in affine term structure models. It finds that the unscented Kalman filter outperforms the extended Kalman filter, especially for highly nonlinear instruments like cap prices, and performs well compared to the particle filter. The authors suggest the unscented Kalman filter as a suitable approach for fixed-income pricing problems.",True,True,False,gemini-2.5-flash-lite,Trine,M,,2025-10-13T16:54:19.258641
77f546f93221829e,Nonlinear autoregressive model with stochastic volatility innovations: Semiparametric and Bayesian approach,"The first-order nonlinear autoregressive model with the stochastic volatility as the model of dependent innovations is considered and a semiparametric method is proposed to estimate the unknown function. Optimal filtering technique based on sequential Monte Carlo perspective is used for estimation of the hidden log-volatility in this model. Bayesian paradigm is applied for estimation of both the unknown parameters and hidden process using particle marginal Metropolis–Hastings scheme. Furthermore, an empirical application on simulated data and on the monthly excess returns of S&P 500 index is presented to study the performance of the schemes implemented. © 2018 Elsevier B.V., All rights reserved.","Hajrajabi, A.; Yazdanian, A.R.; Farnoosh, R.",2018,10.1016/j.cam.2018.05.036,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047786510&doi=10.1016%2Fj.cam.2018.05.036&partnerID=40&md5=02e79d768d049c5f4dbb0017d24c72aa,scopus,This paper proposes a semiparametric method to estimate a first-order nonlinear autoregressive model with stochastic volatility innovations. It utilizes optimal filtering and Bayesian inference (particle marginal Metropolis–Hastings) for parameter and hidden process estimation. The method is demonstrated on simulated data and S&P 500 index returns.,True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:54:41.045309
c362129692c13803,Nonlinear equilibrium adjustment dynamics and predictability of the term structure of interest rates,"We analyze money market dynamics under a long-run equilibrium framework where commonly-monitored spreads serve as error correction terms, derived from a structural model incorporating autocorrelated risk premia, interest rate smoothing and monetary policy feedback. Using a dataset of monthly observations of the spot next and four-, thirteen-, twenty six- and fifty two-week Treasury Bills rates for the United States, Germany and United Kingdom from January 1999 to April 2016, we investigate the power of the expectations hypothesis theory of interest rates taking into account long-run deviations from equilibrium and inherent nonlinearities. We reveal short-run dynamic adjustments for the term structure of the USA, Germany and the UK, which are subject to regime switches. When forecastability is tested during May 2016–October 2017, the MSIH-VECM outperforms systematically the VECM. This is the first attempt to explore the possibility of parameter instability as a crucial factor in deriving the rejection of the restricted version of the cointegration space. Moreover, we investigate the dynamic out-of-sample forecasts of the term structure to assess the effectiveness of nonlinear MS-VECM modeling in capturing the after-effects of the global crisis. Overall, our results suggest that regime shifts in the mean and variance of the term structure may be intertwined with changes in fundamentals, that play a role in driving interest rate regimes, in particular business cycle and inflation fluctuations. © 2017 Elsevier B.V., All rights reserved.","Bekiros, S.; Avdoulas, C.; Hassapis, C.",2018,10.1016/j.irfa.2017.11.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037364609&doi=10.1016%2Fj.irfa.2017.11.009&partnerID=40&md5=fbbb976cfd76f77641d8202840001708,scopus,"This study analyzes the dynamics and predictability of the term structure of interest rates using a long-run equilibrium framework and a structural model. It incorporates autocorrelated risk premia, interest rate smoothing, and monetary policy feedback. The research uses monthly data for US, German, and UK Treasury Bills from 1999 to 2016 to examine the expectations hypothesis, considering long-run deviations and nonlinearities. The findings indicate short-run dynamic adjustments in the term structure, subject to regime switches. A nonlinear MS-VECM model outperforms a standard VECM in forecasting, especially after the global crisis. The study suggests that regime shifts in the term structure are linked to changes in economic fundamentals like business cycles and inflation.",True,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T16:55:04.496432
62ca100ade73914d,"Nonlinear term structure dependence: Copula functions, empirics, and risk implications","This paper documents nonlinear cross-sectional dependence in the term structure of US-Treasury yields and points out risk management implications. The analysis is based on a Kalman filter estimation of a two-factor affine model which specifies the yield curve dynamics. We then apply a broad class of copula functions for modeling dependence in factors spanning the yield curve. Our sample of monthly yields in the 1982-2001 period provides evidence of upper tail dependence in yield innovations; i.e., large positive interest rate shocks tend to occur under increased dependence. In contrast, the best-fitting copula model coincides with zero lower tail dependence. This asymmetry has substantial risk management implications. We give an example in estimating bond portfolio loss quantiles and report the biases which result from an application of the normal dependence model. © 2005 Elsevier B.V. All rights reserved. © 2006 Elsevier B.V., All rights reserved.","Junker, M.; Szimayer, A.; Wagner, N.",2006,10.1016/j.jbankfin.2005.05.014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645876764&doi=10.1016%2Fj.jbankfin.2005.05.014&partnerID=40&md5=32ab47b0aeff8bdd8f432aa07d529247,scopus,"This paper investigates nonlinear dependence in the term structure of US Treasury yields using a Kalman filter and copula functions. It finds evidence of upper tail dependence in yield innovations, suggesting that large positive interest rate shocks tend to occur together. This asymmetry has significant implications for risk management, particularly in estimating bond portfolio loss quantiles, and highlights biases from using normal dependence models.",True,True,False,gemini-2.5-flash-lite,Trine,M,,2025-10-13T16:55:36.877876
68770456e00079eb,"Nonparametric conditional density estimation of short-term interest rate movements: procedures, results and risk management implications","This article shows how to estimate the conditional density of daily changes in the 3-month T-bill rate, using an extension of the kernel-based estimator proposed by Rosenblatt (1969). The shape of the estimated density is allowed to vary with both the level and the lagged change in rates. Due to the nonparametric character of the estimation procedure, the model produces conditional quantile estimates that are based only on the data and are independent of the modellers' assumptions. The obtained results do not support the assumption of systematically mean-reverting behaviour underlying some theoretical models of short-term interest rate dynamics. However, they clearly indicate the presence of nonlinear first-order autocorrelation and volatility clustering effects, as well as a positive relationship between yield volatility and level. Reprinted by permission of Routledge, Taylor and Francis Ltd.",,2013,10.1080/09603107.2012.741677,,proquest,"This article proposes a nonparametric kernel-based method to estimate the conditional density of daily changes in the 3-month T-bill rate. The method allows the density shape to vary with the level and lagged change in rates, providing data-driven conditional quantile estimates. The findings suggest nonlinear autocorrelation, volatility clustering, and a positive relationship between yield volatility and level, contradicting systematic mean reversion.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-13T16:56:06.895783
9d6021375ec7e4f4,Nonparametric long term prediction of stock returns with generated bond yields,"Recent empirical approaches in forecasting equity returns or premiums found that dynamic interactions among the stock and bond are relevant for long term pension products. Automatic procedures to upgrade or downgrade risk exposure could potentially improve long term performance for such products. The risk and return of bonds is more easy to predict than the risk and return of stocks. This and the well known stock-bond correlation motivates the inclusion of the current bond yield in a model for the prediction of excess stock returns. Here, we take the actuarial long term view using yearly data, and focus on nonlinear relationships between a set of covariates. We employ fully nonparametric models and apply for estimation a local-linear kernel smoother. Since the current bond yield is not known, it is predicted in a prior step. The structure imposed this way in the final estimation process helps to circumvent the curse of dimensionality and reduces bias in the estimation of excess stock returns. Our validated stock prediction results show that predicted bond returns improve stock prediction significantly. © 2018 Elsevier B.V., All rights reserved.","Scholz, M.; Sperlich, S.; Nielsen, J.P.",2016,10.1016/j.insmatheco.2016.04.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969264273&doi=10.1016%2Fj.insmatheco.2016.04.007&partnerID=40&md5=17bf2ebd89d43d3a897d4e7079a14d3f,scopus,"This paper proposes a nonparametric model using local-linear kernel smoothing to predict excess stock returns by incorporating generated bond yields. The model leverages the predictability of bond returns and the stock-bond correlation to improve long-term stock market forecasting, especially for pension products. A two-step approach predicts bond yields first, then uses these predictions to estimate stock returns, mitigating the curse of dimensionality and bias.",True,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T16:56:23.896151
bf41f43cc6cb3759,Nonparametric model validations for hidden Markov models with applications in financial econometrics,"We address the nonparametric model validation problem for hidden Markov models with partially observable variables and hidden states. We achieve this goal by constructing a nonparametric simultaneous confidence envelope for transition density function of the observable variables and checking whether the parametric density estimate is contained within such an envelope. Our specification test procedure is motivated by a functional connection between the transition density of the observable variables and the Markov transition kernel of the hidden states. Our approach is applicable for continuous-time diffusion models, stochastic volatility models, nonlinear time series models, and models with market microstructure noise. (C) 2011 Elsevier B.V. All rights reserved.","Zhao, Zhibiao",2011,10.1016/j.jeconom.2011.01.002,,wos,"This paper proposes a nonparametric model validation method for hidden Markov models with partially observable variables and hidden states. It constructs a confidence envelope for the transition density function of observable variables and checks if the parametric density estimate falls within it. This approach is applicable to various models including diffusion models, stochastic volatility models, and nonlinear time series models, with potential applications in financial econometrics.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:56:28.896853
c2c226c46dd988d9,Nonparametric modeling and analysis of association between huntington's disease onset and CAG repeats,"Huntington's disease (HD) is a neurodegenerative disorder with a dominant genetic mode of inheritance caused by an expansion of CAG repeats on chromosome 4. Typically, a longer sequence of CAG repeat length is associated with increased risk of experiencing earlier onset of HD. Previous studies of the association between HD onset age and CAG length have favored a logistic model, where the CAG repeat length enters the mean and variance components of the logistic model in a complex exponential-linear form. To relax the parametric assumption of the exponential-linear association to the true HD onset distribution, we propose to leave both mean and variance functions of the CAG repeat length unspecified and perform semiparametric estimation in this context through a local kernel and backfitting procedure. Motivated by including family history of HD information available in the family members of participants in the Cooperative Huntington's Observational Research Trial (COHORT), we develop the methodology in the context of mixture data, where some subjects have a positive probability of being risk free. We also allow censoring on the age at onset of disease and accommodate covariates other than the CAG length. We study the theoretical properties of the proposed estimator and derive its asymptotic distribution. Finally, we apply the proposed methods to the COHORT data to estimate the HD onset distribution using a group of study participants and the disease family history information available on their family members. © 2013 John Wiley & Sons, Ltd. © 2013 John Wiley & Sons, Ltd. © 2014 Elsevier B.V., All rights reserved.","Ma, Y.; Wang, Y.",2014,10.1002/sim.5971,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896717351&doi=10.1002%2Fsim.5971&partnerID=40&md5=8093cb412464361671de561844e3824e,scopus,"This study proposes a semiparametric method using local kernel and backfitting to model the association between Huntington's disease onset and CAG repeat length, relaxing previous parametric assumptions. It incorporates family history, allows for risk-free individuals, handles censored data, and includes other covariates. The method is applied to the COHORT data.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:56:33.162777
e0d700e7e0724a0d,Oil price dynamics and speculation: A multivariate financial approach,"This paper assesses empirically whether speculation affects oil price dynamics. The growing presence of financial operators in the oil markets has led to the diffusion of trading techniques based on extrapolative expectations. Strategies of this kind foster feedback trading that may cause considerable departures of prices from their fundamental values. We investigate this hypothesis using a modified CAPM following Shiller (1984) and Sentana and Wadhwani (1992). First, a univariate GARCH(1,1)-M is estimated assuming the risk premium to be a function of the conditional oil price volatility. The single factor model, however, is outperformed by the multifactor ICAPM (Merton, 1973), which takes into account a larger investment opportunity set. Analysis is then carried out using a trivariate CCC GARCH-M model with complex nonlinear conditional mean equations where oil price dynamics are associated with both stock market and exchange rate behavior. We find strong evidence that oil price shifts are negatively related to stock price and exchange rate changes and that a complex web of time-varying first and second order conditional moment interactions affects both the CAPM and feedback trading components of the model. Despite the difficulties, we identify a significant role played by speculation in the oil market, which is consistent with the observed large daily upward and downward shifts in prices - a clear evidence that it is not a fundamental-driven market. Thus, from a policy point of view - given the impact of volatile oil prices on global inflation and growth - actions that monitor speculative activities on commodity markets more effectively are to be welcomed. [Copyright Elsevier B.V.]",,2010,10.1016/j.eneco.2009.08.014,,proquest,"This paper empirically investigates the impact of speculation on oil price dynamics using a multivariate financial approach. It employs modified CAPM and GARCH-M models, including a trivariate CCC GARCH-M model incorporating stock market and exchange rate behavior. The findings suggest that speculation plays a significant role, leading to price volatility not solely driven by fundamentals, and advocates for enhanced monitoring of speculative activities in commodity markets.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:56:38.147208
7bede30395f8b5fa,"On Crop Biodiversity, Risk Exposure, and Food Security in the Highlands of Ethiopia","This paper investigates the effects of crop genetic diversity on farm productivity and production risk in the highlands of Ethiopia. Using a moment‐based approach, the analysis uses a stochastic production function capturing mean, variance, and skewness effects. Welfare implications of diversity are evaluated using a certainty equivalent, measured as expected income minus a risk premium (reflecting the cost of risk). We find that the effect of diversity on skewness dominates its effect on variance, meaning that diversity reduces the cost of risk. The analysis also shows that the beneficial effects of diversity become of greater value in degraded land.",,2009,10.1111/j.1467-8276.2009.01265.x,,proquest,"This paper examines how crop genetic diversity impacts farm productivity and production risk in Ethiopia's highlands. It uses a stochastic production function to analyze mean, variance, and skewness effects, finding that diversity reduces risk costs, especially on degraded land. The study evaluates welfare implications through a certainty equivalent.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:56:39.713457
0a36a7f19f6f627b,On LASSO for predictive regression,"Explanatory variables in a predictive regression typically exhibit low signal strength and various degrees of persistence. Variable selection in such a context is of great importance. In this paper, we explore the pitfalls and possibilities of the LASSO methods in this predictive regression framework. In the presence of stationary, local unit root, and cointegrated predictors, we show that the adaptive LASSO cannot asymptotically eliminate all cointegrating variables with zero regression coefficients. This new finding motivates a novel post-selection adaptive LASSO, which we call the twin adaptive LASSO (TAlasso), to restore variable selection consistency. Accommodating the system of heterogeneous regressors, TAlasso achieves the well-known oracle property. In contrast, conventional LASSO fails to attain coefficient estimation consistency and variable screening in all components simultaneously. We apply these LASSO methods to evaluate the short- and long-horizon predictability of S&P 500 excess returns.",,2022,10.1016/j.jeconom.2021.02.002,,proquest,"This paper investigates the use of LASSO methods for predictive regression, particularly when explanatory variables have low signal strength and persistence. It introduces a novel method, the twin adaptive LASSO (TAlasso), to improve variable selection consistency in the presence of stationary, local unit root, and cointegrated predictors. The study applies these methods to evaluate the predictability of S&P 500 excess returns.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:56:45.729679
2a9a49ec44da48fb,On multicollinearity and the value of the shape parameter in the term structure Nelson-Siegel model,"This paper investigates the sensitivity of the dynamic Nelson-Siegel factor loadings to the value of the shape parameter, λ. It also analyses the multicollinearity problem and addresses how to mitigate this issue in the estimation process. First, we find that the selection of a fixed λ is not optimal due to the collinearity problems. Second, we observe a substantial difference between the forecasting performance of the traditional estimation procedures and that of the ridge regression approach. Finally, we implement a Monte Carlo simulation exercise in order to study the statistical distribution of the estimates of the model parameters and thus determine the extent to which they differ from the real values. Furthermore, we find that multicollinearity between the factors of the NS model can, in the case of ordinary least squares estimation with a fixed parameter λ, result in greater differences between the estimates and the actual parameter values. Ridge regression corrects such differences and produces more stable estimates than the ordinary linear and nonlinear least squares methods.",,2018,10.5605/ieb.16.1,,proquest,"This paper examines how the shape parameter (λ) affects the dynamic Nelson-Siegel factor loadings and addresses multicollinearity in the model. It finds that fixing λ is suboptimal and that ridge regression outperforms traditional methods in forecasting. A Monte Carlo simulation confirms that multicollinearity leads to less stable estimates with OLS, which ridge regression mitigates.",False,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T16:57:31.811592
8c483dd1af8aab26,On the specification of the drift and diffusion functions for continuous-time models of the spot interest rate,This paper explores the specification of drift and diffusion functions for continuous-time short-term interest rate models. Various forms for the drift and diffusion of 7-day Eurodollar rates are proposed and then estimated by discrete maximum-likelihood. The results suggest that a nonparametric specification of drift and volatility in terms of orthogonal polynomial expansions is effective in eliminating problems of parameter identification encountered previously. Some evidence is found to support the claim that the drift of the short term interest rate is nonlinear.,"Hurn, AS; Lindsay, KA",2002,10.1111/1468-0084.00277,,wos,This paper investigates the specification of drift and diffusion functions for continuous-time models of short-term interest rates. It proposes and estimates various forms for the drift and diffusion of 7-day Eurodollar rates using discrete maximum-likelihood. The findings indicate that a nonparametric specification using orthogonal polynomial expansions effectively addresses parameter identification issues and suggests a nonlinear drift for short-term interest rates.,True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:57:37.182066
e4db19deb8251edb,On the time-varying relation between monetary policy uncertainty and bond risk premia,"This paper examines the time-varying relationship between monetary policy uncertainty (MPU) and bond excess returns. To do so, we introduce a nonparametric time-varying coefficient predictive regression model for bond returns, and employ a kernel-based two-step method to estimate the time-varying coefficients. Next, we apply the methodologies to analyze the dynamic forecasting relationship between zero-coupon bond returns and MPU from 1985 to 2022. We find that MPU significantly and positively predicts bond returns in over 75% of the sample period, with the strongest effect observed in 2005. Thus, the expectations hypothesis is only transiently valid. After controlling for the shape of the yield curve, MPU still retains its ability to predict bond returns in 50% to 80% of the sample period. Our conclusions are robust to the so-called embedded endogeneity. Additionally, we find that bond excess returns are less responsive to MPU during periods of high economic activities and are more responsive during periods of low economic activities.","Li, Luyang; Yin, Ximing; Yu, Deshui",2025,10.1016/j.irfa.2025.104465,,wos,"This paper investigates the dynamic relationship between monetary policy uncertainty (MPU) and bond excess returns using a nonparametric time-varying coefficient model. The study finds that MPU significantly predicts bond returns for a substantial portion of the 1985-2022 period, with varying effects depending on economic activity levels. The findings suggest the expectations hypothesis is only temporarily valid.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-13T16:57:59.898766
fe0b869162d09c8f,One idea of portfolio risk control for absolute return strategy risk adjustments by signals from correlation behavior,"Absolute return strategy provided from fund of funds (FOFs) investment schemes is the focus in Japanese Financial Community. FOFs investment mainly consists of hedge fund investment and it has two major characteristics which are low correlation against benchmark index and little impact from various external changes in the environment given maximizing return. According to the historical track record of survival hedge funds in this business world, they maintain a stable high return and low risk. However, one must keep in mind that low risk would not be equal to risk free. The failure of Long-term capital management (LTCM) that took place in the summer of 1998 was a symbolized phenomenon. The summer of 1998 exhibited a certain limitation of traditional value at risk (VaR) and some possibility that traditional VaR could be ineffectual to the nonlinear type of fluctuation in the market. In this paper, I try to bring self-organized criticality (SOC) into portfolio risk control. SOC would be well known as a model of decay in the natural world. I analyzed nonlinear type of fluctuation in the market as SOC and applied SOC to capture complicated market movement using threshold point of SOC and risk adjustments by scenario correlation as implicit signals. Threshold becomes the control parameter of risk exposure to set downside floor and forecast extreme nonlinear type of fluctuation under a certain probability. Simulation results would show synergy effect of portfolio risk control between SOC and absolute return strategy. © 2001 Elsevier Science B.V. All rights reserved. © 2004 Elsevier Science B.V., Amsterdam. All rights reserved.","Nishiyama, N.",2001,10.1016/s0378-4371(01)00411-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035576026&doi=10.1016%2FS0378-4371%2801%2900411-3&partnerID=40&md5=d03382ca37a1fbaacc14f9cfe5da4686,scopus,"This paper proposes using self-organized criticality (SOC) to manage portfolio risk for absolute return strategies, particularly in light of the limitations of traditional Value at Risk (VaR) demonstrated by events like the Long-Term Capital Management failure. The authors analyze nonlinear market fluctuations as SOC and apply it to capture market movements using a threshold point and scenario correlation signals. Simulations suggest a synergistic effect between SOC and absolute return strategies for risk control.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:58:04.830508
7cae5492184c7382,Online Investor Sentiment via Machine Learning,"In this paper, we propose utilizing machine learning methods to determine the expected aggregated stock market risk premium based on online investor sentiment and employing the multifold forward-validation method to select the relevant hyperparameters. Our empirical studies provide strong evidence that some machine learning methods, such as extreme gradient boosting or random forest, show significant predictive ability in terms of their out-of-sample performances with high-dimensional investor sentiment proxies. They also outperform the traditional linear models, which shows a possible unobserved nonlinear relationship between online investor sentiment and risk premium. Moreover, this predictability based on online investor sentiment has a better economic value, so it improves portfolio performance for investors who need to decide the optimal asset allocation in terms of the certainty equivalent return gain and the Sharpe ratio.",,2024,10.3390/math12203192,,proquest,"This paper proposes using machine learning (ML) methods, specifically extreme gradient boosting and random forest, to predict stock market risk premium based on online investor sentiment. The ML models demonstrated significant out-of-sample predictive ability, outperforming traditional linear models and showing potential economic value for portfolio performance.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:58:07.879618
c9bb2437e26f8d84,Optimal Filtering of Jump Diffusions: Extracting Latent States from Asset Prices,"This paper provides an optimal filtering methodology in discretely observed continuous-time jump-diffusion models. Although the filtering problem has received little attention, it is useful for estimating latent states, forecasting volatility and returns, computing model diagnostics such as likelihood ratios, and parameter estimation. Our approach combines time-discretization schemes with Monte Carlo methods. It is quite general, applying in nonlinear and multivariate jump-diffusion models and models with nonanalytic observation equations. We provide a detailed analysis of the filter's performance, and analyze four applications: disentangling jumps from stochastic volatility, forecasting volatility, comparing models via likelihood ratios, and filtering using option prices and returns. (JEL C11, C13, C15, C51, C52, G11, G12, G17)","Johannes, Michael S.; Polson, Nicholas G.; Stroud, Jonathan R.",2009,10.1093/rfs/hhn110,,wos,"This paper presents an optimal filtering method for discretely observed continuous-time jump-diffusion models, useful for estimating latent states, forecasting, and parameter estimation. The approach combines time-discretization and Monte Carlo methods, applicable to nonlinear, multivariate, and nonanalytic models. Applications include disentangling jumps from stochastic volatility, volatility forecasting, model comparison, and filtering using option prices and returns.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:58:12.031118
b7733e59a767a900,Optimal hedging of commodity price risks in highway contracts,"Macroeconomic conditions, such as commodity prices, labor wages, and inflation rates, affect the cost of construction projects. In a volatile market environment, highway agencies often pass such risks to contractors by using fixed-price contracts. In turn, contractors respond by adding premiums in bid prices. How much of this risk highway agencies should pass to contractors is the topic of this paper. More specifically, the objective of this paper is to develop a model that can help highway agencies manage cost risks associated with commodity prices. The weighted least squares regression model is used to estimate the risk premium; the solution to a multiobjective optimization formulation considers a genetic algorithm approach to nonconvex optimization. Crude oil prices are used as an example of volatile commodities. The results of this study suggest that the optimal risk mitigation actions are conditional on owners' risk preferences. © 2023 Elsevier B.V., All rights reserved.","Zhou, X.; Damnjanović, I.D.",2011,10.3141/2228-03,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155159114&doi=10.3141%2F2228-03&partnerID=40&md5=2c006c7e567e45da4d1cba2011fb7b10,scopus,"This paper develops a model to help highway agencies manage cost risks associated with commodity prices in construction projects. It uses weighted least squares regression and a genetic algorithm approach to multiobjective optimization to estimate risk premiums and determine optimal risk mitigation strategies, considering crude oil prices as an example. The findings indicate that optimal risk mitigation depends on the owner's risk preferences.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:58:14.480237
70d6446af9a229d8,Optimal profit-making strategies in stock market with algorithmic trading,"Machine learning (ML) techniques are being increasingly applied to financial markets for analyzing trends and predicting stock prices. In this study, we compared the price prediction and profit-making performance of various ML algorithms embedded into stock trading strategies. The dataset comprised daily data from the CSI 300 Index of the China stock market spanning approximately 17 years (2006-2023). We incorporated investor sentiment indicators and relevant financial elements as features. Our trained models included support vector machines (SVMs), logistic regression, and random forest. The results show that the SVM model outperforms the others, achieving an impressive 60.52% excess return in backtesting. Furthermore, our research compared standard prediction models (such as LASSO and LSTM) with the proposed approach, providing valuable insights for users selecting ML algorithms in quantitative trading strategies. Ultimately, this work serves as a foundation for informed algorithm choice in future financial applications. © 2024 Elsevier B.V., All rights reserved.","Wang, H.; Xie, D.",2024,10.3934/qfe.2024021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205123614&doi=10.3934%2FQFE.2024021&partnerID=40&md5=02b39feb6c9d88a93900c795feb2dade,scopus,"This study compares the performance of various machine learning algorithms (SVM, logistic regression, random forest, LASSO, LSTM) for stock price prediction and profit-making in the China stock market (CSI 300 Index). The SVM model achieved the highest excess return (60.52%) in backtesting, demonstrating its effectiveness in quantitative trading strategies.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:58:17.483828
06584a65bc1a10e7,Optimizing filter rule parameters with genetic algorithm and stock selection with artificial neural networks for an improved trading: The case of Borsa Istanbul,"Filter rule along with other trading algorithms is used to identify potentially profitable trading points in stock markets. In this study, the scope of the filter rule has been expanded to include different moving average types. The filter rule parameters that will provide the highest return for each of the stocks listed in Borsa Istanbul have been optimized by using genetic algorithm. A number of 357 stocks traded in Borsa Istanbul is included in the dataset of the study between 06-07-2012 and 31-03-2020 period. To improve the poor performance in out-of-sample sets of optimal rules, the stock selection process was performed by means of artificial neural networks. The artificial neural network model predicts the performance of the stock in the test set by using the performance values in the training set. Results indicate that the returns of the selected stocks are significantly higher than the returns of the buy and hold strategy. Parameter optimization of filter rule with genetic algorithms and stock selection with the artificial neural networks can be used as a decision support system for investors, where they can make a profit above the market return. When only the genetic algorithm results are taken into account, it can be stated that Borsa Istanbul is a weak form efficient market. However, selecting the stocks with the assistance of artificial neural networks made it possible to obtain excess returns over the market. © 2022 Elsevier B.V., All rights reserved.","Özçalıcı, M.; Bumin, M.",2022,10.1016/j.eswa.2022.118120,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134630841&doi=10.1016%2Fj.eswa.2022.118120&partnerID=40&md5=4ccc46612a7b221c336ef63c952ea36c,scopus,"This study optimizes filter rule parameters using a genetic algorithm and employs artificial neural networks for stock selection on Borsa Istanbul. The approach aims to improve trading performance by identifying profitable trading points and selecting stocks with better out-of-sample performance. Results show that the optimized strategy yields significantly higher returns than a buy-and-hold strategy, suggesting its utility as a decision support system for investors.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T16:58:21.780660
a6849cec0ad130d5,"Option-implied preferences adjustments, density forecasts, and the equity risk premium","The main objective of this paper is to analyse the value of information contained in prices of options on the IBEX 35 index at the Spanish Stock Exchange Market. The forward looking information is extracted using implied risk-neutral density functions estimated by a mixture of two-lognormals and several alternative risk adjustments. Our results show that, between October 1996 and March 2000, we can reject the hypothesis that the risk-neutral densities provide accurate predictions of the distributions of future realisations of the IBEX 35 index at 4- and 8-week horizons. When forecasting through risk-adjusted densities the performance of this period is statistically improved and we no longer reject that hypothesis. We show that risk adjustments based on a power specification for the stochastic discount factor-which is the approach used so far in the literature that derives the objective density function from option prices- generates an excessive volatility of risk premia. We use alternative risk adjustments and find that the forecasting performance of the distribution improves slightly in some cases when risk aversion is allowed to be time-varying. Finally, from October 1996 to December 2004, the ex-ante risk premium perceived by investors and that are embedded in option prices is between 12 and 18% higher than the premium required to compensate the same investors for the realised volatility in stock market returns.","Alonso, Francisco; Blanco, Roberto; Rubio, Gonzalo",2009,10.1007/s10108-008-9049-3,,wos,"This paper analyzes the information in IBEX 35 index options to extract implied risk-neutral densities. It finds that these densities do not accurately predict future index distributions but that risk adjustments improve forecasting performance. The study also critiques existing risk adjustment methods and explores time-varying risk aversion, concluding that implied risk premiums are higher than those compensating for realized volatility.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:07:36.616579
9c03ca737f0a4eb9,Out-of-Sample Predictability of the Equity Risk Premium,"A large set of macroeconomic variables have been suggested as equity risk premium predictors in the literature. Acknowledging the different predictability of the equity premium in expansions and recessions, this paper proposes an approach that combines equity premium forecasts from two-state regression models using an agreement technical indicator as the observable state variable. A comprehensive out-of-sample forecast evaluation exercise based on statistical and economic loss functions demonstrates the superiority of the proposed approach versus combined forecasts from linear models or Markov switching models and forecasts from machine learning methods such as random forests and gradient boosting. The parsimonious state-dependent aspect of risk premium forecasts delivers large improvements in forecast accuracy. The results are robust to sub-period analyses and different investors’ risk aversion levels.",,2025,10.3390/math13020257,,proquest,"This paper proposes a novel approach to forecasting the equity risk premium by combining forecasts from two-state regression models, using an agreement technical indicator as the observable state variable. This method demonstrates superior out-of-sample performance compared to linear models, Markov switching models, and machine learning techniques like random forests and gradient boosting, showing significant improvements in forecast accuracy, particularly in a state-dependent manner. The findings are robust across different sub-periods and investor risk aversion levels.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:07:39.078366
9e28d710498e479b,PHANGS-JWST First Results: Dust-embedded Star Clusters in NGC 7496 Selected via 3.3 μm PAH Emission,"The earliest stages of star formation occur enshrouded in dust and are not observable in the optical. Here we leverage the extraordinary new high-resolution infrared imaging from JWST to begin the study of dust-embedded star clusters in nearby galaxies throughout the Local Volume. We present a technique for identifying dust-embedded clusters in NGC 7496 (18.7 Mpc), the first galaxy to be observed by the PHANGS-JWST Cycle 1 Treasury Survey. We select sources that have strong 3.3 mu m PAH emission based on a F300M - F335M color excess and identify 67 candidate embedded clusters. Only eight of these are found in the PHANGS-HST optically selected cluster catalog, and all are young (six have SED fit ages of similar to 1 Myr). We find that this sample of embedded cluster candidates may significantly increase the census of young clusters in NGC 7496 from the PHANGS-HST catalog; the number of clusters younger than similar to 2 Myr could be increased by a factor of 2. Candidates are preferentially located in dust lanes and are coincident with the peaks in the PHANGS-ALMA CO (2-1) maps. We take a first look at concentration indices, luminosity functions, SEDs spanning from 2700 angstrom to 21 mu m, and stellar masses (estimated to be between similar to 10(4) and 10(5) M (circle dot)). The methods tested here provide a basis for future work to derive accurate constraints on the physical properties of embedded clusters, characterize the completeness of cluster samples, and expand analysis to all 19 galaxies in the PHANGS-JWST sample, which will enable basic unsolved problems in star formation and cluster evolution to be addressed.","Rodriguez, M. Jimena; Lee, Janice C.; Whitmore, B. C.; Thilker, David A.; Maschmann, Daniel; Chandar, Rupali; Deger, Sinan; Boquien, Mederic; Dale, Daniel A.; Larson, Kirsten L.; Williams, Thomas G.; Kim, Hwihyun; Schinnerer, Eva; Rosolowsky, Erik; Leroy, Adam K.; Emsellem, Eric; Sandstrom, Karin M.; Kruijssen, J. M. Diederik; Grasha, Kathryn; Watkins, Elizabeth J.; Barnes, Ashley. T.; Sormani, Mattia C.; Kim, Jaeyeon; Anand, Gagandeep S.; Chevance, Melanie; Bigiel, F.; Klessen, Ralf S.; Hassani, Hamid; Liu, Daizhong; Faesi, Christopher M.; Cao, Yixian; Belfiore, Francesco; Pessa, Ismael; Kreckel, Kathryn; Groves, Brent; Pety, Jerome; Indebetouw, Remy; Egorov, Oleg V.; Blanc, Guillermo A.; Saito, Toshiki; Hughes, Annie",2023,10.3847/2041-8213/aca653,,wos,"This study uses JWST infrared imaging to identify dust-embedded star clusters in NGC 7496 by analyzing 3.3 μm PAH emission. The technique identified 67 candidate clusters, significantly increasing the census of young clusters compared to previous optical catalogs. These candidates are often found in dust lanes and CO emission peaks. The study also examines cluster properties like concentration, luminosity, SEDs, and stellar masses, laying the groundwork for future analysis of embedded clusters in star formation and evolution.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:07:42.041471
d501874745d997c5,PMCMC for Term Structure of Interest Rates under Markov Regime Switching and Jumps,"A parameter estimation method, called PMCMC in this paper, is proposed to estimate a continuous-time model of the term structure of interests under Markov regime switching and jumps. There is a closed form solution to term structure of interest rates under Markov regime. However, the model is extended to be a CKLS model with non-closed form solutions which is a typical nonlinear and non-Gaussian state-space model(SSM) in the case of adding jumps. Although the difficulty of parameter estimation greatly prevents from researching models, we prove that the nonlinear and non-Gaussian state-space model has better performances in studying volatility. The method proposed in this paper will be implemented in simulation and empirical study for SHIBOR. Empirical results illustrate that the PMCMC algorithm has powerful advantages in tackling the models. © 2023 Elsevier B.V., All rights reserved.","Liu, X.; Li, X.; Zheng, S.; Qian, H.",2020,10.21078/jssi-2020-159-11,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151287143&doi=10.21078%2FJSSI-2020-159-11&partnerID=40&md5=c49a814702a6f8c4d212641b9b08fbb4,scopus,"This paper proposes a Particle Markov Chain Monte Carlo (PMCMC) method for estimating a continuous-time model of the term structure of interest rates, incorporating Markov regime switching and jumps. While a closed-form solution exists for Markov regime models, the extension to a CKLS model with jumps results in a nonlinear and non-Gaussian state-space model without a closed-form solution. The authors demonstrate that this complex model offers better performance in volatility studies and implement the PMCMC method in simulations and an empirical study using SHIBOR data, showing its effectiveness.",True,True,False,gemini-2.5-flash-lite,Trine,M,,2025-10-13T17:08:00.593611
93fd092bafa0fe9d,PROPERTY AND NUMERICAL SIMULATION OF THE AIT-SAHALIA-RHO MODEL WITH NONLINEAR GROWTH CONDITIONS,"The Ait-Sahalia-Rho model is an important tool to study a number of financial problems, including the term structure of interest rate. However, since the functions of this model do not satisfy the linear growth condition, we cannot study the properties for the solution of this model by using the traditional techniques. In this paper we overcome the mathematical difficulties due to the nonlinear growth condition by using numerical simulation. Thus we first discuss analytical properties of the model and the convergence property of numerical solutions in probability for the Ait-Sahalia-Rho model. Finally, an example for option pricing is given to illustrate that the numerical solution is an effective method to estimate the expected payoffs.","Jiang, Feng; Yang, Hua; Tian, Tianhai",2017,10.3934/dcdsb.2017005,,wos,"This paper analyzes the Ait-Sahalia-Rho model, commonly used for interest rate term structure, overcoming mathematical challenges posed by its nonlinear growth conditions through numerical simulation. It discusses analytical and convergence properties of the model's solutions and provides an example of option pricing to demonstrate the effectiveness of numerical estimation for expected payoffs.",False,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T17:08:13.993955
85ea7436fd04653c,Package CovRegpy: Regularized covariance regression and forecasting in Python,"This paper will outline the functionality available in the CovRegpy package which was written for actuarial practitioners, wealth managers, fund managers, and portfolio analysts in the language of Python 3.11. The objective is to develop a new class of covariance regression factor models for covariance forecasting, along with a library of portfolio allocation tools that integrate with this new covariance forecasting framework. The novelty is in two stages: the type of covariance regression model and factor extractions used to construct the covariates used in the covariance regression, along with a powerful portfolio allocation framework for dynamic multi-period asset investment management. The major contributions of package CovRegpy can be found on the GitHub repository for this library in the scripts: CovRegpy.py, CovRegpy_DCC.py, CovRegpy_RPP.py, CovRegpy_SSA.py, CovRegpy_SSD.py, and CovRegpy_X11.py. These six scripts contain implementations of software features including multivariate covariance time series models based on the regularized covariance regression (RCR) framework, dynamic conditional correlation (DCC) framework, risk premia parity (RPP) weighting functions, singular spectrum analysis (SSA), singular spectrum decomposition (SSD), and X11 decomposition framework, respectively. These techniques can be used sequentially or independently with other techniques to extract implicit factors to use them as covariates in the RCR framework to forecast covariance and correlation structures and finally apply portfolio weighting strategies based on the portfolio risk measures based on forecasted covariance assumptions. Explicit financial factors can be used in the covariance regression framework, implicit factors can be used in the traditional explicit market factor setting, and RPP techniques with long/short equity weighting strategies can be used in traditional covariance assumption frameworks. We examine, herein, two real-world case studies for actuarial practitioners. The first of these is a modification (demonstrating the regularization of covariance regression) of the original example from Hoff & Niu ((2012). Statistica Sinica, 22(2), 729-753) which modeled the covariance and correlative relationship that exists between forced expiratory volume (FEV) and age and FEV and height. We examine this within the context of making probabilistic predictions about mortality rates in patients with chronic obstructive pulmonary disease. The second case study is a more complete example using this package wherein we present a funded and unfunded UK pension example. The decomposition algorithm isolates high-, mid-, and low-frequency structures from FTSE 100 constituents over 20 years. These are used to forecast the forthcoming quarter’s covariance structure to weight the portfolio based on the RPP strategy. These fully funded pensions are compared against the performance of a fully unfunded pension using the FTSE 100 index performance as a proxy. © 2024 Elsevier B.V., All rights reserved.","van Jaarsveldt, C.; Peters, G.; Ames, M.; Chantler, M.",2024,10.1017/s1748499524000101,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193064038&doi=10.1017%2FS1748499524000101&partnerID=40&md5=44609d79e4c0defef3678b5fb8df5c39,scopus,"This paper introduces the Python package CovRegpy, designed for actuarial practitioners and financial professionals. It offers a new class of covariance regression factor models for covariance forecasting and integrates them with a portfolio allocation framework. The package implements techniques such as regularized covariance regression (RCR), dynamic conditional correlation (DCC), singular spectrum analysis (SSA), and others for forecasting covariance and correlation structures. Two case studies are presented: one modifying a statistical model for mortality rate prediction and another demonstrating a pension fund portfolio allocation strategy using FTSE 100 data.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:08:21.425926
9a9d7a03be6464dd,Pairs selection and outranking: An application to the S&P 100 index,Pairs trading is a popular quantitative speculation strategy. This article proposes a general and flexible framework for pairs selection. The method uses multiple return forecasts based on bivariate information sets and multi-criteria decision techniques. Our approach can be seen as a sort of forecast combination but the output of the method is a ranking. It helps to detect potentially under- and overvalued stocks. A first application with S&P 100 index stocks provides promising results in terms of excess return and directional forecasting. (C) 2008 Elsevier B.V. All rights reserved.,"Huck, Nicolas",2009,10.1016/j.ejor.2008.03.025,,wos,"This article proposes a general framework for pairs selection in pairs trading, utilizing multiple return forecasts from bivariate information sets and multi-criteria decision techniques. The method generates a ranking to identify potentially under- and overvalued stocks, and an application to the S&P 100 index shows promising results for excess return and directional forecasting.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:08:27.143722
7b3f1098ac1d9f11,Penalized Averaging of Parametric and Non-Parametric Quantile Forecasts,"We propose a hybrid penalized averaging for combining parametric and non-parametric quantile forecasts when faced with a large number of predictors. This approach goes beyond the usual practice of combining conditional mean forecasts from parametric time series models with only a few predictors. The hybrid methodology adopts the adaptive LASSO regularization to simultaneously reduce predictor dimension and obtain quantile forecasts. Several recent empirical studies have considered a large set of macroeconomic predictors and technical indicators with the goal of forecasting the S&P 500 equity risk premium. To illustrate the merit of the proposed approach, we extend the mean-based equity premium forecasting into the conditional quantile context. The application offers three main findings. First, combining parametric and non-parametric approaches adds quantile forecast accuracy over and above the constituent methods. Second, a handful of macroeconomic predictors are found to have systematic forecasting power. Third, different predictors are identified as important when considering lower, central and upper quantiles of the equity premium distribution. © 2020 Elsevier B.V., All rights reserved.","De Gooijer, J.G.; Zerom, D.",2020,10.1515/jtse-2019-0021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077739820&doi=10.1515%2Fjtse-2019-0021&partnerID=40&md5=55add6393cbc0da4186bc41ac893f6c4,scopus,"This paper introduces a hybrid penalized averaging method to combine parametric and non-parametric quantile forecasts, particularly useful when dealing with a large number of predictors. It utilizes adaptive LASSO regularization for dimension reduction and quantile forecasting. The method is applied to forecast the S&P 500 equity risk premium, demonstrating that combining parametric and non-parametric methods improves accuracy, identifying key macroeconomic predictors, and revealing that different predictors are important for different quantiles of the equity premium distribution.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:08:33.644024
008bfdd1c354cda8,Permutation betting markets: Singleton betting with extra information,"We study permutation betting markets, introduced by Chen et al. (Proceedings of the ACM Conference on Electronic Commerce, 2007). For these markets, we consider subset bettings in which each trader can bet on a subset of candidates ending up in a subset of positions. We consider the revenue maximization problem for the auctioneer in two main frameworks: the risk-free revenue maximization (studied in Chen et al., Proceedings of the ACM Conference on Electronic Commerce, 2007), and the probabilistic revenue maximization. We also explore the use of some certain knowledge or extra information about the possible outcomes of the market. We first show that finding the optimal revenue in the risk-free model for the subset betting problem is inapproximable. This resolves an open question posed by Chen et al. (Proceedings of the ACM Conference on Electronic Commerce, 2007). In order to identify solvable variants of the problem, we propose the singleton betting language which allows traders to bet an arbitrary value on one candidate for one position. For singleton bettings, we first provide a linear-time implementable necessary and sufficient condition for existence of a solution with positive revenue for any possible outcome. Furthermore, we develop an LP-based polynomial-time algorithm to find the optimum solution of this problem. In addition, we show how to extend this LP-based method to handle some extra information about the possible outcomes. Finally, we consider the revenue maximization problem in a probabilistic setting. For this variant, we observe that the problem of maximizing the expected revenue is polynomial-time solvable, but we show that maximizing the probability of achieving a pre-specified revenue is #P-Complete. © 2009 Springer Science+Business Media, LLC. © 2012 Elsevier B.V., All rights reserved.","Ghodsi, M.; Mahini, H.; Mirrokni, V.S.; Zadimoghaddam, M.",2011,10.1007/s00453-009-9378-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959223300&doi=10.1007%2Fs00453-009-9378-0&partnerID=40&md5=50e50d408bf76c116353904d5f36e299,scopus,"This paper studies permutation betting markets, focusing on subset and singleton bettings. It addresses risk-free and probabilistic revenue maximization for the auctioneer, considering the impact of extra information. The study finds the optimal revenue problem for subset betting to be inapproximable, resolves an open question, and proposes a linear-time condition for positive revenue in singleton betting. An LP-based algorithm is developed for singleton betting optimization, extendable to handle extra information. In the probabilistic setting, maximizing expected revenue is polynomial-time solvable, but maximizing the probability of a pre-specified revenue is #P-Complete.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:08:36.978907
ea1a6443c4789db2,Political uncertainty and financial market reactions: A new test,"Recent literature highlights the crucial role of understanding the mechanism between political uncertainty and financial market reactions. Along the lines of this topic, our study stresses a clear causal framework. Exploiting one unique natural experiment of the Taiwan Strait Crisis (1995-96), we provide a simple testing strategy which could precisely quantify the effects of political shocks on stock markets. This approach combines the features of one innovative panel estimator and new statistical learning methods for causal inference. Our results indicate, separating true signal from noise via the optimal benchmark, the political crisis had a substantial and significant negative impact on Taiwan's stock prices. This finding is consistent with the empirical evidence of risk premium in recent studies. Moreover, the optimal counterfactual could be an alternative option for the ceteris paribus assumption in non-lab controlled settings. Finally, this study shows predictor selection is needed for a convincing causal estimate in counterfactual studies. © 2019 Elsevier B.V., All rights reserved.","Wang, H.; Boatwright, A.L.",2019,10.1016/j.inteco.2019.07.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075956787&doi=10.1016%2Fj.inteco.2019.07.004&partnerID=40&md5=7644f7e70f84f5bfd12bd3d90c207b0f,scopus,"This study uses a natural experiment (Taiwan Strait Crisis 1995-96) and a combination of an innovative panel estimator and statistical learning methods to quantify the impact of political shocks on stock markets. The findings suggest a significant negative impact on Taiwan's stock prices, consistent with risk premium theories. The study also highlights the importance of predictor selection in causal inference for counterfactual studies.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:08:40.177722
b6af4763daadded6,Portfolio creation using artificial neural networks and classification probabilities: a Canadian study,"This study aims to verify whether using artificial neural networks (ANNs) to establish classification probabilities generates portfolios with higher excess returns than using ANNs in their traditional role of predicting portfolio returns. Our sample includes all companies listed on the Toronto Stock Exchange from 1994 to 2014 with a monthly average of 16,324 company-month observations. Results indicate that portfolios based on the classification probabilities yield mean returns ranging from 7.81 to 14.40% annually over a 16-year period and that portfolios based on both predicted returns and classification probabilities generate returns that are superior to the market index. In addition, there is evidence that ranking securities based on their probability of beating the market has some benefit. © 2020 Elsevier B.V., All rights reserved.","Morris, T.; Comeau, J.",2020,10.1007/s11408-020-00350-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083775124&doi=10.1007%2Fs11408-020-00350-8&partnerID=40&md5=6e1777fb9bc75bc769cdc7de72b223df,scopus,"This Canadian study investigates whether using artificial neural networks (ANNs) to generate classification probabilities leads to portfolios with higher excess returns compared to traditional ANN methods for predicting returns. The study analyzed companies listed on the Toronto Stock Exchange from 1994 to 2014. Results showed that portfolios based on classification probabilities achieved annual mean returns between 7.81% and 14.40%, outperforming the market index. The findings suggest that ranking securities by their probability of market outperformance is beneficial.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:08:44.910217
8a75482548bbd8de,Post-deregulation bank-deposit-rate pricing: The multivariate dynamics,"The relationship between wholesale and retail interest rates since deregulation is of substantial interest to economists and policymakers, because the predictability of the monetary aggregates and their relationship to bank reserves depend on adjustment patterns in the wholesale and retail money markets. We provide evidence on the nature of wholesale-retail interest rate relationships by examining the dynamic interactions among two wholesale interest rates (federal funds and six-month treasury bills) and three retail deposit rates (six-month consumer certificates of deposit, money market deposit accounts, and super NOW’s). We perform a multivariate time series analysis, with particular attention paid to causal patterns and the shapes of impulse- response functions. A number of stylized facts, related to size of adjustment, speed of adjustment, and pattern of adjustment, are established for the response of retail rates to unanticipated shocks in wholesale rates. © 1990 American Statistical Association. © 2016 Elsevier B.V., All rights reserved.","Diebold, F.X.; Sharpe, S.A.",1990,10.1080/07350015.1990.10509799,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002423521&doi=10.1080%2F07350015.1990.10509799&partnerID=40&md5=8d5caf65bff06c5f7bf019341d859000,scopus,"This study investigates the dynamic relationship between wholesale and retail interest rates post-deregulation using multivariate time series analysis. It examines the interactions among federal funds, treasury bills, and various retail deposit rates, focusing on causal patterns and impulse-response functions to understand how retail rates adjust to wholesale rate shocks.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:08:46.111277
1d531db1659c660e,Practical Bayesian support vector regression for financial time series prediction and market condition change detection,"Support vector regression (SVR) has long been proven to be a successful tool to predict financial time series. The core idea of this study is to outline an automated framework for achieving a faster and easier parameter selection process, and at the same time, generating useful prediction uncertainty estimates in order to effectively tackle flexible real-world financial time series prediction problems. A Bayesian approach to SVR is discussed, and implemented. It is found that the direct implementation of the probabilistic framework of Gao et al. returns unsatisfactory results in our experiments. A novel enhancement is proposed by adding a new kernel scaling parameter μ to overcome the difficulties encountered. In addition, the multi-armed bandit Bayesian optimization technique is applied to automate the parameter selection process. Our framework is then tested on financial time series of various asset classes (i.e. equity index, credit default swaps spread, bond yields, and commodity futures) to ensure its flexibility. It is shown that the generalization performance of this parameter selection process can reach or sometimes surpass the computationally expensive cross-validation procedure. An adaptive calibration process is also described to allow practical use of the prediction uncertainty estimates to assess the quality of predictions. It is shown that the machine-learning approach discussed in this study can be developed as a very useful pricing tool, and potentially a market condition change detector. A further extension is possible by taking the prediction uncertainties into consideration when building a financial portfolio. © 2017 Elsevier B.V., All rights reserved.","Law, T.; Shawe- Taylor, J.",2017,10.1080/14697688.2016.1267868,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014617588&doi=10.1080%2F14697688.2016.1267868&partnerID=40&md5=c7c6bf60d97eacfc0bafb852768ddf4a,scopus,"This study proposes an automated Bayesian Support Vector Regression (SVR) framework for financial time series prediction and market condition change detection. It introduces a novel kernel scaling parameter and uses multi-armed bandit Bayesian optimization for automated parameter selection. The framework is tested on various asset classes and demonstrates competitive generalization performance compared to cross-validation. The prediction uncertainty estimates can be used for adaptive calibration, pricing tools, and market condition detection.",True,False,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T17:09:12.624524
c1bf075f530f6e7a,Predictability and habit persistence,"This paper highlights the role of persistence in explaining predictability of excess returns. To this end, we develop a CCAPM model with habit formation when the growth rate of endowments follows a first order Gaussian autoregression. We provide a closed form solution of the price-dividend ratio and determine conditions that guarantee the existence of a bounded equilibrium. The habit stock model is found to possess internal propagation mechanisms that increase persistence. It outperforms the time separable and a 'Catching up with the Joneses' version of the model in terms of predictability therefore highlighting the role of persistence in explaining the puzzle. (c) 2005 Elsevier B.V. All rights reserved.","Collard, Fabrice; Feve, Patrick; Ghattassi, Imen",2006,10.1016/j.jedc.2005.06.016,,wos,"This paper develops a CCAPM model with habit formation to explain the predictability of excess returns, emphasizing the role of persistence. The model, which incorporates a first-order Gaussian autoregression for endowment growth, yields a closed-form solution for the price-dividend ratio and establishes conditions for a bounded equilibrium. The habit stock model demonstrates internal propagation mechanisms that enhance persistence and outperforms simpler models in predictability, thus underscoring the significance of persistence in addressing this financial puzzle.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:09:15.763817
daa22f7f68680d55,Predictability of HK-REITs returns using artificial neural network,"Purpose: The purpose of this paper is to determine if artificial neural network (ANN) works better than linear regression in predicting Hong Kong real estate investment trusts’ (REITs) excess return. Design/methodology/approach: Both ANN and the regression were applied in this study to forecast the Hong Kong REITs’ (HK-REITs) return using the capital asset pricing model and Fama and French’s three-factor models. Each result was further split into annual time series as a measure to investigate the consistency of the performance across time. Findings: ANN had produced a better forecasting results than the regression based on their trading performance. However, the forecasting performance varied across individual REITs and time periods. Practical implications: ANN should be considered for use when one were to attempt forecasting the HK-REITs excess returns. However, the trading performance should be always compared with buy and hold strategy prior to make any investment decisions. Originality/value: This paper tested the predicting power of ANN on the HK-REITs and the consistency of its predicting power. © 2021 Elsevier B.V., All rights reserved.","Loo, W.K.",2020,10.1108/jpif-07-2019-0090,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075160296&doi=10.1108%2FJPIF-07-2019-0090&partnerID=40&md5=dd4d84e73e04fa2b017a3e756beadbe1,scopus,"This paper investigates the predictability of Hong Kong REITs' excess returns using Artificial Neural Networks (ANNs) and compares their performance against linear regression. Both methods were applied using the capital asset pricing model and Fama-French three-factor models. The study found that ANNs generally produced better forecasting results than regression, although performance varied across individual REITs and time periods. The authors suggest considering ANNs for forecasting HK-REITs returns but advise comparing trading performance with a buy-and-hold strategy before making investment decisions.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:09:21.078548
c167176d7031a0a1,Predicting EU energy industry excess returns on EU market index via a constrained genetic algorithm,"This article introduces an automated procedure to simultaneously select variables and detect outliers in a dynamic linear model using information criteria as objective functions and diagnostic tests as constraints for the distributional properties of errors. A robust scaling method is considered to take into account the sensitiveness of estimates to abnormal data. A genetic algorithm is developed to these purposes. Two examples are presented where models are designed to produce short-term forecasts for the excess returns of the MSCI Europe Energy sector on the MSCI Europe index and a recursive estimation-window is used to shed light on their predictability performances. In the first application the data-set is obtained by a reduction procedure from a very large number of leading macro indicators and financial variables stacked at various lags, while in the second the complete set of 1-month lagged variables is considered. Results show a promising capability to predict excess sector returns through the selection, using the proposed methodology, of most valuable predictors. Reprinted by permission of Springer",,2009,10.1007/s10614-009-9176-4,,proquest,"This article presents an automated procedure for variable selection and outlier detection in dynamic linear models, utilizing information criteria and diagnostic tests. A genetic algorithm is employed for this purpose, with a robust scaling method to handle sensitive estimates. The methodology is applied to forecast excess returns of the MSCI Europe Energy sector on the MSCI Europe index, demonstrating promising predictive capabilities by selecting valuable predictors.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:09:26.195577
a8ce067fd156e398,Predicting Interest Rate Volatility Using Information on the Yield Curve,"This study examines whether information on the yield curve is useful for predicting volatility of the yield curve. The information is used within dynamic models by specifying the covariance matrix of changes in yield factors as nonlinear functions of the factors. Using such models, it is found that the information (i) is useful for predicting volatility of the slope factor, achieving the accuracy comparable with the GARCH model; (ii) has incremental value for predicting volatility of the curvature factor when combined with a volatility-specific factor; and (iii) does not much improve prediction of volatility of the level factor once the volatility-specific factor is introduced. © 2021 Elsevier B.V., All rights reserved.","Takamizawa, H.",2015,10.1111/irfi.12053,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940795801&doi=10.1111%2Firfi.12053&partnerID=40&md5=401dea9b3f3e7e9a0708eba14f080be1,scopus,"This study investigates the predictive power of yield curve information for yield curve volatility. It employs dynamic models where the covariance matrix of yield factor changes is modeled as a nonlinear function of these factors. The findings indicate that yield curve information is effective in predicting the volatility of the slope factor, rivaling GARCH model accuracy. It also offers incremental predictive value for the curvature factor when integrated with a volatility-specific factor, but shows limited improvement for the level factor after accounting for the volatility-specific factor.",True,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T17:09:37.377161
dc45f56e400e0bc4,Predicting Recessions with Leading Indicators: Model Averaging and Selection over the Business Cycle,"Four methods of model selectionequally weighted forecasts, Bayesian model-averaged forecasts, and two models produced by the machine-learning algorithm boostingare applied to the problem of predicting business cycle turning points with a set of common macroeconomic variables. The methods address a fundamental problem faced by forecasters: the most useful model is simple but makes use of all relevant indicators. The results indicate that successful models of recession condition on different economic indicators at different forecast horizons. Predictors that describe real economic activity provide the clearest signal of recession at very short horizons. In contrast, signals from housing and financial markets produce the best forecasts at longer forecast horizons. A real-time forecast experiment explores the predictability of the 2001 and 2007 recessions. Copyright (c) 2015 John Wiley & Sons, Ltd.","Berge, Travis J.",2015,10.1002/for.2345,,wos,"This study compares four methods (equal weighting, Bayesian model averaging, and two boosting models) for predicting business cycle turning points using macroeconomic variables. It finds that different indicators are useful at different forecast horizons, with real activity indicators best for short horizons and housing/financial market indicators best for longer horizons. A real-time experiment tests the models' ability to predict the 2001 and 2007 recessions.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:09:43.360576
a924be2f5f701f16,Predicting Wheat Futures Prices in India,"Futures markets perform their economic roles of price discovery and hedging only when they are efficient. One of the important features of efficient market is that one cannot make abnormal profits from the futures markets by trading in it. This paper addresses the question of whether Indian wheat futures prices can be forecast. This would add to our knowledge whether wheat futures market is efficient, and would enable brokers, traders and speculators to develop profitable trading strategy. We employ the economic variable model to predict the wheat futures prices, and employ out of sample point forecasts. We also evaluate the robustness of our results by employing several alternative specifications, viz. ARMA process and artificial neural network technique. We then test the statistical significance of point forecast using the Diebold and Mariano test. We consider random walk orecast as the bench mark. In order to predict the evolution of wheat futures prices, we use traders' expectations about the futures prices, a number of economic variables and futures prices (lagged) of wheat. The study finds that the futures price of wheat cannot be forecast, and the wheat futures market is efficient.","Kumar, Raushan",2021,10.1007/s10690-020-09320-6,,wos,"This paper investigates the efficiency of the Indian wheat futures market by attempting to forecast wheat futures prices using an economic variable model, ARMA process, and artificial neural networks. The study concludes that wheat futures prices cannot be forecast, indicating an efficient market.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:09:46.928844
b789cc7bea10aacd,Predicting equity premium using news-based economic policy uncertainty: Not all uncertainty changes are equally important,"This study contributes to the growing research that uses the news-based measure of U.S. economic policy uncertainty (EPU) suggested in Baker et al. (2016) to predict economic variables out-of-sample. Using simple predictive regressions à la Goyal and Welch (2008), we evaluate the predictive power afforded by various nonlinear transformations of the U.S. EPU measure suggested in Baker et al. (2016) to predict excess returns on the S&P 500 index one-month ahead. Using data from 1985m1 through 2020m12, we find that not all EPU movements are equally important for obtaining point prediction improvements relative to the historical average benchmark at the population level. Particularly, we document that the one-year net EPU increase, defined as EPU increases beyond the peak over the last year, otherwise zero delivers the most consistent pattern of prediction improvements relative to the benchmark. Conversely, other nonlinear specifications as well as the linear models using log-EPU and the first difference of log-EPU do not deliver the same performance. Overall, this study documents that the predictive impact of the U.S. EPU index suggested in Baker et al. (2016) on equity premium is nonlinear in that EPU increases matter only to the extent that they exceed the maximum value over the last twelve months. In other words, there is evidence of threshold nonlinearity. The statistical predictive power afforded by the one-year net EPU increase also translates into economic gains. © 2021 Elsevier B.V., All rights reserved.","Nonejad, N.",2021,10.1016/j.irfa.2021.101818,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108704036&doi=10.1016%2Fj.irfa.2021.101818&partnerID=40&md5=de440df2664db41659ed758b9ca17144,scopus,"This study investigates the predictive power of the news-based U.S. Economic Policy Uncertainty (EPU) measure for the S&P 500 equity premium. It finds that not all EPU changes are equally important, with a one-year net EPU increase (exceeding the past year's peak) showing the most consistent predictive improvements. The study suggests a threshold nonlinearity in the relationship between EPU and equity premium, indicating that EPU increases only matter when they surpass a recent maximum.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:09:50.863154
101def3e12ba19c0,Predicting failure risk using financial ratios: Quantile hazard model approach,"This study examines the role of financial ratios in predicting companies’ default risk using the quantile hazard model (QHM) approach and compares its results to the discrete hazard model (DHM). We adopt the LASSO method to select essential predictors among the variables mentioned in the literature. We show the preeminence of our proposed QHM through the fact that it presents a different degree of financial ratios’ effect over various quantile levels. While DHM only confirms the aftermaths of “stock return volatilities” and “total liabilities” and the positive effects of “stock price” “stock excess return” and “profitability” on businesses, under high quantile levels QHM is able to supplement “cash and short-term investment to total assets” “market capitalization” and “current liabilities ratio” into the list of factors that influence a default. More interestingly, “cash and short-term investment to total assets” and “market capitalization” switch signs in high quantile levels, showing their different influence on companies with different risk levels. We also discover evidence for the distinction of default probability among different industrial sectors. Lastly, our proposed QHM empirically demonstrates improved out-of-sample forecasting performance. © 2018 Elsevier B.V., All rights reserved.","Dong, M.C.; Tian, S.; Chen, C.W.S.",2018,10.1016/j.najef.2018.01.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042146925&doi=10.1016%2Fj.najef.2018.01.005&partnerID=40&md5=b61603138dc1c51707d88463ab14c1a7,scopus,"This study uses a quantile hazard model (QHM) and LASSO to predict corporate default risk using financial ratios, comparing it to a discrete hazard model (DHM). The QHM reveals varying effects of financial ratios across different risk levels, identifying additional predictors and sign changes for certain ratios at high quantiles. The model also shows improved out-of-sample forecasting performance and highlights differences in default probability across industrial sectors.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:09:54.676128
77577a29281bb651,"Predicting interest rates using shrinkage methods, real‐time diffusion indexes, and model combinations","In the context of predicting the term structure of interest rates, we explore the marginal predictive content of real‐time macroeconomic diffusion indexes extracted from a “data rich” real‐time data set, when used in dynamic Nelson–Siegel (NS) models of the variety discussed in Svensson (NBER technical report, 1994; NSS) and Diebold and Li (Journal of Econometrics, 2006, 130, 337–364; DNS). Our diffusion indexes are constructed using principal component analysis with both targeted and untargeted predictors, with targeting done using the lasso and elastic net. Our findings can be summarized as follows. First, the marginal predictive content of real‐time diffusion indexes is significant for the preponderance of the individual models that we examine. The exception to this finding is the post “Great Recession” period. Second, forecast combinations that include only yield variables result in our most accurate predictions, for most sample periods and maturities. In this case, diffusion indexes do not have marginal predictive content for yields and do not seem to reflect unspanned risks. This points to the continuing usefulness of DNS and NSS models that are purely yield driven. Finally, we find that the use of fully revised macroeconomic data may have an important confounding effect upon results obtained when forecasting yields, as prior research has indicated that diffusion indexes are often useful for predicting yields when constructed using fully revised data, regardless of whether forecast combination is used, or not. Nevertheless, our findings also underscore the potential importance of using machine learning, data reduction, and shrinkage methods in contexts such as term structure modeling.",,2020,10.1002/jae.2768,,proquest,"This study investigates the predictive power of real-time macroeconomic diffusion indexes for interest rates, using dynamic Nelson-Siegel models. While diffusion indexes show some predictive content, forecast combinations of yield variables alone yield the most accurate predictions. The use of fully revised data can confound results, but the study highlights the potential of machine learning and shrinkage methods in term structure modeling.",True,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T17:10:21.633473
c77ed07cec4b8ecf,Predicting recessions using trends in the yield spread,"The yield spread, measured as the difference between long- and short-term interest rates, is widely regarded as one of the strongest predictors of economic recessions. In this paper, we propose an enhanced recession prediction model that incorporates trends in the value of the yield spread. We expect our model to generate stronger recession signals because a steadily declining value of the yield spread typically indicates growing pessimism associated with the reduced future business activity. We capture trends in the yield spread by considering both the level of the yield spread at a lag of 12 months as well as its value at each of the previous two quarters leading up to the forecast origin, and we evaluate its predictive abilities using both logit and artificial neural network models. Our results indicate that models incorporating information from the time series of the yield spread correctly predict future recession periods much better than models only considering the spread value as of the forecast origin. Furthermore, the results are strongest for our artificial neural network model and logistic regression model that includes interaction terms, which we confirm using both a blocked cross-validation technique as well as an expanding estimation window approach.","Kozlowski, Steven E.; Sim, Thaddeus",2019,10.1080/02664763.2018.1537364,,wos,"This paper proposes an enhanced recession prediction model using trends in the yield spread, which is the difference between long- and short-term interest rates. The model incorporates historical yield spread values (12 months and previous two quarters) to capture trends, hypothesizing that a declining spread indicates pessimism about future business activity. The predictive abilities are evaluated using logit and artificial neural network models. Results show that models incorporating time series information of the yield spread predict recessions better than those only considering the current spread value, with the artificial neural network model and logistic regression with interaction terms performing strongest.",True,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T17:11:00.994005
5f36e752a1a66b22,"Prediction and Allocation of Stocks, Bonds, and REITs in the US Market","This study employs dynamic model averaging and selection of Vector Autoregressive and Time-Varying Parameters Vector Autoregressive models to forecast out-of-sample monthly returns of US stocks, bonds, and Real Estate Investment Trusts (REITs) indexes from October 2006 to December 2021. The models were recursively estimated using 17 additional predictors chosen by a genetic algorithm applied to an initial list of 155 predictors. These forecasts were then used to dynamically choose portfolios formed by these assets and the riskless asset proxied by the 3-month US treasury bills. Although we did not find any predictability in the stock market, positive results were obtained for REITs and especially for bonds. The Bayesian-based approaches applied to just the returns of the three risky assets resulted in portfolios that remarkably outperform the portfolios based on the historical means and covariances and the equally weighted portfolio in terms of certainty equivalent return, Sharpe ratio, Sortino ratio and even Conditional Value-at-Risk at 5%. This study points out that Constant Relative Risk Averse investors should use Bayesian-based approaches to forecast and choose the investment portfolios, focusing their attention on different types of assets.",,2025,10.1007/s10614-024-10589-2,,proquest,"This study uses dynamic model averaging and selection of VAR and TVP-VAR models to forecast monthly returns of US stocks, bonds, and REITs from October 2006 to December 2021. It employs 17 predictors selected by a genetic algorithm from an initial list of 155. The forecasts are used to dynamically select portfolios including these assets and riskless assets. While stock market predictability was not found, positive results were observed for REITs and bonds. Bayesian-based approaches for forecasting and portfolio selection outperformed traditional methods in terms of various performance metrics, suggesting their utility for Constant Relative Risk Averse investors.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-13T17:11:11.957546
6c48d26410f11006,Prediction of the implied volatility surface–An empirical analysis of the SSE 50ETF option based on CNNs,"With advancements in artificial intelligence, deep learning techniques have been widely used in predicting financial market volatility. This study forecasts the implied volatility of stock options of the top 50 companies listed on the Shanghai Stock Exchange(SSE) using a convolutional neural network (CNN) with a scaled exponential linear unit activation function and no pooling layer. The CNN model is compared to a back-propagation (BP) neural network to evaluate predictive performance. Results show that the CNN model shows superior performance in predicting implied volatility compared to the BP neural network, accurately fitting data patterns as well as smile and term structures. © 2025 Elsevier B.V., All rights reserved.","Shao, H.; Zhou, B.; Gong, S.",2025,10.1016/j.frl.2025.107119,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219544528&doi=10.1016%2Fj.frl.2025.107119&partnerID=40&md5=55f23bdf3a42f4f7eba6a6d771cac644,scopus,"This study uses a convolutional neural network (CNN) to predict the implied volatility surface of SSE 50ETF options, comparing its performance against a back-propagation (BP) neural network. The CNN model demonstrated superior accuracy in fitting data patterns and capturing smile and term structures.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:11:14.197618
e9d4cb3cc9f23a8f,Prediction-Based Portfolio Optimization Models Using Deep Neural Networks,"Portfolio optimization is a hot research topic, which has attracted many researchers in recent decades. Better portfolio optimization model can help investors earn more stable profits. This paper uses three deep neural networks (DNNs), i.e., deep multilayer perceptron (DMLP), long short memory (LSTM) neural network and convolutional neural network (CNN) to build prediction-based portfolio optimization models which own the advantages of both deep learning technology and modern portfolio theory. These models first use DNNs to predict each stock's future return. Then, predictive errors of DNNs are applied to measure the risk of each stock. Next, the portfolio optimization models are built by integrating the predictive returns and semi-absolute deviation of predictive errors. These models are compared with three equal weighted portfolios, where their stocks are selected by DMLP, LSTM neural network and CNN respectively. Also, two prediction-based portfolio models built with support vector regression are used as benchmarks. This paper applies component stocks of China securities 100 index in Chinese stock market as experimental data. Experimental results present that the prediction-based portfolio model based on DMLP performs the best among these models under different desired portfolio returns, and high desired portfolio return can further improve the performance of this model. This paper presents the promising performance of DNNs in building prediction-based portfolio models.",Y. Ma; R. Han; W. Wang,2020,10.1109/access.2020.3003819,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9121212,ieeexplore,"This paper proposes portfolio optimization models using Deep Neural Networks (DNNs) including DMLP, LSTM, and CNN. These models predict stock returns and use predictive errors to measure risk, integrating these with semi-absolute deviation for optimization. The models are compared against equal-weighted portfolios and support vector regression benchmarks using data from the China Securities 100 index. The DMLP-based model showed the best performance, especially with high desired returns, highlighting DNNs' potential in portfolio modeling.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:11:17.843810
7e7371ae9f087279,Pricing Climate Change Exposure,"We estimate the risk premium for firm-level climate change exposure among S&P 500 stocks and its time-series evolution between 2005 to 2020. Exposure reflects the attention paid by market participants in earnings calls to a firm's climate-related risks and opportunities. When extracted from realized returns, the unconditional risk premium is insignificant but exhibits a period with a positive risk premium before the financial crisis and a steady increase thereafter. Forward-looking expected return proxies deliver an unconditionally positive risk premium with maximum values of 0.5%–1% p.a., depending on the proxy, between 2011 and 2014. The risk premium has been lower since 2015, especially when the expected return proxy explicitly accounts for the higher opportunities and lower crash risks that characterize high-exposure stocks. This finding arises as the priced part of the risk premium primarily originates from uncertainty about climate-related upside opportunities. In the time series, the risk premium is negatively associated with green innovation; Big Three holdings; and environmental, social, and governance fund flows and positively associated with climate change adaptation programs.",,2023,10.1287/mnsc.2023.4686,,proquest,"This study estimates the risk premium associated with firm-level climate change exposure in S&P 500 stocks from 2005-2020. The risk premium, derived from market participants' attention to climate risks and opportunities in earnings calls, shows varying patterns over time. While unconditional premiums are often insignificant, forward-looking proxies reveal a positive premium between 2011-2014. The premium is influenced by factors like green innovation, fund flows, and adaptation programs, with uncertainty about climate-related upside opportunities being a key driver.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:11:21.930403
72f324761d5c5db3,Pricing real options based on linear loss functions and conditional value at risk,"The main purpose of this paper is to expand real option analysis out of the realm of pure financial option pricing techniques. To overcome many of the well-known concerns by adopting the financial option pricing techniques for modeling real options problems such as replicating portfolio concept, geometric Brownian motion as underlying stochastic process, and estimating project volatility, we propose an alternative real option valuation based on the loss function approach. The option value determined by the loss function approach is equivalent to the expected value of perfect information (EVPI) in decision analysis. It basically sets the upper bound of risk premium to pay in retaining the options. In practice, many firms utilize the concept of Value at Risk to manage their portfolio risk. If a firm sets a target VAR, then we may be able to link this VAR in refining the actual risk premium to pay in hedging the risk embedded in the investment. With this practice in mind, we present a logic to figure out an appropriate amount of real option premium to pay for a given level of risk tolerance. A comprehensive example is presented to demonstrate the computational procedures as well as economic interpretations on the outcomes.","Kim, Kyongsun; Park, Chan S.",2020,10.1080/0013791x.2020.1867273,,wos,"This paper proposes an alternative method for valuing real options by using a loss function approach, which is equivalent to the expected value of perfect information. It aims to overcome limitations of traditional financial option pricing techniques for real options. The approach incorporates Value at Risk (VaR) to determine an appropriate risk premium for hedging investment risks, with a comprehensive example provided.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:11:23.644943
b5f453c341dd0e18,Probabilistic forecasts of volatility and its risk premia,"The object of this paper is to produce distributional forecasts of asset price volatility and its associated risk premia using a non-linear state space approach. Option and spot market information on the latent variance process is captured by using dual 'model-free' variance measures to define a bivariate observation equation in the state space model. The premium for variance diffusive risk is defined as linear in the latent variance (in the usual fashion) whilst the premium for variance jump risk is specified as a conditionally deterministic dynamic process, driven by a function of past measurements. The inferential approach adopted is Bayesian, implemented via a Markov chain Monte Carlo algorithm that caters for the multiple sources of non-linearity in the model and for the bivariate measure. The method is applied to spot and option price data on the S&P500 index from 1999 to 2008, with conclusions drawn about investors' required compensation for variance risk during the recent financial turmoil. The accuracy of the probabilistic forecasts of the observable variance measures is demonstrated, and compared with that of forecasts yielded by alternative methods. To illustrate the benefits of the approach, it is used to produce forecasts of prices of derivatives on volatility itself. In addition, the posterior distribution is augmented by information on daily returns to produce value at risk predictions. Linking the variance risk premia to the risk aversion parameter in a representative agent model, probabilistic forecasts of (approximate) relative risk aversion are also produced. All rights reserved, Elsevier",,2012,10.1016/j.jeconom.2012.06.006,,proquest,"This paper uses a non-linear state space approach to generate distributional forecasts of asset price volatility and its risk premia. It incorporates option and spot market information through a bivariate observation equation and models variance risk premia. The methodology is Bayesian, implemented with Markov chain Monte Carlo, and applied to S&P500 data from 1999-2008 to analyze variance risk compensation during financial turmoil. The study demonstrates the accuracy of its probabilistic forecasts, compares them to alternative methods, and uses the approach to forecast prices of volatility derivatives and produce value at risk predictions. It also forecasts relative risk aversion by linking variance risk premia to a representative agent model.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:11:25.427600
1f915ca874b0d1e8,Quadratic stochastic intensity and prospective mortality tables,"We consider a quadratic stochastic intensity model with a Gaussian autoregressive factor, derive explicit formulas for predictive mortality tables and recursive updating formulas are also provided. We also explain how to use appropriately the Kalman filter to estimate the parameters of the model and to approximate the values of the underlying factor. This methodology is applied to French human mortality tables. (C) 2008 Elsevier B.V. Ail rights reserved.","Gourieroux, C.; Monfort, A.",2008,10.1016/j.insmatheco.2008.05.010,,wos,"This paper presents a quadratic stochastic intensity model with a Gaussian autoregressive factor, offering explicit formulas for predictive mortality tables and recursive updating. It also details the use of the Kalman filter for parameter estimation and factor approximation, with an application to French human mortality data.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:11:30.258204
c2f42ac2b768296a,Quantitative assessment of common practice procedures in the fair evaluation of embedded options in insurance contracts,"This work analyses the common industry practice used to evaluate financial options written on with profit policies issued by European insurance companies. In the last years regulators introduced, with the Solvency II directive, a market consistent valuation framework for determining the fair value of asset and liabilities of insurance funds. A relevant aspect is how to deal with the estimation of sovereign credit and liquidity risk, that are important components in the valuation of the majority of insurance funds, which are usually heavily invested in treasury bonds. The common practice is the adoption of the certainty equivalent approach (CEQ) for the risk neutral evaluation of insurance liabilities, which results in a deterministic risk adjustment of the securities cash flows. In this paper, we propose an arbitrage free stochastic model for interest rate, credit and liquidity risks, that takes into account the dependences between different government bond issuers. We test the impact of the common practice against our proposed model, via Monte Carlo simulations. We conclude that in the estimation of options whose pay-off is determined by statutory accounting rules, which is often the case for European traditional with-profit insurance products, the deterministic adjustment for risk of the securities cash flows is not appropriate, and that a more complete model such as the one described in this article is a viable and sensible alternative in the context of market consistent evaluations. (C) 2017 Elsevier B.V. All rights reserved.","Gambaro, Anna Maria; Casalini, Riccardo; Fusai, Gianluca; Ghilarducci, Alessandro",2018,10.1016/j.insmatheco.2017.10.005,,wos,"This paper analyzes the common industry practice for evaluating financial options in European insurance contracts, specifically the certainty equivalent approach (CEQ) for risk-neutral valuation. It proposes an arbitrage-free stochastic model for interest rate, credit, and liquidity risks, considering inter-issuer dependencies. The study concludes that the CEQ approach is inappropriate for options based on statutory accounting rules, common in European with-profit insurance products, and advocates for more comprehensive models like the one presented for market-consistent evaluations.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:11:35.596364
457adc903852506e,Quantitative law describing market dynamics before and after interest-rate change,"We study the behavior of U.S. markets both before and after U.S. Federal Open Market Commission meetings and show that the announcement of a U.S. Federal Reserve rate change causes a financial shock, where the dynamics after the announcement is described by an analog of the Omori earthquake law. We quantify the rate n(t) of aftershocks following an interest-rate change at time T and find power-law decay which scales as n(t-T) similar to (t-T)(-Omega), with Omega positive. Surprisingly, we find that the same law describes the rate n'(vertical bar t-T vertical bar) of preshocks before the interest-rate change at time T. This study quantitatively relates the size of the market response to the news which caused the shock and uncovers the presence of quantifiable preshocks. We demonstrate that the news associated with interest-rate change is responsible for causing both the anticipation before the announcement and the surprise after the announcement. We estimate the magnitude of financial news using the relative difference between the U.S. Treasury Bill and the Federal Funds effective rate. Our results are consistent with the sign effect, in which bad news has a larger impact than good news. Furthermore, we observe significant volatility aftershocks, confirming a market under-reaction that lasts at least one trading day.","Petersen, Alexander M.; Wang, Fengzhong; Havlin, Shlomo; Stanley, H. Eugene",2010,10.1103/physreve.81.066121,,wos,"This study investigates U.S. market dynamics before and after Federal Reserve interest-rate changes, revealing that rate changes act as financial shocks. The authors found that both aftershocks and preshocks follow a power-law decay, similar to the Omori earthquake law. They quantified the magnitude of financial news and observed that bad news has a larger impact than good news, with significant volatility aftershocks indicating market under-reaction.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:11:37.028194
002bc07ecf503c9e,Radial Basis Functions With Adaptive Input and Composite Trend Representation for Portfolio Selection,"We propose a set of novel radial basis functions with adaptive input and composite trend representation (AICTR) for portfolio selection (PS). Trend representation of asset price is one of the main information to be exploited in PS. However, most state-of-the-art trend representation-based systems exploit only one kind of trend information and lack effective mechanisms to construct a composite trend representation. The proposed system exploits a set of RBFs with multiple trend representations, which improves the effectiveness and robustness in price prediction. Moreover, the input of the RBFs automatically switches to the best trend representation according to the recent investing performance of different price predictions. We also propose a novel objective to combine these RBFs and select the portfolio. Extensive experiments on six benchmark data sets (including a new challenging data set that we propose) from different real-world stock markets indicate that the proposed RBFs effectively combine different trend representations and AICTR achieves state-of-the-art investing performance and risk control. Besides, AICTR withstands the reasonable transaction costs and runs fast; hence, it is applicable to real-world financial environments.",Z. -R. Lai; D. -Q. Dai; C. -X. Ren; K. -K. Huang,2018,10.1109/tnnls.2018.2827952,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8356708,ieeexplore,"This paper introduces novel adaptive input and composite trend representation (AICTR) radial basis functions (RBFs) for portfolio selection. The system improves price prediction by combining multiple trend representations and adaptively switching inputs based on performance. Experiments show AICTR achieves state-of-the-art performance and risk control, is robust to transaction costs, and is computationally efficient for real-world applications.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:11:42.597345
b042472b0d9b86fb,Re-examination of the predictability of economic activity using the yield spread: A nonlinear approach,"This paper examines the feasibility of using the term structure of nominal interest rates in empirical predictive relationships with future real activity growth serving as the dependent variable. In particular, we will focus on the strength and stability of the spread-output relationship. We employ smooth transition nonlinear models that can accommodate (a) regime switching type nonlinear behaviour and (b) time-varying parameters. We verify that the link exhibits strong threshold effects with respect to near past spread values implying that the relation is sufficiently strong in economic terms if past spread values did not exceed a positive threshold value. Furthermore, we are able to explicitly model time-variation in the preceding effects reaching the conclusion that the importance of the spread as an output predictor has been significantly diminished if not eradicated during recent years. The timing of the change in the information content of the spread appears to be related to a turn in certain monetary policy practices, in particular, the turn towards stronger inflation targeting practices. © 2002 Elsevier Science Inc. All rights reserved. © 2004 Elsevier Science B.V., Amsterdam. All rights reserved.","Venetis, I.A.; Paya, I.; Peel, D.A.",2003,10.1016/s1059-0560(02)00147-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037278128&doi=10.1016%2FS1059-0560%2802%2900147-8&partnerID=40&md5=4669263ed695d853f5f4317c4773535b,scopus,"This paper investigates the predictability of economic activity using the yield spread, employing nonlinear models to account for regime switching and time-varying parameters. It finds that the spread's predictive power is strong only when past spread values do not exceed a positive threshold. The study also notes a significant decline in the spread's importance as an output predictor in recent years, potentially linked to changes in monetary policy and inflation targeting.",True,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:12:20.880564
eae365d7b19c2a45,Real economic activity leading indicators: should we have paid more attention?,"The ability to predict business cycle activity is an invaluable skill for governments and policy makers alike, especially before an economy enters a downturn. We analyse causality relationships between key leading economic indicators and economic growth for three countries from 1970 to 2010. We find that while many indicators do not help explain current movements in GDP growth, lags of these indicators do. In addition, the direction of the change and the size of the change in the lagged economic indicators are very important in many cases. This is particularly true for housing indicators.","Ryan, Geraldine; Shinnick, Edward",2011,10.1080/17487870.2011.577645,,wos,"This study investigates the predictive power of leading economic indicators for GDP growth in three countries between 1970 and 2010. It finds that while current indicator values are not always predictive, lagged values, particularly changes in housing indicators, are significant predictors of economic growth.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:12:25.796292
7098321857875dd1,Real estate climate index and aggregate stock returns: Evidence from China,"We show that China's real estate climate index (RECI) can be used to forecast the aggregate stock market return. It outperforms popular return predictors both in- and out-of-sample, especially at the monthly horizon. Additionally, RECI's predictive ability is stronger among stocks of small market capitalization and low momentum. For a typical mean-variance investor, RECI's predictive power may provide an additional utility gain of 3.41%. We discuss three potential sources of RECI's predictive ability and present the corresponding evidence, including the cash flow channel, the firm fundamental channel, and the investment substitution channel.","Jiang, Yuexiang; Fu, Tao; Long, Huaigang; Zaremba, Adam; Zhou, Wenyu",2022,10.1016/j.pacfin.2022.101841,,wos,"This study demonstrates that China's Real Estate Climate Index (RECI) can predict aggregate stock market returns, outperforming existing predictors, particularly at the monthly horizon. The predictive power is more pronounced for small-cap and low-momentum stocks. The research explores three potential mechanisms behind this predictability: the cash flow channel, the firm fundamental channel, and the investment substitution channel.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:12:27.324939
cc99eea515945c08,Recent empirical evidence on the impact of the primary budget deficit on nominal longer term treasury note interest rate yields,"This study empirically investigates the impact of the federal budget deficit on the nominal interest rate yields on seven and ten year treasury notes over the 1992-2003 period. To measure the budget deficit, the primary budget deficit, which excludes net interest payments by the treasury, is adopted. In a loanable funds model that includes the monetary base, expected inflation, an ex ante real short term interest rate yield and an ex ante real intermediate term interest rate yield, the percentage growth rate of real GDP, and the percentage growth rate of the S&P 500 stock index, instrumental variables estimations using quarterly data reveal that the primary deficit has raised the nominal interest rate yields on both seven year and ten year treasury notes over the study period, raising serious concerns about the currently surging national debt.",,2005,10.1504/gber.2005.006919,,proquest,"This empirical study examines the effect of the primary budget deficit on nominal interest rates of 7- and 10-year Treasury notes from 1992-2003. Using a loanable funds model and instrumental variables estimation, the research finds that the primary deficit increased these interest rates, raising concerns about the national debt.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:12:46.562917
4ab5048307f07e1c,Recovering Yield Curves from Dynamic Term Structure Models with Time-Varying Factors,"A dynamic version of the Nelson-Siegel-Svensson term structure model with time-varying factors is considered for predicting out-of-sample maturity yields. Simple linear interpolation cannot be applied to recover yields at the very short- and long- end of the term structure where data are often missing. This motivates the use of dynamic parametric term structure models that exploit both time series and cross-sectional variation in yield data to predict missing data at the extreme ends of the term structure. Although the dynamic Nelson–Siegel–Svensson model is weakly identified when the two decay factors become close to each other, their predictions may be more accurate than those from more restricted models depending on data and maturity. © 2024 Elsevier B.V., All rights reserved.","Kawakatsu, H.",2020,10.3390/stats3030020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138041887&doi=10.3390%2Fstats3030020&partnerID=40&md5=745ede452bb52fd755252d659d8e40c8,scopus,"This paper explores a dynamic Nelson-Siegel-Svensson term structure model with time-varying factors to predict bond yields, particularly addressing the challenge of missing data at the short and long ends of the yield curve. It suggests that while the model has identification issues, its predictions can be more accurate than simpler models in certain scenarios.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-13T17:12:57.430263
0a7b16e8ef5ab3d7,Recovering default risk from CDS spreads with a nonlinear filter,"We propose a nonlinear filter to estimate the time-varying default risk from the term structure of credit default swap (CDS) spreads. Based on the numerical solution of the Fokker-Planck equation (FPE) using a meshfree interpolation method, the filter performs a joint estimation of the risk-neutral default intensity and CIR model parameters. As the FPE can account for nonlinear functions and non-Gaussian errors, the proposed framework provides outstanding flexibility and accuracy. We test the nonlinear filter on simulated spreads and apply it to daily CDS data of the Dow Jones Industrial Average component companies from 2005 to 2010 with supportive results. © 2013 Elsevier B.V. © 2013 Elsevier B.V., All rights reserved.","Guarín, A.; Liu, X.; Ng, W.L.",2014,10.1016/j.jedc.2013.09.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890117707&doi=10.1016%2Fj.jedc.2013.09.006&partnerID=40&md5=abd3e9e9a830f35514ed25356804906c,scopus,"This paper introduces a nonlinear filter to estimate time-varying default risk from credit default swap (CDS) spreads. It uses a meshfree interpolation method to solve the Fokker-Planck equation, jointly estimating default intensity and CIR model parameters. The method is tested on simulated data and applied to CDS data of Dow Jones Industrial Average companies from 2005-2010.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:13:00.126810
187d1db1c854c026,Recovering the probability density function of asset prices using garch as diffusion approximations,"This paper uses Garch models to estimate the objective and risk-neutral density functions of financial asset prices and by comparing their shapes, recover detailed information on economic agents' attitudes toward risk. It differs from recent papers investigating analogous issues because it uses Nelson's result that Garch schemes are approximations of the kind of differential equations typically employed in finance to describe the evolution of asset prices. This feature of Garch schemes usually has been overshadowed by their well-known role as simple econometric tools providing reliable estimates of unobserved conditional variances. We show instead that the diffusion approximation property of Garch gives good results and can be extended to situations with (i) non-standard distributions for the innovations of a conditional mean equation of asset price changes and (ii) volatility concepts different from the variance. The objective PDF of the asset price is recovered from the estimation of a nonlinear Garch fitted to the historical path of the asset price. The risk-neutral PDF is extracted from cross-sections of bond option prices, after introducing a volatility risk premium function. The direct comparison of the shapes of the two PDF<inf>s</inf> reveals the price attached by economic agents to the different states of nature. Applications are carried out with regard to the futures written on the Italian 10-year bond. © 2001 Elsevier Science B.V. © 2005 Elsevier Science B.V., Amsterdam. All rights reserved.","Fornari, F.; Mele, A.",2001,10.1016/s0927-5398(01)00021-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0041830388&doi=10.1016%2FS0927-5398%2801%2900021-4&partnerID=40&md5=19f6c9e38044f3279ef58d4ab148d647,scopus,"This paper estimates objective and risk-neutral probability density functions (PDFs) of financial asset prices using Garch models as diffusion approximations. By comparing the shapes of these PDFs, the study extracts information about economic agents' risk attitudes. The methodology leverages Nelson's result that Garch schemes approximate differential equations used in finance for asset price evolution. The research extends this to non-standard innovation distributions and alternative volatility concepts. The objective PDF is recovered from historical asset price data using a nonlinear Garch model, while the risk-neutral PDF is derived from bond option prices with a volatility risk premium function. The comparison of these PDFs reveals the market's valuation of different states of nature. An application is demonstrated using futures on the Italian 10-year bond.",True,True,False,gemini-2.5-flash-lite,Trine,M,,2025-10-13T17:13:30.134501
c47d7837392e4b40,Regime-specific exchange rate predictability,"We study temporary phases of exchange rate predictability in a two-regime threshold predictive regression framework allowing for persistent predictors. Regime switches are triggered by an observable transition variable which relates to media news, expectations, uncertainty and global financial conditions. As predictors for G7 currencies and effective US-Dollar exchange rates, we study various interest rate spreads, yield curve factors, uncertainty measures and deviations from fundamental exchange rate parities. Besides established uncertainty measures, we use a wide range of measures for media coverage and construct uncertainty measures from survey data as transition variables for the activation of the predictability regime. Our results emphasize that short recurring phases of significant predictability are characterized by nonlinear patterns. Phases of predictability are triggered by increased media coverage and high uncertainty with interest rate dynamics emerging as the most important predictor. We find broadly similar results for a contemporaneous threshold analysis where our regressors are allowed to affect the exchange rate in the same period. From a theoretical point of view, we argue that our empirical results are useful for the empirical identification of scapegoat effects and that media coverage and uncertainty affect the exchange rate via the heterogeneity of private signals and the precision of public signals. © 2025 Elsevier B.V., All rights reserved.","Beckmann, J.; Kerkemeier, M.; Kruse, R.",2025,10.1016/j.jedc.2025.105095,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003202194&doi=10.1016%2Fj.jedc.2025.105095&partnerID=40&md5=1f794f64bc449a52a369b9e0c25980fb,scopus,"This study investigates temporary periods of exchange rate predictability using a two-regime threshold predictive regression model. Regime switches are influenced by observable variables like media news, expectations, uncertainty, and global financial conditions. The research examines various interest rate spreads, yield curve factors, uncertainty measures, and deviations from fundamental exchange rate parities as predictors for G7 currencies and effective US-Dollar exchange rates. Findings indicate that short, recurring phases of significant predictability are linked to nonlinear patterns, triggered by increased media coverage and high uncertainty, with interest rate dynamics being the most crucial predictor. The results are consistent with a contemporaneous threshold analysis and offer theoretical insights into scapegoat effects, media coverage, and uncertainty influencing exchange rates through private and public signal heterogeneity and precision.",True,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:14:02.545655
116127da35f3f613,Removing Bias in Estimating Financial Contagion: An Empirical Analysis Based on European Economies,"The degree of contagion is frequently measured by the size and significance of linear correlation coefficients. In this paper, we show that such linear measures are inappropriate for three reasons: contagion is likely to be nonlinear, the structural contagion model is unknown, and the contagion itself will be time-varying. Instead, we use a time-varying coefficient method to give a time-varying, unbiased measure of bilateral contagion between two countries, which shows that simple correlation measures over-estimate the average contagion from the source country and how the degree of contagion varies over the sample period. To illustrate, we use Greece as an exemplar source country and Belgium, France, Italy, Ireland, Netherlands, Portugal, and Spain as recipient countries over the period 2009 to 2022.",,2025,10.1007/s11079-024-09788-z,,proquest,"This paper proposes a time-varying coefficient method to measure bilateral financial contagion between countries, arguing that linear correlation coefficients are inappropriate due to nonlinearity, unknown structural models, and time-varying contagion. The method is illustrated using Greece as a source country and several European countries as recipients from 2009-2022, showing that simple correlation measures overestimate contagion.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:14:11.413391
b46293a022ea475b,Research on characterization and prediction of bond risk factors based on machine learning: evidence from the China,"Introduction: The scale of default on credit bonds in China has been expanding. Credit bond defaults not only increase the financing costs of enterprises but also affect the efficiency of debt issuance and even lead to the spread of risks in the financial market. Accurately identifying bond default risks, clarifying the characteristics of bond defaults, and understanding the default risk mechanism are of crucial importance. Methods: This paper takes corporate credit bonds as the research object and analyzes bond defaults from both macro and micro perspectives. From a macro perspective, it confirms the logical transmission between macro factors and bond defaults through causal relationships and grasps the overall characteristics of bond defaults by combining association rule mining and descriptive statistical research methods. Bonds are divided into a risk-free bond group and a risky bond group, and association rules are mined in four dimensions: the bond issuance region of the enterprise, whether the issuer is listed, the attributes of the issuing enterprise, and whether the enterprise bond is guaranteed. Based on these rules, a cross-analysis of bond risk factors is conducted. From a micro perspective, taking each bond as the research object, a bond default identification system is established, and default predictions are made based on the ensemble learning algorithm. The important characteristics of default bonds are analyzed from the perspective of whether the issuer is a state-owned enterprise, and further cause difference analysis is conducted. Results: The results show that M1 and M2 have an impact on bond defaults, and the ensemble machine learning algorithm can accurately predict bond default risks and obtain key factors for bond risk identification. It is reasonable to choose macro indicators to predict bond defaults. Discussion: Based on the experimental conclusions, this paper discusses and analyzes the bond risk evolution process and the reasons for risk concentration in certain industries, which is helpful for a comprehensive understanding of bond default risks. Our research can provide tool references and guidance for risk management in the actual bond market. © 2025 Elsevier B.V., All rights reserved.","Zhang, Y.; Cui, W.",2025,10.3389/fphy.2025.1559283,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002254323&doi=10.3389%2Ffphy.2025.1559283&partnerID=40&md5=0a0beaee8fb234ad9a3519e69b26697d,scopus,"This study investigates bond default risk in China using machine learning. It analyzes defaults from macro and micro perspectives, employing association rule mining and ensemble learning for prediction. The research identifies key factors influencing defaults and discusses risk evolution, offering insights for bond market risk management.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:14:23.862587
b8d5dbfcb1824da4,Research on quantitative investment strategies based on deep learning,"This paper takes 50 ETF options in the options market with high transaction complexity as the research goal. The Random Forest (RF) model, the Long Short-Term Memory network (LSTM) model, and the Support Vector Regression (SVR) model are used to predict 50 ETF price. Firstly, the original quantitative investment strategy is taken as the research object, and the 15 min trading frequency, which is more in line with the actual trading situation, is used, and then the Delta hedging concept of the options is introduced to control the risk of the quantitative investment strategy, to achieve the 15 min hedging strategy. Secondly, the final transaction price, buy price, highest price, lowest price, volume, historical volatility, and the implied volatility of the time segment marked with 50 ETF are the seven key factors affecting the price of 50 ETF. Then, two different types of LSTM-SVR models, LSTM-SVR I and LSTM-SVR II, are used to predict the final transaction price of the 50 ETF in the next time segment. In LSTM-SVR I model, the output of LSTM and seven key factors are combined as the input of SVR model. In LSTM-SVR II model, the hidden state vectors of LSTM and seven key factors are combined as the inputs of the SVR model. The results of the two LSTM-SVR models are compared with each other, and the better one is applied to the trading strategy. Finally, the benefit of the deep learning-based quantitative investment strategy, the resilience, and the maximum drawdown are used as indicators to judge the pros and cons of the research results. The accuracy and deviations of the LSTM-SVR prediction models are compared with those of the LSTM model and those of the RF model. The experimental results show that the quantitative investment strategy based on deep learning has higher returns than the traditional quantitative investment strategy, the yield curve is more stable, and the anti-fall performance is better. © 2019 Elsevier B.V., All rights reserved.","Fang, Y.; Chen, J.; Xue, Z.",2019,10.3390/a12020035,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061984447&doi=10.3390%2Fa12020035&partnerID=40&md5=72e2e75bf78b20e8f6b2ebc3a1549289,scopus,"This paper investigates quantitative investment strategies for 50 ETF options using deep learning models, specifically Random Forest (RF), Long Short-Term Memory (LSTM), and Support Vector Regression (SVR). It introduces a 15-minute trading frequency and Delta hedging for risk control. Two LSTM-SVR models are developed to predict ETF prices, with the better performing model applied to the trading strategy. The strategy's performance is evaluated based on returns, stability, and maximum drawdown, showing superior results compared to traditional strategies.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:14:28.097633
f3768e92a7ac96fb,"Research on the Risks of Financial Informatization Construction in Colleges and Universities, Its Prevention and Control and Path Optimization","With the rapid growth of the demand for college and university funding, the financial management of colleges and universities from the traditional meaning of the risk-free state to the risky mode of change, for the college and university financial risk of accurate and reasonable prevention and control has become an important issue that needs to be resolved at this stage. This paper selects a university finance as the research object, designs 12 financial indicators as sample data, processes the financial risk indicator system through principal component analysis, and obtains 8 principal factor components as the input data of the risk prediction model. Then the particle swarm algorithm is combined with BP neural network to overcome the defects of BP neural network as a way to strengthen its prediction accuracy of financial risk. Through simulation experiments, comparative analysis of prediction rates of different models, it is found that the PSO-BP prediction model achieves an identification accuracy of 91.7% for 60 test samples, which improves the identification correctness by 23.7% and 8.4% compared with the traditional BP model and GA-BP model, respectively. It confirms that the PSO-BP neural network model has a higher prediction rate and is effective in introducing university finance for risk prediction. Finally, the article proposes an optimization strategy for the path of university finance information technology construction, in order to improve the effectiveness of university finance information technology construction.",,2025,10.2478/amns-2025-1039,,proquest,"This study investigates financial risks in university funding, proposing a prediction model using Principal Component Analysis and a Particle Swarm Optimization-optimized BP neural network (PSO-BP). The PSO-BP model achieved 91.7% accuracy in identifying financial risks, outperforming traditional BP and GA-BP models. The research also suggests an optimization strategy for university financial IT construction.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:14:33.626049
bdf7a90ab5ce3943,Restrictions on Risk Prices in Dynamic Term Structure Models,"Restrictions on the risk-pricing in dynamic term structure models (DTSMs) tighten the link between cross-sectional and time-series variation of interest rates, and make absence of arbitrage useful for inference about expectations. This article presents a new econometric framework for estimation of affine Gaussian DTSMs under restrictions on risk prices, which addresses the issues of a large model space and of model uncertainty using a Bayesian approach. A simulation study demonstrates the good performance of the proposed method. Data for U.S.Treasury yields calls for tight restrictions on risk pricing: only level risk is priced, and only changes in the slope affect term premia. Incorporating the restrictions changes the model-implied short-rate expectations and term premia. Interest rate persistence is higher than in a maximally flexible model, hence expectations of future short rates are more variablerestrictions on risk prices help resolve the puzzle of implausibly stable short-rate expectations in this literature. Consistent with survey evidence and conventional macro wisdom, restricted models attribute a large share of the secular decline in long-term interest rates to expectations of future nominal short rates. Supplementary materials for this article are available online.","Bauer, Michael D.",2018,10.1080/07350015.2016.1164707,,wos,"This article proposes a Bayesian econometric framework for estimating affine Gaussian dynamic term structure models (DTSMs) with restrictions on risk prices. The method addresses challenges related to a large model space and model uncertainty. Empirical analysis of U.S. Treasury yields suggests that only level risk is priced, and term premia are influenced by changes in the slope. These restrictions lead to higher interest rate persistence and more variable short-rate expectations compared to more flexible models, helping to resolve issues of implausibly stable short-rate expectations in existing literature. The restricted models attribute a significant portion of the long-term decline in interest rates to expectations of future nominal short rates, aligning with survey data and conventional macroeconomic understanding.",False,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T17:15:12.792976
ceb068473580003b,Retail Demand Forecasting Using Spatial-Temporal Gradient Boosting Methods,"With the significant growth of the e-commerce business, the retail industry is experiencing rapid developments, leading to the explosion of the number of stock-keeping units (SKUs). Therefore, it calls for forecasting algorithms to forecast a large number of product-level demands over a short forecasting horizon. We developed a novel machine learning algorithm—the spatial-temporal gradient boosting tree (ST-GBT)—for demand forecasting for the retail industry. By incorporating the cross-section and time-series information in the existing gradient-boosting decision tree algorithm, our new algorithm can accurately forecast tremendous SKUs in one process. Furthermore, we show potential factors related to the retail industry, while new factors, such as higher-order statistics and risk-free interest, are also proposed for demand forecasting tasks. The numerical experiment results based on a large e-commerce company’s historical transaction records support the comparative merits of the new algorithm with superior accuracy and automation ability. © 2024 Elsevier B.V., All rights reserved.","Wang, J.; Chong, W.K.; Lin, J.; Hedenstierna, C.P.T.",2024,10.1080/08874417.2023.2240753,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166741941&doi=10.1080%2F08874417.2023.2240753&partnerID=40&md5=23943f325a6dac965bd1f19dd163e2b1,scopus,"This paper introduces a novel spatial-temporal gradient boosting tree (ST-GBT) algorithm for retail demand forecasting, designed to handle a large number of SKUs and short forecasting horizons. The algorithm integrates cross-sectional and time-series data, proposing new factors like higher-order statistics and risk-free interest. Experiments on e-commerce data demonstrate its superior accuracy and automation compared to existing methods.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:15:26.129943
7dd667d475419652,Retrospectives: Irving fisher's Appreciation and Interest (1896) and the fisher relation,"Irving Fisher's monograph Appreciation and Interest (1896) proposed his famous equation showing expected inflation as the difference between nominal interest and real interest rates. In addition, he drew attention to insightful remarks and numerical examples scattered through the earlier literature, and he derived results ranging from the uncovered interest arbitrage parity condition between currencies to the expectations theory of the term structure of interest rates. As J. Bradford DeLong wrote in this journal (Winter 2000), ""The story of 20th century macroeconomics begins with Irving Fisher"" and specifically with Appreciation and Interest because ""the transformation of the quantity theory of money into a tool for making quantitative analyses and predictions of the price level, inflation, and interest rates was the creation of Irving Fisher."" I discuss the message of Appreciation and Interest, and assess how original he was. © 2013 Elsevier B.V., All rights reserved.","Dimand, R.W.; Gomez Betancourt, R.G.",2012,10.1257/jep.26.4.185,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873335341&doi=10.1257%2Fjep.26.4.185&partnerID=40&md5=d258eb0252514708325ae1e7caafec1c,scopus,"This article discusses Irving Fisher's 1896 monograph ""Appreciation and Interest,"" focusing on his famous equation relating nominal interest, real interest, and expected inflation. It also examines Fisher's contributions to other areas like interest arbitrage parity and the expectations theory of the term structure, highlighting his foundational role in 20th-century macroeconomics and his transformation of the quantity theory of money into a predictive tool.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:15:27.746622
1283bdc00da780ff,Re‐Investigating the UIP Hypothesis: Recent Evidence From BRICS Economies,"The study re‐investigates the existence of the Uncovered interest parity (UIP) hypothesis and substantially adds to the literature by offering the most recent evidence during the period from 2000 to 2022 from developing and emerging economies. The study further augments the literature by extending the standard UIP hypothesis to account for the monetary policy stance and risk premium. The estimates of nonlinear autoregressive distributed lag (NARDL) and component generalised autoregressive conditional heteroscedasticity (C‐GARCH) show that the UIP hypothesis does not exist in any of the BRICS economies. Nevertheless, after accounting for the risk premium and monetary policy stance using inflation levels, the interest rate differential significantly and positively influences the expected changes in the spot exchange rates. This indicates three important aspects: first, the necessity of risk premium to make up for the higher risk that comes with holding the foreign bond for the benefit of domestic investors. Second that the UIP puzzle does not hold, such that higher interest differential depreciates the domestic currency. Third, the analysis underscores the substantial and direct impact of US inflation level, particularly for Brazil, Russia and India, in determining the changes in the spot exchange rate. These insights hold crucial implications for policymakers and regulators.",,2025,10.1111/ecno.70002,,proquest,"This study examines the Uncovered Interest Parity (UIP) hypothesis in BRICS economies from 2000-2022, incorporating risk premium and monetary policy. Using NARDL and C-GARCH models, it finds that the standard UIP hypothesis does not hold. However, when accounting for risk premium and inflation, interest rate differentials significantly predict exchange rate changes, suggesting the importance of risk premium and the impact of US inflation on exchange rates in these economies.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:15:29.978386
9f6e559ccffae23b,Risk factors selection with data mining methods for insurance premium ratemaking·,"Osiguravajuća društva koja su prva usvojila primjenu metoda rudarenja podataka u svom poslovanju postali su konkurentniji na tržištu osiguranja. Metode rudarenja podataka osiguravajućoj industriji pružaju brojne prednosti: kraće vrijeme obrade podataka, sofisticiranije metode za precizniju analizu podataka, bolje donošenje odluka itd. Osiguravajuća društva koriste metode rudarenja podataka u razne svrhe, od marketinških kampanja do sprečavanja prijevara, a med strok signu prvima je ta metoda bila u postupku odred strok signivanja premija osiguranja. Primjena metode rudarenja podataka u ovom radu ima za cilj poboljšati rezultate u procesu izračuna stope premije neživotnih osiguranja. Poboljšanje se ogleda u odabiru varijabli predvid strok signanja ili faktora rizika koji utječu na stope premija osiguranja. Istražene su sljedeće metode rudarenja podataka za odabir varijabli predvid strok signanja: Postepena regresija, Stabla odlučivanja i Neuronske mreže. Za izračun premijskih stopa korišteni su Generalizirani linearni modeli (GLM), koji su danas glavni statistički model odred strok signivanja premija neživotnih osiguranja u većini razvijenih tržišta osiguranja u svijetu.",,2020,10.18045/zbefri.2020.2.667,,proquest,"This paper explores the use of data mining techniques, specifically stepwise regression, decision trees, and neural networks, to select risk factors for non-life insurance premium ratemaking. The goal is to improve the accuracy of premium calculations by identifying key predictive variables. Generalized Linear Models (GLM) are used for the final premium rate calculation.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:15:32.996980
c717e8521a127885,Risk-based optimal bidding patterns in the deregulated power market using extended Markowitz model,"Deregulation of power industry has entailed important changes in the energy market. With the power industry being restructured, a generation company (GenCo) sells energy through auctions in a daily market, and submission of the appropriate amount of electricity with the right bidding price is important for a GenCo to maximize their profits and minimize the acceptance risk. The objective of this paper is to propose a novel approach for determination of the optimal biding patterns among GenCos in the deregulated power market using a hybrid of Markowitz Model and Genetic Algorithm (GA). While Markowitz Model as an optimization model considers the risk premium for biding patterns and GA as a search engine, considering the acceptance risk in deregulated market. A case study is used to examine the findings of the proposed approach. Also, to compare the proposed model, neural network by back propagation learning algorithm and real proposed pattern were considered. The numerical results indicate that the proposed model is statistically efficient and offers effective curves and biding patterns by lesser risk and equal profitability in day-ahead market as it is able to achieve better results compared to the neural network. © 2020 Elsevier B.V., All rights reserved.","Ostadi, B.; Motamedi Sedeh, O.; Husseinzadeh Kashan, A.",2020,10.1016/j.energy.2019.116516,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075892372&doi=10.1016%2Fj.energy.2019.116516&partnerID=40&md5=38a0c00b1e470d79a0ad16e40e62044f,scopus,"This paper proposes a novel approach using a hybrid of the Markowitz Model and Genetic Algorithm (GA) to determine optimal bidding patterns for generation companies (GenCos) in deregulated power markets. The approach aims to maximize profits while minimizing acceptance risk by considering risk premiums and using GA as a search engine. A case study compares the proposed model with a neural network, showing the proposed model is statistically efficient, offers effective bidding patterns with lesser risk and equal profitability.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:15:35.331042
d380412620ad2d3a,Robust term structure estimation in developed and emerging markets,"Despite powerful advances in interest rate curve modeling for data-rich countries in the last 30 years, comparatively little attention has been paid to the key practical problem of estimation of the term structure of interest rates for emerging markets. This may be partly due to limited data availability. However, emerging bond markets are becoming increasingly important and liquid. It is, therefore, important to be understand whether conclusions drawn from developed countries carry over to emerging markets. We estimate model parameters of fully flexible Nelson–Siegel–Svensson term structures model which has become one of the most popular term structure model among academics, practitioners, and central bankers. We investigate four sets of bond data: U.S. Treasuries, and three major emerging market government bond data-sets (Brazil, Mexico and Turkey). By including both the very dense U.S. data and the comparatively sparse emerging market data, we ensure that are results are not specific to a particular data-set. We find that gradient and direct search methods perform poorly in estimating term structures of interest rates, while global optimization methods, particularly the hybrid particle swarm optimization introduced in this paper, do well. Our results are consistent across four countries, both in- and out-of-sample, and for perturbations in prices and starting values. For academics and practitioners interested in optimization methods, this study provides clear evidence of the practical importance of choice of optimization method and validates a method that works well for the NSS model.",,2018,10.1007/s10479-016-2282-5,,proquest,"This paper estimates the Nelson–Siegel–Svensson term structure model for developed (U.S.) and emerging markets (Brazil, Mexico, Turkey), comparing the performance of different optimization methods. It finds that global optimization methods, specifically a hybrid particle swarm optimization, outperform gradient and direct search methods for term structure estimation, with results consistent across countries and data conditions. The study highlights the practical importance of choosing the right optimization method for the NSS model.",True,True,False,gemini-2.5-flash-lite,Trine,M,,2025-10-13T17:16:02.461666
32f440cd8991d935,SIMULTANEOUS SPECIFICATION TESTING OF MEAN AND VARIANCE STRUCTURES IN NONLINEAR TIME SERIES REGRESSION,"This paper proposes a nonparametric simultaneous test for parametric specification of the conditional mean and variance functions in a time series regression model. The test is based on an empirical likelihood (EL) statistic that measures the goodness of fit between the parametric estimates and the nonparametric kernel estimates of the mean and variance functions. A unique feature of the test is its ability to distribute natural weights automatically between the mean and the variance components of the goodness-of-fit measure. To reduce the dependence of the test on a single pair of smoothing bandwidths, we construct an adaptive test by maximizing a standardized version of the empirical likelihood test statistic over a set of smoothing bandwidths. The test procedure is based on a bootstrap calibration to the distribution of the empirical likelihood test statistic. We demonstrate that the empirical likelihood test is able to distinguish local alternatives that are different from the null hypothesis at an optimal rate.","Chen, Song Xi; Gao, Jiti",2011,10.1017/s0266466610000502,,wos,"This paper introduces a nonparametric simultaneous test for parametric specification of conditional mean and variance functions in nonlinear time series regression. It utilizes an empirical likelihood (EL) statistic comparing parametric and nonparametric kernel estimates, with automatic weighting between mean and variance components. An adaptive test is developed by maximizing a standardized EL statistic over smoothing bandwidths, and bootstrap calibration is used for its distribution. The test can distinguish local alternatives from the null hypothesis at an optimal rate.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:16:12.662958
8338ecaf338c20d8,Semiparametric Estimates of Monetary Policy Effects: String Theory Revisited,"We develop flexible semiparametric time series methods for the estimation of the causal effect of monetary policy on macroeconomic aggregates. Our estimator captures the average causal response to discrete policy interventions in a macrodynamic setting, without the need for assumptions about the process generating macroeconomic outcomes. The proposed estimation strategy, based on propensity score weighting, easily accommodates asymmetric and nonlinear responses. Using this estimator, we show that monetary tightening has clear effects on the yield curve and on economic activity. Monetary accommodation, however, appears to generate less pronounced responses from both. Estimates for recent financial crisis years display a similarly dampened response to monetary accommodation. © 2018 Elsevier B.V., All rights reserved.","Angrist, J.D.; Jordà, Ò.; Kuersteiner, G.M.",2018,10.1080/07350015.2016.1204919,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019125628&doi=10.1080%2F07350015.2016.1204919&partnerID=40&md5=c8ba3071db31226cdbb135e277f4b0e5,scopus,"This paper introduces a semiparametric time series method to estimate the causal effect of monetary policy on macroeconomic aggregates. The method uses propensity score weighting to capture average causal responses to discrete policy interventions, allowing for asymmetric and nonlinear responses without assuming the process generating macroeconomic outcomes. The study finds that monetary tightening affects the yield curve and economic activity, while monetary accommodation shows less pronounced responses, especially during recent financial crisis years.",True,True,False,gemini-2.5-flash-lite,Trine,M,,2025-10-13T17:16:40.362748
d4806ca198cba917,Sentiment lost: the effect of projecting the pricing kernel onto a smaller filtration set,"This paper provides a theoretical analysis on the impacts of using a suboptimal information set for the estimation of the pricing kernel and, more in general, for the validity of the fundamental theorems of asset pricing. While inferring the risk-neutral measure from options data provides a naturally forward-looking estimate, extracting the real world measure from historical returns is only partially informative, thus suboptimal with respect to investors’ future beliefs. As a consequence of this disalignment, the two measures no longer share the same nullset, thus distorting the investors’ risk premium and the validity of the pricing measure. From a probabilistic viewpoint, the missing beliefs are totally unaccessible stopping times on the coarser filtration set, so that an absolutely continuous strict local martingale, once projected on it, becomes continuous with jumps. Some empirical examples complete the paper.",,2020,10.1080/07362994.2019.1711119,,proquest,"This paper theoretically analyzes how using a suboptimal information set (a smaller filtration set) to estimate the pricing kernel affects the fundamental theorems of asset pricing. It explains that this misalignment causes the real-world and risk-neutral measures to have different nullsets, distorting risk premiums and pricing validity. Probabilistically, missing beliefs manifest as inaccessible stopping times, causing continuous processes to exhibit jumps when projected onto the coarser filtration. Empirical examples are included.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:16:45.896319
f566ead501f33867,Shrinking return forecasts,"We develop a new approach that shrinks a given model forecast to the benchmark model forecast in order to improve forecasting performance. Simulation results show the superior performance of our approach, relative to popular methods such as forecast combination and the robustness to model misspecification. We apply our method to forecasting the returns on the S&P 500 index and find significant predictability when shrinking the principal component (PC) regression forecasts based on statistical and economic evaluation criteria. The forecast improvement from our shrinkage approach can be explained by the ability of its hyperparameters to be better predict real economic changes.","Liu, Li; Pan, Zhiyuan; Wang, Yudong",2022,10.1111/fire.12297,,wos,"This paper introduces a novel method for improving forecast accuracy by shrinking a given model's forecast towards a benchmark model's forecast. The approach demonstrates superior performance in simulations compared to existing methods and is applied to forecasting S&P 500 returns, showing significant predictability. The improvement is attributed to hyperparameters better predicting economic changes.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:16:54.030471
60af6446b18c4e40,Sign realized jump risk and the cross-section of stock returns: Evidence from China's stock market,"Using 5-minute high frequency data from the Chinese stock market, we employ a non-parametric method to estimate Fama-French portfolio realized jumps and investigate whether the estimated positive, negative and sign realized jumps could forecast or explain the cross-sectional stock returns. The Fama-MacBeth regression results show that not only have the realized jump components and the continuous volatility been compensated with risk premium, but also that the negative jump risk, the positive jump risk and the sign jump risk, to some extent, could explain the return of the stock portfolios. Therefore, we should pay high attention to the downside tail risk and the upside tail risk.Using 5-minute high frequency data from the Chinese stock market, we employ a non-parametric method to estimate Fama-French portfolio realized jumps and investigate whether the estimated positive, negative and sign realized jumps could forecast or explain the cross-sectional stock returns. The Fama-MacBeth regression results show that not only have the realized jump components and the continuous volatility been compensated with risk premium, but also that the negative jump risk, the positive jump risk and the sign jump risk, to some extent, could explain the return of the stock portfolios. Therefore, we should pay high attention to the downside tail risk and the upside tail risk.",,2017,10.1371/journal.pone.0181990,,proquest,"This study uses high-frequency data from China's stock market to estimate realized jumps and their components (positive, negative, and sign jumps). It then investigates whether these jump risks, along with continuous volatility, can explain or forecast cross-sectional stock returns using Fama-MacBeth regressions. The findings suggest that both jump components and continuous volatility are compensated with risk premiums, and that negative, positive, and sign jump risks contribute to explaining stock portfolio returns, highlighting the importance of tail risks.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:16:58.996509
1045c81233fea906,Some benefits and costs of genetic improvement in new zealand’s sheep and beef cattle industry:ii discounted costs and returns on a farm basis following selection,"Estimated physical returns and their calculated value net of food costs have been used as the basis for an analysis of discounted costs and returns from a selection programme for a 200-ewe flock and a 100-cow beef herd. Recording costs (computing, labour, tags, and scales) of $1–2 per ewe and $2–3 per cow are assumed, with a discount rate of 10% as used by the New Zealand Treasury. It is also assumed that no stock for breeding were purchased or sold. Results are expressed in 1979 dollars, for 1 year or round of selection applied. The net present value of 10 years of selection is $4950 or $3600 in sheep ($1 or $2 per ewe in costs) and $3670–4870 in beef cattle, depending on recording costs and the assumed level of food costs incurred by the herd in achieving higher weight-for-age. With sheep, net production is much more important than fleece weight in its contribution to higher profits. A very important contribution in the beef herd under consideration is the return from a higher net calf crop; even small percentage biological gains are of great economic importance. Net returns for a breeder also selling stock for breeding would probably be much higher. © 1980 Taylor & Fracis Group, LLC. © 2016 Elsevier B.V., All rights reserved.","Morris, C.A.",1980,10.1080/03015521.1980.10426284,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34447574295&doi=10.1080%2F03015521.1980.10426284&partnerID=40&md5=52a22c886f9f1a8745ae7df917205a97,scopus,"This study analyzes the discounted costs and returns of genetic improvement programs for sheep and beef cattle in New Zealand. It estimates physical returns, net of food costs, and applies a 10% discount rate. The analysis considers recording costs and assumes no stock purchases or sales. Results, expressed in 1979 dollars, indicate positive net present values for 10 years of selection in both sheep and beef cattle, with net production being crucial for sheep profits and higher net calf crop for beef herds.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:17:00.343394
40b1a6f001f4f67c,Sovereign bond yield spreads: A time-varying coefficient approach,"We study the determinants of sovereign bond yield spreads across 10 EMU countries between Q1/1999 and Q1/2010. We apply a semiparametric time-varying coefficient model to identify, to what extent an observed change in the yield spread is due to a shift in macroeconomic fundamentals or due to altering risk pricing. We find that at the beginning of EMU, the government debt level and the general investors' risk aversion had a significant impact on interest differentials. In the subsequent years, however, financial markets paid less attention to the fiscal position of a country and the safe haven status of Germany diminished in importance. By the end of 2006, two years before the fall of Lehman Brothers, financial markets began to grant Germany safe haven status again. One year later, when financial turmoil began, the market reaction to fiscal loosening increased considerably. The altering in risk pricing over time period confirms the need of time-varying coefficient models in this context. © 2011 Elsevier Ltd. © 2012 Elsevier B.V., All rights reserved.","Bernoth, K.; Erdogan, B.",2012,10.1016/j.jimonfin.2011.10.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857451801&doi=10.1016%2Fj.jimonfin.2011.10.006&partnerID=40&md5=b27a4c46ffb5293d5ad3fdea6037a760,scopus,"This study investigates the determinants of sovereign bond yield spreads in 10 EMU countries from 1999 to 2010 using a semiparametric time-varying coefficient model. It distinguishes between changes in macroeconomic fundamentals and shifts in risk pricing. The findings indicate that initially, government debt and investor risk aversion were significant, but their importance waned over time. Germany's safe haven status also fluctuated. The study highlights the increased market reaction to fiscal loosening during financial turmoil and emphasizes the necessity of time-varying coefficient models.",True,True,False,gemini-2.5-flash-lite,Trine,M,,2025-10-13T17:17:09.247618
5b09e441b05c7dfd,Spatio-Temporal Momentum: Jointly Learning Time-Series and Cross-Sectional Strategies,"The authors introduce spatio-temporal momentum strategies, a class of models that unify both time-series and cross-sectional momentum strategies by trading assets based on their cross-sectional momentum features over time. Although both time-series and cross-sectional momentum strategies are designed to systematically capture momentum risk premiums, these strategies are regarded as distinct implementations and do not consider the concurrent relationship and predictability between temporal and cross-sectional momentum features of different assets. They model spatio-temporal momentum with neural networks of varying complexities and demonstrate that a simple neural network with only a single fully connected layer learns to simultaneously generate trading signals for all assets in a portfolio by incorporating both their time-series and cross-sectional momentum features. Back testing on portfolios of 46 actively traded US equities and 12 equity index futures contracts, they demonstrate that the model is able to retain its performance over benchmarks in the presence of high transaction costs of up to 5–10 basis points. In particular, they find that the model when coupled with least absolute shrinkage and turnover regularization results in the best performance over various transaction cost scenarios. © 2023 Elsevier B.V., All rights reserved.","Tan, W.L.; Roberts, S.; Zohren, S.",2023,10.3905/jfds.2023.1.130,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179918020&doi=10.3905%2Fjfds.2023.1.130&partnerID=40&md5=f0dcc0bfe7da5374392a0b389a7636f9,scopus,"This paper introduces spatio-temporal momentum strategies, which combine time-series and cross-sectional momentum by analyzing cross-sectional momentum features over time. The authors use neural networks to model these strategies, finding that even a simple neural network can generate trading signals for all assets by considering both their time-series and cross-sectional momentum. Backtesting on US equities and equity index futures shows the model's effectiveness, especially when combined with regularization, even with high transaction costs.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:17:23.747367
bfec2270e5af77cc,"Statistical actuarial estimation of the Capitation Payment Unit from copula functions and deep learning: historical comparability analysis for the Colombian health system, 2015–2021","The Capitation Payment Unit (CPU) financing mechanism constitutes more than 70% of health spending in Colombia, with a budget allocation of close to 60 trillion Colombian pesos for the year 2022 (approximately 15.7 billion US dollars). This article estimates actuarially, using modern techniques, the CPU for the contributory regime of the General System of Social Security in Health in Colombia, and compares it with what is estimated by the Ministry of Health and Social Protection. Using freely available information systems, by means of statistical copulas functions and artificial neural networks, pure risk premiums are calculated between 2015 and 2021. The study concludes that the weights by risk category are systematically different, showing historical pure premiums surpluses in the group of 0–1 years and deficits (for the regions normal and cities) in the groups over 54 years of age.",,2023,10.1186/s13561-022-00416-5,,proquest,"This study estimates the Capitation Payment Unit (CPU) for Colombia's health system using copula functions and deep learning, comparing its findings to official estimates. The analysis reveals systematic differences in risk category weights, leading to surpluses in younger age groups and deficits in older age groups.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:17:27.246662
f2cb1b203a86e3d4,Stochastic period and cohort effect state-space mortality models incorporating demographic factors via probabilistic robust principal components,"In this study we develop a multi-factor extension of the family of Lee-Carter stochastic mortality models. We build upon the time, period and cohort stochastic model structure to extend it to include exogenous observable demographic features that can be used as additional factors to improve model fit and forecasting accuracy. We develop a dimension reduction feature extraction framework which (a) employs projection based techniques of dimensionality reduction; in doing this we also develop (b) a robust feature extraction framework that is amenable to different structures of demographic data; (c) we analyse demographic data sets from the patterns of missingness and the impact of such missingness on the feature extraction, and (d) introduce a class of multi-factor stochastic mortality models incorporating time, period, cohort and demographic features, which we develop within a Bayesian state-space estimation framework; finally (e) we develop an efficient combined Markov chain and filtering framework for sampling the posterior and forecasting. We undertake a detailed case study on the Human Mortality Database demographic data from European countries and we use the extracted features to better explain the term structure of mortality in the UK over time for male and female populations when compared to a pure Lee-Carter stochastic mortality model, demonstrating our feature extraction framework and consequent multi-factor mortality model improves both in sample fit and importantly out-off sample mortality forecasts by a non-trivial gain in performance. © 2021 Elsevier B.V., All rights reserved.","Toczydłowska, D.; Peters, G.; Fung, M.C.; Shevchenko, P.V.",2017,10.3390/risks5030042,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055749896&doi=10.3390%2Frisks5030042&partnerID=40&md5=8e1387341d859cffdee037ca8e59109b,scopus,"This study extends the Lee-Carter stochastic mortality model by incorporating exogenous demographic factors using a probabilistic robust principal components framework for dimension reduction. The developed multi-factor model, estimated within a Bayesian state-space framework, improves both in-sample fit and out-of-sample mortality forecasts compared to the standard Lee-Carter model, as demonstrated by a case study on European demographic data.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:17:46.314019
4d2e4e1efd54f90f,Stock market volatility predictability in a data-rich world: A new insight,"This study develops a shrinkage method, LASSO with a Markov regime-switching model (MRS-LASSO), to predict US stock market volatility. A set of 17 well-known macroeconomic and financial factors are used. The out-of-sample results reveal that the MRS-LASSO model yields statistically and economically significant volatility predictions. We further investigate the predictability of MRS-LASSO with respect to different market conditions, business cycles, and variable selection. Three factors (equity market returns, a short-term reversal factor, and a consumer sentiment index) are the most frequent predictors. To investigate the practical implications, we construct the expected variance risk premium (VRP) by using volatility forecasts generated from the LASSO and MRS-LASSO models to forecast future stock returns and find that those models are also powerful. © 2023 Elsevier B.V., All rights reserved.","Ma, F.; Wang, J.; Wahab, M.I.M.; Ma, Y.",2023,10.1016/j.ijforecast.2022.08.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140291431&doi=10.1016%2Fj.ijforecast.2022.08.010&partnerID=40&md5=2f88412fe379d55e2ce23fed80d4a33d,scopus,"This study introduces a new shrinkage method, MRS-LASSO, to predict US stock market volatility using macroeconomic and financial factors. The model demonstrates significant out-of-sample predictive power and identifies key predictors like equity market returns, a short-term reversal factor, and a consumer sentiment index. The forecasts also prove useful in predicting future stock returns.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:17:50.077990
b583a5af78b01dc0,Surrogate-assisted optimal re-dispatch control for risk-aware regulation of dynamic total transfer capability,"To enable reliable power delivery through transmission tie-lines, total transfer capability (TTC) must be calculated and regulated to accommodate the transferred amount. However, the traditional optimal power flow (OPF)-based total transfer capability calculation is computationally expensive for efficient total transfer capability control due to the inclusion of a large set of differential-algebraic equations (DAEs) to verify transient stability constraints. In order to enable practicable total transfer capability regulation, a novel risk-aware deep learning-assisted paradigm is proposed here. First, a deep belief network (DBN) is employed to establish the total transfer capability predictor and surrogate the computation-intensive differential-algebraic equations in original optimal power flow formulas, simplifying the high-dimensional and intractable constraints deep belief networks without loss of nonlinearity. Particularly, in order to be aware of control risk from the predictive error of the deep belief networks, prediction intervals (PIs) are produced improved by using ensemble learning and used to disclose the probability of insufficient actions, further guaranteeing the sufficient and cost-effective control by compromising the tradeoff between cost and risk. Symbiotic organisms search (SOS) is then applied to solve the proposed risk-aware deep belief network-assisted total transfer capability control problem globally. The numerical studies testify that the proposed method enables economical, reliable, and full nonlinearity-retained dynamic total transfer capability regulation control within a risk-free surrogate-assisted and tractable physical model-driven hybrid framework. © 2021 Elsevier B.V., All rights reserved.","Qiu, G.; Liu, Y.; Liu, J.; Wang, L.; Liu, T.; Gao, H.; Jawad, S.",2021,10.1049/gtd2.12147,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101158129&doi=10.1049%2Fgtd2.12147&partnerID=40&md5=73bcf2eadd06cf306adf48ea591817a5,scopus,"This paper proposes a novel risk-aware deep learning-assisted paradigm for total transfer capability (TTC) regulation in power systems. It uses a deep belief network (DBN) as a surrogate for computationally expensive differential-algebraic equations (DAEs) to predict TTC. Ensemble learning is used to generate prediction intervals (PIs) to quantify control risk, and Symbiotic Organisms Search (SOS) is applied to solve the risk-aware control problem. The method aims for economical, reliable, and nonlinearity-retained dynamic TTC regulation within a tractable framework.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:17:52.484432
6c0ac1888b8f742d,TEND: A Target-Dependent Representation Learning Framework for News Document,"Real-time news documents published on the Internet have global financial and political impacts. Pioneering statistical approaches investigate manually defined features to capture lexical, sentiment, and event information, which suffer from feature sparsity. As a remedy, recent work has considered learning dense vector representations for documents. Such representations are general, which can not model target-dependent scenarios, such as stance detection towards a specific claim. There has been work on target-specific word and sentence representations, but little was done on target-dependent document representation. Moreover, documents contain more potentially helpful information, but also noise compared to events and sentences. To address the above issues, we focus on models that are: 1. task-driven, which optimize the neural network representations for the end task; 2. target-specific, learning news representations by considering the influence of specific targets. In particular, we propose a novel document-level target-dependent learning framework TEND. The framework employs the information of the target and the news abstract as clues, obtaining relatively informative sentences from the entire document for our objectives. The framework assembles a document representation by integrating the news abstract representation and a weighted sum of sentence representations in the document. To the best of our knowledge, we are among the first to investigate target-dependent document representation. Existing text representation models can be easily integrated into our TEND framework, and it is general enough to be applied to different target-dependent document representation tasks. We empirically evaluate our framework on two target-dependent document-level tasks, including a cumulative abnormal return prediction task and a news stance detection task. Results show that our models give the best performances compared to state-of-the-art document embedding methods, yielding robust and consistent performances across datasets.",J. Duan; X. Ding; Y. Zhang; T. Liu,2019,10.1109/taslp.2019.2947364,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8868172,ieeexplore,"This paper introduces TEND, a novel framework for learning target-dependent document representations, particularly for news documents. TEND addresses limitations of general document representations by incorporating target-specific information and optimizing for end tasks. It integrates news abstract and weighted sentence representations to create a document representation. The framework is evaluated on cumulative abnormal return prediction and news stance detection, outperforming existing methods.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:17:54.713873
fe46b71e2ee515f3,THE SEARCH FOR TIME-SERIES PREDICTABILITY-BASED ANOMALIES,". This paper introduces a new algorithm for exploiting time-series predictability-based patterns to obtain an abnormal return, or alpha, with respect to a given benchmark asset pricing model. The algorithm proposes a deterministic daily market timing strategy that decides between being fully invested in a risky asset or in a risk-free asset, with the trading rule represented by a parametric perceptron. The optimal parameters are sought in-sample via differential evolution to directly maximize the alpha. Successively using two modern asset pricing models and two different portfolio weighting schemes, the algorithm was able to discover an undocumented anomaly in the United States stock market cross-section, both out-of-sample and using small transaction costs. The new algorithm represents a simple and flexible alternative to technical analysis and forecast-based trading rules, neither of which necessarily maximizes the alpha. This new algorithm was inspired by recent insights into representing reinforcement learning as evolutionary computation. © 2022 Elsevier B.V., All rights reserved.","Ospina-Holguin, J.H.; Padilla-Ospina, A.M.",2021,10.3846/jbem.2021.15650,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123798331&doi=10.3846%2Fjbem.2021.15650&partnerID=40&md5=24f80df9c903c2537c0327df647bf65c,scopus,"This paper presents a novel algorithm that uses time-series predictability to generate abnormal returns (alpha) relative to a benchmark asset pricing model. The algorithm employs a deterministic daily market timing strategy, deciding between investing in a risky or risk-free asset using a parametric perceptron. Optimal parameters are found using differential evolution to maximize alpha. Applied to US stock markets with modern asset pricing models and different portfolio weighting schemes, the algorithm identified an undocumented anomaly out-of-sample, even with small transaction costs. It's presented as a simpler, more flexible alternative to technical analysis and forecast-based trading rules, drawing inspiration from reinforcement learning and evolutionary computation.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:18:01.980712
b1aa1d6583414fbd,THE VALUE OF MORTGAGE PREPAYMENT AND DEFAULT OPTIONS,"We use an implicit alternating direction numerical procedure to estimate the value of a fixed-rate mortgage (FRM) with embedded default and prepayment options. The value of FRMs depends on interest rates, the house value, and mortgage maturity. Our numerical results suggest that the joint option value of prepayment and default is considerably high, even at loan origination. We extend the model to include prepayment penalties in FRM valuation. (C) 2009 Wiley Periodicals, Inc. Jrl Fut Mark 29:840-861, 2009","Chen, Yong; Connolly, Michael; Tang, Wenjin; Su, Tie",2009,10.1002/fut.20388,,wos,"This paper uses a numerical procedure to estimate the value of fixed-rate mortgages with embedded default and prepayment options, considering interest rates, house value, and mortgage maturity. The study finds that the combined option value of prepayment and default is significant even at loan origination and explores the impact of prepayment penalties.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:18:03.625248
80e3352b0bafafdc,Technical trading rules and the size of the risk premium in security returns,"Among analysts, technical trading rules are widely used for forecasting security returns. Recent literature provides evidence that these rules may provide positive profits after accounting for transaction costs. This would be contrary to the theory of the efficient market hypothesis which states that security prices cannot be forecasted from their past values or other past variables. This paper uses the daily Dow Jones Industrial Average Index from 1963 to 1988 to examine the linear and nonlinear predictability of stock market returns with simple technical trading rules, by using the nearest neighbors and the feedforward network regressions. Evidence of nonlinear predictability is found in the stock market returns by using the past returns and the buy and sell signals of the moving average rules.","Gencay, R; Stengos, T",1997,10.2202/1558-3708.1026,,wos,"This paper investigates the predictability of stock market returns using technical trading rules and machine learning techniques like nearest neighbors and feedforward network regressions. The study finds evidence of nonlinear predictability in the Dow Jones Industrial Average Index returns from 1963-1988, contradicting the efficient market hypothesis.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:18:07.813881
8b3b7d7aa07bfbd6,Techno-Economic Investment Risk Modeling of Battery Energy Storage System Participating in Day-Ahead Frequency Regulation Market,"Owing to its high capital cost, Battery Energy Storage System (BESS) investment risk has received considerable attention in recent years. Currently, day-ahead frequency regulation service is one major revenue source for BESS, and the revenue is exposed to a compound of stochastic market risk and technical risk. On the market risk side, due to a lack of long-term contracts, investors are exposed to “price risk” and “volume (revenue hours) risk” over the entire investment horizon of 5–10 years. On the technical side, performance issues such as equipment degradation and inherent defects can result in reduced or even negative revenue for BESS under the pay-for-performance remuneration structure. Quantifying these risks is important for investors and banks to assess a project’s investability and bankability. However, existing BESS techno-economic literature has mostly focused on developing optimal control strategies to maximize revenue or optimal battery sizing to reduce capital expenditure. To our knowledge, none of the literature to date has addressed the long-term risk perspective of BESS investment. This study aims to fill this gap by developing a long-term probabilistic revenue estimate that considers these risk factors using Monte Carlo simulations. A case study using Taiwan’s newly launched day-ahead market, which has similar grid dynamics and revenue risk factors as in most markets, is also presented in this paper. Simulation result shows that, for the conservative P90 (90% exceedance probability) scenario, expected return of a hypothetical 10MW (half-hour battery) BESS investment is 8.65% and its debt service coverage ratio is 1.189.",P. -H. Hsi; J. C. P. Shieh,2024,10.1109/access.2024.3390439,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10504254,ieeexplore,"This study addresses the long-term investment risk of Battery Energy Storage Systems (BESS) participating in the day-ahead frequency regulation market. It develops a probabilistic revenue estimation model using Monte Carlo simulations to account for market risks (price and volume) and technical risks (equipment degradation). A case study in Taiwan demonstrates the model's application, showing an expected return of 8.65% under a P90 scenario for a hypothetical BESS investment.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:18:11.594220
4c66380b2d740598,Term Structure Modeling and Forecasting of Government Bond Yields,"Accurate modelling and precise estimation of the term structure of interest rate are of crucial importance in many areas of finance and macroeconomics as it is the most important factor in the capital market and probably the economy. This study compares the in-sample fit and out-of-sample forecast accuracy of the Cox-Ingersoll-Ross (CIR) and Nelson-Siegel models. For the in-sample fit, there is a significant lack of information on the short-term CIR model. The CIR model should also be considered too poor to describe the term structure in a simulation-based context. It generates a downward slope average yield curve. Contrary to CIR model, Nelson-Siegel model is not only compatible to fit attractively the yield curve but also accurately forecast the future yield for various maturities. Furthermore, the non-linear version of the Nelson-Siegel model outperforms the linearised one. In a simulation-based context, the Nelson-Siegel model is capable to replicate most of the stylised facts of the Japanese market yield curve. Therefore, it turns out that the Nelson-Siegel model (non-linear version) could be a good candidate among various alternatives to study the evolution of the yield curve in Japanese market. Adapted from source document.",,2013,10.1111/1759-3441.12046,,proquest,"This study compares the Cox-Ingersoll-Ross (CIR) and Nelson-Siegel models for term structure modeling and forecasting of government bond yields. The Nelson-Siegel model, particularly its non-linear version, demonstrates superior in-sample fit and out-of-sample forecasting accuracy compared to the CIR model. The Nelson-Siegel model is also capable of replicating stylized facts of the Japanese market yield curve, making it a suitable candidate for studying yield curve evolution in that market.",True,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T17:18:21.697745
88ec6de29272b73e,Term structure estimation with liquidity-adjusted Affine Nelson Siegel model: A nonlinear state space approach applied to the Indian bond market,"Efficient term structure estimation in emerging markets is difficult not only because of overall lack of liquidity, but also because of the concentration of liquidity in a few securities. Using the arbitrage-free Affine Nelson-Siegel model, we explicitly incorporate this phenomenon using a proxy for liquidity based on observable data in the bond pricing function and estimate the term structure for Indian Government bond markets in a nonlinear state space setting using the Unscented Kalman Filter. We find strong empirical evidence in support of the extended model with both i) a better in-sample fit to bond prices, and ii) the likelihood ratio test rejecting the restrictions assumed in the standard AFNS specification. In an alternative specification, we also model liquidity as a latent risk factor within the AFNS framework. The estimated latent liquidity factor is found to be strongly correlated with the standard market benchmarks of overall liquidity and the India VIX index.",,2022,10.1080/00036846.2021.1967866,,proquest,This study estimates the term structure of the Indian bond market using a liquidity-adjusted Affine Nelson-Siegel model within a nonlinear state space framework. The model explicitly accounts for liquidity issues prevalent in emerging markets. The findings suggest the extended model provides a better fit to bond prices and that a latent liquidity factor is strongly correlated with market liquidity benchmarks.,True,True,False,gemini-2.5-flash-lite,Trine,M,,2025-10-13T17:18:41.830619
5b36dfbcd718497b,"Testing for UIP-Type Relationships: Nonlinearities, Monetary Announcements and Interest Rate Expectations","This paper tests for UIP-type relationships by estimating first a benchmark linear Cointegrated VAR including the nominal exchange rate and the interest rate differential as well as central bank announcements, and then a Smooth Transition Cointegrated VAR (STCVAR) model incorporating nonlinearities and also taking into account the role of interest rate expectations. The analysis is conducted for five inflation targeting countries (the UK, Canada, Australia, New Zealand and Sweden) and three non-targeters (the US, the Euro-Area and Switzerland) using daily data from January 2000 to December 2020. While we cannot confirm the validity of UIP in its strictest theoretical sense, we find evidence for the existence of an equilibrium relationship between the exchange rate and the interest rate differential. Specifically, the nonlinear framework appears to be more appropriate to capture the adjustment towards the long-run equilibrium, since the estimated speed of adjustment is substantially faster and the short-run dynamic linkages more significant. Further, interest rate expectations play an important role: a fast adjustment only occurs when the market expects the interest rate to increase in the near future, namely central banks are perceived as more credible when sticking to their goal of keeping inflation at a low and stable rate. Also, central bank announcements have a more sizeable short-run effect in the nonlinear model. Finally, the equilibrium relationship between the exchange rate and the interest rate differential holds better in inflation targeting countries, where monetary authorities appear to achieve a higher degree of credibility.","Anderl, Christina; Caporale, Guglielmo Maria",2022,10.1007/s11079-021-09640-8,,wos,"This paper investigates the UIP-type relationship between exchange rates and interest rate differentials using linear and nonlinear (STCVAR) models for several inflation targeting and non-targeting countries. It finds that nonlinearities and interest rate expectations are crucial for capturing the equilibrium relationship, with faster adjustments and more significant short-run dynamics in the nonlinear framework. Central bank credibility, particularly in inflation targeting countries, enhances this relationship and the impact of announcements.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:18:45.181581
045ccef9485f38e1,Testing the empirical performance of stochastic volatility models of the short-term interest rate,"I introduce two-factor discrete time stochastic volatility models of the short-term interest rate to compare the relative performance of existing and alternative empirical specifications. I develop a nonlinear asymmetric framework that allows for comparisons of non-nested models featuring conditional heteroskedasticity and sensitivity of the volatility process to interest rate levels. A new class of stochastic volatility models with asymmetric drift and nonlinear asymmetric diffusion process is introduced in discrete time and tested against the popular continuous time and symmetric and asymmetric GARCH models. The existing models are rejected in favor of the newly proposed models because of the asymmetric drift of the short rate, and the presence of nonlinearity, asymmetry, GARCH, and level effects in its volatility. I test the predictive power of nested and non-nested models in capturing the stochastic behavior of the risk-free rate. Empirical evidence on three-, six-, and 12-month U.S. Treasury bills indicates that two-factor stochastic volatility models are better than diffusion and GARCH models in forecasting the future level and volatility of interest rate changes. © 2018 Elsevier B.V., All rights reserved.","Bali, T.G.",2000,10.2307/2676190,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034386111&doi=10.2307%2F2676190&partnerID=40&md5=9adf8aee423204a0e105653f3f21f549,scopus,"This paper introduces and empirically tests two-factor discrete time stochastic volatility models for the short-term interest rate. It compares these models against existing specifications, including GARCH models, and finds that the proposed models, which account for asymmetric drift and nonlinear asymmetric diffusion, perform better in forecasting interest rate levels and volatility. The study uses U.S. Treasury bill data.",True,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T17:19:15.196168
02493d3e75cc8c16,"The ""forward premium puzzle"" and the sovereign default risk","Carry-trade strategies which consist of buying forward high-yield currencies tend to yield positive excess returns when global financial markets are booming, whereas they generate losses during crises. Firstly, we show that the sovereign default risk, which is taken on by investing in high-yield currencies, may increase the magnitude of the gains during the boom periods and the losses during crises. We empirically test for this hypothesis on a sample of 18 emerging currencies over the period from June 2005 to September 2010, the default risk being proxied by the sovereign credit default swap spread. Relying on smooth transition regression (STR) models, we show that default risk contributes to the carry-trade gains during booms, and worsens the losses during busts. Secondly, we turn to the ""Fama regression"" linking the exchange-rate depreciation to the interest-rate differential. We propose a nonlinear estimation of this equation, explaining the puzzling evolution of its coefficient by the change in the market volatility along the financial cycle. Then, we introduce the default risk into this equation and show that the ""forward bias"", usually evidenced by a coefficient smaller than unity in this regression, is somewhat alleviated, as the default risk is significant to explain the exchange-rate change. © 2012 Elsevier Ltd. © 2017 Elsevier B.V., All rights reserved.","Coudert, V.; Mignon, V.",2013,10.1016/j.jimonfin.2012.05.025,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874775246&doi=10.1016%2Fj.jimonfin.2012.05.025&partnerID=40&md5=b7fab6ec75aa72f841ac0789d409a42e,scopus,"This study investigates the ""forward premium puzzle"" and sovereign default risk in relation to carry-trade strategies. It demonstrates that sovereign default risk amplifies both gains during economic booms and losses during crises for carry trades involving high-yield currencies. The research uses a sample of 18 emerging currencies from 2005-2010, with sovereign credit default swap spreads as a proxy for default risk. Smooth transition regression (STR) models are employed to show the impact of default risk on carry-trade performance. Additionally, the study examines the Fama regression, linking exchange-rate depreciation to interest-rate differentials, and proposes a nonlinear estimation to account for market volatility changes. The findings suggest that default risk helps explain the ""forward bias"" observed in this regression.",True,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:19:39.264256
1206b2b863e2d10a,The Effect of Risk Factor Disclosures on the Pricing of Credit Default Swaps,"This study examines the relation between narrative risk disclosures in mandatory reports and the pricing of credit risk. In particular, we investigate whether and how the Securities and Exchange Commission (SEC) mandate of risk factor disclosures (RFDs) affects credit default swap (CDS) spreads. Based on the theory of Duffie and Lando (2001), we predict and find that CDS spreads decrease significantly after RFDs are made available in 10‐K/10‐Q filings. These results suggest that RFDs improve information transparency about the firm's underlying risk, thereby reducing the information risk premium in CDS spreads. The content analysis further reveals that disclosures pertinent to financial and idiosyncratic risk are especially relevant to credit investors. In cross‐sectional analyses, we document that RFDs are more useful for evaluating the business prospects and default risk of firms with greater information uncertainty/asymmetry. Overall, our findings imply that the SEC requirement for adding a risk factor section to periodic reports enhances the transparency of firm risk and facilitates credit investors in evaluating the credit quality of the firm.",,2018,10.1111/1911-3846.12362,,proquest,"This study investigates how mandatory risk factor disclosures (RFDs) in SEC filings impact the pricing of credit default swaps (CDS). The findings indicate that RFDs lead to a significant decrease in CDS spreads, suggesting improved information transparency about firm risk and a reduced information risk premium. Disclosures related to financial and idiosyncratic risks are particularly important for credit investors, and RFDs are more beneficial for firms with higher information uncertainty.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:19:48.984829
8df5c5a1183a28ea,The London Business School With Gower Publishing: FIGHTING YESTERDAY'S BATTLES,"With still no firm evidence at home of a recovery in non‐oil GDP, the government's main worries centre on the path of output ahead of the General Election. In a forecast, which relies heavily on exports to stimulate demand in 1992, the Treasury cannot regard the rising probability of renewed recession in the US or the very sharp slowdown currently taking place in Europe as the post‐unification German boom runs out of steam with equanimity. The fear mist remain in Conservative politicians' minds that there will be no meaningful recovery within an electorally significant timescale. We sketch out this background, but our focus here is not on the prospects for recovery; rather we ask whether the recession has achieved its objectives. The recession was, it should be remembered, the direct product of government policy ‐ interest rates were raised to 15per cent ahead of ERM membership ‐ aimed at reversing the excesses of the late 198Os'boom and in particular at bringing inflation quickly down to acceptable European levels and reducing the deficit on the current account, which at its peak in 1989 amounted to 4 per cent of GDP. Our answer is that, over the last year of recession, considerable progress has been made: the rate of inflation is now in line with that in Germany and the current account deficit has fallen to under 1 per cent of GDP. But, on the government's own forecasts contained in the Autumn Statement, there will be some slippage on both counts in 1992. It is this worrying feature that we consider here. Our overall conclusion is that the recession has not completely delivered its objectives and that, even as the politicians turn their attention to recovery, we still have to fight yesterday's battles. Copyright © 1991, Wiley Blackwell. All rights reserved © 2016 Elsevier B.V., All rights reserved.","Dicks, G.",1991,10.1111/j.1468-0319.1991.tb00158.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978558611&doi=10.1111%2Fj.1468-0319.1991.tb00158.x&partnerID=40&md5=b960fc38fda667cdd4f0e4365e15cf2e,scopus,"This article analyzes the effectiveness of the UK government's recessionary policies, implemented to curb inflation and reduce the current account deficit. While progress has been made, the authors argue that the recession has not fully achieved its objectives, and further efforts are needed to address lingering economic issues.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:19:52.446374
6e5ee856b4eecc9e,The Markov-switching jump diffusion LIBOR market model,"In this paper, we introduce an extension to the LIBOR Market Model (LMM) that is suitable to incorporate both sudden market shocks as well as changes in the overall economic climate into the interest rate dynamics. This is achieved by substituting the simple diffusion process of the original LMM by a regime-switching jump diffusion. We demonstrate that the new Markov-switching jump diffusion (MSJD) LMM can be embedded into a generalized regime-switching Heath-Jarrow-Morton model and prove that the considered market is arbitrage-free. We derive pricing formulas for caps, floors and swaptions using Fourier pricing techniques and show how the model can be calibrated to real market data.","Steinruecke, L.; Zagst, R.; Swishchuk, A.",2015,10.1080/14697688.2014.962594,,wos,"This paper extends the LIBOR Market Model (LMM) by incorporating a regime-switching jump diffusion process to capture market shocks and economic climate changes. The proposed Markov-switching jump diffusion (MSJD) LMM is shown to be embeddable in a generalized Heath-Jarrow-Morton model, ensuring an arbitrage-free market. The authors derive pricing formulas for caps, floors, and swaptions using Fourier techniques and demonstrate model calibration to market data.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:19:54.315145
dd8b9653412c131a,The Pricing of Time-Varying Exchange Rate Risk in the Stock Market: A Nonparametric Approach,"This paper reexamines the pricing of exchange rate risk in the U.S. stock market. We first construct stock portfolios based on the Foreign Exchange Income (FEI), a measure of currency exposure of firms, reported in their annual reports. We then develop two-factor and multi-factor nonparametric models that allow time variation in risk exposure and risk premium, and nonlinearity in the return generating process. When we assume that risk exposure can be time-varying but risk premium is constant, the estimated premium for exchange rate risk is significant only for the most positive FEI-ranked portfolio and marginally significant for the most negative FEI-ranked portfolio. When we further assume that both risk exposure and risk premium can be time-varying, results suggest that exchange rate risk is significantly priced for all the FEI-ranked portfolios except the one with little exposure.","Chung, Y. Peter; Zhou, Zhong-guo",2012,10.1515/1558-3708.1634,,wos,"This paper investigates the pricing of exchange rate risk in the U.S. stock market using a nonparametric approach. It constructs stock portfolios based on Foreign Exchange Income (FEI) and develops time-varying multi-factor models to account for nonlinearities and time variations in risk exposure and premium. The study finds that exchange rate risk is priced, particularly for portfolios with significant currency exposure.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:19:58.580645
3c22a3ab919ed331,The Pruned State-Space System for Non-Linear DSGE Models: Theory and Empirical Applications,"This article studies the pruned state-space system for higher-order perturbation approximations to dynamic stochastic general equilibrium (DSGE) models. We show the stability of the pruned approximation up to third order and provide closed-form expressions for first and second unconditional moments and impulse response functions. Our results introduce generalized method of moments (GMM) estimation and impulse-response matching for DSGE models approximated up to third order and provide a foundation for indirect inference and simulated method of moments (SMM). As an application,we consider a New Keynesian model with Epstein–Zin preferences and two novel feedback effects from long-term bonds to the real economy, allowing us to match the level and variability of the $10$-year term premium in the U.S. with a low relative risk aversion of $5$.",,2018,10.1093/restud/rdx037,,proquest,"This article develops a pruned state-space system for higher-order perturbation approximations of non-linear DSGE models, demonstrating stability up to third order and providing analytical solutions for moments and impulse responses. It introduces GMM and impulse-response matching for DSGE models approximated to third order, laying groundwork for indirect inference and SMM. An application to a New Keynesian model with Epstein-Zin preferences and novel feedback effects matches U.S. term premium data with low relative risk aversion.",True,True,False,gemini-2.5-flash-lite,Trine,M,,2025-10-13T17:20:34.698471
26ee0779c0e53b1a,The Stern Review on the Economic Effects of Climate Change,"In a study of the economics of climate change commissioned by the British government, released on 30 October, the former World Bank chief economist Sir Nicholas Stern presents a vigorously argued case for early curtailment of greenhouse gas emissions and proposes mitigation strategies that appear to offer highly favorable benefit-cost ratios. An excerpt from the Executive Summary of the Stern Review, concerned with the nature and magnitude of the deleterious economic consequences of anticipated climate change, is printed below. The principal scientific reviews of knowledge of climate change, its consequences, and mitigation strategies are the (roughly) quinquennial reports of the Intergovernmental Panel on Climate Change (IPCC)-the work of hundreds of lead authors, subjected in turn to elaborate peer review and line-by-line scrutiny by interested governments. They represent a broad, though not total, expert consensus. The third IPCC assessment was issued in 2001; the fourth, already in draft, will be released next year. The Stern Review draws heavily on this scientific underpinning, but goes further than the IPCC exercise in computing economic values for the projected changes and costing out remedial policy responses. More forthright in style and emphatic in its conclusions, it reads as a resounding call to international action. The Review explores the implications of atmospheric concentrations of carbon dioxide and other greenhouse gases being capped at 550ppm (parts per million), double the preindustrial level, an objective it argues is feasible. That concentration would be reached by 2050 at current emission rates, or by 2035 if emissions rise as expected. The resulting warming, it believes, would be 2-5 degree C, roughly in accord with the IPCC's third-assessment estimates (see the Documents section of PDR 27, no. 1 for the IPCC projections). The positive feedbacks identified in some recent studies, generated by processes such as release of methane from permafrost, could lead to still higher temperatures. The forecast effects described are by now familiar, though no less grim for being so: species extinctions, expanding disease zones, reductions in surface water availability, coastal flooding, ocean acidification, and so on. The Review translates these effects into economic losses, adjusting for risk, using Monte Carlo simulation applied to an integrated assessment model (the so-called PAGE 2002 model). The exercise, requiring many heroic-and often contestable-assumptions, produces the most quoted figures in the report: that climate change 'will reduce welfare by an amount equivalent to a reduction in consumption per head of between 5 and 20%'-now and into the future. The absolute magnitude of those projected economic losses is made arbitrarily large by their permanence. Typical benefit-cost calculations applied to appraisal of development projects convert such long-term trajectories into a present value using a discount rate comparable to a market interest rate or some (lower) assumed rate of time preference. The Stern Review, however, argues that any discounting is ethically inappropriate for this global issue: 'if a future generation will be present, we suppose that it has the same claim on our ethical attention as the current one' (p. 31). The only exception is an allowance for the possibility that future generations are not present-through human extinction-which is held to justify a minuscule discount rate of 0.1 percent per annum (p. 161). The percentage economic losses from climate change appear less daunting if set against the recent pace of expansion in the world economy. Real per capita income growth since 1990 has averaged about 1.5 percent per year worldwide, and about 3 percent in developing countries. In such a regime, a 5 percent one-time drop to a lower expansion path is no more than a two- or three-year delay in attaining a given income level. For China and India, whose economies are doubling in size each decade, even a 20 percent reduction in income would be a mere hiccough on the path to affluence-hardly enough to motivate major shifts in lifestyle ambitions. The dire repercussions on global environments of a greenhouse warming at the upper end of the forecast range are poorly captured by those percentages. Demography has a marginal place in the Review. The underlying IPCC emission scenarios incorporate expected population growth, using the UN medium projections. Many of the climate-change effects incur costs that are similarly magnified by population growth. One-sixth of the world's population is 'threatened' by water scarcities; 1 in 20 people may be displaced by a rising sea level; mortality may increase from vector-borne diseases and from malnutrition linked to income losses. The later part of the Review is concerned with mitigation and adaptation strategies. It lays out an ambitious set of policies for transition to a low-carbon economy that could stabilize greenhouse gas concentrations over the next several decades. By 2050, emissions would have to be 25 percent below today's and emissions per unit of GDP 75 percent below. In perhaps the most problematic part of the exercise the Review asserts that such cuts could be achieved at a cost of only around 1 percent of annual global GDP-implying that investment in mitigation should be strongly favored on straightforward economic grounds. (This figure, like others in the Review, is acknowledged to lie within a substantial envelope of uncertainty-here a range of -1.0 percent to +3.5 percent of global GDP (p. 212), or, drawing on a wider range of models, -4 percent to +15 percent (p. 241).) In the decades before the investment pays off, adverse consequences of the warming trends already underway must be dealt with by adaptation, such as through better disaster preparedness, lessening the vulnerability of infrastructure, and risk-pooling measures. The excerpt is from pp. iii-iv and vi-xi. The full Stern Review (579 pages), the executive summary, and the commissioned background papers are available online at 'http://www.hm-treasury.gov.uk/independent_reviews/stern_review_ec o nomics_climate_change/sternreview_index.cfm'. A hard copy of the Review will be issued by Cambridge University Press.",,2006,10.1111/j.1728-4457.2006.00153.x,,proquest,"This article summarizes the Stern Review on the Economic Effects of Climate Change, highlighting its argument for early greenhouse gas emission curtailment and proposing mitigation strategies with favorable benefit-cost ratios. It discusses the review's reliance on IPCC scientific assessments, its economic valuation of climate change impacts (estimated as a 5-20% reduction in per capita consumption), and its controversial stance against discounting future welfare. The review also outlines ambitious mitigation policies to transition to a low-carbon economy, suggesting these could be achieved at a low cost (around 1% of global GDP), and emphasizes the need for adaptation strategies.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:20:37.980743
56291c6ae5cabc13,The causality link between political risk and stock prices,"PurposePrior studies have paid close attention to the impact of political risk on financial markets. Following this strand of literature, this paper aims to focus on the causality link between political shocks and their impacts on emerging stock markets.Design/methodology/approachThis paper highlights an innovative counterfactual model for political risk assessment. Based on a natural experiment, i.e. the Taiwan Strait Crisis in 1995-1996, this study utilizes one data-driven approach, e.g. the synthetic control methods (SCMs), to estimate causal impact of this political shock on Taiwan’s stock market.FindingsMajor findings in this study are consistent with existing literature on the price of political risk, e.g. political uncertainty commands a risk premium. The SCM estimations suggest that Taiwan’s stock prices dramatically underperformed its newly industrialized peers and other developed markets during the crisis. The SCM results are statistically significant and robust to various cross-validation tests.Research limitations/implicationsFindings in this study indicate that political risks could generate enormous impacts on emerging financial markets. In particular, political uncertainty following new geopolitical dynamics requires proper identification and assessment.Originality/valueTo the author’s knowledge, this paper is the first rigorous counterfactual study to the causality relationship between political uncertainty and stock prices in emerging markets. This paper is distinct from previous studies in applying a data-driven approach to combine the features of learning from others (cross-sectional) and learning from the past (time series).",,2019,10.1108/jfep-07-2018-0106,,proquest,"This study uses the synthetic control method (SCM), a data-driven approach, to investigate the causal relationship between political shocks and emerging stock markets, specifically examining the Taiwan Strait Crisis of 1995-1996. The findings indicate that political uncertainty leads to a risk premium, causing Taiwan's stock market to underperform significantly during the crisis.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:20:42.480448
d85d0baae566aaa4,The cross-sectional stock return predictions via quantum neural network and tensor network,"In this paper, we investigate the application of quantum and quantum-inspired machine learning algorithms to stock return predictions. Specifically, we evaluate the performance of quantum neural network, an algorithm suited for noisy intermediate-scale quantum computers, and tensor network, a quantum-inspired machine learning algorithm, against classical models such as linear regression and neural networks. To evaluate their abilities, we construct portfolios based on their predictions and measure investment performances. The empirical study on the Japanese stock market shows the tensor network model achieves superior performance compared to classical benchmark models, including linear and neural network models. Though the quantum neural network model attains the lowered risk-adjusted excess return than the classical neural network models over the whole period, both the quantum neural network and tensor network models have superior performances in the latest market environment, which suggests capability of model’s capturing non-linearity between input features. © 2023 Elsevier B.V., All rights reserved.","Kobayashi, N.; Suimon, Y.; Miyamoto, K.; Mitarai, K.",2023,10.1007/s42484-023-00136-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178906367&doi=10.1007%2Fs42484-023-00136-x&partnerID=40&md5=5778bbe192db850d188778151b988196,scopus,"This paper explores the use of quantum and quantum-inspired machine learning algorithms, specifically quantum neural networks and tensor networks, for stock return predictions. The study compares these models against classical methods like linear regression and neural networks using the Japanese stock market. Results indicate that the tensor network model outperforms classical benchmarks, and both quantum and tensor network models show superior performance in recent market conditions, suggesting an ability to capture non-linear relationships.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:20:45.847025
58601be935f1ee81,"The demand for M1 in the U.S.A., 1960-1988","Estimated U.S. M1 demand functions appear unstable, regularly “breaking down,” over 1960-1988 (e.g. missing money, great velocity decline, M1-explosion). We propose a money demand function whose arguments include inflation, real income, long-term bond yield and risk, T-bill interest rates, and learning curve weighted yields on newly introduced instruments in Ml and non-transactions M2. The model is estimated in dynamic error-correction form; it is constant and, with an equation standard error of 0.4%, variance-dominates most previous models. Estimating alternative specifications explains earlier “breakdowns,” showing the model’s distinctive features to be important in accounting for the data. © 1992 The Review of Economic Studies Limited. © 2016 Elsevier B.V., All rights reserved.","Baba, Y.; Hendry, D.F.; Starr, R.M.",1992,10.2307/2297924,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963027196&doi=10.2307%2F2297924&partnerID=40&md5=8fb6ec0e52860e6e8e381822049eedc4,scopus,"This paper estimates U.S. M1 demand functions from 1960-1988, addressing apparent instability by proposing a new model that includes inflation, real income, bond yields, interest rates, and yields on new instruments. The dynamic error-correction model is shown to be stable and more accurate than previous models, explaining earlier ""breakdowns"" by highlighting the importance of its distinctive features.",True,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:20:53.531026
3a2fe3ca49a5826d,The determinants of main stock exchange index changes in emerging countries: evidence from Turkey in COVID-19 pandemic age,"With the emergence and spreading of COVID-19 pandemic all over the world, the uncertainty has been increasing for countries. Depending on this condition, especially emerging countries have been affected negatively by foreign portfolio investment outflows from stock exchanges, and main stock exchange indices have been collapsed. The study examines the causes of the main stock exchange index changes in Turkey in the COVID-19 period. In this context, 14 variables (3 global, 6 country-level, 5 market-level) are analyzed by employing random forest and support vector machine algorithms and using daily data between 01.02.2020 and 05.15.2020, which includes the pre-pandemic and the pandemic periods. The findings prove that (i) the most important variables are the retention amount of foreign investors in the equity market, credit default swap spreads, government bonds interest rates, Morgan Stanley Capital International (MSCI) emerging markets index, and volatility index in the pre-pandemic period; (ii) the importance of variables changes as MSCI emerging markets index, the volatility index, retention amount of foreign investors in the equity market, amount of securities held by the Central Bank of Republic of Turkey (CBRT), equity market traded value in the pandemic period; (iii) support vector machine has superior estimation accuracy concerning random forest algorithms in both pre-pandemic and pandemic period.","Kartal, Mustafa Tevfik; Depren, Ozer; Depren, Serpil Kilic",2020,10.3934/qfe.2020025,,wos,"This study investigates the determinants of changes in Turkey's main stock exchange index during the COVID-19 pandemic using daily data from February to May 2020. It employs Random Forest and Support Vector Machine algorithms to analyze 14 variables, including global, country-level, and market-level factors. The findings highlight the varying importance of factors like foreign investor retention, credit default swap spreads, government bond interest rates, and market indices in pre-pandemic versus pandemic periods, with SVM showing superior accuracy.",True,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:21:22.048618
1254037e95e027a2,The dynamic relationship between the prices of ADRs and their underlying stocks: evidence from the threshold vector error correction model,"This paper sets out to estimate the dynamic relationship that exists between the prices of ADRs and their underlying stocks, in both the short run and the long run, using a number of recent developments of the threshold cointegration framework. The empirical results support the notion of nonlinear mean reversion of the prices of ADRs and their underlying stocks.","Chung, HM; Ho, TW; Wei, LJ",2005,10.1080/00036840500218729,,wos,"This paper investigates the dynamic relationship between ADR prices and their underlying stocks using a threshold vector error correction model, finding evidence of nonlinear mean reversion.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:21:24.296234
d8878047801020d8,The economic value of advanced time series methods for modelling and trading 10-year government bonds,"The motivation for this paper is to determine the potential economic value of advanced modelling methods for devising trading decision tools for 10-year Government bonds. Two advanced methods are used: time-varying parameter models with the implementation of state space modelling using a Kalman filter and nonparametric nonlinear models with Neural Network Regression (NNR). These are benchmarked against more traditional forecasting techniques to ascertain their potential as a forecasting tool and their economic value as a base for a trading decision tool. The models were developed using data from the UK Gilt market, US T-Bond market and German Bund market. Using in-sample data from April 2001 to January 2003 to develop the models, their results were assessed using the out-of-sample period of January 2003 to June 2003. Performance evaluation was based upon forecasting accuracy measures and financial criteria using a simulated trading strategy incorporating realistic trading costs. It is concluded that for the time series studied and for the period under investigation, the performance of the advanced models is mixed. While the NNR models have the ability to forecast the 10-year Government bond yield and add economic value as a trading decision tool, the Kalman filter models' performance is not as conclusive. The Kalman filter models outperformed the traditional techniques using forecasting accuracy measures, however they did not perform as well in the simulated trading strategy. Reprinted by permission of Routledge, Taylor and Francis Ltd.",,2007,10.1080/13518470600880010,,proquest,"This paper evaluates the economic value of advanced time series methods, specifically time-varying parameter models with Kalman filtering and Neural Network Regression (NNR), for modeling and trading 10-year government bonds. These methods are compared against traditional forecasting techniques using data from UK, US, and German bond markets. The study found that NNR models showed potential for forecasting bond yields and adding economic value to trading strategies, while Kalman filter models had mixed results, outperforming traditional methods in accuracy but not in simulated trading.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-13T17:21:36.264152
fcd1ed39b63802de,The effect of managerial ownership on the cost of debt: Evidence from Japan,"This article examines the effect of managerial ownership (MO) on the cost of debt as measured by the interest rate spread on corporate bonds for Japanese firms. First, the authors find that the MO is positively associated with interest rate spread after controlling for the other Japanese ownership structure, cross-shareholdings, and the stable shareholdings by financial institutions. Second, by employing factor analysis to measure the agency cost of debt (ACD) based on financial variables, the authors also find that MO has higher correlation with interest rate spread when the ACD at the time of bond issue is already larger. The results are robust to additional analyses, including the possibility of nonlinear relationship, bond rating, endogeneity problem, and Fama and MacBeth approach. The results suggest that prospective bondholders use MO information to anticipate a firm's future ACD and estimate it higher when the current ACD at issuing bond is already larger. The results also suggest that accounting information is useful to estimate the ACD and increase the efficiency of bond contracting. Finally, although previous studies are often prone to emphasizing the findings on the Japanese unique ownership structure, the results of this article reveal that traditional agency theory on MO apply to Japanese bond market, which is consistent with the findings of U.S. firms. © The Author(s) 2011. © 2011 Elsevier B.V., All rights reserved.","Shuto, A.; Kitagawa, N.",2011,10.1177/0148558x11401553,https://www.scopus.com/inward/record.uri?eid=2-s2.0-81255200514&doi=10.1177%2F0148558X11401553&partnerID=40&md5=27550a2d37ccf3c421fd165a5c3d8a92,scopus,"This study investigates the relationship between managerial ownership (MO) and the cost of debt (interest rate spread on corporate bonds) in Japanese firms. It finds a positive association between MO and the interest rate spread, suggesting that higher MO leads to a higher cost of debt. The study also explores the role of agency costs of debt and concludes that traditional agency theory regarding MO is applicable to the Japanese bond market, similar to findings in the U.S.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:21:39.097977
09eb13bc70199528,The fundamentals of commodity futures returns,"Commodity futures risk premiums vary across commodities and over time depending on the level of physical inventories. The convenience yield is a decreasing, nonlinear function of inventories. Price measures, such as the futures basis, prior futures returns, prior spot returns, and spot price volatilities reflect the state of inventories and are informative about commodity futures risk premiums. We verify these theoretical predictions using a comprehensive data set on 31 commodity futures and physical inventories between 1971 and 2010. We find no evidence that the positions of participants in futures markets predict risk premiums on commodity futures. © 2012 The Authors. © 2012 Elsevier B.V., All rights reserved.","Gorton, G.B.; Hayashi, F.; Rouwenhorst, K.G.",2013,10.1093/rof/rfs019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871192702&doi=10.1093%2Frof%2Frfs019&partnerID=40&md5=16b0c06ebb4692992c6f341dd79c6265,scopus,"This paper investigates the relationship between commodity futures risk premiums and physical inventory levels. It proposes that the convenience yield is a nonlinear function of inventories and that various price measures can predict risk premiums. The study uses data from 31 commodity futures between 1971 and 2010 to test these predictions, finding no evidence that futures market participant positions predict risk premiums.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:21:41.432442
d9eba663f81460f3,The impact of oil price shocks on Turkish sovereign yield curve,"Purpose>This paper aims to investigate the impact of oil price shocks on the Turkish sovereign yield curve factors.Design/methodology/approach>To extract the latent factors (level, slope and curvature) of the Turkish sovereign yield curve, we estimate conventional Nelson and Siegel (1987) model with nonlinear least squares. Then, we decompose oil price shocks into supply, demand and risk shocks using structural VAR (structural VAR) models. After this separation, we apply Engle (2002) dynamic conditional correlation GARCH (DCC-GARCH (1,1)) method to investigate time-varying co-movements between yield curve factors and oil price shocks. Finally, using the LP (local projections) proposed by Jorda (2005), we estimate the impulse-response functions to examine the impact of different oil price shocks on yield curve factors.Findings>Our results demonstrate that the various oil price shocks influence the yield curve factors quite differently. A supply shock leads to a statistically significant increase in the level factor. This result shows that elevated oil prices due to supply disruptions are interpreted as a signal of a surge in inflation expectations since the cost channel prevails. Besides, unanticipated demand shocks have a positive impact on the slope factor as a result of the central bank policy response for offsetting the elevated inflation expectations. Finally, a risk shock is associated with a decrease in the curvature factor indicating that risk shocks influence the medium-term bonds due to the deflationary pressure resulting from depressed economic conditions.Practical implications>Our results provide new insights to understand the driving forces of yield curve movements induced by various oil shocks to formulate appropriate policy responses.Originality/value>The study contributes to the literature by two main dimensions. First, the recent oil shock identification scheme of Ready (2018) is modified using the “geopolitical oil price risk index” to capture the changes in the risk perceptions of oil markets driven by geopolitical tensions such as terrorism and conflicts and sanctions. The modified identification scheme attributes more power to demand shocks in explaining the variation of the oil price compared to that of the baseline scheme. Second, it provides recent evidence that distinguishes the impact of oil demand and supply shocks on Turkey's yield curve.",,2022,10.1108/ijoem-06-2020-0681,,proquest,"This paper investigates how different types of oil price shocks (supply, demand, and risk) affect the Turkish sovereign yield curve factors (level, slope, and curvature). Using advanced econometric models like structural VAR, DCC-GARCH, and local projections, the study finds that supply shocks increase the level factor, demand shocks impact the slope factor, and risk shocks affect the curvature factor. The research contributes by modifying oil shock identification and providing recent evidence specific to Turkey.",True,True,False,gemini-2.5-flash-lite,Trine,Y,,2025-10-13T17:22:08.166863
ae0436eaafdd494a,The impact of serial correlation on testing for structural change in binary choice model: Monte Carlo evidence,"This paper examines the finite sample properties of structural change tests with an unknown breakpoint for the probit model in the presence of serial correlation. The combination of structural change and serial correlation renders model estimation challenging, affecting the consistency of coefficient estimates. Although there is vast literature concerning structural change tests for linear time series models, the literature for such tests in the context of binary choice models is somewhat sparse. More importantly, the empirical literature has applied the standard tests of structural change on the discrete choice model, despite the fact that most of these tests were developed specifically for the linear regression model. Subsequently, the theoretical properties of these tests in the context of non-linear models are unknown. This includes the class of discrete choice models, such as probit and logit. The issue becomes even more complicated in the presence of serial correlation, since typical tests for structural change often require the assumption of independence in the error terms. Even when the tests allow for a weakly dependent structure in the data, their finite sample performance remains unknown. This paper conducts simulation analysis on the size of 'supremum' Wald, LR and LM tests for structural change in the context of the probit model with varying levels of serial correlation. It is found that the shortcomings of the tests in linear models are magnified in probit models. In particular, the tests exhibit greater size distortion for the probit model than the linear model with the same level of serial correlation. Bootstrapping is also considered as an alternative approach to obtaining critical values, and though it reduces the size distortion in finite samples, it is unable to accommodate the distortion associated with a high level of serial correlation. (C) 2012 IMACS. Published by Elsevier B.V. All rights reserved.","Chan, Felix; Pauwels, Laurent L.; Wongsosaputro, Johnathan",2013,10.1016/j.matcom.2012.11.001,,wos,"This paper investigates the performance of structural change tests in probit models with serial correlation, finding that standard tests designed for linear models perform poorly and exhibit significant size distortion in non-linear contexts. Bootstrapping offers some improvement but cannot fully address the issues caused by high serial correlation.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:22:13.315851
3708ca7e2a4ab89a,The impact of sponsorship announcements on shareholder wealth in Australia,"Purpose – The purpose of this paper is to examine the impact of 51 sponsorship announcements upon the stock prices of firms sponsoring in Australia. The research examines the broader question of whether sponsorship has the potential to transcend cultural boundaries and contribute to financial performance in regional markets. Design/methodology/approach – The methodology is based on the event study technique which is applied to the estimation of excess returns that arise in response to announcements of corporate sponsorship made by leading industrial stocks trading on the Australian Stock Exchange. Regressions examine whether the cost and duration of sponsorship signal information of importance to investors regarding the financial prospects of sponsoring firms. Findings – A small, fleeting positive increase in wealth effects is observed indicating that economically, sponsorship expenditure in Australia is more or less value neutral. While investors appear indifferent to sponsorship cost, they value shortterm sponsorships of less than two years in particular. Research limitations/implications – Future research needs to examine the role of associated variables such as contract size and length, and the type and level of sponsorship investment. Originality/value – For firms, the study indicates that sponsorship in smaller regional markets should be valued by investors especially when firms keep the duration of the sponsorship as short. As stock prices tend to rise briefly following sponsorship announcements, marketers should leverage sponsorships immediately to gain the attention of investors. For a regional market, short and sharp sponsorships appear to be the optimal approach. © 2010, Emerald Group Publishing Limited © 2016 Elsevier B.V., All rights reserved.","Johnston, M.A.",2010,10.1108/13555851011026926,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958474981&doi=10.1108%2F13555851011026926&partnerID=40&md5=d22fd09480a4e4c7d08d82235d6f52f7,scopus,"This study investigates the impact of 51 sponsorship announcements on the stock prices of Australian sponsoring firms using an event study technique. It found a small, temporary positive increase in wealth effects, suggesting sponsorship expenditure is largely value-neutral. Investors seem indifferent to sponsorship cost but value shorter-term sponsorships (under two years). The research implies that for regional markets, short and sharp sponsorships are optimal, and marketers should leverage them immediately.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:22:15.031894
2c0b739e8ae2211a,The information content of option-implied volatility for credit default swap valuation,"Credit default swaps (CDS) are similar to out-of-the-money put options in that both offer a low cost and effective protection against downside risk. This study investigates whether put option-implied volatility is an important determinant of CDS spreads. Using a large sample of firms with both CDS and options data, we find that individual firms put option-implied volatility dominates historical volatility in explaining the time-series variation in CDS spreads. To understand this result, we show that implied volatility is a more efficient forecast for future realized volatility than historical volatility. More importantly, the volatility risk premium embedded in option prices covaries with the CDS spread. These findings complement existing empirical evidence based on market-level data. (C) 2010 Elsevier B.V. All rights reserved.","Cao, Charles; Yu, Fan; Zhong, Zhaodong",2010,10.1016/j.finmar.2010.01.002,,wos,"This study examines the relationship between option-implied volatility and Credit Default Swap (CDS) spreads. It finds that implied volatility is a significant predictor of CDS spreads, outperforming historical volatility. The research suggests that the volatility risk premium in option prices is linked to CDS spreads, providing empirical evidence for this connection.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:22:17.664943
73a39603932c7788,The informational content of the embedded deflation option in TIPS,"We estimate the value of the embedded option in U.S. Treasury Inflation-Protected Securities (TIPS). The embedded option value exhibits time variation that is correlated with periods of deflationary expectations. We construct embedded option explanatory variables that are statistically and economically significant for explaining future inflation, even in the presence of traditional inflation variables such as lagged inflation, the gold return, the crude oil return, the VIX return, liquidity, surveys, and the yield spread between nominal Treasuries and TIPS. After conducting robustness tests, we conclude that the TIPS embedded option contains useful information for future inflation. (C) 2016 Elsevier B.V. All rights reserved.","Grishchenko, Olesya V.; Vanden, Joel M.; Zhang, Jianing",2016,10.1016/j.jbankfin.2015.12.004,,wos,"This study estimates the value of the embedded option in U.S. Treasury Inflation-Protected Securities (TIPS), finding that its value varies with deflationary expectations. The authors demonstrate that this option provides significant explanatory power for future inflation, even when controlling for traditional inflation predictors. Robustness tests confirm the informational content of the TIPS embedded option for predicting future inflation.",False,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:22:25.063833
f87c91979270a5eb,The market for commonwealth government securities,"This study of the market for Commonwealth government securities in Australia pays particular attention to the influence of interest rate expectations on demand. Attempts to estimate expectations with current and past information, including the use of a Box-Jenkins time series model are unsuccessful. An assumption that interest rate expectations are based on perfect foresight proves valuable in explaining the demand for government securities. The picture of the market which emerges is one of a private sector which acts on new information more quickly than it is incorporated in the official yield curve. © 1980, SAGE Publications. All rights reserved. © 2016 Elsevier B.V., All rights reserved.","Evans, W.H.; Rozenstein, H.A.",1980,10.1177/031289628000500206,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965949166&doi=10.1177%2F031289628000500206&partnerID=40&md5=f511b472fcc843f2453db0ea9c6b3733,scopus,"This study examines the Australian Commonwealth government securities market, focusing on how interest rate expectations affect demand. It found that attempts to estimate expectations using current and past data, including time series models, were unsuccessful. However, assuming perfect foresight regarding interest rates helped explain demand. The market appears to be characterized by a private sector that reacts to new information faster than it is reflected in the official yield curve.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:22:54.858950
02ab8350d2328fd4,The maturity premium,"We show that firms with longer debt maturities earn risk premia not explained by unconditional factors. Embedding dynamic capital structure choices in an asset-pricing framework where the market price of risk evolves with the business cycle, we find that firms with long-term debt exhibit more countercyclical leverage. The induced covariance between betas and the market price of risk generates a maturity premium similar in size to our empirical estimate of 0.21% per month. We also provide direct evidence for the model mechanism and confirm that the maturity premium is consistent with observed leverage dynamics of long- and short-maturity firms.",,2022,10.1016/j.jfineco.2021.07.008,,proquest,"This paper proposes a theoretical asset-pricing model where firms with longer debt maturities earn a risk premium. The model incorporates dynamic capital structure choices and a business-cycle-dependent market price of risk, leading to countercyclical leverage for long-term debt firms. This mechanism generates a maturity premium consistent with empirical estimates.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:22:56.830890
01e6238a15a50b30,The predictive distributions of thinning-based count processes,"This paper shows that the term structure of conditional (i.e. predictive) distributions allows for closed form expression in a large family of (possibly higher order or infinite order) thinning-based count processes such as INAR(p), INARCH(p), NBAR(p), and INGARCH(1,1). Such predictive distributions are currently often deemed intractable by the literature and existing approximation methods are usually time consuming and induce approximation errors. In this paper, we propose a Taylor's expansion algorithm for these predictive distributions, which is both exact and fast. Through extensive simulation exercises, we demonstrate its advantages with respect to existing methods in terms of the computational gain and/or precision. © 2021 Elsevier B.V., All rights reserved.","Lu, Y.",2021,10.1111/sjos.12438,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077400529&doi=10.1111%2Fsjos.12438&partnerID=40&md5=51d29189308bafffdf7533af066f2877,scopus,"This paper presents an exact and fast Taylor's expansion algorithm for the predictive distributions of thinning-based count processes, including INAR(p), INARCH(p), NBAR(p), and INGARCH(1,1). The proposed method overcomes the intractability and computational issues of existing approximation methods, demonstrating superior performance in simulations.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:22:59.015308
d28b32f410fc35cb,The role of term spread and pattern changes in predicting stock returns and volatility of the United Kingdom: Evidence from a nonparametric causality-in-quantiles test using over 250 years of data,"Given the existence of nonlinear relationship between equity premium and term spread, as well as pattern changes and the interaction of pattern changes with the term-spread and changes in the shape of the yield curve, we use a nonparametric k-th order causality-in-quantiles test to predict the movement in excess returns and volatility based on changes in the shape of the yield curve. With the test applied to over 250 years of monthly data for the UK covering the period 1753:08 to 2017:02, we find that pattern changes and the interaction of pattern changes with the term-spread, besides the term spread itself, tends to also play an important role in predicting volatility at the upper end of its conditional distribution. In addition, the effect on excess returns from term spread, pattern changes and the interaction is found to have improved markedly over time, barring at the conditional median of the equity premium. Finally, comparisons are made with historical data of the US and South Africa, and implications of our results are discussed. © 2019 Elsevier B.V., All rights reserved.","Gupta, R.; Risse, M.; Volkman, D.A.; Wohar, M.E.",2019,10.1016/j.najef.2018.05.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047273301&doi=10.1016%2Fj.najef.2018.05.006&partnerID=40&md5=0b1ce5fd52c3e2ea5edef836d785eb8c,scopus,"This study employs a nonparametric causality-in-quantiles test to investigate the predictive power of term spread and pattern changes on UK stock returns and volatility over 250 years. The findings indicate that term spread, pattern changes, and their interaction are significant predictors of volatility, particularly at higher levels. The predictive impact on excess returns has also increased over time, except at the median. Comparisons with US and South African data are also presented.",True,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:23:15.784915
e3dd71ea367283cb,The term structure of Eurozone peripheral bond yields: an asymmetric regime-switching equilibrium correction approach,"Several studies have established the predictive power of the yield curve i.e. the difference between long and short-term bond rates and the role of asymmetries in the term structure of bond yields with respect to real economic activity. Using an extensive dataset, comprising 3-month, 1-year, 5-year and 10-year constant maturity Treasury bonds for the Eurozone southern periphery countries - the so-called PIIGS - from January 1999 to April 2019, we investigate the links between bond yields of different maturities for the Eurozone southern peripheral countries and we find they co-evolve in line with the predictions of the Expectations Hypothesis theory. We demonstrate the presence of nonlinearities in the term structure, and utilize a multivariate asymmetric two-regime Markov-switching VAR methodology to model them properly. Moreover, we address the economic reasoning behind the introduction of an equilibrium-correction regime-switching approach, hence providing potentially important insights on the behaviour of the entire yield curve. We reveal that the regime shifts are related to the state of the business cycle, particularly in economies in which monetary policy decisions are implemented via changes in short-term rates as a response to deviations of output from equilibrium levels. Our results may have important statistical and economic implications on the behaviour of the term structure of bond yields.","Avdoulas, Christos; Bekiros, Stelios; Lucey, Brian",2020,10.1515/snde-2018-0105,,wos,"This study investigates the term structure of Eurozone peripheral bond yields using a multivariate asymmetric two-regime Markov-switching VAR methodology. It finds that bond yields co-evolve according to the Expectations Hypothesis and demonstrates nonlinearities and regime shifts related to the business cycle. The approach provides insights into the behavior of the entire yield curve, particularly in economies where monetary policy responds to output deviations.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-13T17:23:30.197523
1c272d0569feb4e8,The term structure of Japanese interest rates: The equilibrium spread with asymmetric dynamics,"This paper examines the dynamic adjustment to long-run relationship between Japanese interest rates of different maturities. We employ a new estimation methodology that permits threshold and the momentum-threshold adjustment towards equilibrium. The results support the expectations hypothesis of the term structure of interest rate using Japanese interest rates. As in the case of the United States, it shown that the error-correction process is best estimated as asymmetric. © 2003 Elsevier Inc. All rights reserved. © 2017 Elsevier B.V., All rights reserved.","Kuo, S.-H.; Enders, W.",2004,10.1016/s0889-1583(03)00046-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1442355381&doi=10.1016%2FS0889-1583%2803%2900046-7&partnerID=40&md5=c7948c0dbdc2c0c80ba7960123a64018,scopus,"This paper investigates the dynamic relationship between Japanese interest rates of different maturities, utilizing a novel estimation method that incorporates threshold and momentum-threshold adjustments to equilibrium. The findings support the expectations hypothesis of the term structure of interest rates, indicating that the error-correction process is best modeled as asymmetric, similar to observations in the United States.",False,True,False,gemini-2.5-flash-lite,Trine,M,,2025-10-13T17:24:02.548682
8247c28423e466eb,The “probability of recession”: Evaluating probabilistic and non-probabilistic forecasts from probit models of U.S. recessions,"This letter evaluates forecasts from probit models that use the slope of the yield curve to forecast recessions. These models give reliable non-probabilistic warnings of recessions, but the estimated probabilities do not match the conditional frequency of recession months.",,2013,10.1016/j.econlet.2013.09.002,,proquest,"This study evaluates probit models that forecast U.S. recessions using the yield curve slope. While the models provide reliable non-probabilistic recession warnings, the estimated probabilities do not accurately reflect the actual frequency of recession months.",True,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:24:16.899437
38b1d9a67bf5a236,Third-party Logistics in Bio-medical Waste System: a Path Towards a Risk-free Sector,"After the sudden advent of COVID-19, the amount of medical waste has escalated to a great extent. The incremented medical waste amidst the pandemic exposes the improper waste management system of various developing countries. India, being one of the prominent developing countries, produces the largest waste in the world. Nonetheless, the Indian waste management system is not able to manage the massive amount of waste generated. Henceforth, this research study approaches to reveal the prominent factors which are causing failure in the system of medical waste management in India. This manuscript mainly focuses on two aspects. Firstly, this paper illuminates the factors which are hindering medical waste management by third-party logistics (3PL). Secondly, this study discusses a unique interval-value intuitionistic fuzzy set (IVIFS) based on Decision Making Trial and Evaluation Laboratory (DEMATEL) to depict graphical causal interrelationships among the factors. In addition, the analytic network process (ANP) is utilized to estimate the influence ranking of each factor. The results of this research anticipate that the transportation and disposal-related constraining factors require more attention from 3PL managers. The current study is unique as it enriches the various hindering factors on 3PL BMW management by discussing the ranking and relationship among factors. © 2022 Elsevier B.V., All rights reserved.","Dwivedi, N.; Sharma, H.; Shanker, S.; Barve, A.",2022,10.1007/s41660-022-00259-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131091442&doi=10.1007%2Fs41660-022-00259-x&partnerID=40&md5=a5ee5ac604d47fbacde57222e113ccfe,scopus,"This study investigates the challenges faced by third-party logistics (3PL) in managing bio-medical waste (BMW) in India, particularly in the context of the COVID-19 pandemic. It identifies hindering factors and uses a combination of Decision Making Trial and Evaluation Laboratory (DEMATEL) with interval-value intuitionistic fuzzy sets (IVIFS) and Analytic Network Process (ANP) to analyze causal relationships and rank the influence of these factors. The research highlights the need for greater attention to transportation and disposal-related constraints within the 3PL BMW management system.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:24:19.114770
ded66d2a8d3a91c4,Threshold Dynamics of Short-term Interest Rates: Empirical Evidence and Implications for the Term Structure,"This paper studies a nonlinear one-factor term structure model in discrete time. The short-term interest rate follows a self-exciting threshold autoregressive (SETAR) process that allows for shifts in the intercept and the variance. In comparison with a linear model, we find empirical evidence in favour of the threshold model for Germany and the US. Based on the estimated short-rate dynamics we derive the implied arbitrage-free term structure of interest rates. Since analytical solutions are not feasible, bond prices are computed by means of Monte Carlo integration. The resulting term structure captures stylized facts of the data. In particular, it implies a nonlinear relation between long rates and the short rate.",,2008,10.1111/j.1468-0300.2008.00189.x,,proquest,"This paper investigates a nonlinear term structure model for short-term interest rates using a SETAR process, finding empirical support for this nonlinear approach over linear models in Germany and the US. It derives the implied arbitrage-free term structure and uses Monte Carlo integration to compute bond prices, capturing stylized facts and implying a nonlinear relationship between long and short rates.",True,True,False,gemini-2.5-flash-lite,Trine,M,,2025-10-13T17:26:08.933713
c0e7db18be904853,"Time-varying market, interest rate, and exchange rate risk premia in the US commercial bank stock returns","This paper examines the role of market, interest rate, and exchange rate risks in pricing a sample of the US Commercial Bank stocks by developing and estimating a multi-factor model under both unconditional and conditional frameworks. Three different econometric methodologies are used to conduct the estimations and testing. Estimations based on nonlinear seemingly unrelated regression (NLSUR) via GMM approach indicate that interest rate risk is the only priced factor in the unconditional three-factor model. However, based on 'pricing kernel' approach by Dumas and Solnik [(1995). J. Finance 50, 445-479], strong evidence of exchange rate risk is found in both large bank and regional bank stocks in the conditional three-factor model with time-varying risk prices. Finally, estimations based on the multivariate GARCH in mean (MGARCH-M) approach where both conditional first and second moments of bank portfolio returns and risk factors are estimated simultaneously show strong evidence of time-varying interest rate and exchange rate risk premia and weak evidence of time-varying world market risk premium for all three bank portfolios, namely those of Money Center bank, Large bank, and Regional bank. © 2000 Elsevier Science B.V. All rights reserved. JEL classification: C32; G12; G21. © 2020 Elsevier B.V., All rights reserved.","Tai, C.S.",2000,10.1016/s1042-444x(00)00031-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034416407&doi=10.1016%2Fs1042-444x%2800%2900031-1&partnerID=40&md5=5c47f8f299b31f1053bf00598a4e8e5c,scopus,"This paper investigates the impact of market, interest rate, and exchange rate risks on US commercial bank stock returns using a multi-factor model. Different econometric methods (NLSUR via GMM, pricing kernel approach, and MGARCH-M) reveal that interest rate risk is priced unconditionally. Conditionally, exchange rate risk is significant, and the MGARCH-M approach shows time-varying interest rate and exchange rate risk premia, with weak evidence for market risk premium.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:26:13.248564
430a63506326c22c,Time-varying risk of nominal bonds: How important are macroeconomic shocks?,"I study the sufficiency of macroeconomic information to explain the time-variation in second moments of stock and bond returns, with a particular attention to stock-bond correlations. I propose an external habit model supplemented with realistic non-Gaussian fundamentals estimated solely from macroeconomic data. Intertemporal smoothing and precautionary savings effects – driven by consumption shocks – combine with a time-varying covariance between consumption and inflation to generate large positive and negative stock-bond return correlations. Macroeconomic shocks are most important in explaining second moments of stock and bond returns from the late 1970’s to mid-1990’s and during the Great Recession. © 2022 Elsevier B.V., All rights reserved.","Ermolov, A.",2022,10.1016/j.jfineco.2022.04.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129401050&doi=10.1016%2Fj.jfineco.2022.04.003&partnerID=40&md5=1d94bf5d3b46c594a19799c91da57432,scopus,"This study investigates whether macroeconomic information can explain the time-varying second moments of stock and bond returns, focusing on stock-bond correlations. An external habit model with non-Gaussian fundamentals, estimated from macroeconomic data, is proposed. Consumption shocks and a time-varying covariance between consumption and inflation generate significant stock-bond return correlations. Macroeconomic shocks are found to be particularly influential during the late 1970s to mid-1990s and the Great Recession.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:26:22.800128
dba4d319451eec06,Tools for non-linear time series forecasting in economics - An empirical comparison of regime switching vector autoregressive models and recurrent neural networks,"The purpose of this study is to contrast the forecasting performance of two non-linear models, a regime-switching vector autoregressive model (RS-VAR) and a recurrent neural network (RNN), to that of a linear benchmark VAR model. Our specific forecasting experiment is U.K. inflation and we utilize monthly data from 1969 to 2003. The RS-VAR and the RNN perform approximately on par over both monthly and annual forecast horizons. Both non-linear models perform significantly better than the VAR model.","Binner, JM; Elger, T; Nilsson, B; Tepper, JA",2004,10.1016/s0731-9053(04)19003-8,,wos,This study compares the forecasting performance of a regime-switching vector autoregressive model (RS-VAR) and a recurrent neural network (RNN) against a linear benchmark VAR model using UK inflation data from 1969 to 2003. Both non-linear models performed similarly and significantly better than the linear model for both monthly and annual forecast horizons.,True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:26:37.749867
3f9abe9daf6dd16e,Towards ubiquitous information supply for individual investors: A decision support system design,"This paper introduces an IT artifact called MoFiN DSS that comprises hard- and software components that provide the basis for a prototype of a financial decision support system (DSS) to support individual investors reacting to unforeseen market events. We have derived our motivation for building such a system design from behavioral finance research. Analyses of the behavior of individual investors provide evidence that this segment does react more significantly to any public news published compared to institutional investors. On the other hand, the analyses show that they react significantly slower than their institutional counterparts. Since empirical intraday event study analyses show that capital markets react promptly to new information and that excess returns decrease over a specific period of time, individual investors miss significant trading opportunities due to their current strategies of information research. We address the problem that this market segment is not able to continuously observe diverse information channels and to assess all the new information available. Our prototype decision support system continuously observes company announcements and forecasts their potential impact on the corresponding stock price. After identifying those events for which significant market reactions can be expected, wireless push-based message services provide the technical basis for prompt and location-independent information supply. Based on a novel simulation-based evaluation methodology we have developed, we demonstrate and quantify the advantages that the developed system provides to the individual investors. © 2009 Elsevier B.V. All rights reserved. © 2009 Elsevier B.V., All rights reserved.","Muntermann, J.",2009,10.1016/j.dss.2009.01.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-64949122389&doi=10.1016%2Fj.dss.2009.01.003&partnerID=40&md5=63342a2f3bab4265f4a2068b33fff64b,scopus,"This paper presents MoFiN DSS, a financial decision support system designed to help individual investors react to market events. It addresses the tendency of individual investors to react slower to news than institutional investors, leading to missed trading opportunities. The system continuously monitors company announcements, predicts their impact on stock prices, and uses push notifications for timely, location-independent information delivery. A novel simulation-based methodology is used to evaluate the system's benefits.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:26:40.347290
f19ff24e943e287a,Tracking market and non-traditional sources of risks in procyclical and countercyclical hedge fund strategies under extreme scenarios: a nonlinear VAR approach,"The subprime crisis was quite damaging for hedge funds. Using the local projection method (Jordà 2004, 2005, 2009), we forecast the dynamic responses of the betas of hedge fund strategies to macroeconomic and financial shocks-especially volatility and illiquidity shocks-over the subprime crisis in order to investigate their market timing activities. In a robustness check, using TVAR (Balke 2000), we simulate the reaction of hedge fund strategies' betas in extreme scenarios allowing moderate and strong adverse shocks. Our results show that the behavior of hedge fund strategies regarding the monitoring of systematic risk is highly nonlinear in extreme scenarios-especially during the subprime crisis. We find that countercyclical strategies have an investment technology which differs from procyclical ones. During crises, the former seek to capture non-traditional risk premia by deliberately increasing their systematic risk while the later focus more on minimizing risk. Our results suggest that the hedge fund strategies' betas respond more to illiquidity uncertainty than to illiquidity risk during crises. We find that illiquidity and VIX shocks are the major drivers of systemic risk in the hedge fund industry.The subprime crisis was quite damaging for hedge funds. Using the local projection method (Jordà 2004, 2005, 2009), we forecast the dynamic responses of the betas of hedge fund strategies to macroeconomic and financial shocks-especially volatility and illiquidity shocks-over the subprime crisis in order to investigate their market timing activities. In a robustness check, using TVAR (Balke 2000), we simulate the reaction of hedge fund strategies' betas in extreme scenarios allowing moderate and strong adverse shocks. Our results show that the behavior of hedge fund strategies regarding the monitoring of systematic risk is highly nonlinear in extreme scenarios-especially during the subprime crisis. We find that countercyclical strategies have an investment technology which differs from procyclical ones. During crises, the former seek to capture non-traditional risk premia by deliberately increasing their systematic risk while the later focus more on minimizing risk. Our results suggest that the hedge fund strategies' betas respond more to illiquidity uncertainty than to illiquidity risk during crises. We find that illiquidity and VIX shocks are the major drivers of systemic risk in the hedge fund industry.",,2022,10.1186/s40854-021-00316-3,,proquest,"This study uses a nonlinear VAR approach to analyze hedge fund strategies' responses to market and non-traditional risks during extreme scenarios, particularly the subprime crisis. It finds that countercyclical strategies increase systematic risk during crises to capture non-traditional risk premia, while procyclical strategies focus on risk minimization. Illiquidity and VIX shocks are identified as key drivers of systemic risk in the hedge fund industry.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:26:48.182805
82d9f4b0a102aaea,Tractable nonlinear production planning models for semiconductor wafer fabrication facilities,"We describe a simulation study of a production planning model for multistage production inventory systems that reflects the nonlinear relationship between resource utilization and lead time. The model is based on the use of clearing functions that capture the nonlinear relationship between workload and throughput. We show how these clearing functions can be estimated from empirical data using a simulation model as a surrogate for observation of the production system under study. We then examine the sensitivity of the estimated clearing function to different dispatching algorithms, different demand patterns, and production planning techniques. Computational experiments based on a scaled-down model of a semiconductor wafer fabrication facility illustrate the potential benefits of the clearing function model relative to conventional linear programming models.",J. Asmundsson; R. L. Rardin; R. Uzsoy,2006,10.1109/tsm.2005.863214,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1588867,ieeexplore,"This paper presents a simulation study of a production planning model for multistage production inventory systems, focusing on the nonlinear relationship between resource utilization and lead time. It uses clearing functions to model workload and throughput, estimating them from empirical data via simulation. The study examines the sensitivity of these functions to dispatching algorithms, demand patterns, and planning techniques, using a semiconductor fabrication facility model to demonstrate benefits over linear programming models.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:26:58.282927
e33e0490e032325b,Trading the FX volatility risk premium with machine learning and alternative data,"In this study, we show how both machine learning and alternative data can be successfully leveraged to improve and develop trading strategies. Starting from a trading strategy that harvests the EUR/USD volatility risk premium by selling one-week straddles every weekday, we present a machine learning approach to more skillfully time new trades and thus prevent unfavorable ones. To this end, we build probability-calibrated Random Forests on various predictors, extracted from both traditional market data and financial news, to predict the closing Sharpe ratio of short one-week delta-hedged straddles. We then demonstrate how the output of these calibrated machine learning models can be used to engineer intuitive new trading strategies. Ultimately, we show that our proposed strategies outperform the original strategy on risk-based performance measures. Moreover, the features that we derived from financial news articles significantly improve the performance of the approach. © 2022 Elsevier B.V., All rights reserved.","Dierckx, T.; Davis, J.; Schoutens, W.",2022,10.1016/j.jfds.2022.07.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135525638&doi=10.1016%2Fj.jfds.2022.07.001&partnerID=40&md5=7121ebeb12981bacae46150f14854560,scopus,"This study demonstrates the use of machine learning and alternative data to enhance trading strategies for the EUR/USD volatility risk premium. A Random Forest model, using traditional market data and financial news, predicts the Sharpe ratio of short straddles to time trades more effectively. The proposed strategies, particularly those incorporating financial news features, show improved risk-based performance compared to the original strategy.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:26:59.932763
361f9221ee0d7559,Transition densities for interest rate and other nonlinear diffusions,"This paper applies to interest rate models the theoretical method developed in Ait-Sahalia (1998) to generate accurate closed-form approximations to the transition function of an arbitrary diffusion. While the main focus of this paper is on the maximum-likelihood estimation of interest rate models with otherwise unknown transition functions, applications to the valuation of derivative securities are also briefly discussed.","Aït-Sahalia, Y",1999,10.1111/0022-1082.00149,,wos,"This paper adapts a theoretical method to approximate transition functions of diffusions, focusing on interest rate models for maximum-likelihood estimation and derivative valuation.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:27:07.065110
deab06de18be4232,U.S. leveraged loan and debt markets: Implications for optimal portfolio and hedging,"This paper offers fresh empirical evidence on the relationship between leverage loans and US debt markets by investigating the distributional predictability and directional predictability between leveraged loans and treasury bonds, fixed income securities and corporate bonds in the U.S economy. We use daily price data from January 2013 to April 2021. First, we analyze the causal relationship between variables by applying non-parametric causality-in-quantiles test and find that quantile causality in variance shows the stronger impact of leverage loan market returns on US debt market returns over the entire quantile range. Second, quantile dependence and directional predictability between leverage loan market and US debt markets are analyzed by applying cross-quantilogram approach and estimated results show the heterogeneous quantile relations from leverage loan market to US debt market. Moreover, the cross-quantile correlation results demonstrate the evidence of negative predictability from leverage loan market to US debt market in low, medium and high quantile range. These evidences are important for US investors and portfolio managers. © 2023 Elsevier B.V., All rights reserved.","Abakah, E.J.A.; Nasreen, S.; Tiwari, A.K.; Lee, C.-C.",2023,10.1016/j.irfa.2023.102514,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148346724&doi=10.1016%2Fj.irfa.2023.102514&partnerID=40&md5=c81900d99e18ad5d1fd4f21f01d0a1fd,scopus,"This paper empirically investigates the relationship between leveraged loans and U.S. debt markets (treasury bonds, fixed income securities, and corporate bonds) from January 2013 to April 2021. Using non-parametric causality-in-quantiles tests and a cross-quantilogram approach, the study finds that leveraged loan market returns have a stronger impact on U.S. debt market returns across quantiles. It also reveals heterogeneous quantile relations and negative predictability from the leveraged loan market to the U.S. debt market, which is significant for investors and portfolio managers.",False,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T17:27:29.951724
1a56df038c2e8350,Uncovering nonlinear dependencies in the Treasury-funds rate spread: Quantile-based explanation,"This study examines the structural dynamics of the spread between the 10-year Treasury yield and the federal funds rate, a key indicator of U.S. financial conditions. Cross-quantilogram analysis reveals a nonlinear dependency across adjacent periods, with stronger connectedness observed in the tail distribution than in the middle. Additionally, the functional quantile autoregression model confirms the spread's nonlinear and asymmetric nature from a distributional perspective. Specifically, higher quantiles of the previous spread exert a stronger influence on the current spread, indicating a positive persistence mechanism. Conversely, lower quantiles of the previous spread negatively affect the higher quantiles of the current spread. These findings suggest that bullish market conditions tend to sustain themselves, whereas bearish conditions hinder upward momentum, underscoring the need for quantile-specific policy interventions.","Meng, Fanyu",2025,10.1016/j.frl.2025.107216,,wos,"This study uses cross-quantilogram and functional quantile autoregression to analyze the nonlinear and asymmetric relationship between the 10-year Treasury yield and the federal funds rate spread. It finds stronger dependencies in the tails of the distribution and suggests that market conditions tend to persist, with higher quantiles influencing current spreads more strongly. The authors advocate for quantile-specific policy interventions.",True,True,False,gemini-2.5-flash-lite,Trine,M,,2025-10-13T17:27:49.933895
164d3e1462675b58,Unstable volatility: the break-preserving local linear estimator,"The objective of this paper is to introduce the break-preserving local linear (BPLL) estimator for the estimation of unstable volatility functions for independent and asymptotically independent processes. Breaks in the structure of the conditional mean and/or the volatility functions are common in Finance. Nonparametric estimators are well suited for these events due to the flexibility of their functional form and their good asymptotic properties. However, the local polynomial kernel estimators are not consistent at points where the volatility function has a break. The estimator presented in this paper generalises the classical local linear (LL). The BPLL estimator maintains the desirable properties of the LL estimator with regard to the bias and the boundary estimation while it estimates the breaks consistently. An extensive Monte Carlo study is shown as well as detailed proofs of the estimator asymptotic behaviour.","Casas, Isabel; Gijbels, Irene",2012,10.1080/10485252.2012.720981,,wos,"This paper introduces the break-preserving local linear (BPLL) estimator for unstable volatility functions in independent and asymptotically independent processes, addressing common breaks in financial conditional mean and volatility functions. The BPLL estimator generalizes the classical local linear estimator, maintaining desirable properties while consistently estimating breaks, unlike standard local polynomial kernel estimators.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:27:52.499251
061c3ab3803363ee,Using Neural Networks to Forecast Volatility for an Asset Allocation Strategy Based on the Target Volatility,"The objective of this study is to use artificial neural networks for volatility forecasting to enhance the ability of an asset allocation strategy based on the target volatility. The target volatility level is achieved by dynamically allocating between a risky asset and a risk-free cash position. However, a challenge to data-driven approaches is the limited availability of data since periods of high volatility, such as during financial crises, are relatively rare. To resolve this issue, we apply a stability-oriented approach to compare data for the current period to a past set of data for a period of low volatility, providing a much more abundant source of data for comparison. In order to explore the impact of the proposed model, the results of this approach will be compared to different volatility forecast methodologies, such as the volatility index, the historical volatility, the exponentially weighted moving average (EWMA), and the generalized autoregressive conditional heteroskedasticity (GARCH) model. Trading measures are used to evaluate the performance of the models for forecasting volatility. An empirical study of the proposed model is conducted using the Korea Composite Stock Price Index 200 (KOSPI 200) and certificate of deposit interest rates from January, 2006 to February, 2016. (C) 2016 The Authors. Published by Elsevier B.V.","Kim, Youngmin; Enke, David",2016,10.1016/j.procs.2016.09.335,,wos,"This study uses neural networks to forecast volatility for an asset allocation strategy based on target volatility. It addresses data limitations by comparing current data to past low-volatility periods and compares its model to other forecasting methods (VIX, historical volatility, EWMA, GARCH) using trading measures for evaluation. The empirical study uses KOSPI 200 and CD rates from 2006-2016.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:27:56.329134
4e973d99311508de,Using the yield curve to forecast economic growth,"This paper finds the yield curve to have a well-performing ability to forecast the real gross domestic product growth in the USA, compared to professional forecasters and time series models. Past studies have different arguments concerning growth lags, structural breaks, and ultimately the ability of the yield curve to forecast economic growth. This paper finds such results to be dependent on the estimation and forecasting techniques employed. By allowing various interest rates to act as explanatory variables and various window sizes for the out-of-sample forecasts, significant forecasts from many window sizes can be found. These seemingly good forecasts may face issues, including persistent forecasting errors. However, by using statistical learning algorithms, such issues can be cured to some extent. The overall result suggests, by scientifically deciding the window sizes, interest rate data, and learning algorithms, many outperforming forecasts be produced for all lags from one quarter to 3 years, although some may be worse than the others due to the irreducible noise of the data.","Yang, Parley Ruogu",2020,10.1002/for.2676,,wos,"This paper investigates the predictive power of the yield curve for US real GDP growth, comparing its performance against professional forecasters and time series models. It highlights that forecast accuracy depends on estimation and forecasting techniques, including the choice of interest rates and window sizes. The study demonstrates that statistical learning algorithms can mitigate issues like persistent forecasting errors, enabling the production of outperforming forecasts across various lags by optimizing window sizes, interest rate data, and learning algorithms.",True,True,True,gemini-2.5-flash-lite,Trine,M,,2025-10-13T17:28:03.801135
b369ae93e8a8645c,VALUATION OF EMBEDDED OPTIONS IN NON-MARKETABLE CALLABLE BONDS: A NEW NUMERICAL APPROACH,"The issue of how to price options embedded in callable bonds has attracted a lot of interest over the years. The usual bond valuation methods rely on yield curves, risk premium, and other parameters to estimate interest rates used in discounted cash flow calculations. The option to retire the bond is, however, neglected in the standard pricing models, causing a systematic overvaluation of callable bonds. In the event of a decline in interest rates, investors are exposed to the risk of a lower return on investment than indicated by the yield to maturity. We propose a novel approach to valuing the risk that the issuer will use the right to buy back the bond at a specific call price. While prior models are focused on valuing marketable callable bonds, we deliver a unique approach to valuing bonds with an embedded European option (or a multiple option) that are traded solely through private transactions. These can typically be characterized by the lack of historical records on transaction prices. The modular character of calculation we propose allows us to take into account additional information, such as probable behaviour of the issuer, available opportunities for achieving alternative earnings or different estimates in terms of interest rate development.","Skalicky, Roman; Zinecker, Marek; Balcerzak, Adam P.; Pietrzak, Michal Bernard; Rogalska, Elzbieta",2022,10.3846/tede.2022.17060,,wos,"This paper proposes a new numerical approach to value embedded options in non-marketable callable bonds, addressing the systematic overvaluation issue in standard pricing models that neglect the issuer's right to retire the bond. The method accounts for factors like issuer behavior and interest rate development, offering a unique approach for bonds traded privately without historical price records.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:28:05.866817
f0282a2efcf8a584,Volatility forecasts embedded in the prices of crude-oil options,"This paper evaluates the ability of alternative option-implied volatility measures to forecast crude-oil return volatility. We find that a corridor implied volatility measure that aggregates information from a narrow range of option contracts consistently outperforms forecasts obtained by the popular Black-Scholes and model-free volatility expectations, as well as those generated by a realized volatility model. This measure ranks favorably in regression-based tests, delivers the lowest forecast errors under different loss functions, and generates economically significant gains in volatility timing exercises. Our results also show that the Chicago Board Options Exchange's oil-VIX index performs poorly, as it routinely produces the least accurate forecasts.","Gilder, Dudley; Tsiaras, Leonidas",2020,10.1002/fut.22114,,wos,"This paper assesses how well different option-implied volatility measures can predict crude-oil return volatility. A novel corridor implied volatility measure, which uses data from a specific range of option contracts, proves superior to common methods like Black-Scholes, model-free expectations, and realized volatility models. This measure also performs well in regression tests, shows lower forecast errors across various loss functions, and offers practical benefits in volatility timing. The study also highlights the poor performance of the Chicago Board Options Exchange's oil-VIX index in forecasting.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:28:16.149470
5e5d1d9ac9087b48,What Does the Individual Option Volatility Smirk Tell Us About Future Equity Returns?,"The shape of the volatility smirk has significant cross-sectional predictive power for future equity returns. Stocks exhibiting the steepest smirks in their traded options underperform stocks with the least pronounced volatility smirks in their options by 10.9% per year on a risk-adjusted basis. This predictability persists for at least 6 months, and firms with the steepest volatility smirks are those experiencing the worst earnings shocks in the following quarter. The results are consistent with the notion that informed traders with negative news prefer to trade out-of-the-money put options, and that the equity market is slow in incorporating the information embedded in volatility smirks.","Xing, Yuhang; Zhang, Xiaoyan; Zhao, Rui",2010,10.1017/s0022109010000220,,wos,"The study finds that the shape of the volatility smirk in stock options predicts future equity returns. Stocks with steeper smirks underperform those with flatter smirks by 10.9% annually on a risk-adjusted basis. This predictability lasts for at least 6 months and is linked to firms experiencing negative earnings shocks, suggesting informed traders use out-of-the-money put options and that markets are slow to incorporate this information.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:28:19.867745
b72cfbf62eb061e3,Yield curve in an estimated nonlinear macro model,"This paper estimates a sticky price macro model with US macro and term structure data using Bayesian methods. The model is solved by a nonlinear method. The posterior distribution of the parameters in the model is found to be bi-modal. The degree of nominal rigidity is high at one mode (''sticky price mode'') but is low at the other mode (''flexible price mode''). I find that the degree of nominal rigidity is important for identifying macro shocks that affect the yield curve. When prices are more flexible, a slowly varying inflation target of the central bank is the main driver of the overall level of the yield curve by changing long-run inflation expectations. In contrast, when prices are more sticky, a highly persistent markup shock is the main driver. The posterior probability of each mode is sensitive to the use of observed proxies for inflation expectations. Ignoring additional information from survey data on inflation expectations significantly reduces the posterior probability of the flexible price mode. Incorporating this additional information suggests that yield curve fluctuations can be better understood by focusing on the flexible price mode. Considering nonlinearities of the model solution also increases the posterior probability of the flexible price mode, although to a lesser degree than using survey data information. All rights reserved, Elsevier",,2011,10.1016/j.jedc.2011.03.003,,proquest,"This paper estimates a sticky price macro model using US macro and term structure data, employing Bayesian methods and a nonlinear solution approach. The model's posterior distribution is bi-modal, indicating either high (sticky price) or low (flexible price) nominal rigidity. The degree of nominal rigidity significantly impacts the identification of macro shocks affecting the yield curve. In the flexible price scenario, central bank inflation targets drive the yield curve's level via long-run inflation expectations. In the sticky price scenario, persistent markup shocks are the primary driver. Survey data on inflation expectations and nonlinear model solutions increase the posterior probability of the flexible price mode, suggesting its greater relevance for understanding yield curve fluctuations.",True,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:29:07.099575
40d5957f898788bd,ZONE-TARGETING MONETARY POLICY PREFERENCES AND FINANCIAL MARKET CONDITIONS: A FLEXIBLE NON-LINEAR POLICY REACTION FUNCTION OF THE SARB MONETARY POLICY,"We estimate a flexible model of the monetary policy reaction function of the South African Reserve Bank based on a representation of the policymaker's preferences that capture asymmetries and zone-targeting behaviours. We augment the analysis to allow for responses to financial market conditions over and above inflation and output stabilisation to address the current debate on the importance of financial asset prices in monetary policy decision making. The empirical results show that the monetary authorities' response to inflation is zone symmetric. Secondly, the monetary authorities' response to output is asymmetric with increased reaction during business cycle downturns relative to upturns. Thirdly, the monetary authorities pay close attention to the financial conditions index by placing an equal weight on financial market booms and recessions.","Naraidoo, Ruthira; Raputsoane, Leroi",2010,10.1111/j.1813-6982.2010.01256.x,,wos,"This study estimates a flexible, non-linear monetary policy reaction function for the South African Reserve Bank, incorporating zone-targeting preferences and responses to financial market conditions. The findings indicate symmetric responses to inflation, asymmetric responses to output (stronger during downturns), and significant attention to financial conditions, with equal weight given to booms and recessions.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-13T17:29:09.233672
6a7a6955695693a6,130 years of fiscal vulnerabilities and currency crashes in advanced economies,"This paper investigates the empirical link between fiscal vulnerabilities and currency crashes in advanced economies over the last 130 years, building on a new data set of real effective exchange rates andfiscal balances for 21 countries since 1880. The paper finds evidence that crashes depend more on prospective fiscal deficits than on actual ones, and more on the composition of public debt (that is, rollover/sudden stop risk) than on its level. The paper also uncovers significant nonlinear effects at high levels of public debt as well as significantly negative risk premiums for major reserve currencies, which enjoy a lower probability of currency crash than other currencies ceteris paribus. Yet, the estimates indicate that such premiums remain small in size relative to the conditional probability of a currency crash if prospective fiscal deficits or rollover/sudden stop risk are high. © 2011 International Monetary Fund. © 2016 Elsevier B.V., All rights reserved.","Fratzscher, M.; Mehl, A.; Vansteenkiste, I.",2011,10.1057/imfer.2011.23,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984735363&doi=10.1057%2Fimfer.2011.23&partnerID=40&md5=10412f6505b457691d3ab1474055c9fc,scopus,"This paper examines the relationship between fiscal vulnerabilities and currency crashes in advanced economies over 130 years using a new dataset. It finds that prospective fiscal deficits and the composition of public debt (rollover/sudden stop risk) are more influential than actual deficits or debt levels. The study also notes nonlinear effects at high debt levels and smaller risk premiums for major reserve currencies, though these premiums are insufficient to mitigate crash probability when fiscal risks are high.",False,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:28:02.344846
adc084ce37ecd5d8,A Bayesian Approach for Dynamic Variation of Specific Sectors in Stock Exchange: A Case Study of Stock Exchange Thailand (SET) Indexes,"This paper aims to investigate the identification of sectors of stock exchange that were positively or negatively driven by fundamental monetary tools.A Bayesian approach for dynamic model averaging (DMA) algorithm is analyzed and implemented to deliver the results for identifying what actually drives specific sector of stock exchange, where the posterior inclusion probability is crucial quantity for this investigation. Stock Exchange Thailand (SET) indexes was used as a case study for this work. The predictors used in this paper are: borrowing rate: BR, policy rate: PR, treasury bill rate: TBR, government bond yield: GBYLT, minimum overdraft rate: MOR, minimum loan rate: MLR, minimum retail rate: MRR, discount rate: DISR, savings rate: SAVER, deposit rate: DEPR and lending rate: LENDR. These factors are considered as the most important monetary policy tools from Bank of Thailand. The empirical results demonstrated that each SET index sector responses to those economic variables differently. Some of them are not actually related for specific point of time; some other times, however, they affect SET indexes. Filtered time-varying parameters allow us to indicate the relationship between stock price return and interest rates. According to statistical evidence, we found that the relationship is as of the time-varying characteristic and is unable to absolute identify whether it is positively or negatively related to stock price return for each sector. The impact of a sector on SET indexes depends on a period of time and can also be considered by using seasoning parameters. © 2020 Elsevier B.V., All rights reserved.","Taveeapiradeecharoen, P.; Aunsri, N.",2020,10.1007/s11277-020-07217-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082804266&doi=10.1007%2Fs11277-020-07217-1&partnerID=40&md5=81cc5fcbff5e37f99c07219bc7a0a7d3,scopus,"This paper uses a Bayesian dynamic model averaging (DMA) algorithm to identify how monetary policy tools affect different sectors of the Stock Exchange of Thailand (SET). The study analyzes the relationship between various interest rates and SET index sectors, finding that these relationships are time-varying and sector-specific. The DMA approach, particularly posterior inclusion probability, is used to determine the influence of predictors.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:28:30.745467
16b768df753f1bf5,A Chaos Analysis of the Dry Bulk Shipping Market,"Finding low-dimensional chaos is a relevant issue as it could allow short-term reliable forecasting. However, the existence of chaos in shipping freight rates remains an open and outstanding matter as previous research used methodology that can produce misleading results. Using daily data, this paper aims to unveil the nonlinear dynamics of the Baltic Dry Index that has been proposed as a measure of the shipping rates for certain raw materials. We tested for the existence of nonlinearity and low-dimensional chaos. We have also examined the chaotic dynamics throughout three sub-sampling periods, which have been determined by the volatility pattern of the series. For this purpose, from a comprehensive view we apply several metric and topological techniques, including the most suitable methods for noisy time series analysis. The proposed methodology considers the characteristics of chaotic time series, such as nonlinearity, determinism, sensitivity to initial conditions, fractal dimension and recurrence. Although there is strong evidence of a nonlinear structure, a chaotic and, therefore, deterministic behavior cannot be assumed during the whole or the three periods considered. Our findings indicate that the generalized autoregressive conditional heteroscedastic (GARCH) model and exponential GARCH (EGARCH) model explain a significant part of the nonlinear structure that is found in the dry bulk shipping freight market.","Inglada-Perez, Lucia; Coto-Millan, Pablo",2021,10.3390/math9172065,,wos,"This paper investigates the presence of low-dimensional chaos in the dry bulk shipping market using the Baltic Dry Index. While evidence suggests a nonlinear structure, chaotic and deterministic behavior could not be confirmed. The study found that GARCH and EGARCH models explain a significant portion of the nonlinearity.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:28:53.905704
32873e86106b0a11,A Class of Non-Gaussian State Space Models With Exact Likelihood Inference,"The likelihood function of a general nonlinear, non-Gaussian state space model is a high-dimensional integral with no closed-form solution. In this article, I show how to calculate the likelihood function exactly for a large class of non-Gaussian state space models that include stochastic intensity, stochastic volatility, and stochastic duration models among others. The state variables in this class follow a nonnegative stochastic process that is popular in econometrics for modeling volatility and intensities. In addition to calculating the likelihood, I also show how to perform filtering and smoothing to estimate the latent variables in the model. The procedures in this article can be used for either Bayesian or frequentist estimation of the model's unknown parameters as well as the latent state variables. Supplementary materials for this article are available online.","Creal, Drew D.",2017,10.1080/07350015.2015.1092977,,wos,"This article presents a method for exact likelihood inference in a class of non-Gaussian state space models, applicable to models with stochastic intensity, volatility, or duration. It also details procedures for filtering and smoothing to estimate latent variables, suitable for both Bayesian and frequentist estimation.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:29:07.841389
0df5735bfdc43af0,A Comparative Analysis of the Choice of Mother Wavelet Functions Affecting the Accuracy of Forecasts of Daily Balances in the Treasury Single Account,"Improving the accuracy of cash flow forecasting in the TSA is key to fulfilling government payment obligations, minimizing the cost of maintaining the cash reserve, providing the absence of outstanding debt accumulation and ensuring investment in financial instruments to obtain additional income. This study aims to improve the accuracy of traditional methods of forecasting the time series compiled from the daily remaining balances in the TSAbased on prior decomposition using a discrete wavelet transform. The paper compares the influence of selecting a mother wavelet out of 570 mother wavelet functions belonging to 10 wavelet families (Haar;Dabeshies; Symlet; Coiflet; Biorthogonal Spline; Reverse Biorthogonal Spline; Meyer; Shannon; Battle-Lemarie; and Cohen–Daubechies–Feauveau) and the decomposition level (from 1 to 8) on the forecast accuracy of time series compiled from the daily remaining balances in the TSA in comparison with the traditional forecasting method without prior timeseries decomposition. The model with prior time series decomposition based on the Reverse Biorthogonal Spline Wavelet [5.5] mother wavelet function, upon the eighth iteration, features the highest accuracy, significantly higher than that of the traditional forecasting models. The choice of the mother wavelet and the decomposition level play an important role in increasing the accuracy of forecasting the daily remaining balances in the TSA.",,2022,10.3390/economies10090213,,proquest,"This study investigates the impact of different mother wavelet functions and decomposition levels on the accuracy of forecasting daily Treasury Single Account (TSA) balances. By comparing wavelet transform-based decomposition with traditional forecasting methods, the research identifies the Reverse Biorthogonal Spline Wavelet [5.5] as yielding the highest accuracy after eight iterations, significantly outperforming traditional models. The findings highlight the importance of wavelet selection and decomposition level for improving TSA balance forecast accuracy.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:29:46.757595
ff08816516693672,"A Comparison of LSTM, GRU, and XGBoost for forecasting Morocco’s yield curve","The field of time series forecasting has grown significantly over the past several years and is now highly active. In numerous application domains, deep neural networks are exact and powerful. They are among the most popular machine learning techniques for resolving big data issues because of these factors. Historically, there have been numerous methods for accurately predicting the subsequent change in time series data. The time series forecasting problem and its mathematical underpinnings are first articulated in this study. Following that, a description of the most popular deep learning architectures used to date with success in time series forecasting is provided, emphasizing both their benefits and drawbacks. Feedforward networks, recurrent neural networks (such as Elman networks), long-and short-term memory (LSTM), and gated recurrent units (GRU) are given special consideration. Furthermore, the advantages of the XGBoost boosting tree method have shown its superiority in numerous data mining competitions in recent years. The high coefficients of the metric measures indicate that the proposed XGBoost model provides good predictive performance, according to the results. © 2024 Elsevier B.V., All rights reserved.","Jeaab, K.; Saoudi, Y.; Falloul, M.E.M.",2024,10.23939/mmc2024.03.674,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205025967&doi=10.23939%2Fmmc2024.03.674&partnerID=40&md5=a547ef809381a9bbc475c0545010746d,scopus,"This study compares LSTM, GRU, and XGBoost for forecasting Morocco's yield curve. It reviews deep learning architectures for time series forecasting and highlights XGBoost's strong predictive performance.",True,True,True,gemini-2.5-flash-lite,Trine,Y,nan,2025-10-24T09:30:34.270058
