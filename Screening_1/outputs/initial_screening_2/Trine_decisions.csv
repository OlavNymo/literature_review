paper_id,title,abstract,authors,year,doi,link,database,llm_summary,i1_hint,i2_hint,i3_hint,model,reviewer,decision,notes,timestamp
22865f3093f9375d,"A Comparison of Neural Network, Statistical Methods, and Variable Choice for Life Insurers' Financial Distress Prediction","This study examines the effect of the statistical-mathematical model selected and the variable set considered on the ability to identify financially troubled life insurers. Models considered are two artificial neural network methods (back-propagation and learning vector quantization (LVQ)) and two more standard statistical methods (multiple discriminant analysis and logistic regression analysis). The variable sets considered are the insurance regulatory information system (IRIS) variables, the financial analysis solvency tracking (FAST) variables, and Texas early warning information system (EWIS) variables, and a data set consisting of twenty-two variables selected by us in conjunction with the research staff at TDI and a review of the insolvency prediction literature. The results show that the back-propagation (BP) and LVQ outperform the traditional statistical approaches for all four variable sets with a consistent superiority across the two different evaluation criteria (total misclassification cost and resubstitution risk criteria), and that the twenty-two variables and the Texas EWIS variable sets are more efficient than the IRIS and the FAST variable sets for identification of financially troubled life insurers in most comparisons.",,2006,10.1111/j.1539-6975.2006.00181.x,,proquest,"This study compares neural network methods (back-propagation and LVQ) with statistical methods (discriminant analysis and logistic regression) for predicting financial distress in life insurers. It evaluates the impact of different variable sets (IRIS, FAST, EWIS, and a custom set) on prediction accuracy. The findings indicate that neural networks, particularly back-propagation, generally outperform statistical methods, and a custom variable set of twenty-two variables along with the Texas EWIS variables are more effective for identifying distressed insurers.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:37:53.802300
aaa155be6197dbf8,A Cox model for gradually disappearing events,"Innovations in medicine provide us longer and healthier life, leading lower mortality. Sooner rather than later, much greater longevity would be possible for us due to artificial intelligence advances in health care. Similarly, Advanced Driver Assistance Systems (ADAS) in highly automated vehicles may reduce or even eventually eliminate accidents by perceiving dangerous situations, which would minimize the number of accidents and lead to fewer loss claims for insurance companies. To model the survivor function capturing greater longevity as well as the number of claims reflecting less accidents in the long run, in this paper, we study a Cox process whose intensity process is piecewise-constant and decreasing. We derive its ultimate distributional properties, such as the Laplace transform of intensity integral process, the probability generating function of point process, their associated moments and cumulants, and the probability of no more claims for a given time point. In general, this simple model may be applicable in many other areas for modeling the evolution of gradually disappearing events, such as corporate defaults, dividend payments, trade arrivals, employment of a certain job type (e.g., typists) in the labor market, and release of particles. In particular, we discuss some potential applications to insurance.","Jang, Jiwook; Qu, Yan; Zhao, Hongbiao; Dassios, Angelos",2023,10.1017/s0269964821000553,,wos,"This paper proposes a Cox process model with a piecewise-constant and decreasing intensity process to capture gradually disappearing events, such as reduced accidents due to advanced driver assistance systems or increased longevity due to medical advances. The model's distributional properties, moments, and the probability of no further events are derived. Potential applications in insurance and other fields are discussed.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:38:01.131742
b5415c8117608d9f,A Discussion of Non-Gaussian Price Processes for Energy and Commodity Operations,"Energy sources and commodities exhibit high price risk. This risk is thus an important feature of operational models of the value chains for these goods. These models typically employ Gaussian-based representations of the evolution of this uncertainty. This approach facilitates the optimization of operational policies but is at odds with empirical facts about energy and commodity prices, which are better captured by non-Gaussian processes. We discuss this alternative modeling strategy, focusing on Lévy processes. As an illustration, we show that it substantially increases the optimal policy value in a simplified merchant natural gas storage setting. Further, we highlight potential implications of using this approach to formulate realistic energy and commodity operations models. Our work has broader relevance for modeling the dynamics of both other market variables and operational quantities, such as exchange rates and demand forecasts. The study of how the adoption of non-Gaussian processes may impact energy and commodity operations is an appealing area for future research. © 2021 Elsevier B.V., All rights reserved.","Gambaro, A.M.; Secomandi, N.",2021,10.1111/poms.13250,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096644292&doi=10.1111%2Fpoms.13250&partnerID=40&md5=5666d8014dd126fe7d35d4bc4040ac34,scopus,"This paper discusses the use of non-Gaussian processes, specifically Lévy processes, to model the price dynamics of energy and commodities, arguing that these are more empirically accurate than traditional Gaussian models. It illustrates the benefits with a natural gas storage example and suggests broader implications for operational models and forecasting.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:38:18.340722
f9836cc4dd014711,A Gaussian Process of Yield Rates Calibrated with Strips,"This paper presents a Gaussian multivariate factor model of the term structure of interest rates. It shows that there exists a martingale valuation law of the factors so that the price function of a zero-coupon bond is an exponential spline. The model’s linear and Gaussian structure yields a simple model where estimation and calibration are relatively easy to do. Using yield data on stripped bonds, the spline model gives a very good approximation of the yield curve at all times. Moreover, the crucial Gaussian assumption is reasonable when modeling the dynamics for short periods like one year. © 2001 Taylor & Francis Group, LLC. © 2017 Elsevier B.V., All rights reserved.","Carrière, J.F.",2001,10.1080/10920277.2001.10595995,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011238970&doi=10.1080%2F10920277.2001.10595995&partnerID=40&md5=b9fd767ad96ca600385575d9c079e5fb,scopus,"This paper introduces a Gaussian multivariate factor model for interest rate term structures, where bond prices are represented by exponential splines. The model's linear and Gaussian properties facilitate straightforward estimation and calibration. Empirical results using stripped bond yield data demonstrate the spline model's effectiveness in approximating yield curves, with the Gaussian assumption being valid for short-term dynamics.",True,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:38:58.204318
3cf2316bbcf2ce76,A Hybrid Vector Autoregressive Model for Accurate Macroeconomic Forecasting: An Application to the U.S. Economy,"Forecasting macroeconomic variables is essential to macroeconomics, financial economics, and monetary policy analysis. Due to the high dimensionality of the macroeconomic dataset, it is challenging to forecast efficiently and accurately. Thus, this study provides a comprehensive analysis of predicting macroeconomic variables by comparing various vector autoregressive models followed by different estimation techniques. To address this, this paper proposes a novel hybrid model based on a smoothly clipped absolute deviation estimation method and a vector autoregression model that combats the curse of dimensionality and simultaneously produces reliable forecasts. The proposed hybrid model is applied to the U.S. quarterly macroeconomic data from the first quarter of 1959 to the fourth quarter of 2023, yielding multi-step-ahead forecasts (one-, three-, and six-step ahead). The multi-step-ahead out-of-sample forecast results (root mean square error and mean absolute error) for the considered data suggest that the proposed hybrid model yields a highly accurate and efficient gain. Additionally, it is demonstrated that the proposed models outperform the baseline models. Finally, the authors believe the proposed hybrid model may be expanded to other countries to assess its efficacy and accuracy.",,2025,10.3390/math13111706,,proquest,"This study proposes a novel hybrid vector autoregressive (VAR) model using a smoothly clipped absolute deviation estimation method for accurate macroeconomic forecasting. Applied to U.S. quarterly data, the model effectively addresses the curse of dimensionality and outperforms baseline models in multi-step-ahead out-of-sample forecasts.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:39:15.528561
f1779cf7208a0e34,A MODEL-SELECTION APPROACH TO ASSESSING THE INFORMATION IN THE TERM STRUCTURE USING LINEAR-MODELS AND ARTIFICIAL NEURAL NETWORKS,"We take a model-selection approach to the question of whether forward-interest rates are useful in predicting future spot rates, using a variety of out-of-sample forecast-based model-selection criteria-forecast mean squared error, forecast direction accuracy, and forecast-based trading-system profitability. We also examine the usefulness of a class of novel prediction models called artificial neural networks and investigate the issue of appropriate window sizes for rolling-window-based prediction methods. Results indicate that the premium of the forward rate over the spot rate helps to predict the sign of future changes in the interest rate. Furthermore, model selection based on an in-sample Schwarz information criterion (SIC) does not appear to be a reliable guide to out-of-sample performance in the case of short-term interest rates. Thus, the in-sample SIC apparently fails to offer a convenient shortcut to true out-of-sample performance measures.","SWANSON, NR; WHITE, H",1995,10.2307/1392186,,wos,"This study uses a model-selection approach to evaluate the predictive power of forward interest rates for future spot rates, employing criteria such as forecast mean squared error, direction accuracy, and trading system profitability. It also explores the utility of artificial neural networks and optimal window sizes for prediction models. The findings suggest that the forward rate premium can predict the direction of future interest rate changes, but in-sample model selection criteria like the Schwarz Information Criterion (SIC) do not reliably predict out-of-sample performance for short-term rates.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T09:39:33.182990
fd4f0e95b6a42d4c,A Machine Learning Approach to Forecast Economic Recessions-An Italian Case Study,"In economic activity, recessions represent a period of failure in Gross Domestic Product (GDP) and usually are presented as episodic and non-linear. For this reason, they are difficult to predict and appear as one of the main problems in macroeconomics forecasts. A classic example turns out to be the great recession that occurred between 2008 and 2009 that was not predicted. In this paper, the goal is to give a different, although complementary, approach concerning the classical econometric techniques, and to show how Machine Learning (ML) techniques may improve short-term forecasting accuracy. As a case study, we use Italian data on GDP and a few related variables. In particular, we evaluate the goodness of fit of the forecasting proposed model in a case study of the Italian GDP. The algorithm is trained on Italian macroeconomic variables over the period 1995:Q1-2019:Q2. We also compare the results using the same dataset through Classic Linear Regression Model. As a result, both statistical and ML approaches are able to predict economic downturns but higher accuracy is obtained using Nonlinear Autoregressive with exogenous variables (NARX) model.","Cicceri, Giovanni; Inserra, Giuseppe; Limosani, Michele",2020,10.3390/math8020241,,wos,"This paper presents a Machine Learning (ML) approach, specifically a Nonlinear Autoregressive with exogenous variables (NARX) model, to forecast economic recessions using Italian GDP data. The ML model is trained on macroeconomic variables from 1995:Q1-2019:Q2 and compared against a Classic Linear Regression Model. The study concludes that while both methods can predict downturns, the NARX model achieves higher accuracy.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:39:47.312367
1f7924612848e55b,A Machine-Learning-Based Approach for Natural Gas Futures Curve Modeling,"This work studies the term structure dynamics in the natural gas futures market, focusing on the Dutch Title Transfer Facility (TTF) daily futures prices. At first, using the whole dataset, we compared the in-sample fitting performance of three models: the four-factor dynamic Nelson–Siegel–Svensson (4F-DNSS) model, the five-factor dynamic De Rezende–Ferreira (5F-DRF) model, and the B-spline model. Our findings suggest that B-spline is the method that achieves the best in-line fitting results. Then, we turned our attention to forecasting, using data from 20 January 2011 to 13 May 2022 as the training set and the remaining data, from 16 May to 13 June 2022, for day-ahead predictions. In this second part of the work we combined the above mentioned models (4F-DNSS, 5F-DRF and B-spline) with a Nonlinear Autoregressive Neural Network (NAR-NN), asking the NAR-NN to provide parameter tuning. All the models provided accurate out-of-sample prediction; nevertheless, based on extensive statistical tests, we conclude that, as in the previous case, B-spline (combined with an NAR-NN) ensured the best out-of-sample prediction.",,2023,10.3390/en16124746,,proquest,"This study compares the performance of three models (4F-DNSS, 5F-DRF, and B-spline) for modeling the term structure of natural gas futures prices, specifically the TTF. The B-spline model demonstrated superior in-sample fitting. For day-ahead predictions, the B-spline model, when combined with a Nonlinear Autoregressive Neural Network (NAR-NN) for parameter tuning, achieved the best out-of-sample prediction accuracy.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:39:55.489454
99d0e8a05fcc941c,A Markov regime-switching model for the semiconductor industry cycles,"Because of the huge fluctuation in the semiconductor business, it has been a challenging work for the industry researchers to predict the turning points of the semiconductor industry cycles. To catch the cyclical behavior of the semiconductor business, we propose a Markov Regime-Switching model with two regimes representing expansion and contraction. The simple nonlinear, two states, regime-switching model shows a successful in-sample prediction on the contraction of semiconductor industry sales during the period of 1990:01-2003:08. (c) 2006 Elsevier B.V. All rights reserved.","Liu, Wen-Hsien; Chyi, Yih-Luan",2006,10.1016/j.econmod.2006.02.007,,wos,This paper proposes a Markov regime-switching model with two states (expansion and contraction) to predict turning points in the semiconductor industry cycles. The model successfully predicted the contraction in sales from 1990 to 2003.,True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:40:03.028253
d476fa9fc94bed8b,A New Linear Estimator for Gaussian Dynamic Term Structure Models,"This article proposes a novel regression-based approach to the estimation of Gaussian dynamic term structure models. This new estimator is an asymptotic least-square estimator defined by the no-arbitrage conditions upon which these models are built. Further, we note that our estimator remains easy-to-compute and asymptotically efficient in a variety of situations in which other recently proposed approaches might lose their tractability. We provide an empirical application in the context of the Canadian bond market.","de los Rios, Antonio Diez",2015,10.1080/07350015.2014.948176,,wos,"This paper introduces a new regression-based estimator for Gaussian dynamic term structure models, which is asymptotically efficient and easy to compute. It is applied to the Canadian bond market.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:41:20.508151
5d4eeb7e05f11ede,A Nonlinear Factor Analysis of S&P 500 Index Option Returns,"Growing evidence suggests that extraordinary average returns may be obtained by trading equity index options, and that at least part of this abnormal performance is attributable to volatility and jump risk premia. This paper asks whether such priced risk factors are alone sufficient to explain these average returns. To provide an answer in as general as possible a setting, I estimate a flexible class of nonlinear models using all S&P 500 Index futures options traded between 1986 and 2000. The results show that priced factors contribute to these expected returns but are insufficient to explain their magnitudes, particularly for short-term out-of-the-money puts.",,2006,10.1111/j.1540-6261.2006.01059.x,,proquest,"This paper investigates whether priced risk factors are sufficient to explain the extraordinary average returns observed in S&P 500 Index options. Using a nonlinear factor analysis on options traded between 1986 and 2000, the study finds that while priced factors contribute to expected returns, they do not fully account for their magnitudes, especially for short-term out-of-the-money puts.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:41:34.821715
e87df0ce8abe00f6,A Novel Particle Swarm Optimizer and Its Application to the Yield Curve Estimation Problem,"Particle swarm optimization (PSO) has been considered as one of the main swarm intelligence algorithms for solving single-objective optimization problem. How to update the velocities and positions of a swarm of particles is key to its optimization performance. In this paper, we propose to use the moving average of the local best positions visited so far as a key information to update the velocities and positions. Further, a central learning strategy is proposed in which the center of the local best positions is computed and used to update the global best position. Combining these two strategies with the updating formulas which are inspired by the free electron model in metal conductors placed in an external electric field, we name the proposed PSO algorithm as PSO with moving average and central learning strategies (dubbed as MAPSO). We test the performance of MAPSO on the CEC 2017 test problems with 10 and 30 dimensions. Experimental results show that MAPSO significantly outperforms some well-known PSOs in general. We then apply MAPSO on a very challenging real-world problem, i.e. the yield curve estimation problem, in macroeconomics. Our experimental study on the yield curve estimation problem with Shanghai interbank offered rates shows that MAPSO can effectively solve the problem and achieve the state-of-the-art performance.",J. Zhang; B. Shi,2022,10.1109/access.2022.3220792,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9943541,ieeexplore,"This paper introduces a novel Particle Swarm Optimization (PSO) algorithm called MAPSO, which incorporates moving average and central learning strategies inspired by the free electron model. MAPSO demonstrates superior performance on benchmark test problems and is effectively applied to the yield curve estimation problem using Shanghai interbank offered rates, achieving state-of-the-art results.",True,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:42:43.067286
f2a47be779f44ec8,A Public Administration Moment: Forging an Agenda for Financial Regulatory Reform,"The numbers are staggering. As the financial crisis deepens, American financial companies will lose an estimated $3.6 trillion in non-performing loans and lost asset value (Lohr 2009). Nationally, home prices have fallen more than 18% from peak levels -- as much as 40% in some states -- and in 2008, lenders initiated more than 2.25 million home foreclosures (Duke 2009). The biggest financial crisis since the Great Depression, as the Treasury Department describes it, is a crisis of confidence, of capital, of credit, and of consumer and business demand. Just as staggering, however, is the lack of attention given to the government administrative capacities that failed going into the crisis, and that require fervent attention as they grope toward a reengineered financial system and regulatory structure. Public administration scholars and practitioners play a vital role in forging the future of finance.",,2009,10.1111/j.1540-6210.2009.02008.x,,proquest,This article discusses the failure of government administrative capacities during the financial crisis and emphasizes the role of public administration scholars and practitioners in reengineering the financial system and regulatory structure. It highlights the significant financial losses and foreclosures resulting from the crisis.,False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:42:48.679936
868c96ac24ae4368,"A Public, Open, and Independently-Curated Database of Happiness Coefficients","We present a nascent database of happiness coefficients. This is a synthesis of evidence on the size of improvements to human life experience that can be expected from changing objective, policy-amenable circumstances. The wealth of data on people’s self-reported satisfaction with life in a wide variety of circumstances, from around the world, including respondents undergoing a diversity of changes and life events and subject to a variety of public policies and policy changes, has provided a rich base of knowledge about what makes life good. This growing research literature has in recent years been met with interest from central governments looking for accountable but more human-centred approaches to measuring progress, as well as for communicating objectives, making policy, and allocating resources. Meanwhile, frameworks for benefit-cost accounting using inference from life satisfaction data have been devised. In some cases central government finance departments and treasuries are incorporating this approach into their formal methodology for budgeting. The body of causal inference about these effects is still somewhat diffuse. Collating, reviewing, and synthesizing such evidence should be led initially by academia and ultimately by a broad academic, civil society, and government collaboration. We report on the assembly of a database of summary estimates for Canada, supplemented where needed by evidence from around the world. The categorized domains of individual experience and circumstances include Education, Environment, Work, Finances, Health, Social Capital, and Crime. The paper also explains the context for and limitations of the use of a database of happiness coefficients.",,2023,10.1007/s10902-023-00652-4,,proquest,"This paper introduces a public, open, and independently curated database of happiness coefficients, which quantify the impact of policy-amenable circumstances on life experience. It synthesizes evidence from global data on life satisfaction, relevant for governments seeking human-centered progress measures and resource allocation. The database covers domains like Education, Environment, Work, Finances, Health, Social Capital, and Crime, with a focus on Canada. The paper also discusses the context and limitations of using such coefficients.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:42:51.283798
baf7dc5cdf61fdca,A Quantitative Investment Model Based on Random Forest and Sentiment Analysis,"In recent years, under the influence of economic globalization and anti-globalization, the stock market has experienced great fluctuations in China. Quantitative investment has attracted a lot of attention because of its characteristics of maintaining stable returns. Existing research is unilaterally based on quantitative data or qualitative data for analysis to construct a quantitative investment model. This paper considers both quantitative and qualitative data to construct a more comprehensive model than that in the past. Based on the optimized database, we present a combinational model named RF-SA, which is composed of random forest and sentiment analysis model. First of all, this paper uses the SBS algorithm to select the characteristics of stock transaction historical data, optimizes the prediction database, reduces data redundancy, and improves the accuracy of the model. Secondly, we analyze the characteristics of the Chinese stock market and study the advantages and disadvantages of many data mining algorithms, and select random forest model, the most suitable model, to build the first step of stock selection model. Then, through the analysis of public opinion, the confidence index of the stockholders is calculated; on this basis, the results of the RF model and the confidence index are combined to make a second choice for the stock, and the quantitative investment portfolio is obtained, and excess returns can be obtained. The results of empirical data show that, the RF-SA model obtains a higher rate of return than the investment model of the Shanghai Stock Index.",,2020,10.1088/1742-6596/1575/1/012083,,proquest,"This paper proposes a quantitative investment model (RF-SA) combining Random Forest and Sentiment Analysis, utilizing both quantitative (historical stock transaction data) and qualitative (public opinion sentiment) data to construct a more comprehensive model than previous approaches. The model optimizes historical data using the SBS algorithm and employs Random Forest for initial stock selection, followed by sentiment analysis to calculate a confidence index for a second selection stage, aiming to achieve excess returns. Empirical results indicate the RF-SA model outperforms the Shanghai Stock Index.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:43:03.995755
b91ed97fec43aa60,A Rational Theory of Mutual Funds' Attention Allocation,"The question of whether and how mutual fund managers provide valuable services for their clients motivates one of the largest literatures in finance. One candidate explanation is that funds process information about future asset values and use that information to invest in high-valued assets. But formal theories are scarce because information choice models with many assets are difficult to solve as well as difficult to test. This paper tackles both problems by developing a new attention allocation model that uses the state of the business cycle to predict information choices, which in turn, predict observable patterns of portfolio investments and returns. The predictions about fund portfolios' covariance with payoff shocks, cross-fund portfolio and return dispersion, and their excess returns are all supported by the data. These findings offer new evidence that some investment managers have skill and that attention is allocated rationally. © 2016 Elsevier B.V., All rights reserved.","Kacperczyk, M.; Van Nieuwerburgh, S.; Veldkamp, L.",2016,10.3982/ecta11412,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961238568&doi=10.3982%2FECTA11412&partnerID=40&md5=7c9b7cee943ebb7a5cb2bd4e745f8a5f,scopus,"This paper develops a new attention allocation model for mutual funds, linking business cycle states to information choices and subsequent investment patterns and returns. The model's predictions regarding portfolio covariance with payoff shocks, return dispersion, and excess returns are empirically supported, suggesting rational attention allocation and manager skill.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:43:07.632598
1eac02bbd502ef6e,A Rigorous Statistical Comparison of Deep Learning Models for US Treasury Yield Prediction,"The intrinsic nonlinearity and dynamic relationships in interest rate fluctuations present a substantial challenge when forecasting financial time series, particularly US Treasury yields. These intricate relationships are sometimes not adequately captured by traditional econometric models. In recent years, deep learning (DL) methodologies have gained prominence in the financial market, offering advanced predictive capabilities by modeling high-dimensional dependencies and nonlinear interactions inside yield curves. To enhance the predictive accuracy of short-term (13-week) and long-term (5-year) US Treasury yields, this study leverages advanced deep learning models, including convolutional neural networks (CNNs), long short-term memory (LSTM) networks, and gated recurrent units (GRUs). A comprehensive statistical evaluation is performed to assess model performance through key error metrics such as root mean squared error (RMSE), mean squared error (MSE), mean absolute error (MAE), the coefficient of determination (R2), maximum error, and minimum error, as well as SAFE metrics (Sustainability, Accuracy, Fairness, Explainability) for a holistic assessment. To ensure a robust comparison, we employed the paired t-test to determine if the differences in model predictions are statistically significant. Additionally, we analyzed correlation metrics using Pearson and Spearman coefficients, which evaluate the models’ ability to capture both linear dependencies and ranking trends in yield fluctuations. This rigorous framework not only benchmarks the predictive power of each model but also provides deeper insights into their effectiveness in forecasting treasury yields across different time horizons.",,2025,10.1007/s43069-025-00497-y,,proquest,"This study compares the performance of deep learning models (CNNs, LSTMs, GRUs) for predicting US Treasury yields (13-week and 5-year). It uses various error metrics, SAFE metrics, t-tests, and correlation coefficients to rigorously evaluate the models' predictive accuracy and ability to capture yield curve dynamics, suggesting deep learning models may outperform traditional methods.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T09:43:16.850438
e5bc9c76cee36aba,A Simulation and Empirical Study of the Maximum Likelihood Estimator for Stochastic Volatility Jump-Diffusion Models,"We investigate the behaviour of the maximum likelihood estimator (MLE) for stochastic volatility jump-diffusion models commonly used in financial risk management. A simulation study shows the practical conditions under which the MLE behaves according to theory. In an extensive empirical study based on nine indices and more than 6000 individual stocks, we nonetheless find that the MLE is unable to replicate key higher moments. We then introduce a moment-targeted MLE - robust to model misspecification - and revisit both simulation and empirical studies. We find it performs better than the MLE, improving the management of financial risk.","Begin, Jean-Francois; Boudreault, Mathieu",2025,10.1515/snde-2023-0028,,wos,"This study examines the Maximum Likelihood Estimator (MLE) for stochastic volatility jump-diffusion models. A simulation and empirical study using financial data show that the standard MLE has limitations in capturing higher moments. The authors propose a moment-targeted MLE, which demonstrates improved performance in both simulations and empirical analyses, leading to better financial risk management.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:43:54.375378
77af4344c3d8f866,A Stock Market Decision-Making Framework Based on CMR-DQN,"In the dynamic and uncertain stock market, precise forecasting and decision-making are crucial for profitability. Traditional deep neural networks (DNN) often struggle with capturing long-term dependencies and multi-scale features in complex financial time series data. To address these challenges, we introduce CMR-DQN, an innovative framework that integrates discrete wavelet transform (DWT) for multi-scale data analysis, temporal convolutional network (TCN) for extracting deep temporal features, and a GRU–LSTM–Attention mechanism to enhance the model’s focus and memory. Additionally, CMR-DQN employs the Rainbow DQN reinforcement learning strategy to learn optimal trading strategies in a simulated environment. CMR-DQN significantly improved the total return rate on six selected stocks, with increases ranging from 20.37% to 55.32%. It also demonstrated substantial improvements over the baseline model in terms of Sharpe ratio and maximum drawdown, indicating increased excess returns per unit of total risk and reduced investment risk. These results underscore the efficiency and effectiveness of CMR-DQN in handling multi-scale time series data and optimizing stock market decisions.",,2024,10.3390/app14166881,,proquest,"This paper proposes CMR-DQN, a novel framework for stock market decision-making that combines discrete wavelet transform (DWT) for multi-scale analysis, temporal convolutional network (TCN) for feature extraction, and a GRU-LSTM-Attention mechanism for enhanced memory and focus. The framework uses Rainbow DQN for reinforcement learning to optimize trading strategies. Empirical results show significant improvements in total return rate, Sharpe ratio, and maximum drawdown compared to a baseline model on six selected stocks.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:44:11.053035
2e9eec71c0351724,A Study on Project Portfolio Models with Skewness Risk and Staffing,"When it comes to business operation, the institutions have to choose some appropriate projects from numerous projects to invest. To this end, they should consider to establish a project portfolio to make decisions. When building such a portfolio, the project selection and the staff assignment are the most essential parts, which greatly affect the profit of project portfolios. As for the project selection, market returns tend to be asymmetric and investors are often concerned about the skewness risk which is ignored by the traditional project portfolio. Meanwhile, as for the staff assignment, the institutional investors aim at achieving the highest returns by adopting a proper assignment of project managers. In addition, since the exact possibility distributions of uncertain parameters in practical project portfolio problems are often unavailable, we adopt variable parametric credibility measure to characterize uncertain model parameters. In view of these problems, this article proposes a project portfolio model with skewness risk constraints and a project portfolio model with staffing based on credibility measure theory and fuzzy theory in uncertain circumstances. Our two models are associated with risk-free assets so that the remaining funds can be utilized effectively. Finally, we use genetic algorithms to solve our proposed models and present some numerical examples to demonstrate the effectiveness of the proposed models. © 2023 Elsevier B.V., All rights reserved.","Xu, W.; Liu, G.; Li, H.; Luo, W.",2017,10.1007/s40815-017-0295-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037379412&doi=10.1007%2Fs40815-017-0295-0&partnerID=40&md5=5d175108a94ab2d9fd4c70c6ba35eaf2,scopus,"This article proposes two project portfolio models addressing skewness risk and staffing issues under uncertainty, utilizing credibility measure and fuzzy theories. The models incorporate risk-free assets and are solved using genetic algorithms, with numerical examples demonstrating their effectiveness.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:44:30.993251
5c56da8a165400ea,A Synthetic Data Generation Technique for Enhancement of Prediction Accuracy of Electric Vehicles Demand,"In terms of electric vehicles (EVs), electric kickboards are crucial elements of smart transportation networks for short-distance travel that is risk-free, economical, and environmentally friendly. Forecasting the daily demand can improve the local service provider’s access to information and help them manage their short-term supply more effectively. This study developed the forecasting model using real-time data and weather information from Jeju Island, South Korea. Cluster analysis under the rental pattern of the electric kickboard is a component of the forecasting processes. We cannot achieve noticeable results at first because of the low amount of training data. We require a lot of data to produce a solid prediction result. For the sake of the subsequent experimental procedure, we created synthetic time-series data using a generative adversarial networks (GAN) approach and combined the synthetic data with the original data. The outcomes have shown how the GAN-based synthetic data generation approach has the potential to enhance prediction accuracy. We employ an ensemble model to improve prediction results that cannot be achieved using a single regressor model. It is a weighted combination of several base regression models to one meta-regressor. To anticipate the daily demand in this study, we create an ensemble model by merging three separate base machine learning algorithms, namely CatBoost, Random Forest (RF), and Extreme Gradient Boosting (XGBoost). The effectiveness of the suggested strategies was assessed using some evaluation indicators. The forecasting outcomes demonstrate that mixing synthetic data with original data improves the robustness of daily demand forecasting and outperforms other models by generating more agreeable values for suggested assessment measures. The outcomes further show that applying ensemble techniques can reasonably increase the forecasting model’s accuracy for daily electric kickboard demand.",,2023,10.3390/s23020594,,proquest,"This study proposes a method to improve the prediction accuracy of electric vehicle (EV) demand, specifically focusing on electric kickboards. The authors developed a forecasting model using real-time and weather data from Jeju Island, South Korea. To address the issue of insufficient training data, they generated synthetic time-series data using a Generative Adversarial Network (GAN) and combined it with the original data. An ensemble model, comprising CatBoost, Random Forest, and XGBoost, was employed to further enhance prediction accuracy. The results indicate that the combination of synthetic data and ensemble techniques significantly improves the robustness and accuracy of daily demand forecasting for electric kickboards.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:44:39.045655
f187ea8de5111e6f,A VECM analysis of Bitcoin price using time-varying cointegration approach,"This study proposed an optimal model to examine the relationship between the Bitcoin price and six macroeconomic variables – the Bitcoin price, Standard and Poor's 500 volatility index, US treasury 10-year yield, US consumer price index, gold price and dollar index. It also examined the effectiveness of the vector error correction model (VECM) in analyzing the interrelationship among these variables. The authors employed the following approach: first, the authors sampled the period August 2010–February 2022. This is because Bitcoin achieved a market capitalization of more than US$1 tn over this period, gaining market attention and acceptance from retail, corporate and institutional investors. Second, the authors employed a VECM with the six macroeconomic variables. Finally, the authors expanded the long-run equilibrium relationship (time-invariant cointegration)-based VECM to develop a time-varying cointegration (TVC) VECM. The authors estimated the TVC VECM using the Chebyshev polynomial specification based on various information criteria. The results showed that the Bitcoin price can be modeled with the VECM (p = 1, r = 1). The TVC approach generated more explanatory power for Bitcoin pricing, indicating the effectiveness of the approach for modeling the long-run relationship between Bitcoin price and macroeconomic variables. © 2023 Elsevier B.V., All rights reserved.","Lee, Y.; Rhee, J.H.",2022,10.1108/jdqs-01-2022-0001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150338859&doi=10.1108%2FJDQS-01-2022-0001&partnerID=40&md5=14ed575c3c7ffa374c533ddd0c3a53b7,scopus,"This study analyzes the relationship between Bitcoin price and six macroeconomic variables using a time-varying cointegration (TVC) Vector Error Correction Model (VECM). The TVC VECM approach, estimated using Chebyshev polynomials, demonstrated greater explanatory power for Bitcoin pricing compared to traditional VECM, suggesting its effectiveness in modeling long-run relationships.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:44:44.407583
644a6f98cf40c70c,A comparison of linear and nonlinear statistical techniques in performance attribution,"Performance attribution is usually conducted under the linear framework of multifactor models. Although commonly used by practitioners in finance, linear multifactor models are known to be less than satisfactory in many situations. After a brief survey of nonlinear methods, nonlinear statistical techniques are applied to performance attribution of a portfolio constructed from a fixed universe of stocks using factors derived from some commonly used cross sectional linear multifactor models. By rebalancing this portfolio monthly, the cumulative returns for procedures based on standard linear multifactor model and three nonlinear techniques-model selection, additive models, and neural networks-are calculated and compared. It is found that the first two nonlinear techniques, especially in combination, outperform the standard linear model. The results in the neural-network case are inconclusive because of the great variety of possible models. Although these methods are more complicated and may require some tuning, toolboxes are developed and suggestions on calibration are proposed. This paper demonstrates the usefulness of modern nonlinear statistical techniques in performance attribution.",Ngai Hang Chan; C. R. Genovese,2001,10.1109/72.935100,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=935100,ieeexplore,"This paper compares linear and nonlinear statistical techniques for portfolio performance attribution. It applies nonlinear methods like model selection, additive models, and neural networks to a stock portfolio and finds that nonlinear techniques, particularly model selection and additive models, outperform standard linear models. The study suggests that modern nonlinear techniques are useful for performance attribution, despite their complexity.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:44:49.289141
a4cfbac9053df77e,A comparison of machine learning and econometric models for pricing perpetual Bitcoin futures and their application to algorithmic trading,"Bitcoin (BTC) perpetual futures contracts are highly leveraged speculative trading instruments with daily market trading of $45 Billion. BTC perpetual futures are derivative contracts, which depend upon the underlying BTC SPOT (current) price. Pricing perpetual futures fairly is hard, using traditional arbitrage arguments, because of the volatile nature of the so called funding rate, which is used as the replacement of risk free rate in the Cryptocurrency market. This work presents a novel technique for pricing BTC futures contracts using conditional volatility and mean models. Intra‐day high‐frequency futures' return volatility and mean are modelled using different ML and econometric techniques. A comparison is made using statistical measures to find the model that best captures the intra‐day conditional mean and volatility. Exponential generalized autoregressive conditional heteroskedasticity is shown to be an almost unbiased predictor of intra‐day volatility, while a constant autoregressive moving average (0, 0) model best captures the conditional mean of the returns. A market directional high frequency trading algorithm is developed using the volatility and mean models. The algorithm first prices the futures contract at some future point of time using the volatility and mean regression models. Next, the slope between the current futures price and the expected price are used to predict the market direction. A long or short position is taken depending upon the expected market direction movement. Extensive back‐testing results show absolute returns of 1500%–8000% depending upon the transaction fees and leverage used. On average, the market direction is predicted correctly 85% of the time by the best model. Finally, the trading technique is market neutral, in that it gives large positive returns, with low SD, in both bull and bear markets.",,2023,10.1111/exsy.13414,,proquest,"This study compares machine learning and econometric models for pricing Bitcoin perpetual futures, considering conditional volatility and mean. An Exponential Generalized Autoregressive Conditional Heteroskedasticity (EGARCH) model performed well for volatility, and a Constant Autoregressive Moving Average (0,0) model for mean. A trading algorithm based on these models achieved significant returns (1500%-8000%) by predicting market direction with 85% accuracy, demonstrating market neutrality.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:44:54.628652
df93e71ba8fb12f7,A comparison of multitask and single task learning with artificial neural networks for yield curve forecasting,"The yield curve is the centrepiece in bond markets, a massive asset class with an overall size of USD 100 trillion that remains relatively under-investigated using machine learning. This paper is the first comprehensive study using artificial neural networks in the context of yield curve forecasting. Specifically, two models were used for forecasting the European yield curve: multivariate linear regression and multilayer perceptron (MLP), at five forecasting horizons, from next day to 20 days ahead. Five variants of the MLP were analysed with different sets of features: target to predict (univariate); the most relevant features; all generated features; and the former two incorporating synthetic data generated by the linear regression model. Additionally, two different techniques of multitask learning were employed: simultaneous modelling and transformation into multiple single task learning. The results show that considering all forecasting horizons, the MLP using the most relevant features achieved the best results and the addition of synthetic data tends to improve accuracy. Furthermore, different targets and forecasting horizons resulted in different relevant features, reinforcing the importance of custom-built models. In the two multitask learning methodologies no clear differentiation could be demonstrated, and several explaining factors are identified. Overall, the outcome is very encouraging for the development of better forecasting systems for fixed income markets.",,2019,10.1016/j.eswa.2018.11.012,,proquest,"This paper presents the first comprehensive study using artificial neural networks (ANNs) for European yield curve forecasting. It compares multivariate linear regression and multilayer perceptron (MLP) models at five forecasting horizons. Various MLP configurations were tested, including different feature sets and the incorporation of synthetic data. Multitask learning techniques were also explored. The MLP with the most relevant features showed the best performance, with synthetic data generally improving accuracy. The study highlights the need for custom models for different targets and horizons, and found no clear advantage for multitask learning in this context.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T09:45:09.382694
bac703195941f32a,A comparison of nonlinear methods for predicting earnings surprises and returns,"We compare four nonlinear methods on their ability to learn models from data. The problem requires predicting whether a company will deliver an earnings surprise a specific number of days prior to announcement. This problem has been well studied in the literature using linear models. A basic question is whether machine learning-based nonlinear models such as tree induction algorithms, neural networks, naive Bayesian learning, and genetic algorithms perform better in terms of predictive accuracy and in uncovering interesting relationships among problem variables. Equally importantly, if these alternative approaches perform better, why? And how do they stack up relative to each other? The answers to these questions are significant for predictive modeling in the financial arena, and in general for problem domains characterized by significant nonlinearities. In this paper, we compare the four above-mentioned nonlinear methods along a number of criteria. The genetic algorithm turns out to have some advantages in finding multiple ""small disjunct"" patterns that can be accurate and collectively capable of making predictions more often than its competitors. We use some of the nonlinearities we discovered about the problem domain to explain these results.",V. Dhar; D. Chou,2001,10.1109/72.935099,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=935099,ieeexplore,"This paper compares four nonlinear machine learning methods (tree induction, neural networks, naive Bayesian learning, and genetic algorithms) for predicting earnings surprises and returns, aiming to determine if they outperform traditional linear models and understand why. The genetic algorithm showed an advantage in identifying multiple predictive patterns.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:45:35.160944
8a149439aecedfa5,A comparison of risk-premium forecasts implied by parametric versus nonparametric conditional mean estimators,"This paper computes parametric estimates of a time-varying risk premium model and compares the one-step-ahead forecasts implied by that model with those given by a nonparametric kernel estimator of the conditional mean function. The conditioning information used for the nonparametric analysis is that implied by the theoretical model of time-varying risk. Thus, the kernel estimator is used, in conjunction with a nonparametric diagnostic test for in-sample residual nonlinear structure, to assess the adequacy of the parametric model in capturing any structure in the excess returns. Our results support the parametric specification of an asset pricing model in which the conditional beta is the ratio of the relevant components of the conditional covariance matrix of returns modelled as a bivariate generalized ARCH process. Although the predictable component of the conditional moments is relatively small, the parametric estimator of the risk premia has somewhat more out-of-sample forecasting ability than does the kernel estimator. Hence, the superior in-sample performance of the latter may be attributed to overfitting. © 1992. © 2014 Elsevier B.V., All rights reserved.","McCurdy, T.H.; Stengos, T.",1992,10.1016/0304-4076(92)90071-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0011463942&doi=10.1016%2F0304-4076%2892%2990071-X&partnerID=40&md5=5201c426cd455c9821ffe152c1cd7b83,scopus,"This paper compares parametric and nonparametric (kernel) estimators for forecasting risk premiums. It finds that while the nonparametric estimator captures more in-sample structure, the parametric estimator has slightly better out-of-sample forecasting ability, suggesting potential overfitting in the nonparametric approach. The study supports a parametric asset pricing model with a conditional beta derived from a bivariate GARCH process.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:46:10.633290
7736932495ba615c,A comprehensive investigation on the predictive power of economic policy uncertainty from non-US countries for US stock market returns,"This study uses economic policy uncertainty (EPU) indices for ten developed countries, three diffusion models, and five combination methods to forecast excess returns in the U.S. stock market. It shows empirically that, over the period January 1997 to January 2022, non-U.S. EPU indices have better predictive power for U.S. equity market excess returns than the U.S. EPU index itself. This illustrates how economic information from interna-tional markets can affect the U.S. stock market. This finding challenges the extensively recognized view that the U.S. is where important market signals are initially transmitted to other markets, suggesting that this belief is incomplete. Our outcomes are robust to a battery of tests covering model selection, model specification, forecast horizons, and the pandemic period, and their economic values are assessed. The findings are essential for the financial field to confront future fierce situations and crises.","Huang, Yisu; Ma, Feng; Bouri, Elie; Huang, Dengshi",2023,10.1016/j.irfa.2023.102656,,wos,"This study investigates the predictive power of economic policy uncertainty (EPU) from ten developed non-US countries for US stock market returns. Using three diffusion models and five combination methods from January 1997 to January 2022, the research finds that non-US EPU indices are better predictors of US equity market excess returns than the US EPU index. This suggests that international economic information impacts the US stock market, challenging the traditional view of US market primacy. The findings are robust across various tests and have implications for the financial field.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:46:27.688671
d3acea4f9be74018,A daily view of the term structure dynamics: Some international evidence,"The paper employs daily interest rates from the short-end of the Eurocurrency market in order to test the validity of the Expectations Hypothesis (EH). In particular, exploiting the stochastic trends embedded in the time series the EH implications are tested in a multivariate cointegration framework. The empirical findings indicate that once daily rates are used the estimated coefficients are very close to their theoretical values as predicted by the EH. Furthermore, we cannot reject the hypothesis that the EH is an adequate description of the US yield curve. Similarly, for the German and UK yield curves the number of common stochastic trends present in their yield curves is consistent with the EH. However, the restrictions imposed by the theory on parameters of the cointegration space are rejected.","Drakos, K",2002,10.1023/a:1014851101861,,wos,"This paper tests the Expectations Hypothesis (EH) using daily Eurocurrency interest rates and a multivariate cointegration framework. The findings suggest that the EH is a valid description for the US, German, and UK yield curves when daily data is used, although theoretical restrictions on cointegration parameters are rejected.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:46:30.103713
6b84e9b3a4dbcbf6,A differential evolution algorithm for yield curve estimation,"Modeling the term structure of government bond yields is of great interest to macroeconomists and financial market practitioners. It is crucial for bonds and derivatives pricing, risk management, and reveals market expectations, which is essential for monetary policy decisions. This paper suggests the use of a differential evolutionary algorithm to estimate yield curves for US Treasury bonds. It considers parsimonious modeling to avoid non-convergence and high instability of traditional optimization algorithms when estimating model parameters caused by the choice of their initial values during curve fitting. In this approach, the whole yield curve for different maturities is obtained by models parameters estimates. Computational experiments show that the differential evolutionary algorithm provides more accurate yield curves than the ones derived by nonlinear least squares and genetic algorithm approaches. © 2016 Elsevier B.V., All rights reserved.","Maciel, L.; Gomide, F.; Ballini, R.",2016,10.1016/j.matcom.2016.04.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971623035&doi=10.1016%2Fj.matcom.2016.04.004&partnerID=40&md5=b9d939ee042f35134b929a881b51fb90,scopus,"This paper proposes a differential evolutionary algorithm for estimating yield curves of US Treasury bonds. It aims to overcome limitations of traditional optimization methods by using a parsimonious modeling approach. The algorithm's accuracy is compared to nonlinear least squares and genetic algorithms, showing superior performance.",True,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:48:22.262986
24e4d383bd202c0a,A dynamic factor model of the yield curve components as a predictor of the economy,"In this paper, we propose an econometric model of the joint dynamic relationship between the Treasury yield curve components and the economy, for predicting business cycle turning points. The nonlinear multivariate dynamic factor model takes into account not only the popular slope, but also information extracted from the level and curvature of the yield curve, and from macroeconomic variables. We investigate the interrelationship between the phases of cyclical fluctuations in yield curve components and the phases of the business cycle. The results indicate a strong interrelationship between the yield curve and the economy. The proposed model has substantial incremental predictive value relative to alternative specifications. This result holds both in-sample and out-of-sample, using revised and real time unrevised data. © 2016 Elsevier B.V., All rights reserved.","Chauvet, M.; Senyuz, Z.",2016,10.1016/j.ijforecast.2015.05.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955313253&doi=10.1016%2Fj.ijforecast.2015.05.007&partnerID=40&md5=a41c2d9e062771db7347649b91b21ab4,scopus,"This paper proposes a nonlinear multivariate dynamic factor model to predict business cycle turning points using Treasury yield curve components (slope, level, curvature) and macroeconomic variables. The model demonstrates significant predictive value, both in-sample and out-of-sample, highlighting a strong interrelationship between the yield curve and the economy.",True,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:49:05.506156
0e5f8a823c00b204,A factor pricing model based on machine learning algorithm,"We adopt Discrete Wavelet Transform (DWT) and Support Vector Regression (SVR) algorithm to predict stock returns and form a time-varying Machine Learning (ML) factor based on the predicted returns for improving the classical asset pricing in the Chinese stock market from 2000 to 2020. The results show that the sorted portfolios formed by the predicted return rates can obtain significant excess returns in the Chinese market. Furthermore, our research shows that incorporating the ML factor into the CH4 and FF5 model can improve the pricing power significantly. It indicates that the ML factor can complement the traditional pricing models. We also find the performance of factor models depend on macroeconomy and market sentiment, that is, the better the macroeconomic and stock market performance, the stronger the pricing power of the factor models in the Chinese market. Additionally, we explore whether ML factor falls under short-term, median-term, and long-term momentum and reversal factors, and our analysis demonstrates that the ML factor is more effective than momentum and reversal factors. © 2023 Elsevier B.V., All rights reserved.","Fang, Y.; Chen, Y.; Ren, H.",2023,10.1016/j.iref.2023.06.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165250152&doi=10.1016%2Fj.iref.2023.06.012&partnerID=40&md5=5d7b8619f3b34ab111872af48b27b9aa,scopus,This study develops a time-varying Machine Learning (ML) factor using Discrete Wavelet Transform (DWT) and Support Vector Regression (SVR) to predict stock returns in the Chinese market (2000-2020). The ML factor significantly improves the pricing power of traditional asset pricing models (CH4 and FF5) and outperforms momentum and reversal factors. The factor's performance is influenced by macroeconomic conditions and market sentiment.,True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:49:13.269265
c25e22f6b2dd0dd1,A further analysis of robust regression modeling and data mining corrections testing in global stocks,"In this analysis of the risk and return of stocks in global markets, we build a reasonably large number of stock selection models and create optimized portfolios to outperform a global benchmark. We apply robust regression techniques, LAR regression, and LASSO regression modeling to estimate stock selection models. Markowitz-based optimization techniques is used in portfolio construction within a global stock universe. We apply the Markowitz–Xu data mining corrections test to a global stock universe. We find that (1) robust regression applications are appropriate for modeling stock returns in global markets; (2) weighted latent root regression robust regression techniques work as well as LAR and LASSO-Regressions in building effective stock selection models; (3) mean–variance techniques continue to produce portfolios capable of generating excess returns above transactions costs; and (4) our models pass several data mining tests such that regression models produce statistically significant asset selection for global stocks. Recent Sturdy-Regression modeling technique may offer the greatest potential for further research for statistically based stock selection modeling. © 2021 Elsevier B.V., All rights reserved.","Guerard, J.B.; Xu, G.; Markowitz, H.",2021,10.1007/s10479-020-03521-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079228702&doi=10.1007%2Fs10479-020-03521-y&partnerID=40&md5=e087ffa22478c084792e3b0edf880973,scopus,"This study analyzes stock returns in global markets using robust regression, LAR, and LASSO regression for stock selection models and Markowitz-based optimization for portfolio construction. The findings suggest that robust regression is suitable for modeling stock returns, weighted latent root regression performs comparably to LAR and LASSO, mean-variance techniques can generate excess returns, and the models pass data mining tests, indicating statistically significant asset selection. The authors suggest further research into Sturdy-Regression modeling.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:49:29.787254
9d117d54bee8c73e,A general equilibrium (GE) model of the term structure applied to Australian securities,"The Double Square Root (DSR) GE model of the term structure is fitted to Australian security yield data over the period 2 January 1984 to 15 December 1995 - a data set of 3041 yields on four securities: 30 and 90-day BAB: and 5 and 10-year bonds. Applying both the OLS and GMM estimators we find a nonlinear, reduced form relationship between these yields and the risk free rate. So we conclude that GE models explain a diverse range of Australian yield curve shapes and that Australian bond prices are not necessarily inversely related to interest rates. © 199S Roulledge. © 2017 Elsevier B.V., All rights reserved.","Felmingham, B.S.; Norton Grey, W.",1998,10.1080/135048598354122,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33744459271&doi=10.1080%2F135048598354122&partnerID=40&md5=14ea2b3eb6434db4e77834f89eebf639,scopus,"This paper applies the Double Square Root (DSR) General Equilibrium (GE) model to Australian security yield data from 1984 to 1995, analyzing 30-day and 90-day BABs, and 5-year and 10-year bonds. Using OLS and GMM estimators, the study finds a nonlinear relationship between yields and the risk-free rate, suggesting GE models can explain various yield curve shapes in Australia and that bond prices are not always inversely related to interest rates.",True,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:49:57.404103
1c4239d6298f5cb3,A generalized bootstrap method to determine the yield curve,"A new technique is described for operationalizing the bootstrap methodology to estimate the yield curve given any available data set of bond yields. The problem of missing data points is dealt with using symbolic cubic spline interpolation. To make such an approach tractable the computer algebra system Maple is employed to symbolically generate the interpolation equations for the missing data points and to solve the nonlinear equation system in order to obtain the points on the yield curve. Several examples with real data demonstrate the usefulness of the methodology. © 2000, Taylor & Francis Group, LLC. © 2019 Elsevier B.V., All rights reserved.","Deaves, R.; Parlar, M.",2000,10.1080/13504860010021162,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066185898&doi=10.1080%2F13504860010021162&partnerID=40&md5=af033d7b30bfd2da8123b454f987e7d0,scopus,"This paper introduces a generalized bootstrap method for estimating the yield curve using any available bond yield data. It addresses missing data points through symbolic cubic spline interpolation, utilizing the Maple computer algebra system to generate and solve interpolation equations. The methodology's effectiveness is demonstrated with real data examples.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:50:13.207688
42306d1389b76f62,A hybrid EMD-AR model for nonlinear and non-stationary wave forecasting,"Accurate wave forecasting with a couple of hours of warning time offers improvements in safety for maritime operation-related activities. Autoregressive (AR) model is an efficient and highly adaptive approach for wave forecasting. However, it is based on linear and stationary theory and hence has limitations in forecasting nonlinear and non-stationary waves. Inspired by the capability of empirical mode decomposition (EMD) technique in handling nonlinear and non-stationary signals, this paper describes the development of a hybrid EMD-AR model for nonlinear and non-stationary wave forecasting. The EMDAR model was developed by coupling an AR model with the EMD technique. Nonlinearity and non-stationarity were overcome by decomposing the wave time series into several simple components for which the AR model is suitable. The EMD-AR model was implemented using measured significant wave height data from the National Data Buoy Center, USA. Prediction results from various locations consistently show that the hybrid EMD-AR model is superior to the AR model. This demonstrates that the EMD technique is effective in processing nonlinear and non-stationary waves.",,2016,10.1631/jzus.a1500164,,proquest,"This paper proposes a hybrid Empirical Mode Decomposition (EMD) and Autoregressive (AR) model (EMD-AR) for forecasting nonlinear and non-stationary wave data. The EMD technique decomposes the wave time series into simpler components, allowing the AR model to perform more effectively. The model was tested using significant wave height data and showed superior performance compared to the standalone AR model, demonstrating the effectiveness of EMD in handling complex wave dynamics.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:50:31.831936
06dce1e4e6c88e26,A hybrid convolutional neural network with long short-term memory for statistical arbitrage,"We propose a CNN-LSTM deep learning model, which has been trained to classify profitable from unprofitable spread sequences of cointegrated stocks, for a large scale market backtest ranging from January 1991 to December 2017. We show that the proposed model can achieve high levels of accuracy and successfully derives features from the market data. We formalize and implement a trading strategy based on the model output which generates significant risk-adjusted excess returns that are orthogonal to market risks. The generated out-of-sample Sharpe ratio and alpha coefficient significantly outperform the reference model, which is based on a standard deviation rule, even after accounting for transaction costs.","Eggebrecht, P.; Luetkebohmert, E.",2023,10.1080/14697688.2023.2181707,,wos,"This paper introduces a hybrid CNN-LSTM deep learning model for statistical arbitrage in financial markets. The model classifies profitable spread sequences of cointegrated stocks and is backtested on data from 1991 to 2017. The strategy based on the model achieves significant risk-adjusted excess returns, outperforming a reference model in terms of Sharpe ratio and alpha, even after transaction costs.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:50:39.317893
d009405b1fa5fcaa,A hybrid novel framework for flood disaster risk control in developing countries based on smart prediction systems and prioritized scenarios,"A Decision Support System (DSS) is a highly efficient concept for managing complex objects in nature or human-made phenomena. The main purpose of the present study is related to designing and implementation of real-time monitoring, prediction, and control system for flood disaster management as a DSS. Likewise, the problem of statement in the research is correlated to implementation of a system for different climates of Iran as a unique flood control system. For the first time, this study coupled hydrological data mining, Machine Learning (ML), and Multi-Criteria Decision Making (MCDM) as smart alarm and prevention systems. Likewise, it created the platform for conditional management of floods in Iran's different clusters of climates. According to the KMeans clustering system, which determines homogeneity of the hydrology of a specific region, Iran's rainfall is heterogeneous with 0.61 score, which is approved high efficiency of clustering in a vast country such as Iran with four seasons and different climates. In contrast, the relation of rainfall and flood disaster is evaluated by Nearest Neighbors Classification (NNC), Stochastic Gradient Descent (SGD), Gaussian Process Classifier (GPC), and Neural Network (NN) algorithms which have an acceptable correlation coefficient with a mean of 0.7. The machine learning outputs demonstrated that based on valid data existence problems in developing countries, just with verified precipitation records, the flood disaster can be estimated with high efficiency. In the following, Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) method as a Game Theory (GT) technique ranked the preventive flood damages strategies through three social (Se 1), environmental (Se 2), and economic (Se 3) crises scenarios. The solutions of flood disaster management are collected from literature review, and the opinion approves them of 9 senior experts who are retired from a high level of water resource management positions of Iran. The outcomes of the TOPSIS method proved that National announcement for public-institutional participation for rapid response and funding (G1-2), Establishment of delay structures to increase flood focus time to give the animals in the ecosystem the opportunity to escape to the upstream points and to preserve the habitat (G 2–8), and Granting free national financial resources by government agencies in order to rebuild sensitive infrastructure such as railways, hospitals, schools, etc. to the provincial treasury (G3-10) are selected as the best solution of flood management in Social, Environmental, and Economic crises, respectively. Finally, the collected data are categorized in Social, Environmental, and Economic aspects as three dimensions of Sustainable Development Goals (SDGs) and ranked based on the opinion of 32 experts in the five provinces of present case studies.",,2022,10.1016/j.jenvman.2022.114939,,proquest,"This study proposes a hybrid framework for flood disaster risk control in developing countries, integrating real-time monitoring, prediction, and control as a Decision Support System (DSS). It combines hydrological data mining, Machine Learning (ML), and Multi-Criteria Decision Making (MCDM) for smart alarm and prevention systems. The framework was applied to Iran's diverse climates, using KMeans clustering for hydrological homogeneity and ML algorithms (Nearest Neighbors, SGD, GPC, NN) to correlate rainfall with flood disasters, achieving a mean correlation coefficient of 0.7. The TOPSIS method, a Game Theory technique, ranked preventive strategies based on social, environmental, and economic crises, with expert opinions guiding the selection of top solutions for each crisis type. The findings were categorized according to Sustainable Development Goals (SDGs).",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:50:49.823663
ac5e1389100fd935,A machine learning based asset pricing factor model comparison on anomaly portfolios,"We frame asset pricing linear factor models in a machine learning context and consider related comparisons of their predictive performance against ordinary least squares linear regression over a dataset of anomaly portfolios. Specific regression models involved in the comparison include regularized linear, support vector machines, neural networks, and tree based models among others. Performance metrics are presented on a model, portfolio group, and sequential basis, and the strongest predictors are recommended as alternative techniques for the problem of excess return forecasting.",,2021,10.1016/j.econlet.2021.109919,,proquest,"This study compares the predictive performance of various machine learning-based asset pricing factor models against ordinary least squares linear regression using anomaly portfolios. The models evaluated include regularized linear models, support vector machines, neural networks, and tree-based models. Performance is assessed across different levels, and the most effective predictors for excess return forecasting are identified.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:51:05.953604
0c90039e02d794b5,A machine learning portfolio allocation system for IPOs in Korean markets using GA-rough set theory,"An initial public offering (IPO) is a type of public offering in which a company's shares are sold to institutional and individual investors. While the majority of studies on IPOs have focused on the efficiency of raising capital and price adequacy in IPOs, studies on portfolio allocation strategies for IPO stocks are relatively scarce. This paper develops a machine learning investment strategy for IPO stocks based on rough set theory and a genetic algorithm (GA-rough set theory). To reduce issues of information asymmetry, we use nonfinancial data that are publicly available to individual and institutional investors in the IPO process. Based on the rule sets generated from the training sets, we conduct 120 tests with various conditions involving the target days and the partition of the training and testing sets, and we find excess returns of the constructed portfolios compared to the benchmark portfolios. Investors in IPO stocks can formulate more efficient investment strategies using our system. In this sense, the system developed in this paper contributes to the efficiency of financial markets and helps achieve sustained economic growth. © 2021 Elsevier B.V., All rights reserved.","Kim, J.; Shin, S.; Lee, H.S.; Oh, K.J.",2019,10.3390/su11236803,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076596987&doi=10.3390%2Fsu11236803&partnerID=40&md5=13ff053dd22d5b0ae2fb194bf4f0dde3,scopus,"This paper proposes a machine learning system for IPO stock portfolio allocation in Korean markets, utilizing GA-rough set theory and nonfinancial data to address information asymmetry. The system demonstrated excess returns compared to benchmarks across 120 tests, suggesting potential for more efficient investment strategies and contributing to financial market efficiency.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:51:14.461424
d87bd95a61553bb9,A micro data approach to the identification of credit crunches,"This article presents a micro data approach to the identification of credit crunches. Using a survey among German firms which regularly queries the firms' assessment of the current willingness of banks to extend credit, we estimate the probability of a restrictive loan supply policy by time taking into account the creditworthiness of borrowers. Creditworthiness is approximated by firm-specific factors, e.g. the firms' assessment of their current business situation and their business expectations. After controlling for the return on the banks' risk-free investment alternative, which is also likely to affect the supply of loans, we derive a credit crunch indicator, which measures that part of the shift in the loan supply that is neither explained by firm-specific factors nor by the opportunity costs of providing risky loans. © 2012 Taylor and Francis Group, LLC. © 2012 Elsevier B.V., All rights reserved.","Rottmann, H.; Wollmershäuser, T.",2013,10.1080/00036846.2012.665604,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859759058&doi=10.1080%2F00036846.2012.665604&partnerID=40&md5=f41992b5331f82089cdad15d091c36c9,scopus,"This article proposes a microdata-based method to identify credit crunches by analyzing German firms' perceptions of bank credit availability. It estimates the probability of restrictive loan supply policies, accounting for borrower creditworthiness (approximated by firm-specific factors like business situation and expectations) and the opportunity cost of lending. A credit crunch indicator is derived, isolating unexplained shifts in loan supply.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:51:24.894212
8fa57119b90ec594,A model-selection approach to assessing the information in the term structure using linear models and artificial neural networks,"We take a model-selection approach to the question of whether forward-interest rates are useful in predicting future spot rates, using a variety of out-of-sample forecast-based model-selection criteria—forecast mean squared error, forecast direction accuracy, and forecast-based trading- system profitability. We also examine the usefulness of a class of novel prediction models called artificial neural networks and investigate the issue of appropriate window sizes for rolling-window- based prediction methods. Results indicate that the premium of the forward rate over the spot rate helps to predict the sign of future changes in the interest rate. Furthermore, model selection based on an in-sample Schwarz information criterion (SIC) does not appear to be a reliable guide to out-of-sample performance in the case of short-term interest rates. Thus, the in-sample SIC apparently fails to offer a convenient shortcut to true out-of-sample performance measures. © 1995 Taylor & Francis Group, LLC. © 2016 Elsevier B.V., All rights reserved.","Swanson, N.R.; White, H.",1995,10.1080/07350015.1995.10524600,https://www.scopus.com/inward/record.uri?eid=2-s2.0-21844518145&doi=10.1080%2F07350015.1995.10524600&partnerID=40&md5=3fb7e2b2653ab9f836c914042957ea00,scopus,"This study uses a model-selection approach to evaluate the predictive power of forward interest rates for future spot rates, employing forecast-based criteria like mean squared error, direction accuracy, and trading system profitability. It also explores artificial neural networks (ANNs) as prediction models and investigates optimal window sizes for rolling-window methods. The findings suggest that the forward rate premium can predict the direction of future interest rate changes, but in-sample model selection criteria like the Schwarz Information Criterion (SIC) do not reliably predict out-of-sample performance for short-term rates.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T09:51:38.291967
6a29710c06b32305,A neural-network analyzer for mortality forecast,"This article proposes a neural-network approach to predict and simulate human mortality rates. This semi-parametric model is capable to detect and duplicate non-linearities observed in the evolution of log-forces of mortality. The method proceeds in two steps. During the first stage, a neural-network-based generalization of the principal component analysis summarizes the information carried by the surface of log-mortality rates in a small number of latent factors. In the second step, these latent factors are forecast with an econometric model. The term structure of log-forces of mortality is next reconstructed by an inverse transformation. The neural analyzer is adjusted to French, UK and US mortality rates, over the period 1946-2000 and validated with data from 2001 to 2014. Numerical experiments reveal that the neural approach has an excellent predictive power, compared to the Lee-Carter model with and without cohort effects. © 2018 Elsevier B.V., All rights reserved.","Hainaut, D.",2018,10.1017/asb.2017.45,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041374415&doi=10.1017%2Fasb.2017.45&partnerID=40&md5=95ab9244ef17be4d7010801baa451b81,scopus,"This article introduces a neural network model for predicting human mortality rates by summarizing log-mortality rates into latent factors, forecasting these factors with an econometric model, and reconstructing the mortality structure. The model was applied to French, UK, and US mortality data and showed superior predictive power compared to the Lee-Carter model.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:52:25.257120
6094397f5a71118e,A new portmanteau test for predictive regression models with possible embedded endogeneity,"In the widely used predictive regression model, any possible serial correlation in innovations leads to estimation bias and statistical inference distortions. Hence, it is important to pretest the existence of such serial correlation. Nevertheless, in the presence of embedded endogeneity, which is a common problem in the predictive regression setting, traditional serial correlation tests such as Box-Pierce (BP) and Ljung-Box (LB) tests are found to perform poorly. Motivated by this, we develop a new portmanteau test in this article as a pretest for serial correlation in predictive regression under possible embedded endogeneity. This test is based on the sample splitting idea and the jackknife empirical likelihood method. The asymptotic distribution of the proposed test has been derived, and the Monte Carlo simulations confirm good finite sample performances. As an illustration, we apply our proposed test in pretesting the serial correlation in predictive regression, where financial variables are used to predict the excess return of S&P 500.","Rao, Yao; Fan, Yawen; Ao, Huimin; Liu, Xiaohui",2024,10.1111/jtsa.12745,,wos,"This paper proposes a new portmanteau test to detect serial correlation in predictive regression models, particularly when endogeneity is present. The test utilizes sample splitting and jackknife empirical likelihood methods. Simulation results show good performance, and the test is illustrated with an application predicting S&P 500 excess returns using financial variables.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:52:38.066263
a5854e7f313205c3,A non-linear dynamic model of the variance risk premium,We propose a new class of non-linear diffusion processes for modeling financial markets data. Our non-linear diffusions are obtained as transformations of affine processes. We show that asset-pricing and estimation is possible and likelihood estimation is straightforward. We estimate a non-linear diffusion model for the VIX index under both the objective measure and the risk-neutral measure where the latter is obtained from futures prices. We find evidence of significant non-linearity under both measures. We define the difference between the P and Q drift as a measure of the variance risk premium and show that it has strong predictive power for stock returns.,,2015,10.1016/j.jeconom.2015.02.038,,proquest,"This paper introduces a novel non-linear diffusion model for financial markets, derived from transformations of affine processes. The model allows for straightforward asset pricing and likelihood estimation. Empirical analysis using the VIX index under both objective and risk-neutral measures reveals significant non-linearity. The variance risk premium, defined as the difference between P and Q drifts, is shown to be a strong predictor of stock returns.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:52:52.338448
f1226e3a19defec0,A nonlinear Bayesian filtering approach to estimating adaptive market efficiency,"The adaptive market hypothesis (AMII) supplies a convincing motivation for why market efficiency should not be regarded as a stable property in time. This paper explores a Bayesian methodology for estimating weak-form market efficiency under the AMII using a test of evolving efficiency (TEE). More precisely, a generalized TEE (GTEE) approach is proposed in which the conditional first moment of a time series is assumed to be a nonlinear function of its conditional second moment, i.e., a nonlinear feedback term is present in the conditional mean equation. We then discuss a maximum likelihood estimation procedure for the resulting nonlinear model using the state-space approach and extended Kalman filtering. This methodology is used to estimate time-varying, weak-form market efficiency in four, specifically chosen, markets over a time period that includes the global financial crisis of 2007/2008.","Kulikov, Gennady Yu.; Taylor, David R.; Kulikova, Maria V.",2019,10.1515/rnam-2019-0003,,wos,"This paper proposes a Bayesian methodology using a generalized test of evolving efficiency (GTEE) to estimate weak-form market efficiency under the adaptive market hypothesis. The GTEE assumes a nonlinear relationship between the conditional first and second moments of a time series. The authors use a state-space approach and extended Kalman filtering for estimation and apply it to four markets, including the period of the 2007/2008 global financial crisis.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:53:07.145020
cf9f62572b2d6956,A nonlinear general equilibrium model of the term structure of interest rates,"We derive and test an alternative closed-form general equilibrium model of the term structure within the Cox, Ingersoll, and Ross theoretical framework in which yields are nonlinear functions of the risk-free rate. We show that equilibrium bond prices and the risk-free rate are not always inversely related and that bond risk need not be strictly increasing in maturity. Using Hansen's generalized method of moments to obtain parameter estimates, this nonlinear model outperforms the Cox, Ingersoll, and Ross square root model in describing actual Treasury bill yields for the 1964-1986 period. © 1989. © 2014 Elsevier B.V., All rights reserved.","Longstaff, F.A.",1989,10.1016/0304-405x(89)90056-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0344971168&doi=10.1016%2F0304-405X%2889%2990056-1&partnerID=40&md5=da37d31c6e2c300ffdead5c18b5cebfc,scopus,"This paper presents a nonlinear general equilibrium model of the term structure of interest rates, building upon the Cox, Ingersoll, and Ross framework. The model suggests that yields are nonlinear functions of the risk-free rate, and that bond prices and risk-free rates are not always inversely related, with bond risk not necessarily increasing with maturity. Empirical testing using the generalized method of moments shows this nonlinear model outperforms the original Cox, Ingersoll, and Ross model in describing Treasury bill yields from 1964-1986.",True,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:53:32.185798
0f09673e5b50a64e,A nonlinear model of the term structure of interest rates,"We present an economically motivated two-factor term structure model that generalizes existing stochastic mean term structure models. By allowing a certain parameter to acquire dynamical behavior we extend the two-factor model to obtain a nonlinear three-factor model that is shown, in a deterministic version, to be equivalent to the Lorenz system of differential equations. With reasonable parameter values the model exhibits chaotic behavior. It successfully emulates certain properties of interest rates including cyclical behavior on a business cycle time scale. Estimation and pricing issues are discussed. Standard PCA techniques used to estimate HJM type models are observed to be equivalent to dimensional estimates commonly applied to 'spatial data' in nonlinear systems analysis. It is concluded that techniques commonly used in the analysis of nonlinear systems may be directly applicable to interest rate models, offering new insights in the development of these models. Tests of nonlinearity in interest rate behavior may need to focus on long cycle times. © 2018 Elsevier B.V., All rights reserved.","Tice, J.; Webber, N.",1997,10.1111/1467-9965.00030,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040657144&doi=10.1111%2F1467-9965.00030&partnerID=40&md5=c5199d870e62e9702b95e8b3acf50c5b,scopus,"This paper introduces a nonlinear, two-factor term structure model for interest rates that generalizes existing stochastic mean models. The model can exhibit chaotic behavior and successfully emulates properties of interest rates, including cyclical behavior on a business cycle timescale. It suggests that techniques from nonlinear systems analysis can be applied to interest rate models, and that tests for nonlinearity in interest rate behavior should consider long cycle times.",True,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:55:41.055349
75bf2bd602a393cf,A parametric factor model of the term structure of mortality,"The prototypical Lee–Carter mortality model is characterized by a single common time factor that loads differently across age groups. In this paper, we propose a parametric factor model for the term structure of mortality where multiple factors are designed to influence the age groups differently via parametric loading functions. We identify four different factors: a factor common for all age groups, factors for infant and adult mortality, and a factor for the “accident hump” that primarily affects mortality of relatively young adults and late teenagers. Since the factors are identified via restrictions on the loading functions, the factors are not designed to be orthogonal but can be dependent and can possibly cointegrate when the factors have unit roots. We suggest two estimation procedures similar to the estimation of the dynamic Nelson–Siegel term structure model. First, a two-step nonlinear least squares procedure based on cross-section regressions together with a separate model to estimate the dynamics of the factors. Second, we suggest a fully specified model estimated by maximum likelihood via the Kalman filter recursions after the model is put on state space form. We demonstrate the methodology for US and French mortality data. We find that the model provides a good fit of the relevant factors and, in a forecast comparison with a range of benchmark models, it is found that, especially for longer horizons, variants of the parametric factor model have excellent forecast performance. © 2019 Elsevier B.V., All rights reserved.","Haldrup, N.; Rosenskjold, C.P.T.",2019,10.3390/econometrics7010009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069524567&doi=10.3390%2Feconometrics7010009&partnerID=40&md5=25f49d47a3a9211b38babce433e20709,scopus,"This paper proposes a parametric factor model for the term structure of mortality, extending the Lee-Carter model by incorporating multiple factors with parametric loading functions that influence age groups differently. It identifies factors for common influence, infant/adult mortality, and the 'accident hump'. The model is estimated using nonlinear least squares or maximum likelihood via the Kalman filter and is demonstrated with US and French mortality data. The model shows good fit and excellent forecast performance, especially for longer horizons, compared to benchmark models.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:55:47.956858
27cdb45de7c97a28,A parametric nonlinear model of term structure dynamics,"Recent nonparametric estimation studies pioneered by Aït-Sahalia document that the diffusion of the short rate is similar to the parametric function, r1.5, estimated by Chan et al., whereas the drift is substantially nonlinear in the short rate. These empirical properties call into question the efficacy of the existing affine term structure models and beg for alternative models which admit the observed behavior. This article presents such a model. Our model delivers closed-form solutions for bond prices and a concave relationship between the interest rate and the yields. We show that in empirical analyses, our model outperforms the one-factor affine models in both time-series as well as cross-sectional tests. © 2016 Elsevier B.V., All rights reserved.","Ahn, D.-H.; Gao, B.",1999,10.1093/rfs/12.4.721,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033410115&doi=10.1093%2Frfs%2F12.4.721&partnerID=40&md5=1194eb86b455e11c6bbe2174bdea396f,scopus,"This article proposes a nonlinear parametric model for term structure dynamics, addressing limitations of existing affine models by incorporating nonlinear drift and a diffusion similar to r1.5. The model provides closed-form solutions for bond prices and a concave interest rate-yield relationship, outperforming one-factor affine models in empirical tests.",True,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:56:17.706265
78921cc3f5612ddb,A re-evaluation of the term spread as a leading indicator,"Forecasting the evolution path of macroeconomic variables has always been of keen interest to policy makers and market participants. A common tool used in the relevant forecasting literature is the term spread of Treasury bond yields. In this paper, we decompose the term spread into an expectation and a term premium component and evaluate the informational content of each component in forecasting the GDP growth rate and inflation in various forecasting horizons. In doing so, we employ alternative decomposition procedures and introduce the Support Vector Regression (SVR) methodology from the field of Machine Learning, coupled with linear and non-linear kernels as a novel forecasting method in the field. Using rolling windows in producing point and conditional probability distribution forecasts we find that neither the term spread, nor its decomposition components possess the ability to accurately forecast output growth or inflation. Our findings extend the existing literature, since they are focused on an explicit out-of-sample evaluation in contrast to most existing empirical studies that produce only in-sample forecasts. To strengthen our findings, we also consider several control variables suggested in the relevant literature without significant qualitative differences from the initial results. The main innovation of our approach stems from the use of the non-linear Support Vectors Machine methodology, that is introduced for the first time in this line of research for forecasting out-of-sample. © 2019 Elsevier B.V., All rights reserved.","Plakandaras, V.; Gogas, P.; Papadimitriou, T.; Gupta, R.",2019,10.1016/j.iref.2019.07.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071871619&doi=10.1016%2Fj.iref.2019.07.002&partnerID=40&md5=b1cce98b67b0a1da2aeeb00c223c6d90,scopus,"This paper re-evaluates the term spread of Treasury bond yields as a leading indicator for GDP growth and inflation. It decomposes the term spread into expectation and term premium components and uses Support Vector Regression (SVR) with linear and non-linear kernels for out-of-sample forecasting. The study finds that neither the term spread nor its components accurately forecast output growth or inflation, even with control variables. The novelty lies in the application of non-linear SVR for out-of-sample forecasting in this research area.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T09:56:52.172836
fade9effb1bee170,A risk index model for uncertain portfolio selection with background risk,"This study proposes a new uncertain risk index model with background risk and presents its deterministic equivalents. The security returns and background asset returns are assumed as uncertain variables and estimated by experts. To discuss the influence of background risk on investment decisions, we compare the proposed model with a variant without background risk and find that the portfolio with background risk produces an equal or lower return than the one without background risk. The effects of changes in the standard deviation of background asset and the risk-free interest rate on optimal expected value are discussed. Two different risk measures for portfolio optimization model with background risk are compared, viz., the risk index model with background risk is further compared with the mean chance model with background risk. The nonlinear risk index model is solved by using a genetic algorithm. The efficiency of the genetic algorithm and the applications of the proposed models are illustrated through numerical experiments.",,2021,10.1016/j.cor.2021.105331,,proquest,"This study introduces an uncertain risk index model incorporating background risk, with its deterministic equivalents presented. It assumes security and background asset returns as uncertain variables, estimated by experts. The research compares the model with background risk to one without, finding that background risk can lead to equal or lower returns. It also analyzes the impact of background asset standard deviation and risk-free interest rates on optimal expected value. The study compares two risk measures for portfolio optimization with background risk and uses a genetic algorithm to solve the nonlinear risk index model, demonstrating its efficiency and applications through numerical experiments.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:57:01.223802
e897cf6b8fd84b04,A semiparametric approach for modelling multivariate nonlinear time series,"AbstractIn this article, a semiparametric time‐varying nonlinear vector autoregressive (NVAR) model is proposed to model nonlinear vector time series data. We consider a combination of parametric and nonparametric estimation approaches to estimate the NVAR function for both independent and dependent errors. We use the multivariate Taylor series expansion of the link function up to the second order which has a parametric framework as a representation of the nonlinear vector regression function. After the unknown parameters are estimated by the maximum likelihood estimation procedure, the obtained NVAR function is adjusted by a nonparametric diagonal matrix, where the proposed adjusted matrix is estimated by the nonparametric kernel estimator. The asymptotic consistency properties of the proposed estimators are established. Simulation studies are conducted to evaluate the performance of the proposed semiparametric method. A real data example on short‐run interest rates and long‐run interest rates of United States Treasury securities is analyzed to demonstrate the application of the proposed approach. The Canadian Journal of Statistics 47: 668–687; 2019 © 2019 Statistical Society of Canada",,2019,10.1002/cjs.11518,,proquest,"This article proposes a semiparametric time-varying nonlinear vector autoregressive (NVAR) model for nonlinear vector time series data. It combines parametric and nonparametric estimation approaches, using a multivariate Taylor series expansion for the parametric part and a nonparametric kernel estimator for adjustment. The method's consistency is established, and its performance is evaluated through simulations and a real-data example on US Treasury securities interest rates.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T09:58:09.147054
4c3e70c64d535aeb,A semiparametric model for the systematic factors of portfolio credit risk premia,"The aim of this paper is to investigate the empirical relationship between daily fluctuations in the risk premium for holding a large diversified credit portfolio, which we approximate by a benchmark credit index, and some tradeable market factors which capture systematic risk. The analysis is based on an adaptive nonparametric modelling approach which allows for the data-driven estimation of the nonlinear dynamic relationship between portfolio credit risk premia and their hypothetical components. Our main finding is that the empirical weights of the systematic factors display sudden jumps during market crises and a less intense time-dependent behaviour during normal market conditions. In addition, we find that during market crises the directions of the empirical relationships are often inconsistent with ordinary economic intuition, as they are influenced by the specific circumstances of financial markets distress. All rights reserved, Elsevier",,2009,10.1016/j.jempfin.2009.05.001,,proquest,"This paper proposes a semiparametric model to analyze the relationship between daily fluctuations in credit portfolio risk premium and systematic market factors. It uses an adaptive nonparametric approach to estimate nonlinear dynamics and finds that factor weights change abruptly during market crises, with relationships sometimes deviating from economic intuition during distress.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:58:31.629314
1cef435702aea303,A simple test of momentum in foreign exchange markets,"This study proposes a new method for testing for the presence of momentum in nominal exchange rates, using a probabilistic approach. We illustrate our methodology estimating a binary response model using information on local currency / US dollar exchange rates of eight emerging economies. After controlling for important variables affecting the behavior of exchange rates in the short-run, we show evidence of exchange rate inertia; in other words, we find that exchange rate momentum is a common feature in this group of emerging economies, and thus foreign exchange traders participating in these markets are able to make excess returns by following technical analysis strategies. We find that the presence of momentum is asymmetric, being stronger in moments of currency depreciation than of appreciation. This behavior may be associated with central bank intervention. © 2014 Elsevier B.V., All rights reserved.","García-Suaza, A.F.; Gomez-Gonzalez, J.E.",2012,10.2753/ree1540-496x480504,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905081033&doi=10.2753%2FREE1540-496X480504&partnerID=40&md5=8cc1c4643bfbf2bfda3ee8a5e71341b6,scopus,"This study introduces a probabilistic method to detect momentum in nominal exchange rates, specifically focusing on eight emerging economies' local currency/USD exchange rates. The findings indicate exchange rate inertia and the potential for excess returns through technical analysis, with momentum being more pronounced during currency depreciation, possibly due to central bank intervention.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:58:44.262548
bf4b6db3d4bf4d55,A sparse enhanced indexation model with chance and cardinality constraints,"Enhanced indexation aims to construct a portfolio to track and outperform the performance of a stock market index by employing both passive and active fund management strategies. This paper presents a novel sparse enhanced indexation model with chance and cardinality constraints. Its goal is to maximize the excess return that can be attained with a high probability, while the model allows a fund manger to limit the number of stocks in the portfolio and specify the maximum tolerable relative market risk. In particular, we model the asset returns as random variables and estimate their probability distributions by the Capital Asset Pricing Model or Fama-French 3-factor model, and measure the relative market risk with the coherent semideviation risk function. We deal with the chance constraint via distributionally robust approach and present a second-order cone programming and a semidefinite programming safe approximation for the model under different sets of potential distribution functions. A hybrid genetic algorithm is applied to solve the NP-hard problem. Numerical tests are conducted on the real data sets from major international stock markets, including USA, UK, Germany and China. The results demonstrate that the proposed model and the method can efficiently solve the enhanced indexation problem and our approach can generally achieve sparse tracking portfolios with good out-of-sample excess returns and high robustness.",,2018,10.1007/s10898-017-0513-1,,proquest,"This paper introduces a sparse enhanced indexation model that aims to maximize excess return with high probability, while limiting the number of stocks and relative market risk. It uses random variables for asset returns, estimates distributions via CAPM or Fama-French 3-factor models, and measures risk with semideviation. The model is approximated using second-order cone programming and semidefinite programming, and solved with a hybrid genetic algorithm. Numerical tests on international stock market data show the model efficiently creates sparse portfolios with good out-of-sample excess returns and robustness.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:58:53.429792
78460dede1ee17a6,A stock selection algorithm hybridizing grey wolf optimizer and support vector regression,"Artificial intelligence remarkably facilitates quantitative investment. A latest intelligent search algorithm, grey wolf optimizer, is well integrated with support vector regression machine to obtain the optimal portfolio. The performance of the hybrid algorithm is empirically investigated through transactional and financial data from stock markets of America and China. The experimental results indicate that (i) the proposed algorithm is able to stably achieve excess returns; (ii) compared with genetic algorithm, particle swarm optimization, gravitational search algorithm and harmony search, the enhanced grey wolf optimizer significantly boots the predictive performance of support vector regression machine; (iii) the proposed algorithm can achieve the better profitability and the higher reliability in Chinese A-share market.",,2021,10.1016/j.eswa.2021.115078,,proquest,"This study proposes a hybrid algorithm combining the Grey Wolf Optimizer (GWO) and Support Vector Regression (SVR) for stock selection and portfolio optimization. The algorithm's performance is evaluated using US and Chinese stock market data, demonstrating its ability to generate excess returns and outperform other metaheuristic algorithms in predictive accuracy. The hybrid approach shows improved profitability and reliability, particularly in the Chinese A-share market.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:58:56.975007
c39f49834e05eba2,A structural econometric model of price discrimination in the French mortgage lending industry,"We propose a model of discrimination in the market for mortgages. The model explains accepted loan applications and simultaneously determines loan sizes and interest rates. A competitive and a discriminating monopoly version of the model are proposed. Offered interest rates and loan sizes are a function of observable borrower characteristics. The competitive model rests on a marginal condition, reflecting contract optimality, to which a zero-profit condition is added. In contrast, the discriminating monopoly maximizes profits under a borrower participation constraint, reflecting the availability of a rental market as an outside option. Each version of the model is a bivariate, nonlinear model, and is estimated by standard maximum likelihood methods. The data used for estimation is a sample of clients of a French network of mortgage lenders. We show the presence of ""social discrimination"" in the data, the loan conditions depending not only on the borrower's wage and down payment, but also on the borrower's occupational status. Abnormally high risk premia in the competitive version of the model suggest the presence of market power, justifying an attempt at estimating its monopolistic version. The discriminating monopoly model estimates show that the borrowers' price-elasticity of demand for housing varies with occupational status, and is inversely related with the lender's interest rate markups. This confirms that the lender exploits structural differences in the preferences to discriminate, as predicted by standard theories. © 2003 Elsevier B.V. All rights reserved. © 2008 Elsevier B.V., All rights reserved.","Gary-Bobo, R.J.; Larribeau, S.",2004,10.1016/j.ijindorg.2003.07.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346962780&doi=10.1016%2Fj.ijindorg.2003.07.002&partnerID=40&md5=10ab500b28b6878f5d1a556e8a2af88b,scopus,"This paper presents a structural econometric model to analyze price discrimination in the French mortgage lending market. It considers both competitive and discriminating monopoly scenarios, explaining loan applications, sizes, and interest rates based on borrower characteristics. The study finds evidence of ""social discrimination,"" where loan terms are influenced by occupational status, and suggests market power exists, justifying the analysis of a monopolistic version. The discriminating monopoly model confirms that lenders exploit differences in borrower preferences and price elasticity of demand to discriminate.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:59:01.257634
2aa4181eccce522e,A three-level nested portfolio optimization model with position allocation,"Existing portfolio optimization models cannot well capture the real position allocation requirement, leading to limited impact in practice. To overcome this challenge, we propose a three-level nested portfolio optimization model with position allocation. Within this model, the inner and middle levels collaboratively determine the optimal portfolio, while the outer level focuses on optimizing the holding proportion of each stock in the optimal portfolio. Compared with existing models with portfolio weights, the proposed model imposes the position allocation constraint that precisely characterizes the limitations on holding each stock. This constraint is crucial for investors to obey securities trading regulations involving position limitations and to mitigate the potential impact of market risks. To address the nonlinear and nonconvex nature of the novel model, we develop an intelligent optimization algorithm by effectively hybridizing the support vector regression and the enhanced grey wolf optimizer. We comprehensively evaluate its performance using eight metrics, including accumulative return, annual return, Sharpe ratio, maximum drawdown, absolute and relative win ratios, predictive precision and accuracy. The experimental results indicate that (i) the proposed model can achieve more excess returns than those stock selection models not considering position allocation, especially for the large-cap stocks; (ii) compared with other state-of-the-art meta-heuristics, the enhanced grey wolf optimizer can yield better portfolio in conjunction with the support vector regression; (iii) in the context of the Chinese A-share stock market, specific financial indicators such as return on equity, inventory turnover rate, net income growth rate, and debt-to-equity ratio should be given greater consideration compared to other financial metrics. © 2024 Elsevier B.V., All rights reserved.","Ma, J.; Yang, K.; Luo, K.; Li, P.; He, A.",2024,10.1016/j.asoc.2024.112054,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200798925&doi=10.1016%2Fj.asoc.2024.112054&partnerID=40&md5=7b415008c2dba69527715c4c4b28a8f5,scopus,"This paper introduces a three-level nested portfolio optimization model that incorporates position allocation, addressing limitations in existing models. It uses a hybrid support vector regression and enhanced grey wolf optimizer algorithm to handle the model's complexity. The model is evaluated using eight metrics and shows improved returns, particularly for large-cap stocks, and demonstrates the effectiveness of the proposed algorithm and highlights key financial indicators for the Chinese A-share market.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:59:14.442789
5b8c2d356e2c63ed,A threshold model for the spread,"Using annual data from two panels, one of 11 Eurozone countries and another of 31 OECD countries, we estimate a two-regime log-linear as well as a nonlinear model for the spread as a function of macroeconomic and quality-of-institutions variables. The two regimes, a high-spread and a low-spread regime, are distinguished by using a threshold, in accordance with the perceived ""fair""value of the spread as a reference point. Our results suggest that government-bond spreads are regime-dependent, as most of the regression coefficients of the determinants of the spread are larger (in absolute value) in the high-spread regime than in the low-spread regime. That is, an improvement in the macroeconomic environment (e.g., lower unemployment, lower inflation, lower growth of the debt-to-GDP ratio, less macroeconomic uncertainty, higher growth of real GDP), and/or an improvement in the quality of institutions (e.g., less corruption) reduce the spread facing a country (by enhancing its creditworthiness) to a greater extent in high-spread situations than in low-spread situations. A possible explanation is that the demand for and the supply of loans are inelastic at higher than ""fair""interest rates and elastic at lower rates. © 2023 Elsevier B.V., All rights reserved.","Hatzinikolaou, D.; Sarigiannidis, G.",2023,10.1515/snde-2020-0007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131677707&doi=10.1515%2Fsnde-2020-0007&partnerID=40&md5=13323e4318936c45cc6c14e3e03f58f0,scopus,This study uses annual data from Eurozone and OECD countries to estimate models for government bond spreads. It identifies two regimes (high and low spread) based on a threshold and finds that macroeconomic and institutional factors have a greater impact on reducing spreads in the high-spread regime. The findings suggest that improvements in economic conditions and institutional quality enhance creditworthiness more significantly when spreads are already high.,True,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T09:59:57.680432
7fe86973d0f86928,A two-regime threshold model with conditional skewed Student t distributions for stock returns,"This paper proposes a two-regime threshold model for the conditional distribution of stock returns in which returns follow a distinct skewed Student t distribution within each regime: the model allows capturing time variation in the conditional distribution of returns, as well as higher order moments. An application of the model to daily U.S. stock returns illustrates the advantages of the proposed model in comparison to alternative specifications: the model performs well in terms of in-sample fit; it more accurately estimates the conditional volatility; and it produces useful risk assessment as measured by the term structure of value at risk. (C) 2014 Elsevier B.V. All rights reserved.","Massacci, Daniele",2014,10.1016/j.econmod.2014.07.032,,wos,"This paper introduces a two-regime threshold model using conditional skewed Student t distributions to analyze stock returns. The model captures time-varying conditional distributions and higher-order moments. An application to U.S. stock returns demonstrates its superior in-sample fit, more accurate conditional volatility estimation, and improved risk assessment compared to other models.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:00:12.979154
412fc4122a3c36ac,AN ARBITRAGE-FREE ESTIMATE OF PREPAYMENT OPTION PRICES IN FIXED-RATE GNMA MORTGAGE-BACKED SECURITIES,"In an efficient market, the no-arbitrage condition implies that the price difference between any two assets must be the market value of ail differences in their cash flows. We use this logic to deduce the price of the prepayment option embedded in fixed-rate Government National Mortgage Association (GNMA) mortgage-backed securities. The option price equals the difference between an observed GNMA price and the cost of a synthetic, nonprepayable GNMA constructed from the least expensive portfolio of Treasury securities that exactly replicates the promised GNMA cash flow stream, assuming prepayment is precluded. We regress the option prices on variables found significant in previous prepayment studies, finding that five key regressors explain more than 90% of the prepayment option value in pooled time-series cross-sectional analysis. We also show that the time value of the prepayment option calculated by our method displays a pattern similar to that produced by the Black-Scholes (1973) option pricing model. An additional empirical result is the existence of negative option prices and negative time value of the option prices. We attribute these to the fact that homeowners sometimes exercise their prepayment options when they are out-of-the-money, and to refinancing transaction costs. Our method is independent of assumptions regarding interest rate processes and the homeowner's prepayment behavior, and it provides a benchmark for testing theoretical prepayment models.","RONN, EI; RUBINSTEIN, PD; PAN, FS",1995,10.1111/1540-6229.00655,,wos,"This paper develops an arbitrage-free method to estimate the price of the prepayment option in fixed-rate GNMA mortgage-backed securities. It constructs a synthetic non-prepayable GNMA from Treasury securities and calculates the option price as the difference between the observed GNMA price and the synthetic price. The study finds that five key variables explain over 90% of the prepayment option value and observes negative option prices and time values, attributing them to homeowners exercising options out-of-the-money and refinancing costs. The method is independent of interest rate process assumptions and homeowner behavior, serving as a benchmark for prepayment models.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:00:25.062765
b4063c4f4752f0dc,Accounting for correlations: Price adjustments in unit-cost construction contracts,"Macroeconomic conditions, such as commodity prices, affect the cost of construction projects. In a volatile market environment, contractors respond by adding premiums in bid prices when highway agencies pass such risk on to contractors by using fixed-price contracts. How much of the commodity cost risk should highway agencies pass on to contractors? More specifically, this study aims to investigate the impact of correlation among commodity prices on optimal risk-hedging decisions. A weighted least-squares regression model is used to estimate the risk premium; both univariate time series and vector time series models are estimated and applied to simulate changes in commodity prices over time, including the effect of correlation. A genetic algorithm is used as a solution approach to a multiobjective optimization problem (cost versus future risk exposure). In a case study, project cost risks are shown to be significantly underestimated if correlations are not accounted for. © 2023 Elsevier B.V., All rights reserved.","Zhou, X.; Damnjanović, I.D.",2012,10.3141/2297-17,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026873013&doi=10.3141%2F2297-17&partnerID=40&md5=e436631f1dfbd0d2ab0a0a1fa0768ffd,scopus,"This study investigates the impact of commodity price correlations on optimal risk-hedging decisions in unit-cost construction contracts. It uses a weighted least-squares regression model, univariate and vector time series models, and a genetic algorithm to simulate commodity price changes and optimize cost versus risk exposure. The findings indicate that project cost risks are significantly underestimated when correlations are not considered.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:00:33.948756
54aa7f1532e8bf7d,"Accounting-based probabilistic prediction of ROE, the residual income valuation model and the assessment of mispricing in the Swedish stock market","Using Swedish stock market data, this study investigates whether an investment strategy based on publicly available accounting information can generate abnormal investment returns. The strategy involves two steps. First, an accounting-based probabilistic prediction model of changes in the medium-term book return on owners' equity (ROE) is estimated. Second, market expectations of changes in medium-term ROE are assessed through observed stock prices and the residual income valuation model. Stock market positions over thirty-six-month holding periods are taken when the accounting-based predictions of ROE and the market expectations differ. Over the period 1983-2003, the investment strategy generated values of Jensen's alpha corresponding to an average monthly excess return for a hedge position of up to 0.8% for a sample of manufacturing companies. In the main this hedge return was caused by strong positive returns to the long positions, and additional analyses show that the returns appear to have been affected by a positive market sentiment bias (i.e. positive ROE surprises being associated with stronger price reactions than negative ROE surprises) making out-of-sample inferences somewhat dubious. Furthermore, most of the investment returns accrued over holding periods up to around 1995, with no indications of market mispricing over the last third (1995-2003) of the investment period. The empirical results are consistent with market investors having become more sophisticated in their use of publicly available accounting information over time. Reprinted by permission of Blackwell Publishers",,2010,10.1111/j.1467-6281.2010.00325.x,,proquest,"This study investigates an investment strategy based on accounting information in the Swedish stock market. It uses a probabilistic prediction model for Return on Equity (ROE) and the residual income valuation model to assess market expectations. The strategy generated abnormal returns, particularly before 1995, but the authors suggest potential biases and increasing market sophistication over time, making out-of-sample inferences questionable.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:00:41.922519
1b82b421c3fc63d2,Adaptive Algorithm for Selecting the Optimal Trading Strategy Based on Reinforcement Learning for Managing a Hedge Fund,"In hedge fund management, the ability to dynamically select optimal trading strategies is paramount for maximizing returns and mitigating risk. This paper presents a pioneering approach that integrates Reinforcement Learning (RL), specifically the Proximal Policy Optimization (PPO) algorithm, into the strategy selection process for hedge fund management. Our model considers a diverse array of strategies, including Mean Reversion and Momentum, and employs advanced mathematical frameworks to evaluate and select the strategies. By leveraging RL, our algorithm learns to adaptively adjusts strategy allocations to maximize cumulative returns while adhering to the risk constraints. We demonstrate the effectiveness of our approach through extensive backtesting and validation of historical market data, demonstrating superior performance compared to traditional methods. Nevertheless, it is important to understand that training trading agents requires a considerable amount of time, computing power, and other resources. Our research offers a novel perspective on leveraging RL to optimize strategy selection in hedge fund management and underscores the potential of AI-driven approaches in finance.",B. Belyakov; D. Sizykh,2024,10.1109/access.2024.3515039,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10792442,ieeexplore,"This paper introduces a novel approach using Proximal Policy Optimization (PPO), a Reinforcement Learning algorithm, for adaptive trading strategy selection in hedge fund management. The model evaluates and selects from strategies like Mean Reversion and Momentum, aiming to maximize returns and manage risk. The approach is validated through backtesting, showing improved performance over traditional methods, though it requires significant computational resources for training.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:00:50.862599
e869368c4a48ce29,Advanced Option Pricing and Hedging with Q-Learning: Performance Evaluation of the QLBS Algorithm,"This article provides an overview of the recent advances in reinforcement learning (RL) for pricing and hedging financial instruments, focusing on the Q-Learning Black-Scholes (QLBS) approach. This RL approach bridges the traditional Black-Scholes-Merton (BSM) model with artificial intelligence algorithms, enabling option pricing and hedging in a completely model-free and data-driven way. The study evaluates the QLBS algorithm’s performance across different state variables and scenarios for a European put option, showing that the model accurately estimates option prices under varying volatility levels, hedging frequencies, risk-free rates, and when dividends are included. Additionally, the QLBS method shows robust performance in pricing and hedging options across different moneyness levels, with more pronounced deviations from the BSM model for deep in-the-money options under higher risk aversion. The empirical analysis also incorporates proportional transaction costs using at-the-money S&P 500 call options, revealing varied implications for profitability and risk mitigation, influenced by the choice of state variables. The article sheds light on the QLBS model’s explainability by providing detailed technical notes and a numerical example, and additionally, the results reveal its high practical relevance. © 2025 Elsevier B.V., All rights reserved.","Stoiljkovic, Z.",2025,10.3905/jod.2025.1.222,https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000150110&doi=10.3905%2Fjod.2025.1.222&partnerID=40&md5=2458dcaba8e795c55b724323df4ee627,scopus,"This article introduces the Q-Learning Black-Scholes (QLBS) algorithm, a reinforcement learning approach for model-free, data-driven option pricing and hedging. The study evaluates QLBS performance for European put options under various conditions (volatility, interest rates, dividends, moneyness) and incorporates transaction costs for S&P 500 call options. The QLBS model demonstrates accurate price estimation and robust hedging, with explainability features and practical relevance highlighted.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:01:01.078174
5b1ba23957effe90,Affine arbitrage-free yield net models with application to the euro debt crisis,"We develop a parsimonious class of affine arbitrage-free yield net models for consistent bond pricing across maturities and issuers of different risk levels. Containing a core curve and multiple peripheral curves, the yield net is spanned by three layers of factors: base factors spanning all curves, and common and individual spread factors. Under the arbitrage-free assumption, we prove a parsimonious solution to the risk-neutral process that guarantees joint identification of parameters and latent states. By using a Bayesian estimation method with a marginal Metropolis–Hastings algorithm and specification tests based on MCMC output, we apply the model to weekly treasury yields of Germany, Italy, Spain, and Greece from 2009 to 2016. The results show that the extracted common credit risk is a level factor in spread, and market liquidity risk is a slope factor. Further, the net structure helps reconstruct the Greek yield curve even with only its 10-year yield available throughout the sample. © 2022 Elsevier B.V., All rights reserved.","Hong, Z.; Niu, L.; Zhang, C.",2022,10.1016/j.jeconom.2021.11.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122961603&doi=10.1016%2Fj.jeconom.2021.11.002&partnerID=40&md5=01905cc014d6ef66741afd239638d650,scopus,"This paper introduces a class of affine arbitrage-free yield net models for bond pricing, incorporating multiple curves and factor layers. It uses a Bayesian estimation method with a Metropolis-Hastings algorithm and applies it to European treasury yields from 2009-2016. The findings highlight the roles of common credit risk and market liquidity risk, and demonstrate the model's ability to reconstruct yield curves.",True,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:01:44.474490
f8e2d8cdac56b7a1,"Air pollution, weather factors, and realized volatility forecasts of agricultural commodity futures","This study investigates the potential effects of environmental factors on fluctuations in agricultural commodity futures markets, by constructing a new category of daily exogenous predictors related to air pollution, weather, climate change, and investor attention. The empirical results from out‐of‐sample analyses suggest that the heterogeneous autoregressive (HAR) model incorporating all these exogenous predictors is more likely to outperform other HAR‐type models. Additionally, economic evaluations demonstrate the superior performance of models incorporating investors' attention to climate change or extreme weather as predictors. While not all exogenous predictors are equally important for volatility forecasts, adopting appropriate variable selection methods to handle different sets of exogenous predictors can lead to better performance than the HAR benchmark. With the inclusion of air pollution or weather factors in the HAR model, a portfolio with an annualized average excess return of 16.2068% or a Sharpe ratio of 10.0431 can be achieved for the wheat futures, respectively.",,2024,10.1002/fut.22467,,proquest,"This study explores how air pollution, weather, climate change, and investor attention impact agricultural commodity futures volatility. It proposes an enhanced Heterogeneous Autoregressive (HAR) model incorporating these factors, showing superior out-of-sample performance compared to standard HAR models. The research highlights the importance of variable selection and demonstrates that including air pollution or weather factors can significantly improve portfolio returns and Sharpe ratios for wheat futures.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:01:49.364713
b812228bd22517dc,Algorithmic estimation of risk factors in financial markets with stochastic drift,"We assume a financial market governed by a diffusion process reverting to a stochastic mean which is itself governed by an unobservable ergodic diffusion, similar to those observed in electricity and other energy markets. We develop a moment method algorithm for the estimation of the parameters of both the observable process and the unobservable stochastic mean. Our approach is contrasted with other methods for parameter estimation of partially observed diffusions, and applications to the modelling of interest rates and commodity prices are discussed. (C) 2010 Elsevier Ltd. All rights reserved.","Hernandez, Janko; Saunders, David; Seco, Luis",2012,10.1016/j.cor.2010.09.007,,wos,"This paper proposes a moment method algorithm to estimate parameters in a financial market model with a stochastic drift, applicable to interest rates and commodity prices. The model assumes a diffusion process reverting to a stochastic mean, which is itself governed by an unobservable ergodic diffusion, drawing parallels to energy markets.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:01:53.499060
bcfa69807c6a868e,Alternative models for stock price dynamics,"This paper evaluates the role of various volatility specifications, such as multiple stochastic volatility (SV) factors and jump components, in appropriate modeling of equity return distributions. We use estimation technology that facilitates nonnested model comparisons and use a long data set which provides rich information about the conditional and unconditional distribution of returns. We consider two broad families of models: (I) the multifactor loglinear family, and (2) the affine-jump family. Both classes of models have attracted much attention in the derivatives and econometrics literatures. There are various tradeoffs in considering such diverse specifications. If pure diffusion SV models are chosen over jump diffusions, it has important implications for hedging strategies. If logarithmic models are chosen over affine ones, it may seriously complicate option pricing. Comparing many different specifications of pure diffusion multifactor models and jump diffusion models, we find that (I) log linear models have to be extended to two factors with feedback in the mean reverting factor, (2) affine models have to have a jump in returns, stochastic volatility or probably both. Models (I) and (2) are observationally equivalent on the data set in hand. In either (I) or (2) the key is that the volatility can move violently. As we obtain models with comparable empirical fit, one must make a choice based on arguments other than statistical goodness-of-fit criteria. The considerations include facility to price options, to hedge and parsimony. The affine specification with jumps in volatility might therefore be preferred because of the closed-form derivatives prices. (C) 2003 Elsevier B.V. All rights reserved.","Chernov, M; Gallant, AR; Ghysels, E; Tauchen, G",2003,10.1016/s0304-4076(03)00108-8,,wos,"This paper compares different models for stock price dynamics, including multifactor stochastic volatility and affine-jump models. It finds that models need to account for significant volatility movements, potentially with jumps. The choice between models depends on factors like option pricing and hedging, with affine models with jumps in volatility being potentially preferred for closed-form derivative prices.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:02:04.665509
6935cf79559f8c33,"Ambiguity reduction by objective model selection, with an application to the costs of the EU 2030 climate targets","I estimate the cost of meeting the EU 2030 targets for greenhouse gas emission reduction, using statistical emulators of ten alternative models. Assuming a first-best policy implementation, I find that total and marginal costs are modest. The statistical emulators allow me to compute the risk premiums, which are small, because the EU is rich and the policy impact is small. The ensemble of ten models allows me to compute the ambiguity premium, which is small for the same reason. I construct a counterfactual estimate of recent emissions without the climate policy and use that to test the predictive skill of the ten models. The models that show the lowest cost of emission reduction also have the lowest skill for Europe in recent times. © 2018 Elsevier B.V., All rights reserved.","Tol, R.S.J.",2014,10.3390/en7116886,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912033606&doi=10.3390%2Fen7116886&partnerID=40&md5=885f426cc74a9fa09f0696aff6b5bf0b,scopus,"This study estimates the cost of meeting the EU 2030 climate targets using statistical emulators of ten alternative models. It finds that total and marginal costs are modest, with small risk and ambiguity premiums due to the EU's wealth and the policy's limited impact. The models with lower emission reduction costs showed lower predictive skill for recent European emissions.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:02:23.968181
a68df8aeccec0afd,An Analysis of Some Effects of Rating Information using an Artificial Market,"Standard & Poor' s (S & P) downgraded American government bonds from AAA to AA+ last year. The effects of the downgrade on financial markets have been studied in financial engineering, economics and computational finance, but not in agent-based simulation studies. In this paper, we investigate the effect of the rating system (e.g.: S & P) on asset price fluctuations in the artificial market, which is the agent-based simulation model of the financial market. The rating information is defined as a discrete version of the fundamental value. Four strategies : the noise trader, the fundamentalist, the trend predictor, and the contrarian trader, were assumed in previous studies of the artificial market, plus we assume a new agent called “rating user” which uses the rating value, defined as the discrete value of the fundamental value of an asset. We investigate if the rating user makes the artificial market unstable. First, the simulation results show that kurtosis of an asset price return in the market, without fundamentalists, is higher than without rating users. This suggests the usage of rating information makes the artificial market unstable. The simulation outcomes also suggest volatility continuity of asset price return is stronger in the market without fundamentalists than without rating users. Second, we investigate how two parameters, the update interval and rating length, which control the rating value, makes the market stable. The simulation outcomes show that both standard deviation of asset price return and kurtosis of asset price return becomes smaller as the update interval increases. The standard deviation gets larger and kurtosis of that gets larger with the increasing length of rating. These results imply that the rating information should be updated at short intervals and the length of rating should be moderate to make the artificial market stable. keywords: raing-information, financial market, artificial market, agent-based simulation. © 2012, The Japanese Society for Artificial Intelligence. All rights reserved. © 2017 Elsevier B.V., All rights reserved.","Katsumi, S.; Shimao, H.; Nishiyama, N.",2012,10.1527/tjsai.27.384,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024750711&doi=10.1527%2Ftjsai.27.384&partnerID=40&md5=14a23a707402b1332239499f679a1415,scopus,"This paper investigates the impact of rating information on asset price fluctuations within an artificial market simulation. It introduces a ""rating user"" agent that incorporates rating values into trading strategies. The study finds that the use of rating information can increase market instability, as indicated by higher kurtosis in asset price returns. It also suggests that shorter update intervals and moderate rating lengths contribute to market stability.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:02:39.836978
e040435996e03d39,An Application of Damped Diffusion for Modeling Volatility Dynamics,"This paper proposes a damped constant elasticity variance (CEV) stochastic volatility (DCEV) model, which remedies the possible explosive behavior of the CEV model and also accommodates the mean-reverting dynamics more appropriately than the nonlinear drift (NLD) stochastic volatility model. As the DCEV model maintains the linear drift, an analytic formula is available to efficiently infer latent variances from VIX levels, after which both its physical and risk-neutral parameters can be simultaneously estimated with the maximum-likelihood approach given S&P 500 returns and inferred variances. The DCEV model outperforms the CEV and NLD models in in-sample fitting performance and in out-of-sample variance forecasting under the physical measure. It also exhibits superior ability in out-of-sample option pricing over the CEV and models under the risk-neutral measure. This satisfactory performance demonstrates the suitability of describing volatility dynamics with the DCEV model and the potential of applying this to study other issues.","Hung, Mao-Wei; Ko, Yi-Chen; Wang, Jr-Yan",2023,10.1093/jjfinec/nbab018,,wos,"This paper introduces a Damped Constant Elasticity Variance (DCEV) stochastic volatility model to address limitations in existing models for volatility dynamics. The DCEV model, which allows for mean-reversion and avoids explosive behavior, enables efficient inference of latent variances from VIX levels. The model's parameters are estimated using maximum likelihood with S&P 500 returns and inferred variances. Empirical results show the DCEV model outperforms CEV and Nonlinear Drift (NLD) models in fitting and forecasting volatility under the physical measure, and in option pricing under the risk-neutral measure.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:02:53.195257
7ac4d584ce43caa8,An Automated Framework for Incorporating News into Stock Trading Strategies,"In this paper we present a framework for automatic exploitation of news in stock trading strategies. Events are extracted from news messages presented in free text without annotations. We test the introduced framework by deriving trading strategies based on technical indicators and impacts of the extracted events. The strategies take the form of rules that combine technical trading indicators with a news variable, and are revealed through the use of genetic programming. We find that the news variable is often included in the optimal trading rules, indicating the added value of news for predictive purposes and validating our proposed framework for automatically incorporating news in stock trading strategies.",W. Nuij; V. Milea; F. Hogenboom; F. Frasincar; U. Kaymak,2014,10.1109/tkde.2013.133,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6574843,ieeexplore,"This paper introduces a framework for automatically integrating news into stock trading strategies. It extracts events from free-text news and combines them with technical indicators using genetic programming to create trading rules. The results show that news information improves the trading rules, validating the framework's utility.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:02:56.953279
dde2fd0b63571dc0,An Efficient Deep Learning Based Model to Predict Interest Rate Using Twitter Sentiment,"In macroeconomics, decision making is highly sensitive and significantly influences the financial and business world, where the interest rate is a crucial factor. In addition, the interest rate is used by the governments to manage the monetary policy. There is a need to design an efficient algorithm for interest rate prediction. The analysis of the social media sentiment impact on financial decision making is also an open research area. In this study, we deploy a deep learning model for the accurate forecasting of the interest rate for the UK, Turkey, China, Hong Kong, and Mexico. For this purpose, daily data of the interest rate and exchange rate covering the period from Jan 2010 to Oct 2019 is used for all the mentioned countries. We also incorporate the input of the twitter sentiments of six mega-events, namely the US election 2012, Mexican election 2012, Gaza under attack 2014, Hong Kong protest 2014, Refugee Welcome 2015, and Brexit 2016. Our results provide evidence that the error of the deep learning model significantly decreases when event sentiment is incorporated. A notable improvement has been observed in the case of the Hong Kong interest rate, i.e., a 266% decline in the error after incorporating event sentiments as an input in the deep learning model.","Yasir, Muhammad; Afzal, Sitara; Latif, Khalid; Chaudhary, Ghulam Mujtaba; Malik, Nazish Yameen; Shahzad, Farhan; Song, Oh-young",2020,10.3390/su12041660,,wos,"This study proposes a deep learning model to predict interest rates for the UK, Turkey, China, Hong Kong, and Mexico, incorporating daily interest rate, exchange rate, and Twitter sentiment data from six major global events between 2010 and 2019. The model demonstrated improved accuracy, particularly for Hong Kong, with a significant reduction in prediction error when event sentiment was included.",True,False,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T10:03:11.729118
f3ace5f4c4fc6584,An Empirical Investigation of the Level Effect in Australian Interest Rates,"An extensive literature examines the dynamics of interest rates, with particular attention given to the positive relationship between interest-rate volatility and the level of interest rates—the so-called level effect. This paper examines the interaction between the estimated level effect and competing parameterisations of interest-rate volatility for the Australian yield curve. We adopt a new methodology that estimates elasticity in a multivariate setting that explicitly accommodates the correlations that exist between various yield factors. Results show that significant correlations exist between the residuals of yield factors and that such correlations do indeed impact on model estimates. Within the multivariate setting, the level of the short rate is shown to be a crucial determinant of the conditional volatility of all three yield factors. Measures of model fit suggest that, in addition to the usual level effect, the incorporation of GARCH effects and possible regime shifts is important. © 2008, SAGE Publications. All rights reserved. © 2019 Elsevier B.V., All rights reserved.","Gray, P.; Smith, D.R.",2008,10.1177/031289620803300103,https://www.scopus.com/inward/record.uri?eid=2-s2.0-54849404302&doi=10.1177%2F031289620803300103&partnerID=40&md5=dd13438b705d6c7e42bc5b6e7c4e98a3,scopus,"This paper empirically investigates the level effect in Australian interest rates, examining the relationship between interest rate volatility and the level of interest rates. Using a new multivariate methodology that accounts for correlations between yield factors, the study finds that the level of the short rate significantly influences the conditional volatility of all yield factors. The results suggest that incorporating GARCH effects and regime shifts is important for model fit, in addition to the standard level effect.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:03:29.522231
60d9d62ef78644a0,An adaptively managed dynamic portfolio selection model using a time-varying investment target according to the market forecast,"In this paper, we propose an adaptive investment strategy (AIS) based on a dynamic portfolio selection model (DPSM) that uses a time-varying investment target according to the market forecast. The DPSM allows for flexible investments, setting relatively aggressive investment targets when market growth is expected and relatively conservative targets when the market is expected to be less attractive. The model further allows investments to be liquidated into risk-free assets when the market forecast is pessimistic. By dynamically determining the investment target, the DPSM allows construction of portfolios that are more responsive to market changes, while eliminating the possibility of the model becoming infeasible under certain market conditions. When the proposed DPSM is implemented in real-life investment scenarios using the AIS, the portfolio is rebalanced according to a predefined rebalancing cycle and the model's input parameters are estimated on each rebalancing date using an exponentially weighted moving average (EWMA) estimator. To evaluate the performance of the proposed approach, a 7-year investment experiment was conducted using historical stock returns data from 10 different stock markets around the world. Performance was assessed and compared using diverse measures. Superior performance was achieved using the AIS proposed herein compared with various benchmark approaches for all performance measures. In addition, we identified a converse relationship between the average trading volume of a market and the value of the weighting parameter prescribed to the EWMA estimator, which maximizes cumulative returns in each market. © 2020 Elsevier B.V., All rights reserved.","Jung, J.; Kim, S.",2015,10.1057/jors.2014.72,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937210799&doi=10.1057%2Fjors.2014.72&partnerID=40&md5=8aa29123405930fe4848e304ee973f23,scopus,"This paper introduces an adaptive investment strategy (AIS) using a dynamic portfolio selection model (DPSM) with a time-varying investment target based on market forecasts. The model adjusts investment aggressiveness based on expected market growth, liquidates into risk-free assets during pessimistic forecasts, and is rebalanced using an exponentially weighted moving average (EWMA) estimator. A 7-year experiment with historical stock data from 10 global markets demonstrated superior performance compared to benchmarks.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:46:29.751435
b51ac46514e0b0ea,An automated financial indices-processing scheme for classifying market liquidity regimes,"A multivariate hidden Markov model (HMM)-based approach is developed to capture simultaneously the regime-switching dynamics of four financial market indicators: Treasury-Euro Dollar rate spread, US dollar index, volatility index and S&P 500 bid-ask spread. These indicators exhibit stochasticity, mean reversion, spikes and state memory, and they are deemed to drive the main characteristics of liquidity risk and regarded to mirror financial markets' liquidity levels. In this paper, an online system is proposed in which observed indicators are processed and the results are then interfaced with an advanced alert mechanism that gives out appropriate measures. In particular, two stochastic models, with HMM-modulated parameters switching between liquidity regimes, are integrated to capture the evolutions of the four time series or their transformations. Parameter estimation is accomplished by deriving adaptive multivariate filters. Indicators' joint empirical characteristics are captured well and useful early warnings are obtained for occurrence prediction of illiquidity episodes.",,2021,10.1080/00207179.2019.1616225,,proquest,"This paper proposes an automated system using a multivariate hidden Markov model (HMM) to classify market liquidity regimes based on four financial indicators: Treasury-Euro Dollar rate spread, US dollar index, volatility index, and S&P 500 bid-ask spread. The system processes these indicators and interfaces with an alert mechanism to provide early warnings for illiquidity episodes.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:47:24.783463
8071539ff0d58c9e,An empirical comparison of transformed diffusion models for VIX and VIX futures,"Transformed diffusions (TDs) are nonlinear functions of continuous-time affine diffusion processes. Since they are flexible models with tractable analytic properties, financial modelling with TDs has become increasing popular in recent years. We first provide a formal classification of TD models into drift-driven, diffusion-driven, and distribution-driven according to their empirical emphases and specification strategies. Motivated by the stylized distributional features of VIX such as skewness and excess kurtosis, we then propose a pair of new distribution-driven TDs for modelling VIX dynamics and pricing VIX futures by directly incorporating such information into the specification of the transformation. We conduct a comprehensive empirical investigation into the relative performance of the three classes of models against several empirically relevant criteria. Our focus is on the in-sample goodness-of-fit measure and the out-of-sample forecast accuracy for modelling VIX and pricing VIX futures, as well as the stock return predictability of the implied Variance Risk Premium. Our findings demonstrate that the newly proposed distribution-driven models have clear advantages over well-established alternatives in most of our exercises. © 2017 Elsevier B.V., All rights reserved.","Bu, R.; Jawadi, F.; Li, Y.",2017,10.1016/j.intfin.2016.08.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994910609&doi=10.1016%2Fj.intfin.2016.08.003&partnerID=40&md5=67c5023992ba98b3ea71cdfb9652d735,scopus,"This paper introduces and empirically compares three classes of Transformed Diffusion (TD) models for VIX dynamics and VIX futures pricing. The authors propose two new distribution-driven TD models that incorporate VIX's stylized distributional features (skewness, excess kurtosis). An empirical investigation assesses model performance based on in-sample fit, out-of-sample forecast accuracy for VIX and VIX futures, and implied Variance Risk Premium's stock return predictability. The proposed models show advantages over existing alternatives.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:47:37.347373
6f33af2fc1cab759,"An empirical investigation into the impact of US federal government budget deficits on the real interest rate yield on intermediate-term treasury issues, 1972-2012","This study provides new empirical evidence on the impact of the federal budget deficit on the real interest rate yields on intermediate-term debt issues of the US Treasury, represented herein by the ex post real interest rate yields on 3-year Treasury notes and 7-year Treasury notes, two interest rate measures that have received essentially no attention in the economics and finance literature in recent years. This study is couched within a loanable funds model that includes two ex post real interest rate yields, the monetary base as a per cent of GDP, the change in per capita real GDP, net financial capital inflows as a per cent of GDP and the budget deficit as a per cent of GDP. This study uses annual data for the study period 1972 to 2012, a time period that includes 'quantitative easing' monetary policies by the Federal Reserve. Two-stage least squares estimations reveal that the federal budget deficit, expressed as a per cent of GDP, exercised a positive and statistically significant impact on the ex post real interest rate yields on both 3-year and 7-year Treasury notes, even after allowing for quantitative easing and other factors. The study also considers the time period 1980 to 2012 and offers simple robustness testing. Reprinted by permission of Routledge, Taylor and Francis Ltd.",,2014,10.1080/00036846.2014.932050,,proquest,"This empirical study investigates the impact of US federal budget deficits on real interest rates of 3-year and 7-year Treasury notes from 1972-2012, using a loanable funds model. The findings indicate a positive and statistically significant relationship between budget deficits (as a percentage of GDP) and these real interest rates, even when accounting for quantitative easing and other economic factors. Robustness tests were also conducted for the period 1980-2012.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:49:46.185067
4b946252a5b5a44e,An indicator of future inflation extracted from the steepness of the interest rate yield curve along its entire length,"The term-structure slope contains information about expected future inflation. Mishkin shows that the spread between the twelve-month and three-month interest rates helps predict the difference between twelve-month and three-month inflation. We apply a simple existing framework, which lets the real interest rate vary in the short run but converge to a constant in the long run, to this problem. The appropriate indicator of expected inflation uses the entire length of the yield curve, estimating the steepness of a specific nonlinear transformation, rather than being restricted to a spread between two points. The resulting indicator better predicts inflation, over 1960–1991. © 1994 by the President and Fellows of Harvard College and the Massachusetts Institute of Technology. © 2016 Elsevier B.V., All rights reserved.","Frankel, J.A.; Lown, C.S.",1994,10.2307/2118472,https://www.scopus.com/inward/record.uri?eid=2-s2.0-21344483312&doi=10.2307%2F2118472&partnerID=40&md5=f5ed43ec582dfc6b2cde18634449f72f,scopus,"This paper proposes an indicator for future inflation derived from the entire length of the interest rate yield curve, rather than just a two-point spread. The method involves estimating the steepness of a nonlinear transformation of the yield curve. The resulting indicator demonstrates improved inflation prediction capabilities over the period 1960-1991.",True,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:50:09.278855
99367e520d9af0be,An infinite hidden Markov model for short-term interest rates,"The time-series dynamics of short-term interest rates are important as they are a key input into pricing models of the term structure of interest rates. In this paper we extend popular discrete time short-rate models to include Markov switching of infinite dimension. This is a Bayesian nonparametric model that allows for changes in the unknown conditional distribution over time. Applied to weekly U.S. data we find significant parameter change over time and strong evidence of non-Gaussian conditional distributions. Our new model with a hierarchical prior provides significant improvements in density forecasts as well as point forecasts. We find evidence of recurring regimes as well as structural breaks in the empirical application. © 2016 Elsevier B.V., All rights reserved.","Maheu, J.M.; Yang, Q.",2016,10.1016/j.jempfin.2016.06.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976337100&doi=10.1016%2Fj.jempfin.2016.06.006&partnerID=40&md5=afcfc5a06eb822eff1844bc60ca17c66,scopus,"This paper introduces an infinite hidden Markov model to capture the time-series dynamics of short-term interest rates. The Bayesian nonparametric model allows for changes in conditional distributions over time and was applied to weekly U.S. data. The findings indicate significant parameter changes, non-Gaussian conditional distributions, and evidence of recurring regimes and structural breaks. The proposed model improved density and point forecasts compared to existing models.",True,False,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T10:50:38.183747
e254ebb1201c5b13,An interval constraint-based trading strategy with social sentiment for the stock market,"Developing effective strategies to earn excess returns in the stock market is a cutting-edge topic in the field of economics. At the same time, stock price forecasting that supports trading strategies is considered one of the most challenging tasks. Therefore, this study analyzes and extracts news media data, expert comments, social opinion data, and pandemic text data using natural language processing, and then combines the data with a deep learning model to forecast future stock price patterns based on historical stock prices. An interval constraint-based trading strategy is constructed. Using data from several typical stocks in the Chinese stock market during the COVID-19 period, the empirical studies and trading simulations show, first, that the sentiment composite index and the deep learning model can improve the accuracy of stock price forecasting. Second, the interval constraint-based trading strategy based on the proposed approach can effectively enhance returns and thus, can assist investors in decision-making.",,2024,10.1186/s40854-023-00567-2,,proquest,"This study proposes an interval constraint-based trading strategy for the stock market by combining social sentiment analysis (extracted using NLP from news, expert comments, social opinions, and pandemic data) with a deep learning model for stock price forecasting. Empirical results from the Chinese stock market during COVID-19 indicate that the sentiment index and deep learning model improve forecasting accuracy, and the proposed strategy enhances returns, aiding investor decision-making.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:50:46.357970
0f172ab615288205,An overview of maintenance management strategies for corroded steel structures in extreme marine environments,"Maintenance is playing an important role in integrity management of marine assets such as ship structures, offshore renewable energy platforms and subsea oil and gas facilities. The service life of marine assets is heavily influenced by the involvement of numerous material degradation processes (such as fatigue cracking, corrosion and pitting) as well as environmental stresses that vary with geographic locations and climatic factors. The composition of seawater constituents (e.g. dissolved oxygen, salinity, temperature content, etc.) is one of the major influencing factors in degradation of marine assets. Improving the efficiency and effectiveness of maintenance management strategies can have a significant impact on operational availability and reliability of marine assets. Many research studies have been conducted over the past few decades to predict the degradation behaviour of marine structures operating under different environmental conditions. The utilisation of structural degradation data – particularly on marine corrosion – can be very useful in developing a reliable, risk-free and cost-effective maintenance strategy. This paper presents an overview of the state-of-the-art and future trends in asset maintenance management strategies applied to corroded steel structures in extreme marine environments. The corrosion prediction models as well as industry best practices on maintenance of marine steel structures are extensively reviewed and analysed. Furthermore, some applications of advanced technologies such as computerized maintenance management system (CMMS), artificial intelligence (AI) and Bayesian network (BN) are discussed. Our review reveals that there are significant variations in corrosion behaviour of marine steel structures and their industrial maintenance practices from one climatic condition to another. This has been found to be largely attributed to variation in seawater composition/characteristics and their complex mutual relationships. © 2020 Elsevier B.V., All rights reserved.","Abbas, M.; Shafiee, M.",2020,10.1016/j.marstruc.2020.102718,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078962228&doi=10.1016%2Fj.marstruc.2020.102718&partnerID=40&md5=06e8029f84c8b4e4d649d53d85207f63,scopus,"This paper provides an overview of maintenance management strategies for corroded steel structures in extreme marine environments. It reviews corrosion prediction models, industry best practices, and discusses applications of advanced technologies like CMMS, AI, and Bayesian networks. The study highlights variations in corrosion behavior and maintenance practices due to differing seawater compositions and climatic conditions.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:50:55.163016
90fdb546691bd626,Analysis of herding behavior in individual investor portfolios using machine learning algorithms,"This paper examines the determinants of herding at both stock and individual investor levels and studies the portfolio performance of herd vs. non-herd portfolios using machine learning algorithms. The disposition effect and the attention effect seem to explain herding behavior at the stock level. At the individual investor level, the cumulative number of buys and portfolio values reduce the prediction of herding behavior, while high values of portfolio return lead to a small increase in herding. Individuals who herd do not outperform either market or non-herd portfolios, suggesting that herding is a behavioral bias. Thus, such behavior seems to destabilize stock markets, creating temporary discrepancies in stock prices followed by reversals back to fundamentals. The most predictive factor in the performance tests of individual portfolios is the market risk premium and using equally-weighted factors rather than value-weighted factors seem to provide more consistent results in the portfolio performance analyses. © 2022 Elsevier B.V., All rights reserved.","Mavruk, T.",2022,10.1016/j.ribaf.2022.101740,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136526119&doi=10.1016%2Fj.ribaf.2022.101740&partnerID=40&md5=9a05e65026a399027b17b7c424c6c9fc,scopus,"This study investigates herding behavior in individual investor portfolios using machine learning. It identifies factors influencing herding at stock and individual levels, finding that herding investors do not outperform non-herding ones, suggesting it's a behavioral bias that can destabilize markets. The analysis highlights market risk premium and factor weighting as key to portfolio performance.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:51:00.818794
1b54463d1a0cc54d,Analysis of multifactor affine yield curve models,"In finance and economics much work has been done on the theoretical modeling and statistical estimation of the yield curve, defined as the relationship between -1/τ logp <inf>t</inf>.(τ) and τ,where p <inf>t</inf>(τ) is the time t price of a zero - coupon bond with payoff 1 at maturity date t + τ. Of considerable current interest are models of the yield curve in which a collection of observed and latent factors determine the market price of factor risks, the stochastic discount factor, and the arbitrage - free bond prices. The model is particularly interesting from a statistical perspective, because the yields are complicated nonlinear functions of the underlying parameters (e.g., those appearing in the evolution dynamics of the factors and those appearing in the model of the factor risks). This nonlinearity tends to produce a likelihood function that is multimodal. In this article we revisit the question of how such models should be fit from the Bayesian viewpoint. Key aspects of the inferential framework include (a) a prior on the parameters of the model that is motivated by economic considerations, in particular, those involving the slope of the implied yield curve; (b) posterior simulation of the parameters in ways to improve the efficiency of the MCMC output, for example, through sampling of the parameters marginalized over the factors and tailoring of the proposal densities in the Metropolis - Hastings steps using information about the mode and curvature of the current target based on the output of a simulating annealing algorithm; and (c) measures to mitigate numerical instabilities in the fitting through reparameterizations and square root filtering recursions. We apply the techniques to explain the monthly yields on nine U.S. Treasury Bills (with maturities ranging from 1 month to 120 months) over the period January 1986 - December 2005. The model contains three factors, one latent and two observed. We also consider the problem of predicting the nine yields for each month of 2006. We show that the (multi-step-ahead) prediction regions properly bracket the actual yields in those months, thus highlighting the practical value of the fitted model. © 2009 American Statistical Association. © 2012 Elsevier B.V., All rights reserved.","Chib, S.; Ergashev, B.",2009,10.1198/jasa.2009.ap08029,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049132711&doi=10.1198%2Fjasa.2009.ap08029&partnerID=40&md5=e3022f035f007aa8736ec76106515f58,scopus,"This article revisits the Bayesian approach to fitting multifactor affine yield curve models, focusing on statistical inference and practical application. It proposes an inferential framework incorporating economically motivated priors, efficient posterior simulation techniques (MCMC, Metropolis-Hastings, simulated annealing), and methods to address numerical instabilities. The techniques are applied to U.S. Treasury Bill yields from 1986-2005, using a three-factor model (one latent, two observed). The study also addresses yield prediction for 2006, demonstrating the practical utility of the fitted model through accurate prediction regions.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T10:51:26.618048
f9ea0fbfd77a9f06,Analysis of the relevance of sentiment data for the prediction of excess returns in a multiasset framework,"In this study, we look at the relevance of sentiment data for the prediction of excess returns in a multiasset analysis. We start by initial exploratory data analysis in order to assess the pertinence of the sentiment data. We then compare the performance of rule-based algorithms with and without the sentiment data. The data considered are provided by RavenPack. Finally, we explore the economic relevance of the forecast model in a long-only and long-short context. Inclusion of sentiment data leads to encouraging results.","Desforges, Perceval; Geissler, Christophe; Liu, Fei",2023,10.1002/for.2967,,wos,"This study investigates the predictive power of sentiment data for excess returns in a multi-asset framework. It compares rule-based algorithms with and without sentiment data, using data from RavenPack, and explores the economic relevance of the forecasting model in different investment contexts. The inclusion of sentiment data yielded positive results.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:51:38.022510
a9d18ef3b66653ed,Analyzing the green bond index: A novel quantile-based high-dimensional approach,"The development of green bond markets is important for advancing energy efficiency, supporting renewable energy, encouraging sustainable investments, and safeguarding the environment. However, the inherent complexity and uncertainty of these markets pose significant challenges for both investors and researchers. In this study, we focus on analyzing the S&P Green Bond Index, a leading benchmark for monitoring the global green bond market. We introduce a new high-dimensional statistical method, the Quantile Group Adaptive Lasso, designed to accurately predict the returns of this index. Our empirical results demonstrate that this model surpasses several established forecasting techniques in both accuracy and stability. Furthermore, our analysis of economic significance highlights the critical influence of traditional energy-related predictors from G7 and BRICS countries on the global green bond markets. We also find that monetary policies and macroeconomic factors, such as M2 money supply, CPI, and government bond yields, play vital roles. Additionally, the robustness of our proposed method is confirmed. Overall, our study provides a powerful tool that not only significantly enhances forecasting performance but also deepens the understanding of the interplay between trends in green bond markets and information from energy sectors and broader economic conditions. © 2024 Elsevier B.V., All rights reserved.","Tao, L.; Jiang, W.; Ren, X.",2024,10.1016/j.irfa.2024.103659,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206793875&doi=10.1016%2Fj.irfa.2024.103659&partnerID=40&md5=2a3ac17fe37094fba78ff153a1937058,scopus,"This study introduces a novel Quantile Group Adaptive Lasso method to analyze and predict the returns of the S&P Green Bond Index. The empirical results show that this high-dimensional approach outperforms existing forecasting techniques. The analysis reveals that traditional energy-related predictors from G7 and BRICS countries, along with monetary policies and macroeconomic factors, significantly influence green bond markets. The robustness of the proposed method is also confirmed.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:51:46.027364
ef29280b50c0dfb2,Anchoring Bias in Consensus Forecasts and Its Effect on Market Prices,"Previous empirical studies on the rationality of economic. and financial forecasts generally test for generic properties such as bias or autocorrelated errors but provide only limited insight into the behavior behind inefficient forecasts. This paper tests for a specific form of forecast bias. In particular, we examine whether expert consensus forecasts of monthly economic releases are systematically biased toward the value of previous months' releases. Such a bias would be consistent with the anchoring and adjustment heuristic described by Tversky and Kahneman (1974) or could arise from professional forecasters' strategic incentives. We find broad-based and significant evidence for this form of bias, which in some cases results in sizable predictable forecast errors. To investigate whether market participants' expectations are influenced by this bias, we examine interest rate reactions to economic news. We find that bond yields react only to the residual, or unpredictable, component of the forecast error and not to the component induced by anchoring, suggesting that expectations of market participants anticipate this bias embedded in expert forecasts.","Campbell, Sean D.; Sharpe, Steven A.",2009,10.1017/s0022109009090127,,wos,"This study investigates anchoring bias in consensus forecasts of economic releases, finding significant evidence that forecasts are influenced by previous months' values. The research also examines the impact of this bias on market prices by analyzing interest rate reactions to economic news. Results indicate that bond yields react only to the unpredictable component of forecast errors, suggesting market participants anticipate the anchoring bias in expert forecasts.",False,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T10:52:30.170938
b4b97658e287ca70,Application of QGA-BP Neural Network in Debt Risk Assessment of Government Platforms,"How to correctly understand the existence of local government debt, study its risk classification and impact, give full play to the “dual nature” of debt with a full-caliber indicator system, and avoid debt risks to the greatest extent. That is the research direction of this article. In order to improve the accuracy and efficiency of risk assessment and effectively reduce the debt risk of government platform companies, a risk assessment method based on optimized back-propagation (BP) neural network is proposed. First, the method uses quantum genetic algorithm (quantum genetic algorithm, QGA) to adjust and determine the initial weight and threshold of BP neural network and realize the optimization of BP neural network model parameter setting. Then, the QGA-BP debt risk assessment of government platforms is verified that it performs well in the debt risk prediction of government platform companies, and its prediction accuracy and prediction speed are improved.",,2024,10.4018/ijitwe.335124,,proquest,"This study proposes a risk assessment method for government platform companies using an optimized back-propagation (BP) neural network, adjusted by a quantum genetic algorithm (QGA). The QGA optimizes the initial weights and thresholds of the BP network, aiming to improve the accuracy and efficiency of debt risk prediction and reduce overall debt risk.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:52:49.987482
73bbac9a2ecd9493,Application of evolutionary computation for rule discovery in stock algorithmic trading: A literature review,"Despite the wide application of evolutionary computation (EC) techniques to rule discovery in stock algorithmic trading (AT), a comprehensive literature review on this topic is unavailable. Therefore, this paper aims to provide the first systematic literature review on the state-of-the-art application of EC techniques for rule discovery in stock AT. Out of 650 articles published before 2013 (inclusive), 51 relevant articles from 24 journals were confirmed. These papers were reviewed and grouped into three analytical method categories (fundamental analysis, technical analysis, and blending analysis) and three EC technique categories (evolutionary algorithm, swarm intelligence, and hybrid EC techniques). A significant bias toward the applications of genetic algorithm-based (GA) and genetic programming-based (GP) techniques in technical trading rule discovery is observed. Other EC techniques and fundamental analysis lack sufficient study. Furthermore, we summarize the information on the evaluation scheme of selected papers and particularly analyze the researches which compare their models with buy and hold strategy (B&H). We observe an interesting phenomenon where most of the existing techniques perform effectively in the downtrend and poorly in the uptrend, and considering the distribution of research in the classification framework, we suggest that this phenomenon can be attributed to the inclination of factor selections and problem in transaction cost selections. We also observe the significant influence of the transaction cost change on the margins of excess return. Other influenced factors are also presented in detail. The absence of ways for market trend prediction and the selection of transaction cost are two major limitations of the studies reviewed. In addition, the combination of trading rule discovery techniques and portfolio selection is a major research gap. Our review reveals the research focus and gaps in applying EC techniques for rule discovery in stock AT and suggests a roadmap for future research. © 2015 Elsevier B.V., All rights reserved.","Hu, Y.; Liu, K.; Zhang, X.; Su, L.; Ngai, E.W.T.; Liu, M.",2015,10.1016/j.asoc.2015.07.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939799429&doi=10.1016%2Fj.asoc.2015.07.008&partnerID=40&md5=7ea181ab6877b4d9d39c04fed89bad36,scopus,"This systematic literature review examines the application of evolutionary computation (EC) techniques for discovering rules in stock algorithmic trading (AT). It analyzes 51 relevant articles published before 2013, categorizing them by analytical methods (fundamental, technical, blending) and EC techniques (evolutionary algorithm, swarm intelligence, hybrid EC). The review highlights a bias towards genetic algorithms and genetic programming for technical trading rules, with other EC techniques and fundamental analysis being less studied. It also discusses evaluation schemes, comparisons with buy-and-hold strategies, and identifies limitations such as the absence of market trend prediction and transaction cost selection methods. The paper suggests future research directions, including combining trading rule discovery with portfolio selection.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:53:00.501436
c5a0693edf0851ee,Application of statistical mechanics methodology to term-structure bond-pricing models,"Recent work in statistical mechanics has developed new analytical and numerical techniques to solve coupled stochastic equations. This paper applies the very fast simulated re-annealing and path-integral methodologies to the estimation of the Brennan and Schwartz two-factor term structure model. It is shown that these methodologies can be utilized to estimate more complicated n-factor nonlinear models. © 1991. © 2014 Elsevier B.V., All rights reserved.","Ingber, L.; Wehner, M.F.; Jabbour, G.M.; Barnhill, T.M.",1991,10.1016/0895-7177(91)90107-i,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4344609903&doi=10.1016%2F0895-7177%2891%2990107-I&partnerID=40&md5=a10cc26f8f9082733f9325f7b75f0961,scopus,"This paper applies statistical mechanics techniques, specifically very fast simulated re-annealing and path-integral methodologies, to estimate the Brennan and Schwartz two-factor term structure model. The authors demonstrate that these methods can also be used for more complex n-factor nonlinear models.",True,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:53:36.221689
ce0e67a56a1db07c,Applications of machine learning for corporate bond yield spread forecasting,"This article considers nine different predictive techniques, including state-of-the-art machine learning methods for forecasting corporate bond yield spreads with other input variables. We examine each method's out-of-sample forecasting performance using two different forecast horizons: (1) the in-sample dataset over 2003–2007 is used for one-year-ahead and two-year-ahead forecasts of non-callable corporate bond yield spreads; and (2) the in-sample dataset over 2003–2008 is considered to forecast the yield spreads in 2009. Evaluations of forecasting accuracy have shown that neural network forecasts are superior to the other methods considered here in both the short and longer horizon. Furthermore, we visualize the determinants of yield spreads and find that a firm's equity volatility is a critical factor in yield spreads. © 2021 Elsevier B.V., All rights reserved.","Kim, J.-M.; Kim, D.H.; Jung, H.",2021,10.1016/j.najef.2021.101540,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113399304&doi=10.1016%2Fj.najef.2021.101540&partnerID=40&md5=5ba74f57fb59d060a2c1775e12c696df,scopus,"This article evaluates nine predictive techniques, including machine learning methods, for forecasting corporate bond yield spreads. It compares their out-of-sample performance over one- and two-year horizons using data from 2003-2007 and 2003-2008. Neural networks showed superior forecasting accuracy, and equity volatility was identified as a critical factor in yield spreads.",True,False,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T10:54:12.969878
30d5228daba342c6,Arbitrage bounds in markets with noisy prices and the puzzle of negative option prices implicit in bonds,"The term structure of interest rates has occupied economists for many years. This is testimony to the importance of the term structure (or, equivalently, valuation operator) and to the difficulty of obtaining reliable estimates of it. Until recently, it was not possible to reconcile the theoretical existence of a multiplicity of valuation operators in an incomplete bond market and the empirical nonexistence of even a single one. MacKay and Prisman [Estimating valuation operators in incomplete market with noises: Can noise complete the market. Working paper, 1996] tackle this problem. In this paper, an amendment to and a generalization of their methodology is presented. The amendment preserves the linearity of valuation operators and allows use of efficient linear programming techniques. Further, it facilitates an admissible consideration of the puzzle of negative option prices embedded in bonds. The empirical results presented in this paper were obtained using data on extendable Government of Canada bonds. The results indicate that although the gap between the lowest and the highest prices assigned to a cash flow by different valuation operators is significant, it is not sufficient to resolve the above-mentioned puzzle. (C) 2002 Elsevier Science B.V. All rights reserved.","Ioffe, LD",2002,10.1016/s0378-4266(01)00172-8,,wos,"This paper presents an amendment and generalization to MacKay and Prisman's methodology for estimating valuation operators in incomplete bond markets with noisy prices. It preserves linearity, allows for efficient linear programming, and addresses the puzzle of negative option prices implicit in bonds. Empirical results using Canadian government bond data show that while valuation operator price gaps are significant, they do not resolve the puzzle.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:54:24.111399
57f111d0ba50fca7,Are corporate bond market returns predictable?,"This paper examines the predictability of corporate bond returns using the transaction-based index data for the period from October 1, 2002 to December 31, 2010. We find evidence of significant serial and cross-serial dependence in daily investment-grade and high-yield bond returns. The serial dependence exhibits a complex nonlinear structure. Both investment-grade and high-yield bond returns can be predicted by past stock market returns in-sample and out-of-sample, and the predictive relation is much stronger between stocks and high-yield bonds. By contrast, there is little evidence that stock returns can be predicted by past bond returns. These findings are robust to various model specifications and test methods, and provide important implications for modeling the term structure of defaultable bonds. All rights reserved, Elsevier",,2012,10.1016/j.jbankfin.2012.04.001,,proquest,"This paper investigates the predictability of corporate bond returns using transaction-based index data from October 2002 to December 2010. It finds significant serial and cross-serial dependence in daily investment-grade and high-yield bond returns, with a complex nonlinear structure in serial dependence. Both types of bond returns can be predicted by past stock market returns, with a stronger relationship for high-yield bonds. Stock returns, however, show little predictability from past bond returns. These results are robust and have implications for modeling the term structure of defaultable bonds.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:54:30.247847
302ced4755753438,Are embedded calls valuable? Evidence from agency bonds,"This paper examines the call option values embedded in callable agency bonds. For FHLB, FNMA, and SLMA bonds, call value estimates range from 1.23% of par to 1.47% on average, which are between those for the treasury and corporate debt securities. FHLMC bonds, on the other hand, have an average call value estimate of 2.85%. Call values are significantly larger for bonds with a longer remaining maturity and greater default risk. Most interestingly, call values in the call protection period are significantly larger than those in the callable period except for the SLMA bonds, whereas previous studies on corporate debt find no significant difference in call values between these two periods.In general, call value exhibits a downward trend over time as the callable bond approaches maturity. Also, call value is inversely related to the level of interest rates. Interest rate drops are usually accompanied by an increase in Call values. An analysis of the determinants of call values suggests the following conclusions. First, call values are negatively related to short-term interest rates and the slope of the yield curve, and positively related to coupon rate and remaining maturity. Second, bonds with a greater amount of call protection have smaller call values, which is in contrast with the finding in a previous study on corporate debt that call protection period has little effect on call value. (c) 2006 Elsevier B.V. All rights reserved.","King, Tao-Hsien Dolly",2007,10.1016/j.jbankfin.2006.02.001,,wos,"This paper analyzes the value of embedded call options in agency bonds (FHLB, FNMA, SLMA, FHLMC). It finds that call values vary across bond types and are influenced by maturity, default risk, and interest rate movements. Notably, call values are higher during the call protection period for most agency bonds, unlike findings for corporate debt. The study also identifies factors affecting call values, including interest rates, yield curve slope, coupon rate, maturity, and the presence of call protection.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:54:40.740450
4414f9a04e60c0a8,Are multi-factor Gaussian term structure models still useful? An empirical analysis on Italian BTPs,"In this paper, we empirically study models for pricing Italian sovereign bonds under a reduced form framework, by assuming different dynamics for the short-rate process. We analyze classical Cox-Ingersoll-Ross and Vasicek multi-factor models, with a focus on optimization algorithms applied in the calibration exercise. The Kalman filter algorithm together with a maximum likelihood estimation method are considered to fit the Italian term-structure over a 17-year horizon, including the global financial crisis, the euro area sovereign debt crisis and the Italian political turmoil in 2018. Analytic formulas for the gradient vector and the Hessian matrix of the likelihood function are provided. © 2022 Elsevier B.V., All rights reserved.","Bianchi, M.L.",2022,10.1080/03610918.2020.1721540,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078925539&doi=10.1080%2F03610918.2020.1721540&partnerID=40&md5=48175a83feef6a419ccf98cb26d0831d,scopus,"This paper empirically analyzes multi-factor Gaussian term structure models (Cox-Ingersoll-Ross and Vasicek) for pricing Italian sovereign bonds. It uses the Kalman filter and maximum likelihood estimation to fit the Italian term structure over a 17-year period, encompassing major financial crises. The study also provides analytic formulas for gradient and Hessian matrices in the likelihood function.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:55:29.103452
91e92775d7b84bf0,Are quantitative easing effects transitory? Evidence from out-of-sample forecasts,"Purpose>Advocates of quantitative easing (QE) policies have emphasized some evidence that structural models do not predict long-term asset yields as well as naive forecasts, implying that predictions of price reversals cannot be profitable and that QE effects are not transitory. The purpose of this study is to reconsider the out-of-sample forecasting performance of structural time series processes relative to that of a random walk with or without drift.Design/methodology/approach>This study uses bivariate vector autoregression and Markov switching representations to generate out-of-sample forecasts of ten-year sovereign bond yields, when the information set is augmented by including the growth rate of the monetary base, and the estimation relies on monthly data from countries that have pursued unconventional policies over the last decade.Findings>The results show that naive forecasts are not better than those of structural time series models, based on root mean squared errors, while the Markov model provides additional information on price reversals, through probabilistic inferences regarding policy regime switches, which can induce agents to counteract QE interventions and reduce their effectiveness.Originality/value>The novel features of this work are the use of a large information set including the instrument of unconventional monetary policy, the use of a structural model (Markov process) that can really inform about potential asset price reversals and the use of a large sample over which QE policies have been pursued.",,2022,10.1108/jfep-04-2022-0099,,proquest,"This study investigates whether quantitative easing (QE) effects are transitory by comparing the out-of-sample forecasting performance of structural time series models against naive forecasts. Using bivariate vector autoregression and Markov switching models with monthly data from countries that have implemented QE, the research finds that naive forecasts are not superior to structural models. The Markov model, in particular, offers insights into price reversals and policy regime switches that can mitigate QE effectiveness.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T10:56:22.754664
31af06ccc96e13d2,Are there nonlinear speculative bubbles in commodities prices?,"Daily price movements of seventeen commodities are tested for the possible presence of nonlinear speculative bubbles during 1991-2012. A VAR model for logarithmic first differences of each is estimated with one-year Treasury bill rates, U.S. dollar value, a world stock market index, and an overall commodities price index using Hamilton regime switching and Hurst rescaled range tests. Residuals after removing ARCH for all seventeen commodity price series are tested for remaining nonlinearity using the BDS test. These tests fail to reject the presence of bubble-like trends and nonlinearity beyond ARCH for all seventeen commodity series. However, we note that we are unable to overcome the misspecified fundamentals problem, which means we cannot argue that we have definitely found speculative bubbles. At most we can argue that our results indicate that these markets appear to exhibit excess volatility and unexplained trends. © 2014 M.E. Sharpe, Inc. All rights reserved. © 2020 Elsevier B.V., All rights reserved.","Ahmed, E.; Rosser, J.B.; Uppal, J.Y.",2014,10.2753/pke0160-3477360302,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898645711&doi=10.2753%2FPKE0160-3477360302&partnerID=40&md5=34a271069a89937d6ad612232df95cda,scopus,"This study investigates the presence of nonlinear speculative bubbles in commodity prices using daily data from 1991-2012 for seventeen commodities. It employs a VAR model with regime switching and Hurst rescaled range tests, and the BDS test on residuals after ARCH modeling. While the tests suggest bubble-like trends and nonlinearity, the authors acknowledge the limitation of not being able to definitively identify speculative bubbles due to the 'misspecified fundamentals' problem, concluding that the markets show excess volatility and unexplained trends.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:56:47.181355
bd15d7787199c49d,Artificial Intelligence and Firm Performance: Does Machine Intelligence Shield Firms from Risks?,"We estimate and compare the impact of the coronavirus pandemic (COVID-19) on the performance of Artificial Intelligence (AI) and conventional listed firms using stock market indices. The single-group and multiple-group Interrupted Time-Series Analyses (ITSA) with panel data were used with four interventions: when the news of COVID-19 spread and the pandemic entered the first, second, third, and fourth months (24 February 2020, 23 March 2020, 20 April 2020, and 18 May 2020, respectively). The results show that the negative impact of COVID-19 on the AI stock market was less severe than on the conventional stock market in the first month of the pandemic. The performance of the AI stock market recovered quicker than the conventional stock market when the pandemic went into its third month. The results suggest that the AI stocks were more resilient than conventional stocks when the financial market was exposed to uncertainty caused by the COVID-19 pandemic. The deployment of AI in firms serves as a resilient, crucial driver for sustainable performance in challenging environments. Observing the performance of AI-adopted firms is an interesting direction for technical and fundamental analysts. Investors and portfolio managers should consider an AI market index to minimize risk or invest in stocks of AI-adopted listed firms to maximize excess returns.",,2022,10.3390/jrfm15070302,,proquest,"This study investigates the impact of the COVID-19 pandemic on the performance of Artificial Intelligence (AI) firms compared to conventional firms using stock market data. The analysis, employing Interrupted Time-Series Analyses (ITSA), found that AI stocks were less affected and recovered faster than conventional stocks during the pandemic. The findings suggest that AI adoption can enhance firm resilience and performance in uncertain environments, making AI-adopted firms an attractive investment.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:57:24.772046
c7505f1ac2274c1a,Artificial Intelligence in Economics Research: What Have We Learned? What Do We Need to Learn?,"Motivated by the recent boom in artificial intelligence (AI) playing a significant role in the economics of individuals, firms, and government bodies, we investigate the role of AI in economics by reviewing the literature (2231 articles) during the last 34 years (1990 to November 2024). We identify five research streams: (1) AI and economic modeling, (2) AI and macroeconomics (eight sub-streams), (3) AI and equity and debt market, (4) AI and prediction modeling (three sub-streams), and (5) AI and economics of innovation. Further, we offer suggestions for future research (20 questions). Additionally, we outline a framework to consider changes in economics before and after AI adoption. Further, the critical AI-based methods are identified and discussed.","Bahoo, Salman; Goodell, John W.; Rhattat, Rachid; Shahid, Subhan",2025,10.1111/joes.12694,,wos,"This review examines the role of AI in economics over the past 34 years, identifying five research streams: AI and economic modeling, macroeconomics, equity and debt markets, prediction modeling, and innovation. It suggests future research directions and outlines a framework for understanding economic changes due to AI adoption, while also discussing critical AI methods.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:57:30.787382
e387f15db5d09f82,Artificial intelligence combined with nonlinear optimization techniques and their application for yield curve optimization,"This study makes use of the artificial intelligence approaches combined with some nonlinear optimization techniques for optimization of a well-known problem in financial engineering called yield curve. Yield curve estimation plays an important role on making strategic investment decisions. In this paper, we use two well-known parsimonious estimation models, Nelson-Siegel and Extended Nelson-Siegel, for the yield curve estimation. The proposed models of this paper are formulated as continuous nonlinear optimization problems. The resulted models are then solved using some nonlinear optimization and meta-heuristic approaches. The optimization techniques include hybrid GPSO parallel trust region-dog leg, Hybrid GPSO parallel trust region-nearly exact, Hybrid GPSO parallel Levenberg-Marquardt and Hybrid genetic electromagnetism like algorithm. The proposed models of this paper are examined using some real-world data from the bank of England and the results are analyzed. © 2017 Elsevier B.V., All rights reserved.","Soltani, R.; Sadjadi, S.J.; Rahnama, M.",2017,10.3934/jimo.2017014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030552095&doi=10.3934%2Fjimo.2017014&partnerID=40&md5=9e10a6ec52920ca48e87b2e5a37a3950,scopus,This paper applies artificial intelligence and nonlinear optimization techniques to estimate the yield curve using Nelson-Siegel and Extended Nelson-Siegel models. It formulates the problem as a continuous nonlinear optimization task and solves it using hybrid GPSO algorithms with various nonlinear optimization methods. The models are tested with real-world data from the Bank of England.,True,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:58:31.186142
5dd844b0966c453d,Asian holding of US Treasury securities: trade integration as a threshold,"This paper empirically investigates if there have been any shifts in regimes with Asian holding of US long-term Treasury securities with particular attention paid to the role of growing regional integration in trade. A panel regression estimation of eight Asian countries for 1998-2004 confirms the striking persistency of the portfolio weight of US Treasury securities. It also reveals, without a surprise, that the traditionally strong trade link with US as well as exchange rate regime and volatility of local currency bond index explain observed overinvestment in US Treasury securities deviating from what can be warranted by the market share of the US Treasury securities. What is interesting, however, is the estimated regime switches as found when examined with a threshold estimation (Hansen, 1999). We find three thresholds which divide the sample into four regimes-a decreasing persistency as intraregional trade link becomes tighter. All rights reserved, Elsevier",,2011,10.1016/j.jjie.2011.07.001,,proquest,"This paper examines the holding of US Treasury securities by Asian countries, focusing on the impact of regional trade integration. Using panel regression for eight Asian countries from 1998-2004, it confirms the persistence of US Treasury security holdings and identifies trade links with the US, exchange rate regimes, and local currency bond volatility as explanatory factors. Notably, threshold estimation reveals regime switches, with decreasing persistence as intra-regional trade tightens.",True,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:58:46.072648
cecdcfa68c964887,Assessing the relevance of sell-side analyst recommendations,"This paper evaluates the informational value and alpha-generating potential of sell-side analyst recommendations. We explore this by employing a monthly portfolio-sorted long-short strategy based on consensus analyst recommendations. Our findings indicate that the long-short equal-weighted and value-weighted portfolios yield significant excess returns. However, the value-weighted excess returns are primarily driven by the predictive power of the lowest decile (sell recommendations). The long-short strategy for the value-weighted portfolios yields a monthly excess return ranging from 1.36% to 1.57%, above the risk-free rate. Our analysis further examines variations in recommendation effectiveness across economic cycles, industries, investment banks/brokers, and firm sizes, providing further insights into the value of analyst recommendations.","Aguegboh, Ekene S.; Onuoha, Uchenna C.; Patel, Poojan",2025,10.1002/rfe.70015,,wos,"This study assesses the informational value and alpha-generating potential of sell-side analyst recommendations using a monthly portfolio-sorted long-short strategy. Significant excess returns were found, particularly for value-weighted portfolios driven by sell recommendations. The effectiveness of recommendations was further analyzed across various factors like economic cycles and firm sizes.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:58:57.815897
6512de4ac6889893,Asset Pricing and Machine Learning: A critical review,"The latest development in empirical Asset Pricing is the use of Machine Learning methods to address the problem of the factor zoo. These techniques offer great flexibility and prediction accuracy but require special care as they strongly depart from traditional Econometrics. We review and critically assess the most recent and relevant contributions in the literature grouping them into five categories defined by the Machine Learning (ML) approach they employ: regularization, dimension reduction, regression trees/random forest (RF), neural networks (NNs), and comparative analyses. We summarize the empirical findings with particular attention to their economic interpretation providing hints for future developments. © 2024 Elsevier B.V., All rights reserved.","Bagnara, M.",2024,10.1111/joes.12532,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137459905&doi=10.1111%2Fjoes.12532&partnerID=40&md5=d514f0cebadfe1ab035f5fdd14a1fc49,scopus,"This review critically assesses recent Machine Learning (ML) applications in empirical Asset Pricing, focusing on addressing the 'factor zoo' problem. It categorizes contributions by ML approach (regularization, dimension reduction, RF, NNs, comparative analyses), summarizes empirical findings, and discusses economic interpretations and future directions.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:59:08.479931
2cc3114ff3eae9ca,"Asset Returns Under Model Uncertainty: Evidence from the Euro Area, the US and the UK","We analyze predictability of risk premium in the context of model uncertainty. Using data for the euro area, the US and the UK, we show that there is a large amount of model uncertainty and one can improve the forecasts of stock returns with a Bayesian Model Averaging (BMA) approach. The empirical evidence for the euro area suggests that several macroeconomic, financial and macro-financial variables are consistently among the most prominent determinants of risk premium. As for the US, only a few number of predictors play an important role. In the case of the UK, future stock returns are better forecasted by financial variables. These results are corroborated for both the M-open and the M-closed perspectives, different model priors and in the context of “in-sample” and “out-of-sample” forecasting. Finally, we highlight that the predictive ability of the BMA framework is stronger at longer periods, and clearly outperforms the constant expected returns and the autoregressive benchmark models.",,2019,10.1007/s10614-017-9696-2,,proquest,"This study investigates risk premium predictability under model uncertainty using Bayesian Model Averaging (BMA) for the euro area, US, and UK. It demonstrates that BMA improves stock return forecasts compared to simpler models and highlights different key predictors across regions (macroeconomic/financial for the euro area, financial for the UK, and few for the US). The BMA framework's predictive ability is stronger for longer periods and outperforms benchmark models.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:59:35.575328
c525f00d6ca91a55,Asset market equilibrium under rational inattention,"We propose a noisy rational expectations equilibrium model of asset markets with rationally inattentive investors. We incorporate any finite number of assets with arbitrary correlation. We also do not restrict the signal form and show that investors optimally choose a single signal, which is a noisy linear combination of all risky assets. This generates comovement of asset prices and contagion of shocks, even when asset payoffs are negatively correlated. The model also provides testable predictions of the impact of risk aversion, aggregate risk, and information capacity on the security market line, the portfolio dispersion, and the abnormal return.",,2023,10.1007/s00199-021-01396-z,,proquest,"This paper presents a model of asset markets where investors are rationally inattentive. It incorporates multiple assets with arbitrary correlations and shows that investors optimally choose a single noisy signal, leading to comovement of asset prices and shock contagion. The model offers testable predictions regarding risk aversion, aggregate risk, and information capacity.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:59:38.162489
db174cb1df2093f1,Asset prices in a production network,"The relative importance of sectoral and aggregate productivity shocks in asset pricing is examined using a nonlinear dynamic equilibrium model where heterogeneous sectors interact in a production network. The model accounts for the heterogeneity in sectoral stock returns and endogenously generates conditional heteroskedasticity and fat tails. The equity risk premium is shown to be driven by sectoral shocks – specially to investment good producers and mining – with a limited contribution from the aggregate shock. SMM estimates of the elasticities of substitution between material inputs and between investment goods support the assumption of gross complementarity employed by previous network literature. © 2024 Elsevier B.V., All rights reserved.","Ruge-Murciá, F.",2024,10.1016/j.euroecorev.2024.104751,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193980037&doi=10.1016%2Fj.euroecorev.2024.104751&partnerID=40&md5=09a671fb1dc467fc6c86b4a434bc87f1,scopus,"This paper uses a nonlinear dynamic equilibrium model with a production network to analyze asset pricing. It finds that sectoral productivity shocks, particularly those affecting investment good producers and mining, are the primary drivers of the equity risk premium, with aggregate shocks playing a limited role. The model also accounts for heterogeneous sectoral returns, conditional heteroskedasticity, and fat tails. The study's findings on input substitution elasticities align with previous network literature.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:59:41.558625
7be43e8757080b63,Asset pricing and foreign exchange risk,"According to the International Capital Asset Pricing Model (ICAPM), the covariance of assets with foreign exchange currency returns should be a risk factor that must be priced when the purchasing power parity is violated. The goal of this study is to re-examine the relationship between stock returns and foreign exchange risk. The novelties of this work are: (a) a data set that makes use of daily observations for the measurement of the foreign exchange exposure and volatility of the sample firms and (b) data from a Eurozone country. The methodology we make use in reference to the estimation of the sensitivity of each stock to exchange rate movements is that it allows regressing stock returns against factors controlling for market risk, size, value, momentum, foreign exchange exposure and foreign exchange volatility. Stocks are then classified according to their foreign exchange sensitivity portfolios and the return of a hedge (zero-investment) portfolio is calculated. Next, the abnormal returns of the hedge portfolio are regressed against the return of the factors. Finally, we construct a foreign exchange risk factor in such manner as to obtain a monotonic relation between foreign exchange risk and expected returns. The empirical findings show that the foreign exchange risk is priced in the cross section of the German stock returns over the period 2000-2008. Furthermore, they show that the relationship between returns and foreign exchange sensitivity is nonlinear, but it takes an inverse U-shape and that foreign exchange sensitivity is larger for small size firms and value stocks. © 2011 Elsevier B.V. © 2011 Elsevier B.V., All rights reserved.","Apergis, N.; Artikis, P.; Sorros, J.",2011,10.1016/j.ribaf.2011.02.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953279666&doi=10.1016%2Fj.ribaf.2011.02.005&partnerID=40&md5=711719c805262e6682f85c3b8ccd3aaf,scopus,"This study re-examines the relationship between stock returns and foreign exchange risk using daily observations from a Eurozone country (Germany) between 2000-2008. It employs a methodology that estimates stock sensitivity to exchange rate movements, controlling for various risk factors. The findings indicate that foreign exchange risk is priced in German stock returns, with a nonlinear, inverse U-shaped relationship between returns and foreign exchange sensitivity. This sensitivity is found to be higher for small-cap and value stocks.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:59:45.830388
34b352d3f2e22998,Asset pricing with financial bubble risk,"This paper characterizes systematic risk stemming from the possible occurrence of price bubbles and measures the impact of this additional risk factor on asset prices. Historical stock market behavior and recent empirical experience have led economists and policy makers to acknowledge that price bubbles in financial markets do occur and need to be accounted for in risk analysis. New econometric tools for analyzing mildly explosive behavior (Phillips and Magdalinos, 2007; Phillips et al., 2011) have made it possible to detect the presence of bubbles in data and to date stamp their origination and collapse, providing empirical confirmation of such episodes in recent data. The potential for price bubbles and market collapse provides another source of stock market risk and adds to the risk premium. We provide an analytic and empirical investigation of this additional risk factor. The standard present value model is extended to allow for possible price bubbles and the effects of integrating bubble behavior into a consumption-based asset pricing model are analyzed. The theory involves attention to the investor time horizon and a study of the validity of conventional log linear approximations in the presence of nonstationary and mildly explosive data. Finite decision horizons accommodate myopic investors and are a component of speculative behavior that focuses on short run market gains rather than long run effects of fundamentals. An econometric approach to estimate bubble risk effects is developed and the methods are applied to composite stock market index data, giving new model-based equity premium and market volatility estimates that more closely match the data than traditional consumption based asset pricing models. © 2016 Elsevier B.V., All rights reserved.","Lee, J.H.; Phillips, P.C.B.",2016,10.1016/j.jempfin.2015.11.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954285389&doi=10.1016%2Fj.jempfin.2015.11.004&partnerID=40&md5=ee8c418940be84168dc9bdae30a188ed,scopus,"This paper investigates the impact of financial bubble risk on asset pricing. It extends the standard present value model to incorporate potential price bubbles and analyzes their effects within a consumption-based asset pricing framework. The study considers investor time horizons and the validity of log-linear approximations with nonstationary data. An econometric approach is developed and applied to stock market data, yielding new estimates for the equity premium and market volatility that better align with observed data compared to traditional models.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T10:59:56.146417
0c787f997c47e0be,Asset-pricing implications of biologically based non-expected utility,"Results in population ecology suggest that evolutionary successful species should have an adaptive (reference-based) S-shaped utility function that is intrinsically more sensitive to aggregate than uninsured idiosyncratic shocks the former cannot be diversified demographically. To test the asset-pricing relevance of these ideas, I embed the non-expected utility specification implied by evolutionary theory into an economy with partial risk sharing due to limited commitment. For the benchmark specification (CARA = 6 over gains), Monte Carlo simulations of a Markov growth economy produce the following results: (i) matching the degree of consumption-smoothing in the cross section, the Sharpe ratio for a Lucas tree is 0.33, an increase of 44 percent relative to expected utility; (ii) the risk-free rate is low, stable and counter-cyclical, hence equity returns, unlike in the expected utility case, have the correct pattern of predictability; (iii) in the cross section, excess returns across equity classes exhibit both a value premium and a size discount with risk adjusted returns that are at least two times higher than their expected utility counterparts. (C) 2012 Elsevier Inc. All rights reserved.","Iantchev, Emil P.",2013,10.1016/j.red.2012.08.002,,wos,"This paper explores the asset-pricing implications of a biologically inspired non-expected utility function, derived from evolutionary theory. The study embeds this utility specification into an economy with partial risk sharing due to limited commitment. Monte Carlo simulations in a Markov growth economy show that this non-expected utility model can better match cross-sectional consumption smoothing, leading to a higher Sharpe ratio for a Lucas tree compared to the expected utility model. It also generates a low, stable, and counter-cyclical risk-free rate, and provides a more accurate pattern of equity return predictability, including value premiums and size discounts.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:00:27.071856
cb2acc4b493b7729,"Association between isolation of staphylococcus aureus one week after calving and milk yield, somatic cell count, clinical mastitis, and culling through the remaining lactation","Cows with isolation of Staphylococcus aureus approximately 1 week after calving and milk yield, somatic cell count (SCC), clinical mastitis (CM), and culling risk through the remaining lactation were assessed in 178 Norwegian dairy herds. Mixed models with repeated measures were used to compare milk yield and SCC, and survival analyses were used to estimate the hazard ratio for CM and culling. On average, cows with an isolate of Staph. aureus had a significantly higher SCC than culture-negative cows. If no post-milking teat disinfection (PMTD) was used, the mean values of SCC were 42 000, 61 000, 68 000 and 77 000 cells/ml for cows with no Staph. aureus isolate, with Staph. aureus isolated in 1 quarter, in 2 quarters and more than 2 quarters respectively. If iodine PMTD was used, SCC means were 36 000; 63 000; 70 000 and 122 000, respectively. Primiparous cows testing positive for Staph. aureus had the same milk yield curve as culture-negative cows, except for those with Staph. aureus isolated in more than 2 quarters. They produced 229 kg less during a 305-d lactation. Multiparous cows with isolation of Staph. aureus in at least 1 quarter produced 94161 kg less milk in 2nd and >3rd parity, respectively, and those with isolation in more than 2 quarters produced 303390 kg less than multiparous culture-negative animals during a 305-d lactation. Compared with culture-negative cows, the hazard ratio for CM and culling in cows with isolation of Staph. aureus in at least 1 quarter was 20 (1624) and 17 (1519), respectively. There was a decrease in the SCC and in the CM risk in culture-negative cows where iodine PMTD had been used, indicating that iodine PMTD has a preventive effect on already healthy cows. For cows testing positive for Staph. aureus in more than 2 quarters at calving, iodine PMTD had a negative effect on the CM risk and on the SCC through the remaining lactation. © 2008 Proprietors of Journal of Dairy Research 2008. © 2009 Elsevier B.V., All rights reserved.","Whist, A.C.; Osterås, O.; Sølverød, L.",2009,10.1017/s0022029908003592,https://www.scopus.com/inward/record.uri?eid=2-s2.0-61849171892&doi=10.1017%2FS0022029908003592&partnerID=40&md5=a117643932e2df01d89e6acaacd31332,scopus,"This study investigated the impact of Staphylococcus aureus isolation one week after calving on milk yield, somatic cell count (SCC), clinical mastitis (CM), and culling risk in Norwegian dairy herds. Results showed that cows with Staph. aureus isolation generally had higher SCC. Milk yield was reduced, particularly in multiparous cows with Staph. aureus in multiple quarters. The risk of CM and culling was significantly elevated in cows with Staph. aureus. The study also examined the effect of post-milking teat disinfection (PMTD), finding a preventive effect of iodine PMTD on healthy cows but a negative effect on CM risk and SCC in cows with Staph. aureus in more than two quarters.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:00:31.490698
4b22c1332c433aaa,Asymmetric impacts of individual investor sentiment on the time-varying risk-return relation in stock market,"This study investigates the impacts of investor sentiments, including individual sentiment and market-wide sentiments, on time-varying risk-return tradeoffs in the U.S. stock market using quantile regressions. Empirical results show that the individual sentiment has a significant negative effect on the time-varying risk-return tradeoff across all quantiles, indicating the heterogeneity of the individual sentiment effect. Specifically, the positive individual sentiment weakens the time-varying risk-return tradeoff while the negative individual sentiment enhances it. Besides, there are asymmetric effects of the individual sentiment at quantiles (0.25, 0.75), that is, a negative individual sentiment associated with bad news has a stronger impact than a positive individual sentiment associated with good news. These findings are robust for alternative estimate methods and individual sentiments. However, the study finds that the time-varying riskreturn tradeoff is less sensitive to the market-wide sentiment than to the individual sentiment, indicating that the individual sentiment is more useful and important in determining the stock price and variation.","He, Zhifang",2022,10.1016/j.iref.2021.11.018,,wos,"This study examines how individual and market-wide investor sentiments affect the time-varying risk-return relationship in the U.S. stock market using quantile regressions. It finds that individual sentiment significantly impacts this relationship across all quantiles, with positive sentiment weakening it and negative sentiment strengthening it. The impact of negative sentiment is stronger than positive sentiment, especially during bad news periods. The study concludes that individual sentiment is more influential than market-wide sentiment in stock price determination.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:00:42.890921
1b83d207f7348a04,Asymmetries in exchange rate pass-through and monetary policy principle: Some Caribbean empirical evidence,"The study shows that nonlinear models yield superior unbiased estimates which are free of serial correlation, heteroscedasticity and functional form instability problems which often affect linear models. All six countries have partial exchange rate pass-through, with only The Bahamas recording the lowest pass-through comparable with developed countries which target inflation. The rest of countries, and to a lesser extent Barbados, have high exchange rate pass-through which signifies their vulnerabilities to external inflation. Depreciation results in higher pass-through than appreciation, especially during rising prices/inflation. Nonlinear/threshold cointegration exists in all six countries. TAR results show asymmetric adjustment towards long-run equilibrium in The Bahamas, Guyana and to a lesser extent Jamaica, and symmetric adjustment in Barbados, Belize and Trinidad-Tobago. M-TAR results show asymmetric adjustment in four countries, while Guyana and Trinidad-Tobago experience symmetric adjustment. Taylor's rule is effective in The Bahamas, insignificant in Trinidad-Tobago; and ineffective in the rest of the countries. Appreciation is the most effective operating target for central banks to reduce inflation in The Bahamas, Barbados, Guyana and Jamaica. © 2019 Elsevier B.V., All rights reserved.","Ghartey, E.E.",2019,10.1016/j.najef.2018.05.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047390611&doi=10.1016%2Fj.najef.2018.05.001&partnerID=40&md5=031863a6cf182b0e8952be0d6eeb64d3,scopus,"This study utilizes nonlinear models to analyze exchange rate pass-through and monetary policy in six Caribbean countries. It finds partial pass-through in all countries, with The Bahamas showing the lowest. Depreciation leads to higher pass-through than appreciation. Nonlinear cointegration is present, and adjustment towards equilibrium is asymmetric in some countries. Taylor's rule effectiveness varies, and appreciation is identified as an effective tool for reducing inflation in several nations.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:00:48.246159
996de93213bec4cf,Audit firm tenure and the equity risk premium,"Although investor perceptions of audit quality play a critical role in maintaining systemic confidence in the integrity of financial accounting reports (Levitt [2000]), prior research on the effects of auditor tenure from an investor perspective is relatively sparse. In this study, we investigate whether investors price audit firm tenure for Big Five audits by examining the relation between tenure and the ex ante equity risk premium, that is, the excess of the company-specific ex ante cost of equity capital over the risk-free interest rate. Based on prior research, whereas the ""auditor learning"" argument predicts that audit quality will change in only one direction (i.e., improve) with tenure, the ""auditor-client closeness"" argument suggests that audit quality may decrease beyond some (albeit unspecified) length of tenure because of impaired auditor independence and objectivity. Consistent with prior theoretical arguments, we find some evidence of a nonlinear relation between audit firm tenure and the ex ante equity risk premium, that is, we find that the equity risk premium decreases in the early years of tenure but increases with additional years of tenure. These findings persist after we control for well-known risk factors and company characteristics that have been shown in prior research to be related to the cost of equity capital. The implications of our findings are discussed. © 2018 Elsevier B.V., All rights reserved.","Boone, J.P.; Khurana, I.K.; Raman, K.K.",2008,10.1177/0148558x0802300107,https://www.scopus.com/inward/record.uri?eid=2-s2.0-39049110814&doi=10.1177%2F0148558X0802300107&partnerID=40&md5=afbbdddf8ba8026c99f86443de6bb403,scopus,"This study investigates the relationship between audit firm tenure and the equity risk premium, examining whether investors price this tenure. The findings suggest a nonlinear relationship, where the equity risk premium initially decreases with tenure but then increases with longer tenure, even after controlling for other risk factors and company characteristics. The research explores the 'auditor learning' versus 'auditor-client closeness' arguments regarding audit quality.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:00:59.346880
9cf59f608ef4415d,Autoencoder-Based Three-Factor Model for the Yield Curve of Japanese Government Bonds and a Trading Strategy,"Interest rates are representative indicators that reflect the degree of economic activity. The yield curve, which combines government bond interest rates by maturity, fluctuates to reflect various macroeconomic factors. Central bank monetary policy is one of the significant factors influencing interest rate markets. Generally, when the economy slows down, the central bank tries to stimulate the economy by lowering the policy rate to establish an environment in which companies and individuals can easily raise funds. In Japan, the shape of the yield curve has changed significantly in recent years following major changes in monetary policy. Therefore, an increasing need exists for a model that can flexibly respond to the various shapes of yield curves. In this research, we construct a three-factor model to represent the Japanese yield curve using the machine learning approach of an autoencoder. In addition, we focus on the model parameters of the intermediate layer of the neural network that constitute the autoencoder and confirm that the three automatically generated factors represent the “Level,” “Curvature,” and “Slope” of the yield curve. Furthermore, we develop a long–short strategy for Japanese government bonds by setting their valuation with the autoencoder, and we confirm good performance compared with the trend-follow investment strategy.",,2020,10.3390/jrfm13040082,,proquest,"This study develops an autoencoder-based three-factor model to represent the Japanese government bond yield curve, identifying factors corresponding to Level, Curvature, and Slope. The model's parameters are used to create a long-short trading strategy for Japanese government bonds, which demonstrates superior performance compared to a trend-following strategy.",True,True,False,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T11:02:07.280405
28706c3e7bae92e6,BEKKs: An R Package for Estimation of Conditional Volatility of Multivariate Time Series,"We describe the R package BEKKs, which implements the estimation and diagnostic analysis of a prominent family of multivariate generalized autoregressive conditionally heteroskedastic (MGARCH) processes, the so-called BEKK models. Unlike existing software packages, we make use of analytical derivatives implemented in efficient C++ code for nonlinear log-likelihood optimization. This allows fast parameter estimation even in higher model dimensions N > 3. The baseline BEKK model is complemented with an asymmetric parameterization that allows for a flexible modeling of conditional (co)variances. Furthermore, we provide the user with the simplified scalar and diagonal BEKK models to deal with high dimensionality of heteroskedastic time series. The package is designed in an object-oriented way featuring a comprehensive toolbox of methods to investigate and interpret, for instance, volatility impulse response functions, risk estimation and forecasting (VaR) and a backtesting algorithm to compare the forecasting performance of alternative BEKK models. For illustrative purposes, we analyze a bivariate ETF return series (S&P, US treasury bonds) and a four-dimensional system comprising, in addition, a gold ETF and changes of a log oil price by means of the suggested package. We find that the BEKKs package is more than 100 times faster for time series systems of dimension N > 3 than other existing packages. © 2024 Elsevier B.V., All rights reserved.","Fülle, M.J.; Lange, A.; Hafner, C.M.; Herwartz, H.",2024,10.18637/jss.v111.i04,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211314976&doi=10.18637%2Fjss.v111.i04&partnerID=40&md5=64e7416bf0ca62d1bfe6d56bc4055e20,scopus,"This paper introduces the R package BEKKs for estimating and analyzing multivariate GARCH (MGARCH) models, specifically BEKK models. It highlights the package's efficiency through C++ implementation of analytical derivatives, enabling faster parameter estimation, especially for higher dimensions. The package supports asymmetric parameterizations, simplified scalar and diagonal BEKK models, and includes tools for volatility impulse response functions, risk estimation (VaR), and backtesting. The authors demonstrate the package's utility by analyzing bivariate and four-dimensional ETF return series, showing significant speed improvements over existing software.",False,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:04:12.334523
83fe6cfa852a80cb,Baltic Dry Index Estimation With NARX Neural Network Model,"Abstract. BDI is a global trade indicator followed by those interested in maritime trade. But it has volatility, seasonality, and uncertain cyclicality. For this reason, in this study, the BDI has been estimated to provide preliminary information to those interested in maritime trade. NARX Neural Network which performs successfully in complex and nonlinear real-life problems is used. In addition, the NARX neural network model has not been found in a previous study used for BDI estimation. Eleven independent variables are used in this study, what increases the predictive power. Independent variables are Bloomberg Commodities Index (BCOM), Twitter-Based Economic Uncertainty Index (TEU), Twitter-Based Market Uncertainty Index (TMU), S&P 500 Index, MSCI World Index, €/$ Parity, VIX (CBOE), US 10-Year Bond Yield (%), Brent Oil (USD/Barrel), Economic Uncertainty Index and World Trade Volume (USD Billion). The Twitter-Based Economic Uncertainty Index (TEU) and Twitter-Based Market Uncertainty Index (TMU), which were not used before in BDI estimation studies, were included in the analysis and contributed to the literature. The data set contains daily data for the period 9.07.2012-31.08.2020. 11-day estimate values covering 1.09.2020-15.09.2020 are calculated. MAPE, MAE and RMSE performance criteria were calculated for the estimation values. Value of MAPE (2.96%), value of MAE (36.6%) and value of RMSE (46.68) were obtained. As a result, the estimate values were compared with the actual values.",,2023,10.15388/ekon.2023.102.1.4,,proquest,"This study estimates the Baltic Dry Index (BDI) using a NARX Neural Network model, incorporating eleven independent variables including commodity indices, uncertainty indices (including novel Twitter-based ones), stock market indices, currency parity, VIX, bond yields, oil prices, and world trade volume. The model was trained on daily data from 2012-2020 and used to forecast 11 days ahead. Performance metrics (MAPE, MAE, RMSE) were calculated, showing promising estimation accuracy.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:04:54.482905
dc9164700f08b9ff,Bank portfolio exposure to emerging markets and its effects on bank market value,"This study estimates a model of banking company equity returns taking into consideration book value and market value measures of their exposure to emerging markets debt. In this estimation, general systematic market factors, such as the rate of return on the S&P500 stock index and yields on a constant maturity 5-year Treasury note, are held constant such that the exposure variables are accounting for effects due to banks' exposure to emerging market debt. The results, although not uniform among banking companies, support the hypothesis that the extent of exposure to emerging market debt are factored into the valuation of banking company equity contemporaneously. The inclusion of a market value indicator adds to the explanation of equity returns of some banks. It is also clear that knowing the extent of the exposure on a book value basis is important information alone that may allow investors to take account of or evaluate the effects of changes in banking company equity valuation from LDC debt exposures. We also perform an event study for three major debt crises to determine whether the market recognizes the effects of these events on bank valuation. The event study results show that there is little information from identifying the time period of the crises on banking company equity returns. Explanations for this are that the information of these possible crises has been embedded in bank changes in exposure and that the market valuation of the emerging market debt is already accounted for by our model. (c) 2005 Elsevier B.V. All rights reserved.","Fissel, GS; Goldberg, L; Hanweck, GA",2006,10.1016/j.jbankfin.2005.05.013,,wos,"This study investigates the impact of bank portfolio exposure to emerging markets debt on bank market value. It uses a model to estimate equity returns, considering both book and market value measures of exposure, while controlling for general market factors. The findings suggest that emerging market debt exposure is contemporaneously factored into banking equity valuation. An event study on debt crises indicated limited direct impact on equity returns, possibly due to embedded information in exposure changes and prior market valuation.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:05:02.787560
9eb0be674d53d7ff,"Bank risks, capital and loan supply: evidence from Sierra Leone","Purpose -- The study aims to investigate the factors that influence banks' loan supply in Sierra Leone. More specifically, it seeks to look into the effects of risk premium, leverage ratio and credit risk on banks' loan supply in Sierra Leone. Design/methodology/approach -- Using annual bank level data on an unbalanced panel of 13 commercial banks data observed over a period of ten years (2002 to 2011), the study employs time and bank-specific fixed effects model for estimation. Findings -- The findings indicate that risk premium, the share of non-performing loans in the banks' loan portfolio, tier 1 capital ratio (leverage ratio) and local currency deposit levels positively and significantly affect the share of loan supply to the private sector in banks' earning assets. On the other hand, advances to local currency deposit ratio and bank size have significant negative effects on the share of loans in banks assets. The study also finds bank type and the growth rate of real GDP (a proxy for economic activity) to be important determinants of the share of loans in banks' earning assets. Practical implications -- The study recommends that the monetary authorities, banking practitioners and the government should pay keen attention to the key risk factors such as non-performing loans and risk premium in the operation of the banking sector to boost commercial banks' loan supply. Originality/value -- Sierra Leone's banking sector presents a unique opportunity to study bank loan supply in relation to bank-specific features in the context of post-war financial reconstruction. Adapted from the source document.",,2013,10.1108/jfep-09-2012-0041,,proquest,"This study examines factors influencing banks' loan supply in Sierra Leone, specifically the impact of risk premium, leverage ratio, and credit risk. Using a panel data fixed-effects model for 13 commercial banks from 2002-2011, the research found that risk premium, non-performing loans, and tier 1 capital ratio positively affect loan supply. Conversely, advances to local currency deposit ratio and bank size negatively impact loan supply. Bank type and GDP growth rate were also identified as significant determinants. The study suggests monetary authorities and practitioners focus on risk factors like non-performing loans to enhance loan supply.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:05:05.234532
b917561dfec7c669,Banking and Currency Crises: Differential Diagnostics for Developed Countries,"We identify a set of ‘rules of thumb’ that characterize economic, financial and structural conditions preceding the onset of banking and currency crises in 36 advanced economies over 1970–2010. We use the classification and regression tree methodology and its random forest extension, which permits the detection of key variables driving binary crisis outcomes, allows for interactions among key variables and determines critical tipping points. We distinguish between basic country conditions, country structural characteristics and international developments. We find that crises are more varied than they are similar. For banking crises, we find that low net interest rate spreads in the banking sector and a shallow, or inverted, yield curve is their most important forerunners in the short term. In the longer term, it is high house price inflation. For currency crises, high domestic short-term rates coupled with overvalued exchange rates are the most powerful short-term predictors. We find that both country structural characteristics and international developments are relevant banking-crisis predictors. Currency crises, however, seem to be driven more by country idiosyncratic, short-term developments. We find that some variables, such as the domestic credit gap, provide important unconditional signals, but it is difficult to use them as conditional signals and, more importantly, to find relevant threshold values. Copyright © 2016 John Wiley & Sons, Ltd. © 2017 Elsevier B.V., All rights reserved.","Joy, M.; Rusnák, M.; Šmídková, K.; Vašíček, B.",2017,10.1002/ijfe.1570,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997815830&doi=10.1002%2Fijfe.1570&partnerID=40&md5=ad5779af0086673cb5576a2c30dc058f,scopus,"This study identifies 'rules of thumb' for banking and currency crises in 36 advanced economies (1970-2010) using classification and regression trees and random forests. Key predictors for banking crises include low net interest rate spreads, inverted yield curves, and high house price inflation. For currency crises, high domestic short-term rates and overvalued exchange rates are significant short-term predictors. The study highlights that banking crises are influenced by both structural and international factors, while currency crises are more idiosyncratic and short-term driven.",True,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:05:46.542349
8f362e0ee7b8047a,Bayesian Factor-adjusted Sparse Regression,"Many sparse regression methods are based on the assumption that covariates are weakly correlated, which unfortunately do not hold in many economic and financial datasets. To address this challenge, we model the strongly-correlated covariates by a factor structure: strong correlations among covariates are explained by common factors and the remaining variations are interpreted as idiosyncratic components. We then propose a factor-adjusted sparse regression model with both common factors and idiosyncratic components as decorrelated covariates and develop a semi-Bayesian method. Parameter estimation rate-optimality and model selection consistency are established by non-asymptotic analyses. We show on simulated data that the semi-Bayesian method outperforms its Lasso analogue, manifests insensitivity to the overestimates of the number of common factors, pays a negligible price when covariates are not correlated, scales up well with increasing sample size, dimensionality and sparsity, and converges fast to the equilibrium of the posterior distribution. Numerical results on a real dataset of U.S. bond risk premia and macroeconomic indicators also lend strong supports to the proposed method.Many sparse regression methods are based on the assumption that covariates are weakly correlated, which unfortunately do not hold in many economic and financial datasets. To address this challenge, we model the strongly-correlated covariates by a factor structure: strong correlations among covariates are explained by common factors and the remaining variations are interpreted as idiosyncratic components. We then propose a factor-adjusted sparse regression model with both common factors and idiosyncratic components as decorrelated covariates and develop a semi-Bayesian method. Parameter estimation rate-optimality and model selection consistency are established by non-asymptotic analyses. We show on simulated data that the semi-Bayesian method outperforms its Lasso analogue, manifests insensitivity to the overestimates of the number of common factors, pays a negligible price when covariates are not correlated, scales up well with increasing sample size, dimensionality and sparsity, and converges fast to the equilibrium of the posterior distribution. Numerical results on a real dataset of U.S. bond risk premia and macroeconomic indicators also lend strong supports to the proposed method.",,2022,10.1016/j.jeconom.2020.06.012,,proquest,"This paper proposes a Bayesian factor-adjusted sparse regression model to handle strongly correlated covariates, common in economic and financial data. The method models correlations using common factors and idiosyncratic components, decorrelating covariates. The authors establish theoretical properties and demonstrate superior performance over Lasso on simulated data, showing robustness and scalability. Empirical results on U.S. bond risk premia and macroeconomic indicators support the method's effectiveness.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T11:06:10.689796
331b25330b0d6585,Bayesian estimation of generalized hyperbolic skewed student GARCH models,"Efficient posterior simulators for two GARCH models with generalized hyperbolic disturbances are presented. The first model, GHt-GARCH, is a threshold GARCH with a skewed and heavy-tailed error distribution; in this model, the latent variables that account for skewness and heavy tails are identically and independently distributed. The second model, ODLV-GARCH, is formulated in terms of observation-driven latent variables; it automatically incorporates a risk premium effect. Both models nest the ordinary threshold t-GARCH as a limiting case. The GHt-GARCH and ODLV-GARCH models are compared with each other and with the threshold t-GARCH using five publicly available asset return data sets, by means of Bayes factors, information criteria, and classical forecast evaluation tools. The GHt-GARCH and ODLV-GARCH models both strongly dominate the threshold t-GARCH, and the Bayes factors generally favor GHt-GARCH over ODLV-GARCH. A Markov switching extension of GHt-GARCH is also presented. This extension is found to be an empirical improvement over the single-regime model for one of the five data sets. © 2010 Elsevier B.V. All rights reserved. © 2012 Elsevier B.V., All rights reserved.","Deschamps, P.J.",2012,10.1016/j.csda.2011.10.021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862002336&doi=10.1016%2Fj.csda.2011.10.021&partnerID=40&md5=8ad3dd3fea5afe72e5ab50f367c7831f,scopus,"This paper presents Bayesian estimation methods for two GARCH models with generalized hyperbolic disturbances: GHt-GARCH and ODLV-GARCH. Both models are compared to the standard threshold t-GARCH model using asset return data. The proposed models generally outperform the standard model, with GHt-GARCH often favored over ODLV-GARCH. A Markov switching extension of GHt-GARCH is also introduced and found to be an improvement for one dataset.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:06:43.286822
3b84aee29f05f089,Bayesian estimation of stable CARMA spot models for electricity prices,"We develop a Bayesian estimation procedure for the electricity spot price model in Benth et al. (2014). This model incorporates a trend and seasonality component, a stable CARMA process for the price spikes, and an additional Lévy process for mid-range price level changes. Our MCMC algorithm has two advantages over the existing stepwise estimation procedure presented in Benth et al. (2014): First, since our algorithm produces samples from the full posterior distribution over all parameters, we can estimate the parameters much more accurately, which is shown in simulation studies. Second, we can provide accuracy measures as credibility intervals in addition to the point estimates. The approach is quite general, so that it can be adapted also to other similar pricing models. For illustration, we analyse spot and future prices from the EEX using the new Bayesian method and provide estimates for the risk premium together with credibility regions. © 2019 Elsevier B.V., All rights reserved.","Müller, G.; Seibert, A.",2019,10.1016/j.eneco.2018.10.016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057586272&doi=10.1016%2Fj.eneco.2018.10.016&partnerID=40&md5=c80f6183b56a7b047b29e1f7324535da,scopus,"This paper presents a Bayesian estimation procedure for a stable CARMA spot model for electricity prices, which includes trend, seasonality, price spikes, and mid-range price level changes. The proposed MCMC algorithm offers more accurate parameter estimation and provides credibility intervals compared to existing methods. The approach is demonstrated by analyzing EEX spot and future prices, estimating risk premiums and their credibility regions.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:06:48.220594
aaff782e22358a7e,Bayesian inference and state number determination for hidden Markov models: an application to the information content of the yield curve about inflation,"This paper is concerned with Bayesian inference in hidden Markov models. Focusing oil switching regression models, we propose a new methodology that delivers a joint estimation of the parameters and the number of regimes that have actually appeared in the studied sample. The only prior information that is required on the latter quantity is an upper bound. We implement a particle filter algorithm to compute the corresponding estimates. Applying this methodology to the information content of the yield curve regarding future inflation in four OECD countries, we show that the predictive content for given country and combination of maturities is subject to regime switching. (C) 2003 Elsevier B.V. All rights reserved.","Chopin, N; Pelgrin, F",2004,10.1016/j.jeconom.2003.12.010,,wos,"This paper proposes a Bayesian inference methodology for hidden Markov models, specifically for switching regression models. It jointly estimates model parameters and the number of regimes, requiring only an upper bound for the number of regimes. A particle filter algorithm is used for computation. The methodology is applied to analyze the information content of the yield curve regarding future inflation in four OECD countries, revealing regime switching in predictive content.",True,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:07:27.886315
b0bcf13b6f1f1508,Bayesian inference for long memory term structure models,"In this study, we propose a novel adaptation of the Dynamic Nelson–Siegel term structure model, incorporating long memory properties to enhance its forecasting accuracy. Our approach involves modelling the evolution of latent factors using fractional Gaussian noise processes, approximated by a weighted sum of independent first-order autoregressive components. The resulting formulation allows for a Gaussian Markov Random Field representation, facilitating the application of computationally efficient Bayesian techniques through Integrated Nested Laplace Approximations. Extensive simulation and empirical analysis demonstrate that integrating long memory significantly improves the model's forecasting performance, particularly for longer time horizons. By shedding light on the potential benefits of incorporating long memory concepts into traditional term structure models, our research highlights its utility in capturing intricate temporal dependencies and enhancing prediction precision.",,2024,10.1080/00949655.2023.2299938,,proquest,"This study adapts the Dynamic Nelson–Siegel term structure model by incorporating long memory properties using fractional Gaussian noise processes. The model is approximated by a weighted sum of first-order autoregressive components, allowing for a Gaussian Markov Random Field representation and efficient Bayesian inference via Integrated Nested Laplace Approximations. Both simulation and empirical analyses show that the long memory integration improves forecasting accuracy, especially for longer horizons, by capturing complex temporal dependencies.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T11:08:35.276593
f94a815f95694f43,Bias in the estimation of the mean reversion parameter in continuous time models,"It is well known that for continuous time models with a linear drift standard estimation methods yield biased estimators for the mean reversion parameter both in finite discrete samples and in large in-fill samples. In this paper, we obtain two expressions to approximate the bias of the least squares/maximum likelihood estimator of the mean reversion parameter in the Ornstein-Uhlenbeck process with a known long run mean when discretely sampled data are available. The first expression mimics the bias formula of Marriott and Pope (1954) for the discrete time model. Simulations show that this expression does not work satisfactorily when the speed of mean reversion is slow. Slow mean reversion corresponds to the near unit root situation and is empirically realistic for financial time series. An improvement is made in the second expression where a nonlinear correction term is included into the bias formula. It is shown that the nonlinear term is important in the near unit root situation. Simulations indicate that the second expression captures the magnitude, the curvature and the non-monotonicity of the actual bias better than the first expression. (C) 2012 Elsevier B.V. All rights reserved.","Yu, Jun",2012,10.1016/j.jeconom.2012.01.004,,wos,"This paper derives two expressions to approximate the bias of the least squares/maximum likelihood estimator for the mean reversion parameter in the Ornstein-Uhlenbeck process using discrete time data. The second expression, which includes a nonlinear correction term, is shown to be more accurate, especially in situations of slow mean reversion (near unit root), which are common in financial time series.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:08:40.871858
d7cc9979d5792ab1,Biased Bayesian learning with an application to the risk-free rate puzzle,"Based on the axiomatic framework of Choquet decision theory, we develop a closed-form model of Bayesian learning with ambiguous beliefs about the mean of a normal distribution. In contrast to rational models of Bayesian learning the resulting Choquet Bayesian estimator results in a long-run bias that reflects the agent's ambiguity attitudes. By calibrating the standard equilibrium conditions of the consumption based asset pricing model we illustrate that our approach contributes towards a resolution of the risk-free rate puzzle. For a plausible parameterization we obtain a risk-free rate in the range of 3.5-5%. This is 1-2.5% closer to the empirical risk-free rate than according calibrations of the rational expectations model. (C) 2013 Elsevier B.V. All rights reserved.","Ludwig, Alexander; Zimper, Alexander",2014,10.1016/j.jedc.2013.11.007,,wos,"This paper presents a model of Bayesian learning with ambiguous beliefs about the mean of a normal distribution, based on Choquet decision theory. The model results in a long-run bias that reflects the agent's ambiguity attitudes. When applied to the consumption-based asset pricing model, this approach helps resolve the risk-free rate puzzle, yielding a risk-free rate closer to empirical observations than rational expectations models.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:08:44.244962
732f5350febe431a,"Big data analytics in economics: What have we learned so far, and where should we go from here?","Research into predictive accuracy testing remains at the forefront of the forecasting field. One reason for this is that rankings of predictive accuracy across alternative models, which under misspecification are loss function dependent, are universally utilized to assess the usefulness of econometric models. A second reason, which corresponds to the objective of this paper, is that researchers are currently focusing considerable attention on so‐called big data and on new (and old) tools that are available for the analysis of this data. One of the objectives in this field is the assessment of whether big data leads to improvement in forecast accuracy. In this survey paper, we discuss some of the latest (and most interesting) methods currently available for analyzing and utilizing big data when the objective is improved prediction. Our discussion includes a summary of various so‐called dimension reduction, shrinkage and machine learning methods as well as a summary of recent tools that are useful for ranking prediction models associated with the implementation of these methods. We also provide a brief empirical illustration of big data in action, in which we show that big data are indeed useful when predicting the term structure of interest rates.",,2018,10.1111/caje.12336,,proquest,"This survey paper discusses methods for analyzing big data to improve prediction accuracy, focusing on dimension reduction, shrinkage, and machine learning techniques. It includes an empirical illustration showing the usefulness of big data in predicting the term structure of interest rates.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T11:09:31.363852
60a879db8106ce03,Bilateral multiple gamma returns: Their risks and rewards,"The bilateral gamma model for returns is naturally derived from the lognormal model. Maximizing entropy in a random time change delivers the symmetric variance gamma model. The asymmetric variance gamma follows on incorporating skewness. Differential speeds for the upward and downward motions lead to the bilateral gamma. A further generalizations results in the bilateral double gamma model when the speed parameter of the bilateral gamma model is itself taken to be gamma distributed on entropy maximization. A rich five to seven parameter specification of preferences renders possible the extraction of physical densities from option prices. The quality of such extraction is measured by examining the uniformity of the estimated distribution functions evaluated at realized forward returns. The economic value of risky returns is seen to embed three/five risk premia for the bilateral gamma/bilateral double gamma. For the bilateral gamma they are up and down side volatilities compensated in up and down side drifts, and the down side drift compensated in the up side drift. For the bilateral double gamma one adds in addition compensations for skewness. Results reveal a drop in the down side risk premiumsince the crisis with an increase in the recent period.","Madan, Dilip B.; Schoutens, Wim; Wang, King",2020,10.1142/s2424786320500085,,wos,"This paper introduces the bilateral gamma model for returns, derived from the lognormal model and incorporating skewness and differential speeds for upward and downward movements. It generalizes to the bilateral double gamma model. The model allows for the extraction of physical densities from option prices using a five to seven parameter specification of preferences. The economic value of risky returns is analyzed through risk premia, identifying three/five such premia for the bilateral gamma/bilateral double gamma models, respectively. The study observes a decrease in downside risk premiums since the crisis and an increase in the recent period.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:11:07.519748
067ccda7c63409f5,Brand Alliances and Stock Reactions,"Purpose: This paper examines the performance and risk of brand alliances by investigating the market value of brand alliances through the analysis of investors’ response, and look into the different reactions of the stock market to brand alliance-type in terms of co-branding and joint-promotion, as well as into the potential different effects in the contexts of B to B versus B to C. Brand alliances, whereby two or more brands are jointly presented to the consumer, have been investigated extensively. The importance of brand alliances is emphasized by two factors: (1) brands are considered critical elements in business-to-business marketing settings; and (2) firms use brand alliances due to the trading costs and investment necessary to buy brands, the increasingly higher costs of launching a new brand onto the market, the high failure rates in new brand launches and brand extensions, the competitive pressures around product launches and diffusion, and the limitations imposed on the extension of a brand by its own identity. Consequently, brand alliances have exploded over recent years. As indicated later, by accomplishing the purpose of this research we fill a gap in the literature as most of the research on brand alliances revolves around consumers’ perspective. Methodology/Approach: The methodology followed is based on the event study method. First, the event study estimates the excess returns of share prices generated by events that were unanticipated by the market. To this end, we estimate the market model and the subsequent abnormal returns. To examine the impact of the publication of a brand alliance announcement on the share prices of the company, we use the cumulative abnormal returns calculated over k days of the event window for 55 announcements. In the second step, we analyze the returns of the different brand alliances. In particular, the abnormal returns are used as dependent variable in a regression analysis, wherein the central explanatory variable is brand alliance type (co-branding vs joint promotion). Finally, the third stage of the methodology analyzes the change in the variance of returns between the periods before and after the brand alliance announcements. Findings: The results show that brand alliance announcements generate positive abnormal returns, which support the hypothesis that brand alliance announcements are positively related to company stock returns. In particular, we observe that the reactions to brand alliances are spread over the event window. In fact, the window (−5,+5) produces returns that stand at 1.6%, which is the greatest abnormal return over the five days around the publication date. The economic impact of a cumulative return of 1.6% in eleven days is tantamount to annual returns of 69.33%. Considering that the average market value of the sample is €17,494 million, it represents an increase of €279 million for the sample stocks on the period (−5,+5). The regression analysis shows that the coefficients of the variable “co-branding” are positive and significant, which supports the hypothesis that co-branding presents higher abnormal returns than joint promotion. However, no differential effect are found between B to B versus B to C paradigms. The results obtained present an increase in the variance of the share prices after the alliance announcement date, which supports the hypothesis that the variance of the company stock returns is positively associated to announcements of brand alliances. Research Implications: The key implication of the measurement of the market value of brand alliances is that research should be reoriented toward a better understanding of the role of marketing in the value creation of a company. Instead of just concentrating on marketing research into consumer behavior, more emphasis should be given to the core company processes that create shareholder value. Practical Implications: The managerial implications of the specific results obtained are the following: the result that companies increase their market value when they implement brand alliance strategies, leads to a better ken of the way alliance activities can be managed when dealing with other organizations. In this way, finding a partner to form a brand alliance with could be a useful objective in terms of firm performance. Moreover, the results show that co-branding presents higher abnormal returns than joint promotion, which suggests that co-branding is the most valuable strategic decision (or long-term decision) for companies, as it implies the simultaneous participation of two or more brands in a single product. In this way, deciding on whether a short- or long-term branding strategy is pursued turns to be fundamental. Originality/Value/Contribution of the paper: The literature has analyzed the consequences of brand alliance, which looks at each partner’s brand attitude after the alliance, the brand equity of the constituent brands after the alliance, and the impact of the allied brand on the evaluation of the host brand. These studies have focused on the area of consumer behavior; that is, by measuring consumers´ attitudes and evaluation. Still, the measurement of dimensions reflecting the other side of the relationship, i.e. the firm, via brand image and equity is critical. Nevertheless, the examination of the impact of brand alliances on the partner company performance and risk has received little attention, despite the fact that “brand perceptions of companies’ products spill over to investment decisions in the market for companies’ stock”. © 2021 Elsevier B.V., All rights reserved.","Mas-Ruiz, F.J.; Nicolau, J.L.; Calderón-Martínez, A.",2021,10.1080/1051712x.2021.1893029,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106229558&doi=10.1080%2F1051712X.2021.1893029&partnerID=40&md5=b86e75377e394e1b65e18c69efb1907f,scopus,"This paper investigates the stock market's reaction to brand alliances, specifically co-branding and joint promotion, using an event study methodology. The findings indicate that brand alliance announcements positively impact company stock returns, with co-branding yielding higher abnormal returns than joint promotion. The study also observed an increase in stock price variance post-announcement but found no differential effect between B2B and B2C contexts. The research emphasizes the importance of marketing's role in value creation and suggests co-branding as a valuable strategic decision.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:11:10.121459
aa51b3156f3ce072,"Business cycles, stock market wealth, and gambling at the racetracks","Purpose: As digital technologies expand access to new forms of legalized gambling, including sports betting and online gaming, it is important to assess the impact of macroeconomic and equity market outcomes on fund flows into gambling. The authors’ findings will be of interest to policymakers and the gambling industry, as various forms of gambling, including day trading, gain broad public acceptance. Design/methodology/approach: The authors examine the impact of macroeconomic forces, business cycles, and financial market wealth on gambling. The authors propose a nonlinear model linking aggregate gambling expenditures to macroeconomic, stock market, and gambling industry variables. The authors estimate the proposed model using nonlinear estimation procedures. Findings: The authors find that price of wagering, incomes, and supply of gambling opportunities are the primary determinants of wagering demand. Aggregate wagering is negatively impacted by realized stock returns and market volatility, but rises during recessions. Originality/value: To the best of the authors’ knowledge, the questions posed and addressed in this manuscript have not been addressed in prior literature. © 2024 Elsevier B.V., All rights reserved.","Ramezani, C.A.; Ahern, J.J.",2024,10.1108/jes-03-2023-0120,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164109195&doi=10.1108%2FJES-03-2023-0120&partnerID=40&md5=f3427f470bd95119b06fc28721e86dd2,scopus,"This study investigates how macroeconomic factors, business cycles, and stock market performance influence gambling expenditures. Using a nonlinear model, the research finds that income, the price of wagering, and the availability of gambling opportunities are key drivers of demand. Notably, aggregate wagering decreases with positive stock returns and market volatility but increases during economic recessions. The authors highlight the novelty of their approach in addressing these questions.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:11:15.984949
f34bce48827c7358,Business performance assessment of small and medium-sized enterprises: Evidence from the Czech Republic,"Business performance assessment is one of the basic tasks of management. Business performance can be assessed using a number of methods. The basic ones include financial analysis, Balanced Scorecard or Economic Value Added (EVA). The paper is focused on SME business performance assessment based on Economic Value Added, calculated using the INFA build-up model. According to this method, companies were divided into four categories. The first category included companies with a positive EVA value. The second category included companies with negative EVA, but with the economic result above the risk-free rate. The third category included companies with a positive economic result above the risk-free rate. The fourth category included companies with a negative economic result. The model did not include companies with negative equity. The input represented 15 predictors based on their financial statements. The data were normalized and all extreme values, likely caused by a data rewriting error, were removed. Company performance is visualized by comparing Principal Component Analysis and Kohonen neural networks. Compared to similar research, the methods are compared using the data that analyzes the performance of companies. Both methods made it possible to visualize the given task. With regard to the purpose of facilitating the interpretation of the results, for the given case, the use of PC seems to be more appropriate.",,2021,10.21511/ppm.19(3).2021.35,,proquest,"This paper assesses the business performance of Czech SMEs using the Economic Value Added (EVA) method, categorizing companies based on their EVA values and economic results relative to the risk-free rate. It employs Principal Component Analysis (PCA) and Kohonen neural networks for visualization, finding PCA more interpretable for this task. The study utilizes 15 financial statement predictors, with data normalization and outlier removal.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:11:19.959192
374c0b6ef5ceccd7,CDS risk premia forecasting with multi-featured deep RNNs: An application on BR[I]CS countries,"Using state-of-the-art recurrent neural network architectures, this study attempts to predict credit default swap risk premia for BR[I]CS countries as accurately as possible. In the time series setting, these recurrent neural networks are ELMAN, NARX, GRU, and LSTM RNNs, considering local and global features. The predictive power of each architecture is compared, and the results differ depending on the country. NARX RNN was the best predictor for Brazil and South Africa in various settings. Meanwhile, ELMAN RNN produces more accurate results in China, whereas Russia's long short-term memory RNN achieves the best predictors among other countries’ RNNs. © 2023 Elsevier B.V., All rights reserved.","Kutuk, Y.",2023,10.1016/j.bir.2023.10.013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175864895&doi=10.1016%2Fj.bir.2023.10.013&partnerID=40&md5=a144e84f118885624d72b08a1f1e2dfa,scopus,"This study forecasts credit default swap (CDS) risk premia for BRICS countries using deep recurrent neural networks (RNNs), including ELMAN, NARX, GRU, and LSTM. The performance of these models varies by country, with NARX excelling in Brazil and South Africa, ELMAN in China, and LSTM in Russia.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:11:26.109223
3c9b8446c50f3d5d,Calendar anomolies and stock market volatility in selected Arab stock exchanges,"While seasonal effects for both advanced and emerging markets have been investigated extensively in mean and variance equations, Arab region asset markets have received much less attention. The objective of this article is to fill this gap in the literature by investigating the day-of-the-week effect in 12 major Arab stock markets using Arab Monetary Fund (AMF) daily index returns from May 2002 to December 2005. Our estimation strategy utilizes Autoregressive (AR) and Generalized Autoregressive Conditional Heteroscedastic (GARCH)-type specifications to allow for a time-varying variance. Among the most important results of this article are, first, is one-third of these markets exhibit significant day-of-the-week effect in returns. Second, two-third of these markets exhibit significant day-of-the-week effect on volatility. Third, most of these day-of-the-week effects are focused within the beginning and the end of the trading week. Finally, the existence of a significant risk premium was confirmed in five of the 12 studied markets.",,2009,10.1080/09603100802359976,,proquest,"This study investigates the day-of-the-week effect on stock returns and volatility in 12 Arab stock markets from May 2002 to December 2005, using AR and GARCH models. Findings indicate significant day-of-the-week effects in returns for one-third of markets and in volatility for two-thirds, often concentrated at the beginning and end of the trading week. A significant risk premium was also confirmed in five markets.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:11:37.943129
0c6ee05d8b799ba1,Can Brazilian Central Bank communication help to predict the yield curve?,"This paper investigates whether Brazilian Central Bank communication helps to forecast the yield curve. Our forecast strategy involves two steps: First, we analyze textual Central Bank documents to extract sentiment variables that describe its communication, and then, we include those sentiment variables as additional factors into the dynamic Nelson–Siegel term structure model. We found that sentiment variables contain predictive information for yield curve forecasting. Specifically, when combined with macroeconomic variables, the sentiment variables improve the accuracy of the forecast for short maturities and forecast horizons. In addition, sentiment variables are useful in forecasting for medium and long forecast horizons for all maturities. Besides finding a new source of information to forecast the yield curve, the results indicate that the information provided by Central Bank affects market participants, proving to be a useful tool for monetary policy. © 2023 Elsevier B.V., All rights reserved.","de Andrade Alves, C.R.; Joseph Abraham, K.; Laurini, M.",2023,10.1002/for.2964,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148706166&doi=10.1002%2Ffor.2964&partnerID=40&md5=b789cdf18b904f4f1c913cf6b72bf13b,scopus,"This study explores the predictive power of Brazilian Central Bank communication for the yield curve. It extracts sentiment from Central Bank documents and incorporates these sentiment variables into a dynamic Nelson–Siegel term structure model. The findings indicate that sentiment variables enhance yield curve forecasting accuracy, particularly for short maturities and horizons when combined with macroeconomic variables, and are also useful for medium and long horizons across all maturities. This suggests that Central Bank communication is a valuable tool for monetary policy and a new source of information for yield curve prediction.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T11:11:42.535813
bb5517a95f6fd771,Can Ensemble Machine Learning Methods Predict Stock Returns for Indian Banks Using Technical Indicators?,"This paper develops ensemble machine learning models (XGBoost, Gradient Boosting, and AdaBoost in addition to Random Forest) for predicting stock returns of Indian banks using technical indicators. These indicators are based on three broad categories of technical analysis: Price, Volume, and Turnover. Various error metrics like Mean Absolute Error (MAE), Mean Squared Error (MSE), Mean Absolute Percentage Error (MAPE), Root-Mean-Squared-Error (RMSE) have been used to check the performance of the models. Results show that the XGBoost algorithm performs best among the four ensemble models. The mean of absolute error and the root-mean-square -error vary around 3–5%. The feature importance plots generated by the models depict the importance of the variables in predicting the output. The proposed machine learning models help traders, investors, as well as portfolio managers, better predict the stock market trends and, in turn, the returns, particularly in banking stocks minimizing their sole dependency on macroeconomic factors. The techniques further assist the market participants in pre-empting any price-volume action across stocks irrespective of their size, liquidity, or past turnover. Finally, the techniques are incredibly robust and display a strong capability in predicting trend forecasts, particularly with any large deviations.",,2022,10.3390/jrfm15080350,,proquest,"This study investigates the efficacy of ensemble machine learning models (XGBoost, Gradient Boosting, AdaBoost, and Random Forest) in predicting stock returns for Indian banks. The models utilize technical indicators derived from price, volume, and turnover data. Performance is evaluated using MAE, MSE, MAPE, and RMSE. XGBoost demonstrated the best performance, with MAE and RMSE around 3-5%. The findings suggest these models can aid traders and investors in predicting stock market trends, complementing traditional macroeconomic analysis.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:11:54.643972
b29c792181020dbb,Can climate change attention predict energy stock returns?,"We propose a climate change attention (CCA) index based on Google search volume index (GSVI) from 2004 to 2021 and show that it is an economically and statistically significant negative predictor for next month’s energy stock returns. The index is extracted using principal component analysis (PCA), but the results are similar by using the equal-weighted average method. Compared with 14 traditional macroeconomic predictors, CCA performs the best and provides complementary information when added into bivariate and multivariate macro predictive models. When further considering the effect of CCA’s forecasting power over different periods, strong evidence is shown that this outperformance is especially prominent in economic depressions and down market conditions. From the asset allocation perspective, CCA can provide a mean-variance investor with significant economic gains under alternative risk aversions. Our empirical results prove that investors’ attention to climate change contains predictive information for excess returns of global traditional energy stock index.",,2023,10.1007/s11356-023-28731-2,,proquest,"This study develops a climate change attention (CCA) index using Google search volume data from 2004-2021. The CCA index is found to be a significant negative predictor of future energy stock returns, outperforming traditional macroeconomic predictors, especially during economic downturns. The findings suggest that investor attention to climate change contains valuable predictive information for energy stock returns and can aid in asset allocation decisions.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:12:13.854885
3e458c1ef4064bc0,Can investor attention predict oil prices?,"This paper sets out to investigate the predictive power of investor attention onto oil prices. We firstly construct investor attention index by using the Google search volume index (SVI) based on a broad set of words related to oil-related variables and terms that are directly linked to real economy to measure investor attention. Then the empirical work is performed via a novel hybrid approach and WN model (Westerlund and Narayan, 2012, 2014) that account for characteristics of persistency, endogeneity, and heteroskedasticity. The empirical results show that investor attention does exhibit statistically and economically significant in-sample and out-of-sample forecasting power to directly forecast oil prices for both daily data and weekly data. In addition, the results exhibit the term structure character, which are helpful for understanding the financial phenomena that irrational attentions have more effect in short-term decision-making. © 2017 Elsevier B.V., All rights reserved.","Han, L.; Lv, Q.; Yin, L.",2017,10.1016/j.eneco.2017.04.018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019673191&doi=10.1016%2Fj.eneco.2017.04.018&partnerID=40&md5=ca9f86fe67dec0c5b3eeae9ad04a2a64,scopus,"This paper investigates the predictive power of investor attention on oil prices using a Google search volume index to construct an investor attention index. It employs a hybrid approach and the WN model to account for persistency, endogeneity, and heteroskedasticity, finding statistically and economically significant in-sample and out-of-sample forecasting power for oil prices. The results also suggest that irrational attention has a greater effect on short-term decision-making.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:12:19.087842
81bda8db83234614,Can investors attention on oil markets predict stock returns?,"This paper sets out to explore the predictability of the U.S. equity risk premium directly based on investor attention to oil. We find that the predictive power of oil attention exhibits statistical and economic significance within different models in both in-sample and out-of-sample tests. Meanwhile, oil attention reveals considerable and robust economic value for asset allocation in the sense of positive utility gains. Furthermore, supportive evidence that oil attention is closely linked to stock market volatility endues it with a macroeconomic meaning, serving as an explanation for its predictive power. Overall, investor attention to oil does have a direct predictive power to forecast the U.S. stock excess returns. © 2019 Elsevier B.V., All rights reserved.","Yin, L.; Feng, J.",2019,10.1016/j.najef.2018.08.017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052227023&doi=10.1016%2Fj.najef.2018.08.017&partnerID=40&md5=571887a6f6c671ca6e8211b013cc00c9,scopus,"This paper investigates whether investor attention to oil markets can predict U.S. stock returns. The findings indicate that oil attention has statistically and economically significant predictive power for the U.S. equity risk premium, demonstrated through various models and tests. The study also suggests that oil attention is linked to stock market volatility, providing a macroeconomic explanation for its predictive ability and offering economic value for asset allocation.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:12:22.837837
9d7c4c949713cefa,Can the Sharia-based Islamic Stock Market returns be forecasted using large number of predictors and models?,"This study employs 14 global economic and financial variables to predict the return of the Islamic stock market as identified by the Dow Jones Islamic Stock Market (DJIM). It implements alternative forecasting methods and allows for nonlinearity in the multivariate predictive regressions by estimating time-varying parameter models. All the methods fail to forecast the returns of the Sharia-based DJIM index over the out- of-sample period. The forecasts are weak at best, with only four predictors, the 3-month Treasury bill rate, inflation, oil price and return on the SandP500 Index, outperforming the benchmark autoregressive model of order one. The study suggests that the DJIM return is best predicted by an autocorrelation(1) model, and that future research should aim at analysing whether the performance of the linear autoregressive model can be improved by using nonlinear methods. Reprinted by permission of Routledge, Taylor and Francis Ltd.",,2014,10.1080/09603107.2014.924296,,proquest,"This study investigates the predictability of the Sharia-based Dow Jones Islamic Stock Market (DJIM) returns using 14 global economic and financial variables and various forecasting methods, including time-varying parameter models to account for nonlinearity. The findings indicate that most methods fail to forecast DJIM returns out-of-sample, with only a few predictors showing marginal improvement over a simple autoregressive model. The study concludes that an autocorrelation(1) model best predicts DJIM returns and suggests exploring nonlinear methods for future research.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:12:46.013180
46aaeaa8f110a015,Capturing the Regime-Switching and Memory Properties of Interest Rates,"We propose a mean-reverting interest rate model whose mean-reverting level, speed of mean-reversion and volatility are all modulated by a weak Markov chain (WMC). This model features a simple way to capture the regime-switching evolution of the parameters as well as the memory property of the data. Concentrating on the second-order WMC framework, we derive the filters of the WMC and other auxiliary processes through a change of reference probability measure. Optimal estimates of model parameters are provided by employing the EM algorithm. The h-step ahead forecasts under our proposed set-up are examined and compared with those under the usual Markovian regime-switching framework. We obtain better goodness-of-fit performance based on our numerical results generated from the implementation of WMC-based filters to a 10-year dataset of weekly short-term-maturity Canadian yield rates. Some statistical inference issues of the proposed modelling approach are also discussed.","Xi, Xiaojing; Mamon, Rogemar",2014,10.1007/s10614-013-9396-5,,wos,"This paper introduces a new interest rate model that uses a weak Markov chain (WMC) to capture regime-switching and memory properties. The model allows for changes in mean-reversion level, speed, and volatility based on the WMC. The authors derive filters for the WMC and other processes, use the EM algorithm for parameter estimation, and provide forecasts. They demonstrate better performance compared to standard Markovian regime-switching models using Canadian yield rate data and discuss statistical inference.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T11:13:05.469585
220322bfc22a7fa8,Cash Flow Forecasting in SAP ERP Enhanced by UiPath Automation: A Predictive Analytics Approach,"Maintaining liquidity, mitigating financial risks, and making strategic business decisions in today’s enterprises require accurate cash flow forecasting. Unfortunately, the native forecasting features of the SAP ERP are often constrained by outdated input streams, static data assumptions, and rigid model structures, severely impeding responsiveness and accuracy. This study proposes and evaluates the results-focused integration of UiPath robotic process automation (RPA) with predictive analytics to improve short and medium-term cash flow forecasting in SAP environments. We automated real-time data extraction from SAP FI, FI-CA, and bank interface modules, then employed machine learning and deep learning models (regression trees, LSTM networks, and ensemble methods) to demonstrate substantial gains in forecasting accuracy, cycle time, and exception handling. The framework was tested on large data sets from multi-currency, multi-business unit enterprises, achieving forecast accuracy improvement estimates of 15% to 28% compared to SAP’s baseline predictions. Aside from significantly reducing manual effort associated with forecast preparation, automation also expedited scenario-based liquidity analysis while enhancing governance through exception-based audit logging. These results provide a proven scaling architecture for intelligent real-time cash forecasting that is reliable and compliant, placing RPA and AI at the core of cash management operations of the future, and integrating deeply within ERP systems. © 2025 Elsevier B.V., All rights reserved.","Jamithireddy, N.S.",2025,10.51983/ijiss-2025.ijiss.15.2.45,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010042509&doi=10.51983%2Fijiss-2025.IJISS.15.2.45&partnerID=40&md5=b869ac625a26cb67b95a63d8895c5ea9,scopus,"This study integrates UiPath RPA with predictive analytics to enhance cash flow forecasting in SAP ERP systems. It automates data extraction from SAP modules and uses machine learning (regression trees, LSTM, ensemble methods) to improve accuracy, reduce cycle time, and enhance exception handling. The approach demonstrated significant forecast accuracy improvements (15-28%) compared to SAP's baseline, offering a scalable architecture for intelligent, real-time cash forecasting.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:13:13.249300
218b2b4f103b094a,Challenges in macro-finance modeling,"This article discusses various challenges in the specification and implementation of ""macrofinance"" models in which macroeconomic variables and term structure variables are modeled together in a no-arbitrage framework. The author classifies macro-finance models into pure latent factor models (""internal basis models"") and models that have observed macroeconomic variables as state variables (""external basis models"") and examines the underlying assumptions behind these models. Particular attention is paid to the issue of unspanned short-run fluctuations in macroeconomic variables and their potentially adverse effect on the specification of external basis models. The author also discusses the challenge of addressing features such as structural breaks and timevarying inflation uncertainty. Empirical difficulties in the estimation and evaluation of macrofinance models are also discussed in detail. © 2009, The Federal Reserve Bank of St. Louis. © 2020 Elsevier B.V., All rights reserved.","Kim, D.H.",2009,10.20955/r.91.519-544,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350561910&doi=10.20955%2Fr.91.519-544&partnerID=40&md5=c0fb7fe87ee07e159e0eeb472a1beb2c,scopus,"This article reviews challenges in macro-finance modeling, distinguishing between internal and external basis models. It highlights issues like unspanned short-run fluctuations, structural breaks, time-varying inflation uncertainty, and empirical estimation difficulties.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:13:27.938463
93d50033bed897f3,ChatGPT and Commodity Return,"This paper investigates the ability of a ChatGPT‐based indicator to forecast excess returns of the commodity futures index. Using ChatGPT to extract information from over 2.5 million articles from nine international newspapers, we demonstrate that our constructed commodity news ratio index significantly predicts future commodity returns, both in‐sample and out‐of‐sample. Furthermore, it outperforms traditional textual analysis methods, including Bidirectional Encoder Representations from Transformers (BERT) and Bag‐of‐Words (BoW), while indicating economic significance within an asset allocation framework. The results highlight the critical role of ChatGPT in forecasting commodity market dynamics and provide valuable insights for both financial market participants and researchers.",,2025,10.1002/fut.22568,,proquest,"This study explores the predictive power of a ChatGPT-derived index for commodity futures returns. By analyzing over 2.5 million news articles, the research shows that the ""commodity news ratio index"" significantly forecasts future commodity returns, outperforming traditional methods like BERT and BoW, and demonstrating economic relevance in asset allocation.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:13:36.252444
d38268c443f9bed1,Climate-sensitive hydrological drought insurance for irrigated agriculture under deep uncertainty. Insightful results from the Cega River Basin in Spain,"This paper assesses the feasibility and robustness of an index-based insurance scheme against hydrological droughts under climate change. To this end, we develop a grand ensemble that samples both modeling and scenario uncertainty in the estimation of the insurance risk premium, so to reveal potential unfavorable surprises and minimize regret in the design of the proposed insurance scheme. The grand ensemble combines four microeconomic models and seven GAMLSS models, which are run for three alternative climate change scenarios: stationary climate/no climate change, RCP 2.6, and RCP 8.5. Methods are illustrated with an application to the Cega River Sub-basin (CRS) in central Spain. Results indicate that for a conventional deductible of 30%, the proposed index-based insurance scheme would be actuarially feasible and affordable under all models for the stationary climate scenario (i.e., robust). For climate change scenarios RCP 2.6 and 8.5 and a 30% deductible, the suggested index-based insurance would be actuarially feasible under most models, albeit some outliers point towards potential unfavorable surprises. Lower deductibles decrease feasibility, particularly for deductibles <10%.",,2022,10.1016/j.agwat.2022.107938,,proquest,"This paper evaluates an index-based insurance scheme for irrigated agriculture against hydrological droughts, considering climate change uncertainty. Using a comprehensive ensemble of microeconomic and GAMLSS models under various climate scenarios (stationary, RCP 2.6, RCP 8.5), the study finds the insurance to be feasible and affordable for a 30% deductible under a stationary climate. For climate change scenarios, it remains feasible for most models, though some outliers suggest potential risks, especially with deductibles below 10%.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:13:44.445597
c5534bf69e592d8c,Combined DEMATEL technique with a novel MCDM model for exploring portfolio selection based on CAPM,"This research proposes a novel MCDM model, including DEMATEL, ANP, and VIKOR for exploring portfolio selection based on CAPM. We probe into the influential factors and relative weights of risk-free rate, expected market return, and beta of the security. The purpose of this research is to establish an investment decision model and provides investors with a reference of portfolio selection most suitable for investing effects to achieve the greatest returns. Taking full consideration of the interrelation effects among criteria/variables of the decision model, this paper examined leading semiconductor companies spanning the hottest sectors of integrated circuit (IC) design, wafer foundry, and IC packaging by experts. Empirical findings revealed that risk-free rate was affected by budget deficit, discount rate, and exchange rate; expected market return was affected by country risk, industrial structure, and macroeconomic factors; and beta of the security was affected by firm-specific risk and financial risk. Also, the factors of the CAPM possessed a self-effect relationship according to the DEMATEL technique. In the eight evaluation criteria, macroeconomic criterion was the most important factor affecting investment decisions, followed by exchange rate and firm-specific risk. In portfolio selection, leading companies in the wafer foundry industry outperformed those in IC design and IC packaging, becoming the optimal portfolio of investors during the time that this study was conducted. (C) 2010 Elsevier Ltd. All rights reserved.","Ho, Wen-Rong Jerry; Tsai, Chih-Lung; Tzeng, Gwo-Hshiung; Fang, Sheng-Kai",2011,10.1016/j.eswa.2010.05.058,,wos,"This research introduces a new MCDM model combining DEMATEL, ANP, and VIKOR for portfolio selection based on CAPM. It analyzes the influence of risk-free rate, expected market return, and security beta, considering interrelations among criteria. The study uses expert evaluations of semiconductor companies and finds that macroeconomic factors, exchange rates, and firm-specific risk are key. The wafer foundry industry emerged as the optimal portfolio choice.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:13:58.054341
8a2720754be4ab08,Comparative analysis of responses of risky and safe haven assets to stock market risk before and after the yield curve inversions in the US,"This study examines the response of safe -haven assets (gold, US dollar) and Bitcoin to market risk before and after yield curve inversions in the U.S. market. Using the VIX volatility index and static and dynamic cross-quantilogram approach, the analysis reveals that gold and the U.S. dollar act as safe havens, showing positive responses to increased VIX values, while Bitcoin behaves as a risky asset, negatively responding to higher VIX values during market turbulence. Changes in the VIX index have an immediate impact on asset price returns, but the effect diminishes over time, suggesting the need for timely updates to investment strategies. Yield curve inversions have altered the VIX-US dollar relationship: pre -inversion changes were influential in calmer markets, but post -inversion, they played a bigger role during turbulent phases, suggesting potential changes in investor behavior and market dynamics. The findings offer practical insights for investors seeking stability and protection during uncertain market conditions.","Sokhanvar, Amin; Hammoudeh, Shawkat",2024,10.1016/j.iref.2024.103376,,wos,"This study analyzes how gold, the US dollar, and Bitcoin respond to stock market risk (measured by VIX) before and after US yield curve inversions. Gold and the US dollar act as safe havens, while Bitcoin acts as a risky asset. The yield curve inversion appears to have altered the relationship between VIX and the US dollar, particularly during turbulent market conditions.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:14:38.828089
d7114845f2509422,Comparisons of mean-variance analysis and entropy-based approaches to portfolio selection under asymmetric returns in bear and bull markets,"This paper considers dynamic portfolio selection models where uncertainty in the financial market is characterized by business cycles. We consider that the financial market is defined by factors and present a regime switching autoregressive model for macro-economic factors to reflect financial cycles. It is assumed that the regime dynamics are Markovian and the parameters in the autoregressive model depend on regime dynamics. We then define a factor model for asset returns, with returns depending on regimes through the factors. The joint distribution of regimes and asset returns is the input to optimal portfolio selection models. Contrasting approaches to risk measurement of returns on investment are variance and exponential Rényi entropy (Rényi, 1960). We compare portfolio models with minimum variance and minimum entropy objectives. In the empirical analysis, we use the select sector ETFs to test the asset pricing model and examine the portfolio performance. Weekly financial data from 04-March-2016 to 26-June-2020 is employed for the estimation of the hidden Markov model including the asset return parameters, while the out-of-sample period from 26-June-2020 and 14-July-2023 is used for portfolio performance testing. It is found that, under both the empirical Sharpe and excess return to entropy ratios, the dynamic portfolio strategy with the entropy objective is an improvement on mean-variance models. © 2025 Elsevier B.V., All rights reserved.","MacLean, L.; Zhao, Y.; Miao, H.",2025,10.1007/s10479-025-06746-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013467192&doi=10.1007%2Fs10479-025-06746-x&partnerID=40&md5=9f90626e433c0bcb688c46baef7ae42a,scopus,"This paper compares mean-variance analysis and entropy-based approaches for dynamic portfolio selection, considering market business cycles modeled by a regime-switching autoregressive model. It uses sector ETFs and weekly financial data from 2016-2023 to test the models, finding that the entropy objective improves portfolio performance over mean-variance models.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:14:45.937669
d03ccef6cf62c8a5,Comprehensive evidence implies a higher social cost of CO2,"The social cost of carbon dioxide (SC-CO2) measures the monetized value of the damages to society caused by an incremental metric tonne of CO2 emissions and is a key metric informing climate policy. Used by governments and other decision-makers in benefit-cost analysis for over a decade, SC-CO2 estimates draw on climate science, economics, demography and other disciplines. However, a 2017 report by the US National Academies of Sciences, Engineering, and Medicine1 (NASEM) highlighted that current SC-CO2 estimates no longer reflect the latest research. The report provided a series ofrecommendations for improving the scientific basis, transparency and uncertainty characterization of SC-CO2 estimates. Here we show that improved probabilistic socioeconomic projections, climate models, damage functions, and discounting methods that collectively reflect theoretically consistent valuation of risk, substantially increase estimates of the SC-CO2. Our preferred mean SC-CO2 estimate is $185 per tonne ofCO2 ($44-$413 per tCO2: 5%-95% range, 2020 US dollars) at a near-term risk-free discount rate of 2%, a value 3.6 times higher than the US government's current value of $51per tCO2. Our estimates incorporate updated scientific understanding throughout all components of SC-CO2 estimation in the new open-source Greenhouse Gas Impact Value Estimator (GIVE) model, in a manner fully responsive to the near-term NASEM recommendations. Our higher SC-CO2 values, compared with estimates currently used in policy evaluation, substantially increase the estimated benefits of greenhouse gas mitigation and thereby increase the expected net benefits of more stringent climate policies.",,2022,10.1038/s41586-022-05224-9,,proquest,"This study presents a significantly higher estimate for the social cost of carbon dioxide (SC-CO2), proposing a mean value of $185 per tonne of CO2. This updated estimate, derived from improved socioeconomic projections, climate models, damage functions, and discounting methods within the new GIVE model, is substantially higher than the current US government estimate. The authors argue that this higher SC-CO2 value strengthens the case for more stringent climate policies by increasing the estimated benefits of greenhouse gas mitigation.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:14:59.422471
465418f6b1f7d557,Conditional risk-return relationship in a time-varying beta model,"We investigate the asymmetric risk-return relationship in a time-varying beta CAPM. A state space model is established and estimated by the Adaptive Least Squares with Kalman foundations proposed by McCulloch. Using SP 500 daily data from 1987:11-2003:12, we find a positive risk-return relationship in the up market (positive market excess returns) and a negative relationship in the down market (negative market excess returns). This supports the argument of Pettengill et al., who use a constant beta model. However, our model outperforms theirs by eliminating the unexplained returns and improving the accuracy of the estimated risk price.","Huang, Peng; Hueng, C. James",2008,10.1080/14697680701191361,,wos,"This study examines the asymmetric risk-return relationship within a time-varying beta Capital Asset Pricing Model (CAPM). Employing a state space model estimated via Adaptive Least Squares with Kalman filters, and using S&P 500 daily data from 1987 to 2003, the research reveals a positive risk-return link during market upturns and a negative link during downturns. This aligns with prior findings using constant beta models but offers improved accuracy and explanation of returns.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:15:07.600084
466b8b36926ebb9e,Connectedness network and dependence structure mechanism in green investments,"We present an empirical study of renewable energy stock returns and their relation to four major investment asset classes—stocks, currency, US Treasury bonds, and oil—and several sources of uncertainty. Applying nonlinear causality and connectedness network analysis on data covering the period 2004–2016, we investigate the directionality and connectedness among different asset classes, as well as between uncertainties. First, from the results of the estimation of directionality and network spillovers, it can be concluded that the European stock market has a strong market dependence on renewable energy stock prices. Second, uncertainties have an economically significant impact on both return and volatility spillover in energy investments. Third, most of the uncertainties are net transmitters of volatility connectedness during the global financial crisis (GFC) and European sovereign debt crisis (ESDC). © 2018 Elsevier B.V., All rights reserved.","Lundgren, A.I.; Milicevic, A.; Uddin, G.S.; Kang, S.H.",2018,10.1016/j.eneco.2018.04.015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045660578&doi=10.1016%2Fj.eneco.2018.04.015&partnerID=40&md5=b1c26dcdb123c802a3c779b455053326,scopus,"This empirical study analyzes the connectedness network and dependence structure of renewable energy stock returns in relation to stocks, currency, US Treasury bonds, and oil, as well as various uncertainty sources. Using nonlinear causality and connectedness network analysis from 2004-2016, the study found that the European stock market is highly dependent on renewable energy stock prices. Uncertainties significantly impact return and volatility spillovers in energy investments, with most uncertainties acting as net transmitters of volatility during the GFC and ESDC.",True,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:15:25.311748
f4767033c2a4eee5,Constrained Factor Models for High-Dimensional Matrix-Variate Time Series,"High-dimensional matrix-variate time series data are becoming widely available in many scientific fields, such as economics, biology, and meteorology. To achieve significant dimension reduction while preserving the intrinsic matrix structure and temporal dynamics in such data, Wang, Liu, and Chen proposed a matrix factor model, that is, shown to be able to provide effective analysis. In this article, we establish a general framework for incorporating domain and prior knowledge in the matrix factor model through linear constraints. The proposed framework is shown to be useful in achieving parsimonious parameterization, facilitating interpretation of the latent matrix factor, and identifying specific factors of interest. Fully utilizing the prior-knowledge-induced constraints results in more efficient and accurate modeling, inference, dimension reduction as well as a clear and better interpretation of the results. Constrained, multi-term, and partially constrained factor models for matrix-variate time series are developed, with efficient estimation procedures and their asymptotic properties. We show that the convergence rates of the constrained factor loading matrices are much faster than those of the conventional matrix factor analysis under many situations. Simulation studies are carried out to demonstrate finite-sample performance of the proposed method and its associated asymptotic properties. We illustrate the proposed model with three applications, where the constrained matrix-factor models outperform their unconstrained counterparts in the power of variance explanation under the out-of-sample 10-fold cross-validation setting. for this article are available online.","Chen, Elynn Y.; Tsay, Ruey S.; Chen, Rong",2020,10.1080/01621459.2019.1584899,,wos,"This article proposes a constrained matrix factor model for high-dimensional matrix-variate time series data, incorporating domain knowledge through linear constraints to improve parameterization, factor interpretation, and overall modeling accuracy. The constrained models demonstrate faster convergence rates and better out-of-sample performance compared to unconstrained models in empirical applications.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:15:41.697106
52cf1b9349ff6fe8,Consumption and equilibrium asset pricing: An empirical assessment,"In the various attempts to solve the equity premium puzzle, the characterization of the utility function has received a lot of attention, along with the postulated nature of the economy. In this paper, we specify and estimate by maximum likelihood over the period 1871 - 1985 a heteroskedastic joint consumption and dividend Markov endowment process in an exchange asset pricing model. To assess the model, we try to replicate both the first and second unconditional moments of the return series, the negative serial correlation present in real and excess returns and the forecasting power of the dividend-price ratio for multiperiod returns. For the real returns, the model captures to some extent the main features of the data for values of the coefficient of risk aversion below 10. The main failure of the model comes from the excess returns. We also assess the model by inferring the consumption growth that rationalizes the observed stock and safe asset returns, but it is too variable to be plausible. © 2022 Elsevier B.V., All rights reserved.","Bonomo, M.; Garcia, R.",1996,10.1016/0927-5398(96)00002-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030243487&doi=10.1016%2F0927-5398%2896%2900002-3&partnerID=40&md5=e7b71931b2764b48a8f6f6888b7ef14b,scopus,"This paper empirically assesses an asset pricing model by estimating a heteroskedastic joint consumption and dividend Markov endowment process. It attempts to replicate key features of return series, including moments, serial correlation, and dividend-price ratio forecasting power. The model shows some success for real returns but struggles with excess returns and implies implausibly variable consumption growth.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:15:54.969483
438204aa4a86c9c1,Corporate Probability of Default: A Single-Index Hazard Model Approach,"Corporate probability of default (PD) prediction is vitally important for risk management and asset pricing. In search of accurate PD prediction, we propose a flexible yet easy-to-interpret default-prediction single-index hazard model (DSI). By applying it to a comprehensive U.S. corporate bankruptcy database we constructed, we discover an interesting V-shaped relationship, indicating a violation of the common linear hazard specification. Most importantly, the single-index hazard model passes the Hosmer-Lemeshow goodness-of-fit calibration test while neither does a state-of-the-art linear hazard model in finance nor a parametric class of Box-Cox transformation survival models. In an economic value analysis, we find that this may translate to as much as three times of profit compared to the linear hazard model. In model estimation, we adopt a penalized-spline approximation for the unknown function and propose an efficient algorithm. With a diverging number of spline knots, we establish consistency and asymptotic theories for the penalized-spline likelihood estimators. Furthermore, we reexamine the distress risk anomaly, that is, higher financially distressed stocks deliver anomalously lower excess returns. Based on the PDs from the proposed single-index hazard model, we find that the distress risk anomaly has weakened or even disappeared during the extended period.","Li, Shaobo; Tian, Shaonan; Yu, Yan; Zhu, Xiaorui; Lian, Heng",2023,10.1080/07350015.2022.2120484,,wos,"This paper introduces a single-index hazard model (DSI) for predicting corporate probability of default (PD), which is crucial for risk management and asset pricing. The model demonstrates a V-shaped relationship, outperforming traditional linear hazard models and Box-Cox transformation models in calibration tests. The study also finds that the DSI model weakens or eliminates the distress risk anomaly. The methodology involves penalized-spline approximation and establishes theoretical consistency for estimators.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:16:30.162613
6ff1318102cc0b35,Corporate relation extraction for the construction of knowledge-bases against tax fraud,"Tax fraud is a criminal activity that entails significant losses for governments. Due to its clandestine nature, it is difficult to reliably estimate the amount of taxes evaded. To fight tax fraud, this investigation details the construction and evaluation of a corporate relation extraction system designed to access an unstructured knowledge-base and extract corporate relations for further validation. The system was developed in response to a need raised by the Treasury and Finance Department of the Provincial Council of Gipuzkoa (Spain). It follows a waterfall architecture that integrates Natural Language Processing (NLP) and Computer Vision (CV) components, including web scraping, optical character recognition, syntactic parsing, and information extraction. The proposed system produces a relational knowledge-base with structured data representing 23 types of corporate operations published in the Official Gazette of the Commercial Registry (e.g., incorporation of companies, terminations, capital increases and reductions, mergers and takeovers, etc.), allowing for comparison with the fiscal information available in the tax agency. Facilitating such comparison across distinct sources is key to identifying discrepancies that might be indicators of tax fraud. © 2025 Elsevier B.V., All rights reserved.","Lopez-Gazpio, I.; Baselga-Pascual, L.; Garmendia-Lazcano, A.",2025,10.1016/j.knosys.2025.113026,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215966196&doi=10.1016%2Fj.knosys.2025.113026&partnerID=40&md5=7d2cbcf4fc0162b01f57c357e1ef6ac4,scopus,"This paper describes the development and evaluation of a corporate relation extraction system to build a knowledge base for detecting tax fraud. The system integrates NLP and CV techniques to process unstructured data from official gazettes, extracting 23 types of corporate operations. This structured data can then be compared with fiscal information to identify potential tax evasion.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:16:36.277234
af087fe826b75cd4,Credit Pit Detection in Subordinate Securities: A French Perspective,"The purpose of this research is to prepare a predictive model for identifying credit crisis using an artificial neural network. The paper also aims to find out the driver and driven relationship between various financial instruments like CDS, FRA, IRS, and the Volatility index (VCAC) and government securities for France. The model, thus, is directed towards finding a threshold for credit pit events and linking various events corresponding to that dates where the threshold is breached to validate the accuracy and usefulness of the model. From the research, it is found that for France, the CDS-FRA-VCAC model derives the threshold for VCAC to indicate the probability of credit crisis or financial market crash. It is also found that sovereign bonds have a huge impact on France economy including various derivatives. This is probably why the Eurozone debt crisis impacted France much more than the 2008 financial crash.",,2019,10.12725/ujbm.48.6,,proquest,"This study develops a predictive model using artificial neural networks to identify credit crises in French subordinate securities. It investigates the relationships between financial instruments like CDS, FRA, IRS, and the Volatility Index (VCAC) with government securities. The model aims to establish a threshold for credit pit events and links these events to breaches of the threshold. Findings indicate that the CDS-FRA-VCAC model can predict credit crises or market crashes in France, with sovereign bonds significantly impacting the French economy, explaining the greater impact of the Eurozone debt crisis compared to the 2008 financial crash.",True,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:32:31.175947
893197e230ff363c,Cross-market volatility forecasting with attention-based spatial-temporal graph convolutional networks,"We propose a cross-market volatility forecasting framework by applying attention-based spatial-temporal graph convolutional network model (ASTGCN) to forecast future volatility of stock indices in 18 financial markets. In our work, we construct cross-market volatility networks to integrate interrelations among financial markets and the corresponding features of each market. ASTGCN combines the spatial-temporal attention mechanisms with the spatial-temporal convolutions to simultaneously capture the dynamic spatial-temporal characteristics of global volatility data. Compared with competitive models, ASTGCN exhibits superiority in multivariate predictive accuracies under multiple forecasting horizons. Our proposed framework demonstrates outstanding stability through several robustness checks. We also inspect the training process of ASTGCN by extracting spatial attention matrices and find that interrelations among global financial markets perform differently in tranquil and turmoil periods. Our study levitates empirical findings in financial networks to practical application with a novel forecasting method in the deep learning community.","Gong, Jue; Wang, Gang-Jin; Zhou, Yang; Xie, Chi",2025,10.1016/j.jempfin.2025.101639,,wos,This paper introduces an attention-based spatial-temporal graph convolutional network (ASTGCN) for forecasting stock index volatility across 18 financial markets. The model integrates inter-market relationships and individual market features to capture dynamic spatial-temporal characteristics of global volatility data. ASTGCN outperforms competitive models in predictive accuracy and demonstrates stability. Analysis of attention matrices reveals differing inter-market relationships during tranquil and turmoil periods.,True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:32:44.732673
14c53eff72879cd2,Cross-predictability of industry return in trade network: Using LASSO,"This study tests the predictability power of returns across economically related industries, using monthly data over 1990-2010 and a machine learning model, the Least Absolute Shrinkage and Selection Operator (LASSO). This article presents evidence that LASSO identifies significant predictors of the returns of an individual industry among its interdependent industries. The results indicate cross-predictability of industry return by showing a significant relation between an industry's returns and the lagged returns of its LASSO-selected trade partners. The study finds that industries that are more central in the input-output network have a greater effect in predicting related industry return. It also computes out of sample (2011-2016) the mean return forecasts of the portfolio of industries based on the predictability effect of the lagged returns of trade partners. The self-financing trading strategy of buying (selling) a high (low) portfolio based on related industry lagged returns yields about 10.44 percent annual excess return (0.83 percent monthly), with an annual Sharpe ratio of about 0.86. The study shows that after controlling for the Fama-French (1993) three factors, the monthly returns from the self-financing trading strategies generate a significant alpha of 0.7 percent monthly over the period 2011-2016. © 2020 Elsevier B.V., All rights reserved.","Ashraf, R.",2019,10.3905/joi.2019.1.095,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090195969&doi=10.3905%2Fjoi.2019.1.095&partnerID=40&md5=c7c7979fbc72b7bbed6ea690a19b5740,scopus,"This study investigates the cross-predictability of industry returns within a trade network using the LASSO machine learning model. It demonstrates that lagged returns of trade partners can predict an industry's returns, with more central industries having a greater predictive impact. A trading strategy based on these predictions yields significant excess returns and alpha, even after controlling for Fama-French factors.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:33:09.431410
7ca6c07a43b0a174,Debt Intolerance: Threshold Level and Composition*,"Fiscal vulnerabilities depend on both the level and composition of government debt. This study examines the role of debt thresholds and debt composition in driving the nonlinear behaviour of long-term interest rates through a novel approach, a panel smooth transition regression with a general logistic model. The main findings are threefold. First, the impact of the expected public debt level on interest rates rises exponentially when the share of foreign private holdings exceeds approximately 20% of government debt denominated in local currency. Second, if the public debt level exceeds a certain level, an increase in foreign private holding of government debt could raise interest rates, offsetting the downward pressure from higher market liquidity. Third, out-of-sample forecasts of this novel non-linear model are more accurate than those of previous methods.","Matsuoka, Hideaki",2022,10.1111/obes.12470,,wos,"This study investigates how the level and composition of government debt influence long-term interest rates using a novel panel smooth transition regression model. It finds that foreign holdings of local currency debt above 20% amplify the impact of debt levels on interest rates, and that increased foreign holdings can raise interest rates beyond a certain debt threshold. The model also demonstrates superior out-of-sample forecasting accuracy compared to previous methods.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T11:33:45.560840
d9568c102a3985ba,Debt-stabilizing properties of GDP-linked securities: A macro-finance perspective,"We study the debt -stabilizing properties of indexing debt to GDP using a consumption -based macro -finance model. To this end, we derive quasi -analytical pricing formulas for any type of bond/equity by exploiting the discretization of the state -space, making large-scale simulations tractable. We find that GDP -linked security prices would embed time -varying risk premiums of about 40 basis points. For a fixed budget surplus, issuing GDP -linked securities does not imply more beneficial debt -to -GDP ratios in the long -run, while the debt -stabilizing budget surplus is more predictable at the expense of being higher. Our findings call into question the view that such securities tame debt.","Mouabbi, Sarah; Renne, Jean -Paul; Sahuc, Jean -Guillaume",2024,10.1016/j.jbankfin.2024.107131,,wos,"This paper investigates the debt-stabilizing effects of GDP-linked securities using a macro-finance model. It derives pricing formulas and conducts simulations, finding that while these securities embed risk premiums, they do not necessarily lead to more beneficial debt-to-GDP ratios or tame debt effectively. The budget surplus required for debt stabilization becomes higher but more predictable.",False,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:34:21.829469
11be73fadb2757bc,Decision analysis under ambiguity,"Abstract In selecting the preferred course of action, decision makers are often uncertain about one or more probabilities of interest. The experimental literature has ascertained that this uncertainty (ambiguity) might affect decision makers' preferences. Then, the decision maker might wish to incorporate ambiguity aversion in the analysis. We investigate the modeling ambiguity attitudes in the solution of decision analysis problems through functionals well-established in the decision theory literature. We obtain the multiple-event problems for subjective expected utility, smooth ambiguity and maximin decision makers. This allows us to establish the conditions under which these alternative decision makers face equivalent problems. Results for certainty equivalents and risk premia in the presence of both risk and ambiguity aversion are obtained. A recent generalization of the classical Arrow-Pratt quadratic approximation allows us to quantify the portions of a premium due to risk-PLXINSERT-, and to ambiguity-aversion. The numerical implementation of the objective functions is addressed, showing that all functionals can be estimated at no additional burden through Monte Carlo simulation. The well known Carter Racing case study is addressed quantitatively to demonstrate the findings. © 2020 Elsevier B.V., All rights reserved.","Borgonovo, E.; Marinacci, M.",2015,10.1016/j.ejor.2015.02.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926408325&doi=10.1016%2Fj.ejor.2015.02.001&partnerID=40&md5=1c2892d7e4971f8f088bace8177405c3,scopus,"This paper investigates how decision-makers incorporate ambiguity aversion into decision analysis problems using established decision theory functionals. It derives multiple-event problems for different decision-maker types (subjective expected utility, smooth ambiguity, maximin) and establishes conditions for problem equivalence. The study quantifies the portions of a premium attributable to risk and ambiguity aversion and demonstrates the application of these findings using a Monte Carlo simulation and the Carter Racing case study.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:34:25.143558
029287686acf7c81,Deep Learning for Bond Yield Forecasting: The LSTM-LagLasso,"We present long short-term memory (LSTM)-LagLasso, a novel explainable deep learning approach applied to bond yield forecasting. Our method involves feature selection from a large universe of potential features and forecasts bond yields using dynamic LSTM networks. It examines the internal gating signals of a trained LSTM and explains their dynamics through exogenous variables that may influence bond price formation. By considering these variables at various lags and using the Lasso technique for feature selection, we demonstrate how different hidden units within the LSTM dynamically adjust to make predictions across different temporal regimes and how their evolution is shaped by various external factors. In an empirical study on government bond yield forecasting, we demonstrate the statistical accuracy of LSTM-LagLasso compared to a multilayer perceptron (MLP) and highlight its explainability. © 2025 Elsevier B.V., All rights reserved.","Nunes, M.; Gerding, E.; McGroarty, F.; Niranjan, M.; Sermpinis, G.",2025,10.1002/ijfe.3116,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215673445&doi=10.1002%2Fijfe.3116&partnerID=40&md5=16a040114703ee0a5003b658cae9572e,scopus,"This paper introduces LSTM-LagLasso, a novel explainable deep learning method for bond yield forecasting. It uses feature selection from a broad set of potential features and dynamic LSTM networks to predict bond yields. The approach analyzes LSTM internal gating signals and explains their dynamics using exogenous variables that influence bond prices. The study empirically validates LSTM-LagLasso on government bond yield forecasting, showing its statistical accuracy and explainability compared to an MLP.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T11:34:28.998167
20d437cdd0b20220,Deep learning with long short-term memory networks for financial market predictions,"Long short-term memory (LSTM) networks are a state-of-the-art technique for sequence learning. They are less commonly applied to financial time series predictions, yet inherently suitable for this domain. We deploy LSTM networks for predicting out-of-sample directional movements for the constituent stocks of the S&P 500 from 1992 until 2015. With daily returns of 0.46 percent and a Sharpe ratio of 5.8 prior to transaction costs, we find LSTM networks to outperform memory-free classification methods, i.e., a random forest (RAF), a deep neural net (DNN), and a logistic regression classifier (LOG). The outperformance relative to the general market is very clear from 1992 to 2009, but as of 2010, excess returns seem to have been arbitraged away with LSTM profitability fluctuating around zero after transaction costs. We further unveil sources of profitability, thereby shedding light into the black box of artificial neural networks. Specifically, we find one common pattern among the stocks selected for trading – they exhibit high volatility and a short-term reversal return profile. Leveraging these findings, we are able to formalize a rules-based short-term reversal strategy that yields 0.23 percent prior to transaction costs. Further regression analysis unveils low exposure of the LSTM returns to common sources of systematic risk – also compared to the three benchmark models. © 2018 Elsevier B.V., All rights reserved.","Fischer, T.; Krauß, C.",2018,10.1016/j.ejor.2017.11.054,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039970639&doi=10.1016%2Fj.ejor.2017.11.054&partnerID=40&md5=b5354f21b79010e8b9e0f8e44362b4ae,scopus,"This study applies Long Short-Term Memory (LSTM) networks to predict directional movements in S&P 500 stocks from 1992-2015. LSTMs outperformed random forest, deep neural net, and logistic regression models, showing significant excess returns until 2009. Profitability decreased after 2010, potentially due to arbitrage. The study identified high volatility and short-term reversal as key factors for LSTM profitability and developed a rules-based strategy. LSTM returns showed low exposure to systematic risk.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:34:40.372904
b97c65a22bac2147,"Development of a Machine Learning Model Using Multiple, Heterogeneous Data Sources to Estimate Weekly US Suicide Fatalities","ImportanceSuicide is a leading cause of death in the US. However, official national statistics on suicide rates are delayed by 1 to 2 years, hampering evidence-based public health planning and decision-making.ObjectiveTo estimate weekly suicide fatalities in the US in near real time.Design, Setting, and ParticipantsThis cross-sectional national study used a machine learning pipeline to combine signals from several streams of real-time information to estimate weekly suicide fatalities in the US in near real time. This 2-phase approach first fits optimal machine learning models to each individual data stream and subsequently combines predictions made from each data stream via an artificial neural network. National-level US administrative data on suicide deaths, health services, and economic, meteorological, and online data were variously obtained from 2014 to 2017. Data were analyzed from January 1, 2014, to December 31, 2017.ExposuresLongitudinal data on suicide-related exposures were obtained from multiple, heterogeneous streams: emergency department visits for suicide ideation and attempts collected via the National Syndromic Surveillance Program (2015-2017); calls to the National Suicide Prevention Lifeline (2014-2017); calls to US poison control centers for intentional self-harm (2014-2017); consumer price index and seasonality-adjusted unemployment rate, hourly earnings, home price index, and 3-month and 10-year yield curves from the Federal Reserve Economic Data (2014-2017); weekly daylight hours (2014-2017); Google and YouTube search trends related to suicide (2014-2017); and public posts on suicide on Reddit (2 314 533 posts), Twitter (9 327 472 tweets; 2015-2017), and Tumblr (1 670 378 posts; 2014-2017).Main Outcomes and MeasuresWeekly estimates of suicide fatalities in the US were obtained through a machine learning pipeline that integrated the above data sources. Estimates were compared statistically with actual fatalities recorded by the National Vital Statistics System.ResultsCombining information from multiple data streams, the machine learning method yielded estimates of weekly suicide deaths with high correlation to actual counts and trends (Pearson correlation, 0.811;P < .001), while estimating annual suicide rates with low error (0.55%).Conclusions and RelevanceThe proposed ensemble machine learning framework reduces the error for annual suicide rate estimation to less than one-tenth of that of current forecasting approaches that use only historical information on suicide deaths. These findings establish a novel approach for tracking suicide fatalities in near real time and provide the potential for an effective public health response such as supporting budgetary decisions or deploying interventions.",,2020,10.1001/jamanetworkopen.2020.30932,,proquest,"This study developed a machine learning model using diverse data sources (health services, economic, meteorological, and online data) to estimate weekly US suicide fatalities in near real time. The model showed a high correlation with actual counts and trends, offering a potential for improved public health responses.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:35:08.446769
1aaadcf8c464fe28,Disagreement in expectations and the credibility of monetary authorities in the Brazilian inflation targeting regime,"Based on market expectations reported by the Central Bank of Brazil for the SELIC interest rate, the IPCA inflation, the exchange rate (BRL/USD) and the growth rate of industrial production for four different forecasting horizons, this work analyzes the term structures of disagreement in expectations regarding the future values of these variables. It also investigates the driving factors of disagreement, paying special attention to the influence of monetary authorities’ credibility. An extensive regression analysis shows that the levels of the term structures of disagreement are negatively related to the output gap (although this result is not very robust); and that the levels of the term structures of disagreement in expectations about the IPCA inflation rate and the SELIC interest rate have a strong negative relationship with central bankers’ credibility; this relationship is positive in the case of the growth rate of industrial production. © 2019 Elsevier B.V., All rights reserved.","Vereda, L.V.; Curi, A.",2016,10.1016/j.econ.2016.03.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031719048&doi=10.1016%2Fj.econ.2016.03.002&partnerID=40&md5=3986f94f0699536c3421af3af3134519,scopus,"This study examines disagreement in market expectations regarding SELIC interest rate, IPCA inflation, exchange rate, and industrial production growth in Brazil. It finds that disagreement is negatively related to the output gap and positively related to central bankers' credibility for inflation and interest rates, but negatively for industrial production growth.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:35:16.323381
e877d6e0c89d5e6e,Discrete-time affine term structure models with generalized market prices of risk,"This article develops a rich class of discrete-time, nonlinear dynamic term structure models (DTSMs). Under the risk-neutral measure, the distribution of the state vector Xt resides within a family of discrete-time affine processes that nests the exact discrete-time counterparts of the entire class of continuous-time models in Duffie and Kan (1996) and Dai and Singleton (2000). Under the historical distribution, our approach accommodates nonlinear (nonaffine) processes while leading to closed-form expressions for the conditional likelihood functions for zero-coupon bond yields. As motivation for our framework, we show that it encompasses many of the equilibrium models with habit-based preferences or recursive preferences and long-run risks. We illustrate our methods by constructing maximum likelihood estimates of a nonlinear discrete-time DTSM with habit-based preferences in which bond prices are known in closed form. We conclude that habit-based models, as typically parameterized in the literature, do not match key features of the conditional distribution of bond yields. © 2010 The Author. © 2010 Elsevier B.V., All rights reserved.","Le, A.; Singleton, K.J.; Dai, Q.",2010,10.1093/rfs/hhq007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951229447&doi=10.1093%2Frfs%2Fhhq007&partnerID=40&md5=357de60632b52de4ee7c87b041b608ad,scopus,"This article presents a class of discrete-time, nonlinear dynamic term structure models (DTSMs) that generalize continuous-time affine models. It allows for nonlinear processes under the historical distribution, providing closed-form expressions for bond yield likelihoods. The framework includes equilibrium models with habit-based or recursive preferences and long-run risks. The authors illustrate their methods by estimating a nonlinear DTSM with habit-based preferences, finding that such models, as typically parameterized, do not fully capture the conditional distribution of bond yields.",True,True,False,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T11:35:53.642096
d6ed6a2f0e967ff5,Discrete-time implementation of continuous-time filters with application to regime-switching dynamics estimation,"This paper details the implementation in discrete time of filters for a mean-reverting model formulated under a continuous-time framework, whereby a hidden Markov chain governs the model's parameters. Parameter estimates are determined via adaptive filters designed to extract hidden information from an observable time series. An application involving the dynamic behaviour of spot interest rates is considered. More specifically, we present an empirical study aimed at capturing accurately, on the basis of some benchmarks and statistical validation, the evolution of three country-specific rates in the European zone. Our analysis reveals some similar yield-rate and risk characteristics as well as independent market behaviours of the three EU sovereign states. (C) 2019 Elsevier Ltd. All rights reserved.","Grimm, Stefanie; Erlwein-Sayer, Christina; Mamon, Rogemar",2020,10.1016/j.nahs.2019.08.001,,wos,"This paper presents a discrete-time implementation of continuous-time filters for a mean-reverting model governed by a hidden Markov chain. It uses adaptive filters to estimate parameters from observable time series and applies this to model the dynamic behavior of spot interest rates in three European countries, revealing similarities and independent behaviors in their markets.",True,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:36:50.297591
f25a821e963e885a,Dissecting climate change risk and financial market instability: Implications for ecological risk management,"This research investigates the impact of climate challenges on financial markets by introducing an innovative approach to measure climate risk, specifically the aggregate climate change concern (ACCC) index. The study aims to assess and quantify the potential influence of climate change and risk-related factors on the performance and dynamics of financial markets. In this paper, concern is defined as the attention paid to the risk of climate change and the associated negative consequences. The findings demonstrate that the aggregate index exhibits robust predictability of market risk premiums, both within the sample and out-of-sample. By comparison, the index contains additional information beyond 14 economic predictors and 12 risk/uncertainty indexes in forecasting stock market return. In addition, the index proves valuable for mean-variance investors in asset allocation, leading to significant economic gains. The study identifies the index's ability to capture the reversal of temporary price crashes caused by overreactions to climate change risk. Furthermore, it exhibits stronger return forecasting capability for green stocks, non-state-owned enterprise (non-SOE) stocks, and stocks in regions with low air pollution. Particularly during periods of low air pollution and relaxed regulation, the index displays an enhanced ability to forecast returns. The study's findings provide valuable insights for policymakers and financial institutions as they address 21st-century environmental challenges. Moreover, these findings can inform the design of adaptive measures and interventions aimed at mitigating ecological risks and promoting sustainable economic growth.This research investigates the impact of climate challenges on financial markets by introducing an innovative approach to measure climate risk, specifically the aggregate climate change concern (ACCC) index. The study aims to assess and quantify the potential influence of climate change and risk-related factors on the performance and dynamics of financial markets. In this paper, concern is defined as the attention paid to the risk of climate change and the associated negative consequences. The findings demonstrate that the aggregate index exhibits robust predictability of market risk premiums, both within the sample and out-of-sample. By comparison, the index contains additional information beyond 14 economic predictors and 12 risk/uncertainty indexes in forecasting stock market return. In addition, the index proves valuable for mean-variance investors in asset allocation, leading to significant economic gains. The study identifies the index's ability to capture the reversal of temporary price crashes caused by overreactions to climate change risk. Furthermore, it exhibits stronger return forecasting capability for green stocks, non-state-owned enterprise (non-SOE) stocks, and stocks in regions with low air pollution. Particularly during periods of low air pollution and relaxed regulation, the index displays an enhanced ability to forecast returns. The study's findings provide valuable insights for policymakers and financial institutions as they address 21st-century environmental challenges. Moreover, these findings can inform the design of adaptive measures and interventions aimed at mitigating ecological risks and promoting sustainable economic growth.",,2025,10.1111/risa.14265,,proquest,"This study introduces the Aggregate Climate Change Concern (ACCC) index to measure climate risk and its impact on financial markets. The ACCC index demonstrates strong predictability of market risk premiums and stock market returns, outperforming traditional economic predictors. It also aids mean-variance investors in asset allocation and shows enhanced forecasting for green stocks, non-SOE stocks, and stocks in low-pollution regions, particularly during periods of relaxed regulation. The findings offer insights for policymakers and financial institutions in managing ecological risks and promoting sustainable growth.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:36:56.178051
4b00fc35dcf2e6b0,Diverging Roads: Theory-Based vs. Machine Learning-Implied Stock Risk Premia,"We compare the performance of theory-based and machine learning (ML) methods for quantifying equity risk premia and assess hybrid strategies that combine the two very different philosophies. The theory-based approach offers advantages at a one-month investment horizon, in particular, if daily frequency risk premium estimates (RPE) are needed. At the one-year horizon, ML has an edge, especially using theory-based RPE as additional feature variables. For a hybrid strategy called Theory with ML Assistance, we employ ML to account for the approximation errors of the theory-based approach. Employing random forests or an ensemble of ML models for theory support yields promising results. © 2025 Elsevier B.V., All rights reserved.","Grammig, J.; Hanenberg, C.; Schlag, C.; Sönksen, J.",2025,10.1093/jjfinec/nbaf005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003374308&doi=10.1093%2Fjjfinec%2Fnbaf005&partnerID=40&md5=a2cce87fcf6aee5edfa0171f297c0c34,scopus,"This study compares theory-based and machine learning (ML) methods for estimating equity risk premia. The theory-based approach is better for short-term (one-month) predictions, especially with daily data. ML performs better at a one-year horizon, particularly when theory-based estimates are used as features. A hybrid strategy, ""Theory with ML Assistance,"" uses ML to correct for approximation errors in the theory-based approach, showing promising results with random forests and ensemble models.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:36:59.887744
10aa5400853f1e0c,Do Foreign Yield Curves Predict US Recessions and GDP Growth?,Foreign term spreads constructed from bond yields of non-U.S. G-7 constituents predict future U.S. recessions and foreign term spreads are stronger predictors of U.S. recessions occurring within the next year than U.S. term spreads. U.S. and foreign term spreads are both informative of the U.S. economy but over different horizons and for different components of economic activity. Smaller U.S. term spreads lead to smaller foreign term spreads and U.S. Dollar appreciation. Smaller foreign term spreads do not lead to significant U.S. Dollar depreciation but do lead to persistent declines in U.S. exports and foreign direct investment (FDI) flows into the United States. These findings are consistent with the proposition that foreign term spreads embed growth spillovers from the U.S. and the resulting Dollar strength and slowdown abroad spill back to the United States.,"Ahmed, Rashad; Chinn, Menzie D.",2024,10.1111/jmcb.13164,,wos,"This study investigates whether foreign yield curves can predict U.S. recessions and GDP growth. It finds that foreign term spreads are stronger predictors of upcoming U.S. recessions than domestic ones. Both U.S. and foreign term spreads offer insights into the U.S. economy, but at different time horizons and for different economic activities. Changes in U.S. term spreads influence foreign term spreads and the U.S. dollar, while foreign term spreads impact U.S. exports and FDI. These results support the idea that foreign term spreads reflect U.S. growth spillovers, which then feedback into the U.S. economy.",False,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:37:04.323700
adb32e1a86c4dbc0,Do industry returns predict the stock market? A reprise using the random forest,"The prior work reports conflicting evidence on the information content of industry returns for the market index return. We reexamine the out of sample predictive ability of industry returns by considering several relatively advanced methods from the statistical learning literature. We show that when the random forest method, which accounts for both linear and nonlinear dynamics, is used for regression, industry returns indeed contain significant out of sample forecasting power for the market index return. Moreover, our analysis also presents evidence for lead-lag relations among individual industry returns. The reported findings are consistent with the implications of the gradual diffusion of information hypothesis. © 2019 Elsevier B.V., All rights reserved.","Ciner, C.",2019,10.1016/j.qref.2018.11.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057465162&doi=10.1016%2Fj.qref.2018.11.001&partnerID=40&md5=81b05d72c2b849b290de0e292f797a33,scopus,"This study re-examines the predictive power of industry returns for market index returns using advanced statistical learning methods, specifically the random forest. The findings indicate that industry returns do possess significant out-of-sample forecasting power, and also reveal lead-lag relationships among individual industry returns, supporting the gradual diffusion of information hypothesis.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:37:16.532903
b0fbff89929bd3a8,Does Rating Consistency Matter? A Micro-Level Study on the Impact of Corporate ESG Rating Divergence on Equity Financing Costs,"The influence of ESG rating divergence on corporate finance is receiving increasing scholarly attention as ESG concerns become more prominent. However, few studies have systematically examined how ESG rating discrepancies affect corporate equity financing costs. This study addresses this gap by analysing data from Chinese A-share listed firms between 2015 and 2022 to explore the impact of ESG rating divergence on equity financing costs and its underlying mechanisms. The results reveal that: (1) ESG rating divergence significantly raises equity financing costs, suggesting that capital markets impose a risk premium on inconsistent ESG information; (2) this effect is more pronounced in firms with greater external analyst forecast dispersion and more severe internal agency problems; (3) firms that are non-state-owned, face significant financial constraints, or have poor information quality are particularly vulnerable to this effect. The study contributes to the understanding of the economic consequences of ESG rating divergence and offers policy implications for regulators, institutions, investors and firms in navigating ESG-related challenges. © 2025 Elsevier B.V., All rights reserved.","Li, J.; Hou, Y.; Tan, C.; Han, L.",2025,10.1111/acfi.70057,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009813119&doi=10.1111%2Facfi.70057&partnerID=40&md5=bad9947ba156708087e4fd04ac55b870,scopus,"This study investigates the impact of ESG rating divergence on the equity financing costs of Chinese A-share listed firms from 2015-2022. Findings indicate that higher ESG rating divergence leads to increased equity financing costs, particularly for firms with greater analyst forecast dispersion, internal agency problems, non-state ownership, financial constraints, or poor information quality. The research suggests that capital markets penalize inconsistent ESG information.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:37:20.477626
bc5db5e2cc4b6eec,Does the mode of financing the budget deficit matter for inflation in Nigeria?,"Purpose: The motivation for this study is to determine whether inflation in Nigeria is driven by the Central Bank’s direct advances and Treasury bills/bonds as modes of financing the budget deficit. Hence, it examines whether the method of deficit financing significantly impacts inflation in Nigeria. Design/methodology/approach: Based on the nature of the study and the availability of data in Nigeria, this study employs the ARDL bound test estimation technique to analyse annual time-series data from 1981 to 2021. Findings: The ARDL bounds test approach to co-integration revealed a long-run co-integrating relationship between Central Bank advances, Treasury bills/bonds, and inflation in Nigeria. Furthermore, the ARDL results provide evidence of a negative and significant relationship between bonds and inflation in both the short and long run. In contrast, Central Bank advances exhibit a statistically significant direct effect on inflation in the short run and an indirect effect in the long run. Research limitations/implications: The study focuses solely on Nigeria, limiting the applicability of the findings to other nations with differing economic structures or fiscal policies. Secondly, while the ARDL bounds testing approach is appropriate for the research context, it may not capture complex nonlinear relationships or structural breaks within the dataset. Lastly, the exclusion of additional potential determinants of inflation, such as external shocks, geopolitical factors, or exchange rate dynamics, could restrict the comprehensiveness of the analysis. Practical implications: This study provides empirical evidence supporting the view that, to achieve lower inflation in Nigeria, policymakers should prioritize using bonds to finance the deficit budget, as they have been shown to have a short-and long-term deflationary effect on the economy. Originality/value: The novelty of this study lies in categorizing deficit budget financing (Central Bank advances and Treasury bills) and identifying which has the greatest impact on inflation in Nigeria. © 2025 Elsevier B.V., All rights reserved.","Abdullahi, S.M.; Kanang, A.A.; Gana, S.S.",2025,10.1108/ajems-04-2024-0265,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002467091&doi=10.1108%2FAJEMS-04-2024-0265&partnerID=40&md5=2af768eadcd1634b02656af15aeca46e,scopus,"This study investigates whether the method of financing Nigeria's budget deficit, specifically through Central Bank advances and Treasury bills/bonds, impacts inflation. Using ARDL bound testing on time-series data from 1981-2021, the research found a long-run relationship between these financing methods and inflation. Bonds showed a negative and significant relationship with inflation in both the short and long run, while Central Bank advances had a significant direct effect in the short run and an indirect effect in the long run. The study suggests prioritizing bonds for deficit financing to achieve lower inflation in Nigeria.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:37:26.839283
4bd64fadf8327592,Dynamic Autoregressive Liquidity (DArLiQ),"We introduce a new class of semiparametric dynamic autoregressive models for the Amihud illiquidity measure, which captures both the long-run trend in the illiquidity series with a nonparametric component and the short-run dynamics with an autoregressive component. We develop a generalized method of moments (GMM) estimator based on conditional moment restrictions and an efficient semiparametric maximum likelihood (ML) estimator based on an iid assumption. We derive large sample properties for our estimators. Finally, we demonstrate the model fitting performance and its empirical relevance on an application. We investigate how the different components of the illiquidity process obtained from our model relate to the stock market risk premium using data on the S&P 500 stock market index. © 2024 Elsevier B.V., All rights reserved.","Hafner, C.M.; Linton, O.B.; Wang, L.",2024,10.1080/07350015.2023.2238790,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171686338&doi=10.1080%2F07350015.2023.2238790&partnerID=40&md5=87817b3788132f686907c6a32d52cff2,scopus,"This paper introduces Dynamic Autoregressive Liquidity (DArLiQ), a semiparametric model for the Amihud illiquidity measure. It combines nonparametric and autoregressive components to capture long-run trends and short-run dynamics. The study proposes GMM and ML estimators, derives their asymptotic properties, and demonstrates the model's performance and relevance through an application to the S&P 500 index, exploring the relationship between illiquidity components and the stock market risk premium.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:37:36.605377
1a2c221241c0c808,"Dynamic effect of Bitcoin, fintech and artificial intelligence stocks on eco-friendly assets, Islamic stocks and conventional financial markets: Another look using quantile-based approaches","Against the milieu of rapidly growing investment in technologically induced assets, this study examines the investment role of Bitcoin, fintech, and artificial intelligence (AI) stocks in relation to major environmentally friendly assets (green bonds, sustainable investments, and clean energy), Islamic stocks, and conventional financial markets using quantile-based approaches. To this end, we specifically examine the distributional and directional predictability between the returns of fintech, Bitcoin, and AI and various markets using the nonparametric causality-in-quantiles method and the cross-quantilogram correlation method respectively. We use daily data spanning March 9, 2018 to January 27, 2021. In terms of the distributional predictability of fintech, Bitcoin, and AI in relation to the traditional markets, Islamic stocks, clean energy stocks, and sustainable investments, we find strong evidence of causal asymmetry across quantiles and strong variations across markets. Likewise, findings in terms of directional predictability between the returns of fintech, Bitcoin, and AI and various markets infer that Islamic stocks act as a good hedge against Bitcoin. The S&P Treasury Bond and S&P Green Bond are also perfect hedges for fintech stocks, while S&P Global Clean Energy is a perfect hedge for AI stocks in terms of long-term dynamics. © 2023 Elsevier B.V., All rights reserved.","Abakah, E.J.A.; Tiwari, A.K.; Ghosh, S.; Doğan, B.",2023,10.1016/j.techfore.2023.122566,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152485944&doi=10.1016%2Fj.techfore.2023.122566&partnerID=40&md5=c1f97b045bdb4bd9b8444cb70ca68f6e,scopus,"This study investigates the relationship between Bitcoin, fintech, and AI stocks and eco-friendly assets, Islamic stocks, and conventional financial markets using quantile-based approaches. It employs nonparametric causality-in-quantiles and cross-quantilogram correlation methods to analyze distributional and directional predictability. The findings indicate causal asymmetry and variations across markets, with Islamic stocks, S&P Treasury Bond, S&P Green Bond, and S&P Global Clean Energy acting as hedges against specific tech assets under certain conditions.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:39:42.637286
ce15cb2f2a6fe0d0,"Dynamic spillovers and linkages between gold, crude oil, S&P 500, and other economic and financial variables. Evidence from the USA","This paper focuses on the price determinants of gold, and on the challenges associated with gold's safe haven property. Specifically, it analyses the interlinkages and the return spillover effect among gold, crude oil, S&P 500, dollar exchange rate, Consumer Price Index (CPI), economic policy uncertainty and Treasury bills, by employing a Vector Autoregression (VAR) and the spillover index of Diebold and Yilmaz (2012), Diebold and Yilmaz (2014). Monthly realized return series, covering the period from 2nd of January 1986 to 31st of December 2019 are used to examine the short-run linkages, and the return spillovers rolling-window estimates in analyzing the transmission mechanism in a time-varying fashion, respectively. Our findings identify gold as a strong dollar hedge, while crude oil and Treasury bills appear to drive inflation; they also indicate strong spillover effects between exchange rate and gold returns. In general, co-movement dynamics display state-dependent characteristics. Both total and directional spillovers increase significantly during market turbulence caused by severe financial crises such as the Global Financial Crisis (GFC) of 2007-2009 and the European Sovereign Debt Crisis of 2010-2012. Net spillovers switch between positive and negative values for all these markets, implying that the recipient/transmitter position changes drastically with market events. Economic policy uncertainty, stock market returns, and crude oil price returns are the main transmitters, while Treasury bills and CPI are the main return shock recipients. Gold and exchange rate act both as receivers and transmitters over the sample period.","Golitsis, Petros; Gkasis, Pavlos; Bellos, Sotirios K.",2022,10.1016/j.najef.2022.101785,,wos,"This paper investigates the dynamic spillovers and return transmission mechanisms among gold, crude oil, S&P 500, dollar exchange rate, CPI, economic policy uncertainty, and Treasury bills using VAR and spillover index models. It finds gold acts as a dollar hedge, crude oil and Treasury bills influence inflation, and significant spillovers exist between exchange rates and gold. Spillovers intensify during financial crises, with economic policy uncertainty, stock returns, and crude oil being key transmitters, and Treasury bills and CPI being key recipients. Gold and exchange rates exhibit dual roles as transmitters and receivers.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:39:45.364473
ae8ad37dda607103,ESTIMATING AN UPPER BOUND ON THE PRATT RISK-AVERSION COEFFICIENT WHEN THE UTILITY FUNCTION IS UNKNOWN,,"MCCARL, BA; BESSLER, DA",1989,10.1111/j.1467-8489.1989.tb00481.x,,wos,This paper focuses on estimating an upper bound for the Pratt risk-aversion coefficient when the utility function is unknown. The abstract does not provide further details on the methodology or specific applications.,False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:39:56.131429
c3b936939fcbc181,Earthquake Insurance via CAT Bonds Utilizing Autoregressive Neural Networks and Active Faults,"Catastrophe (CAT) bonds necessitate a robust construction with regard to the estimated probability measure of their triggering parameter. This article concentrates on earthquakes as the primary natural catastrophe of concern. By leveraging the geometry of active faults for estimating default probability, we utilize seismic event information spanning up to 15,000 years in the past—thereby surpassing the restricted time range of available historical catalogs commonly used in other analyses, which typically cover only a few hundred years. This article introduces the design and pricing methodology of CAT bonds employing autoregressive neural networks, extending the standard VAR Nelson-Siegel model for yield curves. It presents a case study focused on the region of Greece, estimating that an additional spread of approximately 500 basis points over LIBOR constitutes the minimum premium required to attract an investor to undertake the associated risk. This premium could be absorbed by insured parties as an alternative to the conventional insurance process. © 2024 Elsevier B.V., All rights reserved.","Louloudis, E.; Zimbidis, A.; Tsekrekos, A.; Yannacopoulos, A.",2024,10.3905/jfi.2024.1.186,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206015299&doi=10.3905%2Fjfi.2024.1.186&partnerID=40&md5=16f2c58895e0134693f922a1467a33d0,scopus,"This article proposes a method for earthquake insurance using Catastrophe (CAT) bonds, leveraging autoregressive neural networks and active fault geometry to estimate default probabilities over a 15,000-year period. A case study in Greece suggests a minimum premium of 500 basis points over LIBOR is needed to attract investors, offering an alternative to traditional insurance.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:40:03.301442
189cf61c7efc6837,"Economic Freedom, Budget Deficits, and Perceived Risk from Larger National Debt-to-GDP Ratios: An Exploratory Analysis of Their Real Interest Rate Effects","Since the early 1980s, there have been a number of principally empirical studies of the impact of government budget deficits on interest rates that have typically tested the hypothesis that larger deficits raise interest rates. However, in more recent years, this topic has received far less attention. Accordingly, this study seeks to “update” the findings of such studies and to do so for the dominant North American economies of Canada and the U.S. Furthermore, in the pursuit of further insights into interest rates, the present study also investigates an effectively heretofore overlooked variable that arguably also might influence interest rates, namely, economic freedom. Finally, given the increased upward trend of government debt (relative to GDP) in recent years in Canada and the U.S., this study investigates the interest rate impact of rising national debt-to-GDP ratios. For the 1995–2024 period (and also in one estimate for the 1985–2001 period), this exploratory study finds compelling evidence (1) that the real interest rate yield on 10-year Treasuries in Canada and the real interest rate yield on 10-year U.S. Treasury notes are increasing functions of the central government budget deficits in both Canada and the U.S., respectively, and (2) the real interest rate yields on 10-year Treasuries in Canada and 10-year U.S. Treasury notes are both decreasing functions of economic freedom in Canada and the U.S., respectively. On the other hand, regarding the impact of a higher national debt-to-GDP ratio on the real ten-year Treasury yield, there is only very mixed support for an impact, with support for its impact coming from the Canadian estimates but no support whatsoever coming from the U.S. estimates.",,2024,10.3390/jrfm17100469,,proquest,"This exploratory study examines the impact of economic freedom, budget deficits, and national debt-to-GDP ratios on real interest rates for Canada and the U.S. from 1995-2024. It finds that larger budget deficits increase real interest rates, while greater economic freedom decreases them. The impact of higher debt-to-GDP ratios on real interest rates is mixed, with support found for Canada but not for the U.S.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:40:31.350640
b9ed21f0c6e8f1fe,Efficient importance sampling maximum likelihood estimation of stochastic differential equations,"Maximum likelihood estimation (MLE) of stochastic differential equations (SDEs) is difficult because in general the transition density function of these processes is not known in closed form, and has to be approximated somehow. An approximation based on efficient importance sampling (EIS) is detailed. Monte Carlo experiments, based on widely used diffusion processes, evaluate its performance against an alternative importance sampling (IS) strategy, showing that EIS is at least equivalent, if not superior, while allowing a greater flexibility needed when examining more complicated models. (C) 2010 Elsevier B.V. All rights reserved.","Pastorello, S.; Rossi, E.",2010,10.1016/j.csda.2010.02.001,,wos,"This paper proposes an efficient importance sampling (EIS) method for maximum likelihood estimation (MLE) of stochastic differential equations (SDEs) when the transition density function is unknown. The EIS method is evaluated through Monte Carlo experiments using diffusion processes and compared to an alternative importance sampling (IS) strategy, demonstrating its effectiveness and flexibility for complex models.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:40:39.022758
03ea7ad65ff3f52e,Empirical evaluation of the market price of risk using the CIR model,"We describe a simple but effective method for the estimation of the market price of risk. The basic idea is to compare the results obtained by following two different approaches in the application of the Cox-Ingersoll-Ross (CIR) model. In the first case, we apply the non-linear least squares method to cross sectional data (i.e., all rates of a single day). In the second case, we consider the short rate obtained by means of the first procedure as a proxy or the real market short rate. Starting from this new proxy, we evaluate the parameters of the CIR model by means of martingale estimation techniques. The estimate of the market price of risk is provided by comparing results obtained with these two techniques, since this approach makes possible to isolate the market price of risk and evaluate, under the Local Expectations Hypothesis, the risk premium given by the market for different maturities. As a test case, we apply the method to data of the European Fixed Income Market. (c) 2006 Elsevier B.V. All rights reserved.","Bernaschi, M.; Torosantucci, L.; Uboldi, A.",2007,10.1016/j.physa.2006.10.072,,wos,This paper proposes a method to estimate the market price of risk using the Cox-Ingersoll-Ross (CIR) model by comparing two estimation approaches: non-linear least squares on cross-sectional data and martingale estimation techniques using a proxy for the short rate. The method is tested on the European Fixed Income Market.,False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:40:45.826255
36465bfe50fa4423,Engineering viable foot-and-mouth disease viruses with increased thermostability as a step in the development of improved vaccines,"We have rationally engineered foot-and-mouth disease virus to increase its stability against thermal dissociation into subunits without disrupting the many biological functions needed for its infectivity. Amino acid side chains located near the capsid intersubunit interfaces and either predicted or found to be dispensable for infectivity were replaced by others that could establish new disulfide bonds or electrostatic interactions between subunits. Two engineered viruses were normally infectious, genetically stable, and antigenically indistinguishable from the natural virus but showed substantially increased stability against irreversible dissociation. Electrostatic interactions mediated this stabilizing effect. For foot-and-mouth disease virus and other viruses, some evidence had suggested that an increase in virion stability could be linked to an impairment of infectivity. The results of the present study show, in fact, that virion thermostability against dissociation into subunits may not be selectively constrained by functional requirements for infectivity. The thermostable viruses obtained, and others similarly engineered, could be used for the production, using current procedures, of foot-and-mouth disease vaccines that are less dependent on a faultless cold chain. In addition, introduction of those stabilizing mutations in empty (nucleic acid-free) capsids could facilitate the production of infection-risk-free vaccines against the disease, one of the economically most important animal diseases worldwide. Copyright © 2008, American Society for Microbiology. All Rights Reserved. © 2009 Elsevier B.V., All rights reserved.; MEDLINE® is the source for the MeSH terms of this document.","Mateo, R.; Luna, E.; Rincón, V.; Mateu, M.G.",2008,10.1128/jvi.01553-08,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57349125715&doi=10.1128%2FJVI.01553-08&partnerID=40&md5=312eebb857432143e3d20a48dc5ffa60,scopus,"This study describes the engineering of foot-and-mouth disease viruses (FMDV) to enhance their thermostability. By introducing specific amino acid changes, the researchers created FMDV variants that are more resistant to thermal dissociation without compromising infectivity or antigenicity. These thermostable viruses could lead to improved FMDV vaccines that are less reliant on cold chain storage and potentially safer if used as empty capsids.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:40:49.172295
f74524ca4a843b41,Equity/bond yield correlation and the FED model: evidence of switching behaviour from the G7 markets,"This paper considers how the strength and nature of the relation between the equity and bond yield varies with the level of the real bond yield. We demonstrate that at low levels of the real bond yield, the correlation between the equity and bond yields turns negative. This arises as the lower bond yield implies heightened macroeconomic risk (e.g. deflation and economic stagnation) and causes equity and bond prices to move in opposite directions. The FED model relies on a positive relation for its success in predicting future returns. Thus, we argue that the mixed empirical evidence regarding the FED model arises due to this switch in correlation behaviour. We present supportive evidence for the switching relation and its link to the level of the bond yield using linear and nonlinear smooth transition panel regression techniques for the G7 markets. The results presented here should be of interest to market practitioners who may wish to use the FED model to aid market timing decisions and for academics interested in understanding the interrelations between markets. © 2018 Elsevier B.V., All rights reserved.","Humpe, A.; McMillan, D.G.",2018,10.1057/s41260-018-0091-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052958299&doi=10.1057%2Fs41260-018-0091-x&partnerID=40&md5=da878cf9fc2d0e5c1d3d00c3aa2fbdb2,scopus,"This paper investigates the relationship between equity and bond yields, finding that it becomes negative at low real bond yield levels due to heightened macroeconomic risk. This switching behavior is proposed as an explanation for the mixed empirical evidence on the FED model's predictive power. The study uses smooth transition panel regression techniques for G7 markets to support these findings.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T11:41:29.731601
c621221de8365a99,Estimating Interest Rate Curves by Support Vector Regression,"A model that seeks to estimate an interest rate curve should have two desirable capabilities in addition to the usual characteristics required from any function-estimation model: it should incorporate the bid-ask spreads of the securities from which the curve is extracted and restrict the curve shape. The goal of this article is to estimate interest rate curves by using Support Vector Regression (SVR), a method derived from the Statistical Learning Theory developed by Vapnik (1995). The motivation is that SVR features these extra capabilities at a low estimation cost. The SVR is specified by a loss function, a kernel function and a smoothing parameter. SVR models the daily U.S. dollar interest rate swap curves, from 1997 to 2001. As expected from a priori and sensibility analyses, the SVR equipped with the kernel generating a spline with an infinite number of nodes was the best performing SVR. Comparing this SVR with other models, it achieved the best cross-validation interpolation performance in controlling the bias-variance trade-off and generating the lowest error considering the desired accuracy fixed by the bid-ask spreads.","Monteiro, Andre d'Almeida",2010,10.1080/07474938.2010.481998,,wos,"This article estimates interest rate curves using Support Vector Regression (SVR), a method that incorporates bid-ask spreads and restricts curve shape. The SVR model, particularly with a spline-generating kernel, demonstrated superior performance in cross-validation, controlling bias-variance trade-off and minimizing error compared to other models.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:42:06.887410
8407ab83849bf0fa,Estimating dynamic equilibrium models using mixed frequency macro and financial data,"We provide a framework for inference in dynamic equilibrium models including financial market data at daily frequency, along with macro series at standard lower frequency. Our formulation of the macro finance model in continuous time conveniently accounts for the difference in observation frequency. We suggest the use of martingale estimating functions (MEF) to infer the structural parameters of the model directly through a nonlinear scheme. This method is compared to regression-based methods and the generalized method of moments (GMM). We illustrate our approaches by estimating various versions of the AK-Vasicek model with mean-reverting interest rates. We provide asymptotic theory and Monte Carlo evidence on the small sample behavior of the estimators and report empirical estimates using 30 years of US macro and financial data. (C) 2016 Elsevier B.V. All rights reserved.","Christensen, Bent Jesper; Posch, Olaf; van der Wel, Michel",2016,10.1016/j.jeconom.2016.04.005,,wos,"This paper proposes a framework for estimating dynamic equilibrium models using mixed-frequency macro and financial data. It utilizes continuous-time modeling and martingale estimating functions (MEF) for parameter inference, comparing this approach to regression-based methods and GMM. The authors illustrate their methods with versions of the AK-Vasicek model and provide empirical estimates using US data.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:42:16.785852
cecb8e178cdb3ad6,Estimating stochastic discount factor models with hidden regimes: Applications to commodity pricing,"We develop new likelihood-based methods to estimate factor-based Stochastic Discount Factors (SDF) that may accommodate Hidden Markov dynamics in the factor loadings. We use these methods to investigate whether it is possible to find a SDF that jointly prices the cross-section of eight U.S. portfolios of stocks, Treasuries, corporate bonds, and commodities. In particular, we test a range of possible different specification of the SDF, including single-state and Hidden Markov models and compare their statistical and pricing performances. In addition, we assess whether and to which extent a selection of these models replicates the observed moments of the return series, and especially correlations. We report that regime-switching models clearly outperform single-state ones both in term of statistical and pricing accuracy. However, while a four-state model is selected by the information criteria, a two-state three-factor full Vector Autoregression model outperforms the others as far as the pricing accuracy is concerned. © 2017 Elsevier B.V., All rights reserved.","Giampietro, M.; Guidolin, M.; Pedio, M.",2018,10.1016/j.ejor.2017.07.045,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029533759&doi=10.1016%2Fj.ejor.2017.07.045&partnerID=40&md5=290458cda6fa18cfc85c520063b195a2,scopus,"This paper proposes new likelihood-based methods to estimate Stochastic Discount Factor (SDF) models with hidden regimes, applying them to commodity pricing. The study investigates whether these models can jointly price various U.S. asset portfolios (stocks, Treasuries, corporate bonds, commodities) and compares the performance of single-state versus Hidden Markov models. Regime-switching models demonstrate superior statistical and pricing accuracy, with a two-state, three-factor model showing the best pricing performance.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T11:43:28.640461
00660d636f49d59e,Estimating the Term Structure with Linear Regressions: Getting to the Roots of the Problem,"Linear estimators of the affine term structure model are inconsistent since they cannot reproduce the factors used in estimation. This is a serious handicap empirically, giving a worse fit than the conventional ML estimator that ensures consistency. We show that a simple self-consistent estimator can be constructed using the eigenvalue decomposition of a regression estimator. The remaining parameters of the model follow analytically. Estimates from this model are virtually indistinguishable from that of the ML estimator. We apply the method to estimate various models of U.S. Treasury yields. These exercises greatly extend the range of models that can be estimated. © 2021 Elsevier B.V., All rights reserved.","Goliǹski, A.; Spencer, P.",2021,10.1093/jjfinec/nbz031,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099520801&doi=10.1093%2Fjjfinec%2Fnbz031&partnerID=40&md5=fefd9e28937e02b26df6e38c509d65ee,scopus,"This paper proposes a self-consistent estimator for the affine term structure model, which is shown to be consistent and empirically performs similarly to the conventional Maximum Likelihood estimator. The method is applied to estimate U.S. Treasury yield models, expanding the scope of estimable models.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:43:34.122521
f0f93a32202f330e,Estimating volatility from ATM options with lognormal stochastic variance and long memory,"In this article we propose a nonlinear state space representation to model At-The-Money (ATM) implied volatilities and to estimate the unobserved Stochastic Volatility (SVOL) for the underlying asset. We derive a polynomial measurement model relating fractionally cointegrated implied and spot volatilities. We then use our state space representation to obtain Maximum Likelihood (ML) estimates of the short-memory model parameters, and for filtering the fractional spot volatility. We are also able to estimate the average volatility risk premia. We applied our methodology to implied volatilities on eurodollar options, from which we filter the unobserved spot local variance. These data arise from Over The Counter (OTC) transactions that account for high liquidity. For these data, we estimated a positive average volatility risk premia, which is consistent with the Intertemporal Capital Asset Pricing Model (ICAPM) setup of Merton (1973). We also had evidence of highly nonlinear relation between eurodollar spot and implied volatilities. From a methodological and computational point of view, the likelihood function, and all the iterative procedures associated with it, converged uniformly in the parameter space at very little computational expense. We illustrated the effectiveness of our approach by evaluating the approximated Information matrix, the Hotelling's T2 test along with other diagnostic procedures. Reprinted by permission of Routledge, Taylor and Francis Ltd.",,2012,10.1080/09603107.2011.624082,,proquest,"This paper proposes a nonlinear state space model to estimate unobserved stochastic volatility (SVOL) from At-The-Money (ATM) implied volatilities. It derives a measurement model relating cointegrated implied and spot volatilities, enabling Maximum Likelihood (ML) estimation of short-memory model parameters and filtering of fractional spot volatility. The study also estimates average volatility risk premia and finds evidence of a nonlinear relationship between eurodollar spot and implied volatilities. The methodology is computationally efficient and validated with diagnostic procedures.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:43:42.807841
cb24c90ea906f755,Estimation and Forecasting of Sovereign Credit Rating Migration Based on Regime Switching Markov Chain,"Our research aims to develop the regime switching Markov chain (RSMC), a discrete time Markov chain whose underlying regime is depending on a hidden Markov model, which express the dynamics of sovereign credit rating migration. Estimated based on a version of the Expectation-Maximization algorithm, the regime in RSMC indicates either economic expansion or contraction. Then, we apply RSMC to the monthly time series of the sovereign credit rating of 41 nations from January 1994 to December 2018. At first, we confirm that the estimation of RSMC is superior to a homogeneous Markov chain. It implies that the credit rating dynamics are subject to the underlying economic condition. Secondly, we observe that the second tier and non-investment credit ratings in economic contractions are likely to be downgraded. We also detect the continental clustering of economic contractions for the Asian currency and European sovereign debt crises. Lastly, we discover that the forecasting performance of RSMC is superior to that of the benchmark, especially for the second tier and non-investment credit ratings. In conclusion, we claim that RSMC can improve the management of sovereign credit risk exposures.","Oh, Sung Youl; Song, Jae Wook; Chang, Woojin; Lee, Minhyuk",2019,10.1109/access.2019.2934516,,wos,"This study introduces a regime-switching Markov chain (RSMC) model to analyze sovereign credit rating migration, considering underlying economic conditions (expansion/contraction). The RSMC model, estimated using an Expectation-Maximization algorithm, demonstrates superior performance over homogeneous Markov chains. Findings indicate that lower credit ratings are more prone to downgrades during economic contractions, with evidence of continental clustering of these contractions. The RSMC model also shows improved forecasting accuracy, particularly for lower credit ratings, suggesting its utility in managing sovereign credit risk.",True,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:44:17.956613
d06548d8a3fc3d66,Estimation and empirical evaluation of the time-dependent Extended-CIR term structure model,"Empirical study of 25 years US Treasury bills data shows that even when the spot interest rate remains fixed, its volatility varies significantly over time. Constant-coefficient models cannot capture these changes as they give rise to time-homogeneous distributions. Maximum likelihood fitting of a one-factor time-dependent Extended-CIR model of the term structure, whose closed-form solution was previously obtained by the author, shows that it can capture these changes, as well as achieve significantly higher likelihood value. It is shown that exploitation of the closed-form solutions substantially improves the accuracy and efficiency of Monte Carlo simulations over high-order discretization algorithms. It is also shown that the feasibility of exact one-to-one calibration of the model to any continuous yield curve allows valuation of bond options significantly more accurately and efficiently.",Y. MAGHSOODI,2000,10.1093/imaman/11.3.161,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8132325,ieeexplore,"This paper empirically evaluates a time-dependent Extended-CIR model for the term structure of interest rates using 25 years of US Treasury bills data. The model captures time-varying volatility, outperforming constant-coefficient models. The study also demonstrates the efficiency of using closed-form solutions for Monte Carlo simulations and bond option valuation.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:45:14.678694
5a9d202cac09c44d,Evaluating credit rating prediction by using the KMV model and random forest,"PurposeAn increasing number of investors have begun using financial data to develop optimal investment portfolios; therefore, the public financial data shared in the capital market plays a critical role in credit ratings. These data enable investors to understand the credit levels of debtors from a bank perspective; this facilitates predicting the debtor default rate to efficiently evaluate investment risks. The paper aims to discuss these issues.Design/methodology/approachA credit rating model can be developed to reduce the risk of adverse selection and moral hazard caused by information asymmetry in the loan market. In this study, a random forest (RF) was used to evaluate financial variables and construct credit rating prediction models. Data-mining techniques, including an RF, decision tree, neural networks, and support vector machine, were used to search for suitable credit rating forecasting methods. The distance to default from the KMV model was then incorporated into the credit rating model as a research variable to increase predictive power of various data-mining techniques. In addition, four-level and nine-level classification were set to investigate the accuracy rates of various models.FindingsThe experimental results indicated that applying the RF in the variable feature selection process and developing a forecasting model was the most effective method of predicting credit ratings; the four-level and nine-level feature-selection settings achieved 95.5 and 87.8 percent accuracy rates, respectively, indicating that RF demonstrated outstanding feature selection and forecasting capacity.Research limitations/implicationsThe experimental cases were based on financial data from public companies in North America.Practical implicationsPractical implication of this study indicates the most effective financial variables were dividends common/ordinary, cash dividends, volatility assumption, and risk-free rate assumption.Originality/valueThe RF model can be used to perform feature selection and efficiently filter numerous financial variables to obtain crediting rating information instantly.",,2016,10.1108/k-12-2014-0285,,proquest,"This study evaluates credit rating prediction using the KMV model and random forest (RF). The RF model was found to be the most effective for variable selection and forecasting, achieving high accuracy rates in four-level and nine-level classifications. Key financial variables for prediction were identified, and the RF model's ability to efficiently filter variables for instant credit rating information is highlighted.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:45:35.070833
821367289e61d536,Event-based approach for probabilistic agricultural drought risk assessment under rainfed conditions,"An event-based approach for the probabilistic risk assessment of agricultural drought under rainfed conditions to estimate the economic impact is proposed. The risk parameters are evaluated in an event-based probabilistic framework for a set of hazard events; these results are probabilistically integrated including, in a formal way, all uncertainties related to every part of the process. The hazard is defined as a stochastic or historic set of events, collectively exhaustive and mutually exclusive, that describes the spatial distribution, the annual frequency, and the randomness of the hazard intensity. The risk is expressed in different economic terms: the average annual loss (or pure risk premium) and the loss exceedance curve; these metrics are of particular importance for risk retention (financing) schemes or risk transfer instruments. As an illustrative example, this approach is applied to probabilistic drought risk assessment of maize under rainfed conditions in Mexico. These results are the base of further studies in defining strategies for financial protection against agricultural losses and disasters.",,2015,10.1007/s11069-014-1550-4,,proquest,"This paper proposes an event-based approach for assessing agricultural drought risk under rainfed conditions, focusing on estimating economic impacts. It integrates hazard events, their frequencies, intensities, and associated uncertainties within a probabilistic framework. The risk is quantified using metrics like average annual loss and loss exceedance curves, which are relevant for risk financing and transfer. The approach is demonstrated with a case study on maize drought risk in Mexico, aiming to inform financial protection strategies against agricultural losses.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:45:42.675465
8aebf3ff75b87c23,Exchange rate predictability in emerging markets,"This paper uses financial and macroeconomic variables to predict currency returns, by using a two-step procedure. The first step consists of a cointegration equation that explains the exchange rate level as a function of global and domestic financial factors. The second step estimates an error-correction equation, for modeling the expected returns. This approach is a factor model analysis, where a Lasso derived technique is used for variable selection. This paper will focus on the five most frequently traded Latin American currencies, Brazilian Real (BRL), Chilean Peso (CLP), Colombian Peso (COL), Mexican Peso (MXN) and Peruvian Sol (PEN), during the time horizon from December 2001 until February 2016. The first finding is that the Global Exchange Rate Factor offers information about the exchange rate movements. In addition, this paper shows that commodity, equity prices and domestic risk premium are important variables for explaining exchange rates. Moreover, it confirms the existing results for the carry and slope variables. © 2019 Elsevier B.V., All rights reserved.","Baku, E.",2019,10.1016/j.inteco.2018.06.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049573296&doi=10.1016%2Fj.inteco.2018.06.003&partnerID=40&md5=fc8de85fd1f55752a97088fd14aeb2fd,scopus,"This paper investigates the predictability of exchange rates in five Latin American currencies (BRL, CLP, COL, MXN, PEN) from December 2001 to February 2016. It employs a two-step procedure involving cointegration and error-correction equations, utilizing factor model analysis with Lasso for variable selection. Key findings indicate that a Global Exchange Rate Factor, along with commodity prices, equity prices, domestic risk premium, carry, and slope variables, are significant in explaining exchange rate movements.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:45:58.309729
9342c5885c2e100c,Experimental Evidence on Socioeconomic Differences in Risk‐Taking and Risk Premiums,"Using a route choice experiment with embedded travel time variability, this study empirically estimates car commuters’ risk attitudes and taste preferences within a nonlinear mixed logit model. In addition to the identified overall risk‐taking behaviour, we find that risk attitudes covary with some sociodemographic characteristics, that is, older commuters are more risk‐taking than young ones and higher‐income commuters are less risk‐taking than low‐income ones. The implications of accounting for systematic risk attitude heterogeneity for valuing travellers’ willingness to pay for travel time improvement are also discussed.",,2020,10.1111/1475-4932.12544,,proquest,"This study uses a route choice experiment to estimate car commuters' risk attitudes and taste preferences using a nonlinear mixed logit model. It finds that older commuters are more risk-taking than younger ones, and higher-income commuters are less risk-taking than lower-income ones. The study also discusses the implications of this risk attitude heterogeneity for valuing travel time improvements.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:46:03.180436
e2d7c330b34a3f2e,Experiments on the application of IOHMMs to model financial returns series,"Input-output hidden Markov models (IOHMMs) are conditional hidden Markov models in which the emission (and possibly the transition) probabilities can be conditioned on an input sequence. For example, these conditional distributions can be linear, logistic, or nonlinear (using for example multilayer neural networks), We compare the generalization performance of several models which are special cases of input-output hidden Markov models on financial time-series prediction tasks: an unconditional Gaussian, a conditional linear Gaussian, a mixture of Gaussians, a mixture of conditional linear Gaussians, a hidden Markov model, and various IOHMMs. The experiments compare these models on predicting the conditional density of returns of market and sector indices. Note that the unconditional Gaussian estimates the first moment with the historical average. The results show that, although for the first moment the historical average gives the best results, for the higher moments, the IOHMMs yielded significantly better performance, as estimated by the out-of-sample likelihood.","Bengio, Y; Lauzon, VP; Ducharme, R",2001,10.1109/72.896800,,wos,"This study compares the performance of various input-output hidden Markov models (IOHMMs) and their special cases for predicting financial time-series returns. While simple models like the historical average performed best for the first moment, IOHMMs significantly outperformed them for higher moments, as measured by out-of-sample likelihood.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:46:34.578442
c053694d929092d7,Exploiting Visual Features in Financial Time Series Prediction,"The possibility to enhance prediction accuracy for foreign exchange rates was investigated in two ways: first applying an outside the box approach to modeling price graphs by exploiting their visual properties, and secondly employing the most efficient methods to detect patterns to classify the direction of movement. The approach that exploits the visual properties of price graphs which make use of density regions along with high and low values describing the shape; hence, the authors propose the name 'Finance Vision.' The data used in the predictive model consists of 1-hour past price values of 4 different currency pairs, between 2003 and 2016. Prediction performances of state-of-the-art methods; Extreme Gradient Boosting, Artificial Neural Network and Support Vector Machines are compared over the same data with the same sets of features. Results show that density based visual features contribute considerably to prediction performance.","Karacor, Adil Gursel; Erkan, Turan Erman",2020,10.4018/ijcini.2020040104,,wos,"This study investigates enhancing foreign exchange rate prediction accuracy by leveraging visual properties of price graphs ('Finance Vision') and pattern detection for movement direction classification. It compares Extreme Gradient Boosting, Artificial Neural Network, and Support Vector Machines using 1-hour past price values of 4 currency pairs (2003-2016). Results indicate that density-based visual features significantly improve prediction performance.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:46:42.089512
728df2bee0b1fd08,Exploring Low-Risk Anomalies: A Dynamic CAPM Utilizing a Machine Learning Approach,"Low-risk pricing anomalies, characterized by lower returns in higher-risk stocks, are prevalent in equity markets and challenge traditional asset pricing theory. Previous studies primarily relied on linear regression methods, which analyze a limited number of factors and overlook the advantages of machine learning in handling high-dimensional data. This study aims to address these anomalies in the Chinese market by employing machine learning techniques to measure systematic risk. A large dataset consisting of 770 variables, encompassing macroeconomic, micro-firm, and cross-effect factors, was constructed to develop a machine learning-based dynamic capital asset pricing model. Additionally, we investigated the differences in factors influencing time-varying beta between state-owned enterprises (SOEs) and non-SOEs, providing economic explanations for the black-box issues. Our findings demonstrated the effectiveness of random forest and neural networks, with the four-layer neural network performing best and leading to a substantial rise in the excess return of the long–short portfolio, up to 0.36%. Notably, liquidity indicators emerged as the primary drivers influencing beta, followed by momentum. Moreover, our analysis revealed a shift in variable importance during the transition from SOEs to non-SOEs, as liquidity and momentum gradually replaced fundamentals and valuation as key determinants. This research contributes to both theoretical and practical domains by bridging the research gap in incorporating machine learning methods into asset pricing research. © 2023 Elsevier B.V., All rights reserved.","Wang, J.; Chen, Z.",2023,10.3390/math11143220,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166206513&doi=10.3390%2Fmath11143220&partnerID=40&md5=d85880f568724fa7255d579695735817,scopus,"This study applies machine learning (random forest and neural networks) to a dynamic CAPM model using a large dataset of 770 variables to analyze low-risk anomalies in the Chinese equity market. It investigates time-varying betas, particularly differences between SOEs and non-SOEs, finding liquidity and momentum to be key drivers. The machine learning approach significantly improved the performance of a long-short portfolio.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:46:50.565866
aa74a4500fdd27b8,Extrapolating Long-Run Yield Curves: An Innovative and Consistent Approach,"This article proposes a method to build term structures that are consistent with market data and that provide interest rates for which the volatility, on average, decreases as maturities increase. The method is designed for continuous repetitive use and is consistent with work by Diebold and Li, providing reasonable extrapolated rates, with an appropriate level of volatility over time. The Svensson model is adopted, and its parameters are estimated by the combination of a genetic algorithm and a quasi-Newton nonlinear optimization method. We innovate with a new objective function that focuses on both parts of the estimated curves (interpolated and extrapolated). For this purpose, a stability component is added. The new objective function aims to solve the problem of estimating long-term rates not observable in the market, for which the estimates are usually artificially stable or excessively volatile. The results show that the estimation method is able to bring the volatility of extrapolated rates to levels consistent with those observed for the longest liquid rate. Estimation errors are small enough and there is no statistical evidence that they are biased. The method is useful for the insurance market, since it provides interest rates that do not lead to artificially stable or excessively volatile technical provisions. © 2023 Elsevier B.V., All rights reserved.","Signorelli, T.P.; Campani, C.H.; Neves, C.D.R.",2023,10.1080/10920277.2022.2102040,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138326527&doi=10.1080%2F10920277.2022.2102040&partnerID=40&md5=a98fb2ad043814ee043fadaf75c64129,scopus,"This paper introduces a novel and consistent method for constructing long-run yield curves, aiming to provide interest rates with decreasing volatility as maturity increases. It adapts the Svensson model, employing a genetic algorithm and quasi-Newton optimization for parameter estimation. A new objective function is proposed to improve the estimation of unobservable long-term rates, addressing issues of artificial stability or excessive volatility. The method's effectiveness is demonstrated by its ability to produce extrapolated rate volatilities consistent with observed liquid rates, with small and unbiased estimation errors. This approach is particularly beneficial for the insurance market, ensuring stable technical provisions.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T11:47:39.817661
8edd8d9e068332ee,FORECASTING DEMAND FOR MONEY UNDER CHANGING TERM STRUCTURE OF INTEREST-RATES - APPLICATION OF RIDGE REGRESSION,,"WATSON, DE; WHITE, KJ",1976,10.2307/1057334,,wos,"This article applies Ridge Regression to forecast demand for money, considering changes in the term structure of interest rates.",True,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:47:50.460323
3fbafc43043d6b52,Failure Propagation in SAP MultiBank Payment Batches Due to Unsynchronized Secure Channel Negotiations,"In SAP MultiBank systems, the payment batch processing failures are usually due to out-of-sync secure channel negotiations between the corporate SAP treasury systems and the banking interfaces that are integrated and connected. This research looks into the failure timing impacts of secure channel handshake intervals such as the time-to-first-byte, SSL certificate validation delays, and out-of-sync cryptographic negotiations occurring asynchronously. Using transaction logs, synthetic datasets, and real-world data, we construct a failure propagation framework that models transaction execution disruptions across various API types and geographical endpoints. The research confirms that there is a strong dependency between the level of channel desynchronization and the average payment failure rate with retry latencies and the volume of batches serving as extreme multipliers. Through recovery strategy analysis based on manual realignments, scheduled syncs, and AI anomaly driven adaptive predictions, we quantify the changes in batch stability and the integrity of transmitted data. The findings illustrate that diverse systems require synchronous coordinated negotiation protocols for secure cryptographic negotiation per transaction, stressing the treasury teams provide seamless payment failure continuity, enhanced compliance standards, and robust compliance standards for interbank integration. © 2025 Elsevier B.V., All rights reserved.","Jamithireddy, N.S.",2025,10.58346/jisis.2025.i2.036,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012485585&doi=10.58346%2FJISIS.2025.I2.036&partnerID=40&md5=4f31951178fd939d4f4e5b7ce6ed7171,scopus,"This research investigates failures in SAP MultiBank payment batches caused by unsynchronized secure channel negotiations between SAP treasury systems and banking interfaces. It models failure propagation using transaction logs and real-world data, confirming a strong link between channel desynchronization and payment failure rates. The study also analyzes recovery strategies, including AI-driven predictions, to improve batch stability and data integrity, emphasizing the need for synchronous negotiation protocols.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:47:57.719240
a940e3bbceaea481,False Safe Haven Assets: Evidence From the Target Volatility Strategy Based on Recurrent Neural Network,"This paper examines which safe haven assets should be used when improving out-of-sample portfolio performance. We define a market state with recurrent neural network (RNN) volatility predictions and construct an investment strategy that dynamically combines equity, cash, and safe havens. The equity is allocated by targeting the volatility, and investing in safe havens depends on the predicted market state. We consider the S&P500 index with 13 safe haven assets, such as long-term government bonds, commodities, gold, and other precious metals. Other indices, NIKKEI225, NIFTY50, and STOXX50, are examined for robustness. With analysis conducted over a 20-year sample period, we find that RNN delivers sound predictions to construct the volatility targeting strategy. Among considered assets, only long-term Treasury bonds act as a safe haven and improve the strategy performance. Other considered assets have no such potential. Our findings are relevant to portfolio managers and investors actively managing portfolio risk. © 2021 Elsevier B.V., All rights reserved.","Kaczmarek, T.; Be̜dowska-Sójka, B.; Grobelny, P.; Perez, K.",2022,10.1016/j.ribaf.2021.101610,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122282703&doi=10.1016%2Fj.ribaf.2021.101610&partnerID=40&md5=19f8d4a3c02c1effd32fe5951dcc2048,scopus,"This paper uses a Recurrent Neural Network (RNN) to predict market states and construct a volatility targeting strategy. The strategy dynamically combines equity, cash, and safe haven assets, with a focus on long-term Treasury bonds as the only effective safe haven among 13 considered assets. The analysis, conducted over 20 years using S&P500 and other indices, confirms the RNN's predictive accuracy and the limited safe haven potential of other assets.",True,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:48:27.329750
0726da9e365f79a8,"Financing anomaly, mispricing and cross-sectional return predictability","This study investigates the persistence of financing anomaly in the Chinese stock market. The results show that the long-short portfolio earns an average monthly excess return of 0.88% for the following 12 months based on this anomaly. We provide a behavioral mispricing-based explanation for this anomaly that financing anomaly is stronger in stocks or period with high investor overconfidence. Moreover, we find that financing anomaly is stronger in stocks with more investor attention, indicating that excessive investor attention increases mispricing and reduces market efficiency. However, idiosyncratic volatility cannot capture the arbitrage costs faced by financing anomaly, and investors' lottery-like preference cannot explain financing anomaly. Our findings are robust after controlling for other firm characteristic variables, considering shell value contamination and different sample periods. © 2022 Elsevier B.V., All rights reserved.","Yang, B.; Ye, T.; Ma, Y.",2022,10.1016/j.iref.2022.02.062,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125476546&doi=10.1016%2Fj.iref.2022.02.062&partnerID=40&md5=ce0991486424799573e1de259da53f4a,scopus,"This study examines the financing anomaly in the Chinese stock market, finding it persists and generates excess returns. The anomaly is linked to behavioral mispricing, particularly investor overconfidence and attention, which increase mispricing and reduce market efficiency. The study rules out idiosyncratic volatility and lottery-like preferences as explanations for arbitrage costs and investor behavior, respectively. Findings remain robust across various controls and sample periods.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:48:35.547315
30116f36dc2b4789,Fiscal policy and asset markets: A semiparametric analysis,"Using a flexible semiparametric varying coefficient model specification, this paper examines the role of fiscal policy on the US asset markets (stocks, Corporate and treasury bonds). We consider two possible roles of fiscal deficits (or surpluses): as a separate direct information variable and as a (indirect) conditioning information variable indicating binding constraints oil monetary policy actions. The results show that the impact of monetary policy on the stock market varies, depending oil fiscal expansion or contraction. The impact of fiscal policy on corporate and treasury bond yields follow similar patterns as in the equity market. The results are consistent with the notion of strong interdependence between monetary and fiscal policies. (C) 2008 Elsevier B.V. All rights reserved.","Jansen, Dennis W.; Li, Qi; Wang, Zijun; Yang, Jian",2008,10.1016/j.jeconom.2008.09.007,,wos,"This paper uses a semiparametric varying coefficient model to analyze the impact of fiscal policy on US asset markets (stocks, corporate bonds, and treasury bonds). It investigates fiscal deficits as both a direct information variable and an indicator of monetary policy constraints. The findings suggest that fiscal policy influences the effect of monetary policy on the stock market and that fiscal policy impacts corporate and treasury bond yields similarly to equity markets, indicating a strong interdependence between monetary and fiscal policies.",True,True,False,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T11:49:02.838946
436fe1047a3403b7,Fluctuations in economic and activity and stabilization policies in the CIS,"In this study, a highly flexible form of nonlinear time series models called artificial neural networks (ANNs) are employed to predict fluctuations in economic activity in selected members (Armenia, Azerbaijan, Georgia, Kazakhstan, and Kyrgyzstan) of the Commonwealth of Independent States (CIS) using macroeconomic time series [treasury bill rate (T-bill), long term bond rate (BondLT), money supply (MS), industrial production (IP), spread (10-year treasury bond rate less 3-month treasury bill rate), BRTB (bank rate less 3-month treasury bill rate), and GDP growth rate]. Forecasting recessions being very important though challenging, recessions in the selected countries are modeled recursively 1-10 quarters ahead out-of-sample using ANNs in conjunction with macroeconomic time series for all the countries. The out-of-sample forecast results show that in general no single macroeconomic variable employed appears to be useful for predicting recessions in any of the series. However, for Armenia, the treasury bill rate, industrial production, money supply, and the spread (the yield curve) are candidate variables for predicting recessions 1-10 quarters ahead. For Georgia, Kazakhstan, and Kyrgyzstan, the treasury bill rate and money supply series are candidate variables for predicting recessions 1-10 quarters ahead. Reprinted by permission of Springer",,2011,10.1007/s10614-010-9233-z,,proquest,"This study uses artificial neural networks (ANNs) to forecast economic activity and recessions in Armenia, Azerbaijan, Georgia, Kazakhstan, and Kyrgyzstan. Macroeconomic time series such as T-bill rates, bond rates, money supply, industrial production, and GDP growth were used. The results suggest that while no single variable is universally predictive, specific variables like the treasury bill rate and money supply show potential for forecasting recessions in certain countries.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:49:43.427259
cff0be28d8a28bbf,Fluoride-treated water and the problem of merit goods,"This paper inquires into the fluoride treatment of community water in the United States to determine why and how conflicts in the production, consumption, and distribution of merit goods arise and are resolved. Primary and secondary data were employed to analyze statewide and municipality-level fluoridation initiatives in one key “battleground” state. We find that obstacles to successful fluoridation include a unidimensional policy space, high risk premia assigned by the affected population to health and environmental hazards, concerns over government interference with personal health choices, perceived adequacy of fluoride sources, “customer bundling,” and lack of a critical middle ground for consensus-building. The accessibility and social desirability of merit goods, like fluoridated water, cannot therefore be considered as value-free choices. How consumer demand is expressed, how fluoridation costs and benefits are estimated, how conflicts over its provision and production are resolved, and how the merits of science-based policies can be equally recast in terms of their presumed demerits require serious attention on the part of decision-makers in formulating and implementing health promotion policies.",,2011,10.2166/wp.2010.127,,proquest,"This paper examines the challenges and conflicts associated with community water fluoridation in the United States, analyzing it as a case study for understanding merit goods. It identifies obstacles such as limited policy options, public perception of risks, concerns about government intervention, and difficulties in consensus-building. The study suggests that the provision of merit goods like fluoridated water is not a neutral choice and requires careful consideration of consumer demand, cost-benefit analysis, and the framing of science-based policies.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:49:45.060749
56a900a32f4d205c,Forecasting NFT coin prices using machine learning: Insights into feature significance and portfolio strategies,"With the rise in popularity of Non-Fungible Tokens (NFTs), the demand for NFT coins has also surged. NFT coins are cryptocurrencies that facilitate NFT ecosystems by supporting NFT trading and platform governance. Accurate price predictions of NFT coins are crucial for risk managing volatility and constructing optimal portfolios. This study employs machine learning techniques to predict the daily price direction of four key NFT coins, namely ENJ, MANA, THETA, and XTZ. The machine learning methods employed include three decision tree-based methods (random forests, extremely randomized trees, XGBoost), support vector machine, Lasso and Naïve Bayes. The findings show that random forests, extremely randomized trees, XGBoost, and support vector machine models have accuracy ranging between 80% and 90% for predictions in the 14 to 21 day range. This adds to the literature showing that machine learning methods have high prediction accuracy for cryptocurrency prices. Conversely, Lasso or Naïve Bayes models yield considerably lower prediction accuracy. Feature importance is assessed using Shapley values. The Shapley value feature importance calculated from random forests highlights that, for 14 and 21-day forecasts, four variables - five-year expected inflation, ten-year bond yields, the interest rate spread, and on balance volume - are consistently highly ranked across all NFT coins. Additionally, the MA50, MA200, and WAD also emerge as important features. These results highlight the importance of including macroeconomic variables which capture business cycle conditions and technical analysis indicators that capture investor psychology as features. NFT coin portfolios constructed using trading signals generated from Extra Trees outperforms a buy and hold portfolio. Extra Trees are easy and fast to implement and investors not making use of this information are likely making sub-optimal investment decisions. © 2023 Elsevier B.V., All rights reserved.","Henriques, I.; Sadorsky, P.",2023,10.1016/j.gfj.2023.100904,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174468022&doi=10.1016%2Fj.gfj.2023.100904&partnerID=40&md5=4aa2aaf1db2dce93e2690576c3902f88,scopus,"This study uses machine learning (ML) models like random forests, extremely randomized trees, XGBoost, and support vector machines to predict the daily price direction of four NFT coins (ENJ, MANA, THETA, XTZ) with 80-90% accuracy for 14-21 day forecasts. Macroeconomic variables (inflation, bond yields, interest rate spread) and technical indicators (on balance volume, moving averages, WAD) are identified as significant features. ML-based trading signals from Extra Trees outperform a buy-and-hold strategy, suggesting potential for improved investment decisions.",True,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:50:02.505488
6e5161b9d8618c33,Forecasting cryptocurrencies returns: Do macroeconomic and financial variables improve tail expectation predictions?,"This study aims to jointly predict conditional quantiles and tail expectations for the returns of the most popular cryptocurrencies (Bitcoin, Ethereum, Ripple, Dogecoin and Litecoin) using financial and macroeconomic indicators as explanatory variables. We adopt a Monotone Composite Quantile Regression Neural Network (MCQRNN) model to make one- and five-steps-ahead predictions of Value-at-Risk (VaR) and Expected Shortfall (ES) based on a rolling window and compare the performance of our model against the Historical simulation and the standard ARMA(1,1)-GARCH(1,1) model used as benchmarks. The superior set of models is then chosen by backtesting VaR and ES using a Model Confidence Set procedure. Our results show that the MCQRNN performs better than both benchmark models for jointly predicting VaR and ES when considering daily data. Models with the implied volatility index, treasury yield spread and inflation expectations sharpen the extreme return predictions. The results are consistent for the two risk measures at the 1% and 5% level both, in the case of a long and short position and for all cryptocurrencies.",,2024,10.1007/s11135-023-01761-1,,proquest,"This study uses a Monotone Composite Quantile Regression Neural Network (MCQRNN) to forecast Value-at-Risk (VaR) and Expected Shortfall (ES) for cryptocurrency returns (Bitcoin, Ethereum, Ripple, Dogecoin, and Litecoin). It incorporates financial and macroeconomic indicators and compares the MCQRNN model against historical simulation and ARMA-GARCH benchmarks. The findings indicate that the MCQRNN model outperforms benchmarks for predicting VaR and ES, with implied volatility index, treasury yield spread, and inflation expectations improving tail return predictions.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:50:49.359710
43890536593eb51e,Forecasting day-ahead expected shortfall on the EUR/USD exchange rate: The (I)relevance of implied volatility,"The existing literature provides mixed results on the usefulness of implied volatility for managing risky assets, while evidence for expected shortfall predictions is almost nonexistent. Given its forward-looking nature, implied volatility might be more valuable than backward-looking measures of realized price fluctuations. Conversely, the volatility risk premium embedded in implied volatility leads to overestimating the observed price variation. This paper explores the benefits of augmenting econometric models used in forecasting the expected shortfall, a risk measured endorsed in the Basel III Accord, with information on implied volatility obtained from EUR/USD option contracts. The day-ahead forecasts are obtained from several classes of econometric models: historical simulation, EGARCH, quantile regression-based HAR, joint VaR and ES model, and combination forecasts. We verify whether the resulting expected shortfall forecasts are well-specified and test the models' accuracy. Our results provide evidence that the information provided by forward-looking implied volatility is more valuable than that in backward-looking realized measures. These results hold across multiple model specifications, are stable over time, hold under alternative loss functions, and are more pronounced during periods of higher market uncertainty when risk modeling matters most. (c) 2023 International Institute of Forecasters. Published by Elsevier B.V. All rights reserved.","Lyocsa, Stefan; Plihal, Tomas; Vyrost, Tomas",2024,10.1016/j.ijforecast.2023.11.003,,wos,"This paper investigates the usefulness of implied volatility for forecasting day-ahead expected shortfall on the EUR/USD exchange rate. It compares various econometric models, including historical simulation, EGARCH, quantile regression-based HAR, and a joint VaR and ES model, augmented with implied volatility information. The findings suggest that implied volatility, despite its potential drawbacks, provides valuable forward-looking information that improves expected shortfall forecasts, especially during periods of high market uncertainty.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:50:53.716042
bd6374063056c122,Forecasting equity risk premium: The role of investor concern on oil price volatility,"We explore the impact of investor concern to oil price volatility on Chinese equity risk premium by constructing an investor oil price volatility concern index (IOPVC) using Baidu Index. Our analysis demonstrates that IOPVC is a strong predictor of equity risk premium, with a negative correlation to future returns. Robustness checks, including the use of different data sources, various prediction windows, and differing levels of risk aversion, confirm that the relationship between IOPVC and equity risk premium is reliable. Furthermore, our analysis reveals that IOPVC is a leading indicator of forthcoming economic scenarios and exerts a significant influence on investor risk aversion.","Li, Dakai",2025,10.1016/j.ribaf.2025.102990,,wos,This study investigates the relationship between investor concern about oil price volatility and the Chinese equity risk premium. The authors construct an index (IOPVC) using Baidu Index data and find it to be a significant negative predictor of future equity returns. The findings are robust across various checks and suggest that IOPVC also acts as a leading indicator for economic conditions and influences investor risk aversion.,False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:50:57.328551
b6b619d5ca55a0f5,Forecasting exchange rate: A bibliometric and content analysis,"The study aims to present a systematic overview of the research in the field of exchange rate projection models through bibliometric techniques and content analysis. First, 775 articles published in journals within the scope of the international Web of Science database from 1966 to May 2021 were analyzed through bibliometric techniques. Second, a selected sample of 69 articles was analyzed through a detail content analysis to identify hot topics and new avenues of interest in the field. The research findings suggest that the scientific production on the subject is in wide development. New approaches have been incorporated, such as neural networks, requiring a broad perspective by the researcher in the evaluation of the empirical results.","Vasconcelos, Camila de Souza; Hadad Jr, Eli",2023,10.1016/j.iref.2022.09.006,,wos,"This study provides a bibliometric and content analysis of research on exchange rate forecasting models, examining 775 articles from 1966-2021. It identifies emerging trends like neural networks and emphasizes the need for a broad perspective in evaluating empirical results.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:51:01.811827
a8145ad99ef09055,Forecasting foreign exchange rates with an intrinsically nonlinear dynamic speed of adjustment model,"Forecasting foreign exchange rates is an important but difficult process; therefore, it is important to use a superior forecasting model. The paper takes up this criterion and proposes to describe and forecast foreign exchange rates by developing an intrinsically nonlinear model with variable and dynamic speeds of adjustment. It is found that the speed of adjusting the random (or expected) to the equilibrium rate is very slow, implying that fiscal policy (statistically insignificat) and monetary policy (statistically significant) may be ineffective to induce changes in the adjustment speed. We also find that the nonlinear dynamic model improves forecasting performance, implying that nonlinearities in the sense of functional forms are exploitable for improved point forecasting of foreign exchange rates.","Lin, WT; Chen, YH",1998,10.1080/000368498325822,,wos,"This paper proposes a nonlinear dynamic model with variable adjustment speeds to forecast foreign exchange rates. The model suggests that the adjustment to equilibrium is slow, and fiscal policy is insignificant while monetary policy is significant in influencing adjustment speed. The nonlinear model demonstrates improved forecasting performance, indicating that exploitable nonlinearities exist for better point forecasting.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:51:04.708491
5c20933515d8c65b,Forecasting gold price using machine learning methodologies,"This study investigates the potential of advanced Machine Learning (ML) methodologies to predict fluctuations in the price of gold. The study employs data from leading global stock indices, the S&P500 VIX volatility index, major commodity futures, and 10-year bond yields from the US, Germany, France, and Japan. Lagged values of these features up to 10 previous days are also used. Four machine learning models are used: Random Forest, Gradient Boosted Regression Trees (GBRT), and Extreme Gradient Boosting (XGBoost), to forecast future gold prices. The study finds that the most influential stocks indices for prediction are one-day lagged data of ASX, S&P500, TA35, IBEX, and AEX, as well as U.S. and Japan bonds yields and delayed data of gas and silver. Furthermore, the study's models identify that one-day lagged VIX score and our VIX dummy variable have a significant impact on gold price, indicating that economic uncertainty affects gold prices. The results suggest that incorporating various financial indicators and moving averages can be a powerful tool for predicting future gold prices. GBRT and XGBoost can be valuable models for making informed decisions about gold investments. © 2023 Elsevier B.V., All rights reserved.","Cohen, G.; Aiche, A.",2023,10.1016/j.chaos.2023.114079,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171768860&doi=10.1016%2Fj.chaos.2023.114079&partnerID=40&md5=242166c0eb01ccf4ae2e69b83f662f2b,scopus,"This study explores the use of Random Forest, GBRT, and XGBoost machine learning models to forecast gold prices, utilizing data from global stock indices, the VIX, commodity futures, and bond yields. It identifies key influential factors such as lagged stock indices, bond yields, and VIX scores, suggesting that economic uncertainty impacts gold prices. The findings indicate that GBRT and XGBoost are valuable for gold investment predictions.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T11:51:23.893107
8459fc80a83ccbd0,Forecasting nonlinear green bond yields in China: Deep learning for improved accuracy and policy awareness,"This study develops a convolutional neural network bidirectional long short-term memory model with an attention mechanism to forecast yields in China's green bond market. The model incorporates macroeconomic indicators, financial variables, policy factors, and issuer heterogeneity to enhance predictive accuracy. Empirical results show the model outperforms traditional approaches in point forecasting. It also offers superior robustness under identical confidence levels, increasing its utility for risk management and policy assessment in green finance. It is a practical tool for regulators, investors, and issuers. © 2025 Elsevier B.V., All rights reserved.","Wang, L.; Wang, Y.; Wang, J.; Yu, L.",2025,10.1016/j.frl.2025.107889,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009696841&doi=10.1016%2Fj.frl.2025.107889&partnerID=40&md5=3f3d8b8a179e9671678fa81f21beab95,scopus,"This study utilizes a deep learning model (CNN-BiLSTM with attention) to forecast yields in China's green bond market, incorporating macroeconomic, financial, policy, and issuer-specific factors. The model demonstrates superior accuracy and robustness compared to traditional methods, offering a practical tool for risk management and policy assessment in green finance.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:51:35.310321
264f50bc1957303c,Forecasting stock market returns with a lottery index: Evidence from China,"This study constructs a Chinese lottery index (LI) based on six popular lottery preference variables by using the partial least squares method and examines the relationship between the LI and future stock market returns during the period from January 2000 to December 2021. We find that the LI can negatively predict stock market excess returns in‐sample and out‐of‐sample. In addition, the LI can generate a large economic gain for a mean–variance investor. Finally, the predictive sources of the LI stem from a cash flow channel and can be explained by the positive volume–volatility relationship and investor attention.",,2024,10.1002/for.3100,,proquest,"This study develops a Chinese lottery index (LI) using lottery preference variables and the partial least squares method. The LI is found to negatively predict future stock market excess returns, offering economic gains for investors. The predictive power is attributed to cash flow, the volume-volatility relationship, and investor attention.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:51:58.753479
4300334f009b5fe1,"Fractional Black-Scholes option pricing, volatility calibration and implied Hurst exponents in South African context","Background: Contingent claims on underlying assets are typically priced under a framework that assumes, inter alia, that the log returns of the underlying asset are normally distributed. However, many researchers have shown that this assumption is violated in practice. Such violations include the statistical properties of heavy tails, volatility clustering, leptokurtosis and long memory. This paper considers the pricing of contingent claims when the underlying is assumed to display long memory, an issue that has heretofore not received much attention.Aim: We address several theoretical and practical issues in option pricing and implied volatility calibration in a fractional Black-Scholes market. We introduce a novel eight-parameter fractional Black-Scholes-inspired (FBSI) model for the implied volatility surface, and consider in depth the issue of calibration. One of the main benefits of such a model is that it allows one to decompose implied volatility into an independent long-memory component - captured by an implied Hurst exponent - and a conditional implied volatility component. Such a decomposition has useful applications in the areas of derivatives trading, risk management, delta hedging and dynamic asset allocation.Setting: The proposed FBSI volatility model is calibrated to South African equity index options data as well as South African Rand/American Dollar currency options data. However, given the focus on the theoretical development of the model, the results in this paper are applicable across all financial markets.Methods: The FBSI model essentially combines a deterministic function form of the 1-year implied volatility skew with a separate deterministic function for the implied Hurst exponent, thus allowing one to model both observed implied volatility surfaces as well as decompose them into independent volatility and long-memory components respectively. Calibration of the model makes use of a quasi-explicit weighted least-squares optimisation routine.Results: It is shown that a fractional Black-Scholes model always admits a non-constant implied volatility term structure when the Hurst exponent is not 0.5, and that 1-year implied volatility is independent of the Hurst exponent and equivalent to fractional volatility. Furthermore, we show that the FBSI model fits the equity index implied volatility data very well but that a more flexible Hurst exponent parameterisation is required to fit accurately the currency implied volatility data.Conclusion: The FBSI model is an arbitrage-free deterministic volatility model that can accurately model equity index implied volatility. It also provides one with an estimate of the implied Hurst exponent, which could be very useful in derivatives trading and delta hedging.",,2017,10.4102/sajems.v20i1.1532,,proquest,"This paper introduces a novel eight-parameter fractional Black-Scholes-inspired (FBSI) model for implied volatility surface, which decomposes implied volatility into a long-memory component (implied Hurst exponent) and a conditional implied volatility component. The model is calibrated to South African equity index and currency options data. The FBSI model accurately models equity index implied volatility and provides an estimate of the implied Hurst exponent, useful for derivatives trading and hedging. However, a more flexible parameterization is needed for currency options data.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:52:12.088631
129b8cefa68fea23,Functional Autoregression for Sparsely Sampled Data,"We develop a hierarchical Gaussian process model for forecasting and inference of functional time series data. Unlike existing methods, our approach is especially suited for sparsely or irregularly sampled curves and for curves sampled with nonnegligible measurement error. The latent process is dynamically modeled as a functional autoregression (FAR) with Gaussian process innovations. We propose a fully nonparametric dynamic functional factor model for the dynamic innovation process, with broader applicability and improved computational efficiency over standard Gaussian process models. We prove finite-sample forecasting and interpolation optimality properties of the proposed model, which remain valid with the Gaussian assumption relaxed. An efficient Gibbs sampling algorithm is developed for estimation, inference, and forecasting, with extensions for FAR(p) models with model averaging over the lag p. Extensive simulations demonstrate substantial improvements in forecasting performance and recovery of the autoregressive surface over competing methods, especially under sparse designs. We apply the proposed methods to forecast nominal and real yield curves using daily U.S. data. Real yields are observed more sparsely than nominal yields, yet the proposed methods are highly competitive in both settings. Supplementary materials, including R code and the yield curve data, are available online. © 2019 Elsevier B.V., All rights reserved.","Kowal, D.R.; Matteson, D.S.; Ruppert, D.",2019,10.1080/07350015.2017.1279058,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019636100&doi=10.1080%2F07350015.2017.1279058&partnerID=40&md5=8ef118146d36427daf34c568a470d5cd,scopus,"This paper introduces a hierarchical Gaussian process model for functional time series forecasting and inference, particularly effective for sparsely or irregularly sampled data with measurement error. It utilizes a functional autoregression (FAR) with Gaussian process innovations and proposes a nonparametric dynamic functional factor model for enhanced applicability and efficiency. The model demonstrates finite-sample forecasting and interpolation optimality and is estimated using an efficient Gibbs sampling algorithm. Simulations show significant improvements in forecasting and autoregressive surface recovery, especially with sparse data. The method is applied to forecast U.S. nominal and real yield curves.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T11:52:31.425182
853f428121b4ba0c,Futures Open Interest and Speculative Pressure Dynamics via Bayesian Models of Long-Memory Count Processes,"In this work, we develop time series regression models for long-memory count processes based on the generalized linear Gegenbauer autoregressive moving average (GLGARMA) framework. We present a comprehensive Bayesian formulation that addresses both in-sample and out-of-sample forecasting within a broad class of generalized count time series regression models. The GLGARMA framework supports various count distributions, including Poisson, negative binomial, generalized Poisson, and double Poisson distributions, offering the flexibility to capture key empirical characteristics such as underdispersion, equidispersion, and overdispersion in the data. We connect the counting process to a time series regression framework through a link function, which is associated with a stochastic linear predictor incorporating the family of long-memory GARMA models. This linear predictor is central to the model's formulation, requiring careful specification of both the GLGARMA Bayesian likelihood and the resulting posterior distribution. To model the stochastic error terms driving the linear predictor, we explore two approaches: parameter-driven and observation-driven frameworks. For model estimation, we adopt a Bayesian approach under both frameworks, leveraging advanced sampling techniques, specifically the Riemann manifold Markov chain Monte Carlo (MCMC) methods implemented via R-Stan. To demonstrate the practical utility of our models, we conduct an empirical study of open interest dynamics in US Treasury Bond Futures. Our Bayesian models are used to forecast speculative pressure, a crucial concept for understanding market behavior and agent actions. The analysis includes 136 distinct time series from the US Commodity Futures Trading Commission (CFTC), encompassing futures-only and futures-and-options data across four US government-issued fixed-income securities. Our findings indicate that the proposed Bayesian GLGARMA models outperform existing state-of-the-art models in forecasting open interest and speculative pressure. These improvements in forecast accuracy directly enhance portfolio performance, underscoring the practical value of our approach for bond futures portfolio construction. This work advances both the methodology for modeling long-memory count processes and its application in financial econometrics, particularly in improving the forecasting of speculative pressure and its impact on investment strategies. © 2025 Elsevier B.V., All rights reserved.","Yan, H.; Peters, G.; Bagnarosa, G.; Chan, J.",2025,10.1002/for.70001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009700778&doi=10.1002%2Ffor.70001&partnerID=40&md5=72ceb973c96f77374b0a2198f1f0c781,scopus,"This paper introduces Bayesian time series regression models for long-memory count processes using the GLGARMA framework. It offers flexibility in count distributions and error term modeling. The models are applied to forecast speculative pressure in US Treasury Bond Futures using CFTC data, demonstrating superior performance over existing models and potential for improved portfolio construction.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T11:54:40.560398
31f65a4ef29b519a,Futures price modeling under exchange rate volatility and its multi-period semi-variance portfolio selection,"Considering the stochastic exchange rate, a four-factor futures model with the underling asset, convenience yield, instantaneous risk-free interest rate and exchange rate, is established. These processes follow jump-diffusion processes (Weiner process and Poisson process). The corresponding partial differential equation (PDE) with terminal boundary condition of the model is drawn. The general solution with parameters of the above PDE is derived. The parameters are estimated by using the weight least squares approach with historical data for special cases. For the objective of risk assessment, downside risk has impacted on the practitioner's view of risk apparently. Variance is substituted by semi-variance. Moreover, one period portfolio selection is extended to multi-period. A class of multi-period semi-variance model is formulated. A hybrid genetic algorithm, which makes use of the position displacement strategy of the particle swarm optimiser as a mutation operation, is applied to solve the multi-period semi-variance model. Finally, in order to demonstrate the effectiveness of the theoretical models and numerical methods, fuel futures in the Shanghai exchange market is selected to be an example. © 2010 Elsevier B.V., All rights reserved.","Yan, W.; Li, S.",2009,10.1080/00207720902985385,https://www.scopus.com/inward/record.uri?eid=2-s2.0-75649121144&doi=10.1080%2F00207720902985385&partnerID=40&md5=76b4efe53044da4e6519f0e46370cf28,scopus,"This paper develops a four-factor futures price model incorporating exchange rate volatility, jump-diffusion processes for asset price, convenience yield, interest rate, and exchange rate. It derives a general solution for the corresponding PDE and estimates parameters using weighted least squares. The study extends portfolio selection to multiple periods using a semi-variance objective and a hybrid genetic algorithm. The model's effectiveness is demonstrated with fuel futures data from the Shanghai exchange.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:54:52.739929
9c9a62d1d4915359,"GDP Forecasting: Machine Learning, Linear or Autoregression?","This paper compares the predictive power of different models to forecast the real U.S. GDP. Using quarterly data from 1976 to 2020, we find that the machine learning K-Nearest Neighbour (KNN) model captures the self-predictive ability of the U.S. GDP and performs better than traditional time series analysis. We explore the inclusion of predictors such as the yield curve, its latent factors, and a set of macroeconomic variables in order to increase the level of forecasting accuracy. The predictions result to be improved only when considering long forecast horizons. The use of machine learning algorithm provides additional guidance for data-driven decision making.","Maccarrone, Giovanni; Morelli, Giacomo; Spadaccini, Sara",2021,10.3389/frai.2021.757864,,wos,"This paper compares the predictive power of machine learning (KNN) and traditional time series models for forecasting real U.S. GDP using quarterly data from 1976-2020. The KNN model outperformed traditional methods, especially for long forecast horizons, and the inclusion of predictors like the yield curve and macroeconomic variables improved accuracy. The study suggests machine learning offers valuable guidance for data-driven decision-making.",True,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:55:08.877761
3495aca6d5866e34,Gaussian scale mixture model for estimating volatility as a function of economic factor,"In this paper the scale mixture of Gaussian distribution is used to model the stock return data in financial market. There are many volatility models and forecasting methods. Some of the models are Historical volatility models, Implied volatility models, Autoregressive Conditional Heteroskedasticity models, models based on Artificial Neural Network. All these models are direct models. In these models the influence of economic factors like price level uncertainty, riskless rate of interest, the equity risk premium and the ratio of expected profit to expected revenue for the economy are not taken into account. Here the volatility parameter 'σ' is treated as a function of an economic factor. The main economic factor considered is the ratio of expected profit to expected revenue. Economic ratio is assumed to follow the exponential distribution. The resultant distribution is fitted to Dow Jones Industrial Average (DJIA) data by estimating the parameters. It is observed that this mixture distribution is a better fit than the GARCH fit. © 2016 Elsevier B.V., All rights reserved.","Seethalakshmi, R.; Saavithri, V.; Vijayabanu, C.",2014,10.5829/idosi.wasj.2014.32.06.783,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975509067&doi=10.5829%2Fidosi.wasj.2014.32.06.783&partnerID=40&md5=0af32d633e8e0428f18b827b74333b0b,scopus,"This paper proposes a Gaussian scale mixture model to estimate stock return volatility, treating volatility as a function of economic factors, specifically the ratio of expected profit to expected revenue. The model is fitted to Dow Jones Industrial Average (DJIA) data and shows a better fit than the GARCH model.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:55:13.797462
07a6141c54a06ec2,Generalist CEOs and Credit Ratings*,"A recent trend is that firms prefer to hire generalist CEOs with transferable skills (across firms or industries) over hiring specialist CEOs, but the consequences of this trend are unclear. In this study, we examine whether credit rating agencies consider a CEO's general skills as a credit risk factor when assessing an entity's overall creditworthiness. We predict and find that generalist CEOs are associated with lower credit ratings, suggesting that the presence of generalist CEOs is a significant credit rating factor. We also find that generalist CEOs are likely to take on more risks, which leads to more volatile performance ex post, and our path analyses confirm default risk is a significant mediator between credit ratings and CEOs' general skills. Our results hold in the presence of additional controls (e.g., CEO characteristics and corporate governance), when applying different fixed‐effect models and different matching methods, and for a subsample with forced CEO turnover. We also find that the negative relationship is attenuated for R&D‐intensive firms and firms in competitive industries. Last, we provide evidence that firms with generalist CEOs face higher borrowing costs, such as bond yields and syndicated loan spreads. Overall, our results contribute to a growing literature on the costs and benefits of hiring generalist CEOs, by providing a full picture of why hiring a generalist CEO may benefit shareholders but also cause misalignments with bondholders' interests.",,2021,10.1111/1911-3846.12662,,proquest,"This study investigates whether credit rating agencies consider CEO generalist skills as a credit risk factor. The findings indicate that generalist CEOs are associated with lower credit ratings, potentially due to increased risk-taking and performance volatility. The research also suggests that firms with generalist CEOs face higher borrowing costs and that this relationship is moderated by firm characteristics like R&D intensity and industry competition. The study contributes to understanding the trade-offs of hiring generalist CEOs for shareholders versus bondholders.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:55:17.216323
abed275e49b577b6,Generalized Forecast Error Variance Decomposition for Linear and Nonlinear Multivariate Models,"We propose a new generalized forecast error variance decomposition with the attractive property that the proportions of the impact accounted for by innovations in each variable sum to unity. Our decomposition is based on the generalized impulse response function, and it can easily be obtained by simulation. The new decomposition is illustrated in an empirical application to US output growth and interest rate spread data.","Lanne, Markku; Nyberg, Henri",2016,10.1111/obes.12125,,wos,"This paper introduces a novel generalized forecast error variance decomposition for multivariate models, applicable to both linear and nonlinear cases. The decomposition ensures that the proportions of impact from each variable's innovations sum to one. It is derived from the generalized impulse response function and can be computed via simulation. The method is demonstrated using US output growth and interest rate spread data.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:55:47.983986
246c475ec932cfd5,Global market factors that impact Baltic Dry Index,"The Baltic Dry Index is used as a strategic tool by shipping companies to monitor the daily movement of freight rates for the transportation of bulk cargoes on predetermined routes for the different types of bulk carriers. Therefore, the management of shipping companies pays great attention to the factors that can contribute to the prediction of the price movement of the Baltic Dry Index. Main goal of this paper is to explore if stock market indices of United States of America (S&P 500 stock index) and China (Shanghai stock exchange Composite index), 10 Year bond yield, CRB index, WTI Crude oil and Gold as global market factors, but also as leading macroeconomic global indicators, have impact on movement of BDI. We explored period from January 1, 2003 to December 31, 2021, with monthly data for which the multiple linear regression method was used to analyse mentioned global market factors impact on BDI. The research found that the movement of S&P 500 and SSECI stock indices and CRB index had a positive impact on the movement of BDI, while the movement of Gold and WTI crude oil had negative impact on BDI for the observed period. The scientific contribution of this paper is manifested through observation and exploring relationship of mentioned global market factors with BDI, previous papers observed shorter time period and included macroeconomic indicators which are lagging, together with some global market factors. © 2023 Elsevier B.V., All rights reserved.","Pepur, P.; Peronja, I.; Laća, S.",2022,10.31217/p.36.2.8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144355072&doi=10.31217%2Fp.36.2.8&partnerID=40&md5=477b721368b0945e8012ecc7dd331876,scopus,"This paper investigates the impact of global market factors, including US and Chinese stock indices, bond yields, the CRB index, WTI crude oil, and gold, on the Baltic Dry Index (BDI) from 2003 to 2021. Using multiple linear regression, the study found that S&P 500, SSECI, and CRB index movements positively influenced the BDI, while gold and WTI crude oil had a negative impact. The research contributes by analyzing a longer time period and including global market factors alongside macroeconomic indicators.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:55:51.958650
03eff6df80dbcb50,Heartbeat classification based on single lead-II ECG using deep learning,"The analysis and processing of electrocardiogram (ECG) signals is a vital step in the diagnosis of cardiovascular disease. ECG offers a non-invasive and risk-free method for monitoring the electrical activity of the heart that can assist in predicting and diagnosing heart diseases. The manual interpretation of the ECG signals, however, can be challenging and time-consuming even for experts. Machine learning techniques are increasingly being utilized to support the research and development of automatic ECG classification, which has emerged as a prominent area of study. In this paper, we propose a deep neural network model with residual blocks (DNN-RB) to classify cardiac cycles into six ECG beat classes. The MIT-BIH dataset was used to validate the model resulting in a test accuracy of 99.51%, average sensitivity of 99.7%, and average specificity of 98.2%. The DNN-RB method has achieved higher accuracy than other state-of-the-art algorithms tested on the same dataset. The proposed method is effective in the automatic classification of ECG signals and can be used for both clinical and out-of-hospital monitoring and classification combined with a single-lead mobile ECG device. The method has also been integrated into a web application designed to accept digital ECG beats as input for analyses and to display diagnostic results.The analysis and processing of electrocardiogram (ECG) signals is a vital step in the diagnosis of cardiovascular disease. ECG offers a non-invasive and risk-free method for monitoring the electrical activity of the heart that can assist in predicting and diagnosing heart diseases. The manual interpretation of the ECG signals, however, can be challenging and time-consuming even for experts. Machine learning techniques are increasingly being utilized to support the research and development of automatic ECG classification, which has emerged as a prominent area of study. In this paper, we propose a deep neural network model with residual blocks (DNN-RB) to classify cardiac cycles into six ECG beat classes. The MIT-BIH dataset was used to validate the model resulting in a test accuracy of 99.51%, average sensitivity of 99.7%, and average specificity of 98.2%. The DNN-RB method has achieved higher accuracy than other state-of-the-art algorithms tested on the same dataset. The proposed method is effective in the automatic classification of ECG signals and can be used for both clinical and out-of-hospital monitoring and classification combined with a single-lead mobile ECG device. The method has also been integrated into a web application designed to accept digital ECG beats as input for analyses and to display diagnostic results.",,2023,10.1016/j.heliyon.2023.e17974,,proquest,"This paper proposes a deep neural network model with residual blocks (DNN-RB) for classifying cardiac cycles into six ECG beat classes using single-lead ECG signals. The model was validated on the MIT-BIH dataset, achieving high accuracy (99.51%), sensitivity (99.7%), and specificity (98.2%), outperforming other state-of-the-art algorithms. The method is suitable for clinical and out-of-hospital monitoring and has been integrated into a web application.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:56:02.311971
48a52c4662bbacda,"Heterogeneity, dividends and complex dynamics in asset pricing; 投资者异质性、分红与资产价格的复杂动态","A lot of empirical researches have shown that the dividend policy can affect investor trading behaviors and price volatility. Different types of Investors may response differently to the dividend policy. For instance, individual investors are insensitive to the dividend policy, while the institutional investors usually prefer to the stocks with stable dividend. Some studies have provided evidences that the unstable dividend may lead to great fluctuations of stock price. In this paper, we attempt to develop a theoretical model based on investor behaviors, and investigate the impacts of dividend policy on price dynamics. In real financial markets, investors are heterogeneous and have asymmetric information. They make trading decisions based on the information they observe and take different trading strategies. Some investors may buy the stock and care about the dividend. Some others may trade based on the fundamentals of the stock. They estimate the fundamental value and buy the stock when the price is below the fundamental value. In this paper, investors are assumed to be boundedly rational, and their behaviors are modeled with the heterogenous agent model. In the model, we consider two types of trading strategies, i. e., the fundamental strategy and the technical strategy. These strategies reflect the heterogeneous expectations of investors. Investors choose between the two strategies according to their performance. The better performance one strategy has, the higher probability the strategy is chosen. The strategy switching mechanism is expressed by the logit response function. It can also depict the dynamics of investor fractions in the market. The price is generated through the trading behaviors of investors. We model the price adjustment process based on the aggregate excess demand. When the aggregate excess demand is positive, the price increases, and vice versa. In Section 1, we establish the model and derive a nonlinear dynamical system, which can characterize the evolutionary dynamics of stock price and investor structure. As it shows, the dynamical system involves a variety of parameters such as investor risk appetites, the level of bounded rationality, market liquidity, trading costs, dividend, fundamental value, and risk-free rate. Using the analysis of dynamical system, we study the impacts of these factors on asset pricing and volatility. In Section 2, we investigate the equilibrium of the dynamical system with mathematical analysis. In the case with rational and homogeneous investors, the equilibrium price equals to the intrinsic value which is determined by dividends. Furthermore, we solve the equilibrium in the case with bounded rational and heterogeneous investors. The equilibrium price is a weighted average of the estimated fundamental value and the intrinsic value. When the estimated fundamental value is equal to the intrinsic value, the equilibrium price is the same as the rational equilibrium price. We study the difference between the estimated fundamental value and the intrinsic value, and analyze its impacts on asset pricing and investor structure. The fraction of fundamentalists in the market is positively correlated with the absolute difference between the estimated fundamental value and the intrinsic value. Moreover, by using the Schur-Cohn criterion, we solve the stability conditions for the equilibrium. The stability conditions are determined by the parameters such as market liquidity, risk appetites and the deviation of the fundamental value from the intrinsic value. To guarantee the stability conditions, the difference between the estimated fundamental value and the intrinsic value should be neither too great nor too small. In Section 3, the simulation analysis is implemented. Under different parameter settings, we investigate the stability region, the bifurcation diagram and the phase portrait. When the stability conditions are not satisfied, the periodic and chaotic dynamics can be observed. The results of this study can explain the empirical findings of previous literatures. If the dividend policy is unstable, the volatility of dividend is great. Then, according to our study, investors would have less demand for stocks, and fundamentalists would take a small proportion in the market. This reflects institutional investors’ preference for dividend policy. In addition, if the dividend policy is unstable, the estimated fundamental value might deviate greatly from the intrinsic value, and the stability conditions cannot be satisfied. As a result, the price might fluctuate greatly. Then, the market might collapse. The findings provide some significant implications for understanding the complex phenomena in financial markets, making dividends policy and implementing market regulations. When companies formulate dividend policies, they should consider investors′ fundamental valuations of stocks, and should not make dividends too low or too high. In addition, market regulators can take measures to promote the disclosure of information, guide investors to have a reasonable valuation of stocks, so as to reduce market instability. © 2024 Elsevier B.V., All rights reserved.","Qingbin, G.; Diao, D.",2024,10.13587/j.cnki.jieem.2024.03.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193751502&doi=10.13587%2Fj.cnki.jieem.2024.03.004&partnerID=40&md5=3e73a674019bd1a2b77639a59c3d526c,scopus,"This paper develops a theoretical model of asset pricing based on heterogeneous agents with bounded rationality. It investigates the impact of dividend policy on price dynamics, considering two trading strategies: fundamental and technical. The model incorporates factors like investor risk appetites, liquidity, and trading costs. Analysis of the resulting nonlinear dynamical system reveals that unstable dividend policies can lead to increased price volatility and potentially market collapse. The study suggests implications for dividend policy formulation and market regulation.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:56:06.953782
4f40b18a5eb1cbfb,Hoarding the herd: the convenience of productive stocks,"This paper investigates the convenience yield that emerges in markets with productive stocks. We isolate the economic fundamentals giving rise to the yield, and show how these map to the empirical convenience yield measure. A model for price dynamics is derived from an economic model for optimal stock levels. We show how the price process reduces to a simple non-linear first-order Markov process. The model is estimated for the Norwegian market for farmed salmon by Generalized Methods of Moments, where stock growth is approximated by sea-water temperature. Our estimation result supports the theorized role of stock growth as a convenience yield component. Our results are relevant for the functioning of futures markets for commodities such as fish and other animal production where systematic stock growth affects the term structure. © 2014 Wiley Periodicals, Inc. Jrl Fut Mark 35:679-694, 2015 Copyright John Wiley & Sons. Reproduced with permission. An electronic version of this article is available online at http://www.interscience.wiley.com",,2015,10.1002/fut.21679,,proquest,"This paper investigates the convenience yield in markets with productive stocks, deriving a model for price dynamics from an economic model of optimal stock levels. The model is estimated using data from the Norwegian farmed salmon market, where stock growth is approximated by sea-water temperature. The findings suggest that stock growth plays a role in the convenience yield and are relevant for futures markets in commodities like fish and other animal production.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:56:24.446696
983957a87777ab21,How Much Can Machines Learn Finance from Chinese Text Data?,"How much can we learn finance directly from text data? This paper presents a new framework for learning textual data based on the factor augmentation model and sparsity regularization, called the factor-augmented regularized model for prediction (FarmPredict), to let machines learn financial returns directly from news. FarmPredict allows the model itself to extract information directly from articles without predefined information, such as dictionaries or pretrained models as in most studies. Using unsupervised learned factors to augment the predictors would benefit our method with a ""doublerobust""feature: that the machine would learn to balance between individual words or text factors/topics. It also avoids the information loss of factor regression in dimensionality reduction. We apply our model to the Chinese stock market with a large proportion of retail investors by using Chinese news data to predict financial returns. We show that positive sentiments scored by our FarmPredict approach from news generate on average 83 basic points (bps) stock daily excess returns, and negative news has an adverse impact of 26 bps on the days of news announcements, where both effects can last for a few days. This asymmetric effect aligns well with the short-sale constraints in the Chinese equity market. The result shows that the machine-learned prediction does provide sizeable predictive power with an annualized return of 54% at most with a simple investment strategy. Compared with other statistical and machine learning methods, FarmPredict significantly outperforms them on model prediction and portfolio performance. Our study demonstrates the far-reaching potential of using machines to learn text data. © 2024 Elsevier B.V., All rights reserved.","Zhou, Y.; Fan, J.; Xue, L.",2024,10.1287/mnsc.2022.01468,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201264430&doi=10.1287%2Fmnsc.2022.01468&partnerID=40&md5=86523d9245d5c88bd1b6868500e9f558,scopus,"This paper introduces FarmPredict, a novel framework using factor augmentation and sparsity regularization to enable machines to learn financial returns directly from Chinese news text data. Unlike previous methods, FarmPredict extracts information without predefined dictionaries or models. The study demonstrates that positive news sentiment predicts an average of 83 bps daily excess stock returns, while negative news has an adverse impact of 26 bps, with effects lasting several days. The model achieves up to 54% annualized return with a simple strategy and outperforms other statistical and machine learning methods in prediction and portfolio performance, highlighting the potential of machine learning in text data analysis.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:56:42.972086
09389e7c8bffb0a4,How to Rate the Financial Performance of Private Companies? A Tailored Integrated Rating Methodology Applied to North-Eastern Italian Districts,"This paper contributes to solving the puzzle of assessing the financial performance of private/unlisted companies. The inner characteristics of these companies make the adoption of traditional best practices in estimating risk premia difficult or impossible. Moreover, the lack of market data and comparable information biases the perception of corporate performance and generates the misallocation of credit fundings (both quantities and pricing). Hence, in this paper, we develop an Integrated Rating Methodology (IRM) to estimate a more efficient corporate “return-to-risk” measure. Our IRM is rooted in the seminal “certainty equivalent” model as developed by Lintner in 1965, but we modify it using a shortfall approach, and then compute a “confident equivalent” that is compliant with Fischer Black’s zero-beta model as well as the Basel agreements. An empirical application of the approach is conducted with a sample of 13,583 non-financial SMEs in the north-east regions of Italy, where there is evidence of inefficient bank financing. We back-test our IRM by rating these companies using corporate financial data during the period 2007–2014, which encompasses both the Great Financial Crisis and the European sovereign debt crisis. Our empirical results depict a clear crowding-out effect of credit allocations when we compare our IRM scoring measure with the actual raising ability and the cost of capital relating to these firms. We find that 36% of companies are underfunded, even if they have a superior IRM score, while 27% of them are funded without merit. Interestingly, this last figure is in line with the average non-performing loan ratio provided by official Italian statistics from 2015 to 2020. Therefore, we conclude that our IRM methodology is promising and may be better at estimating risk financing in small private companies (including start-ups) than internal banking models. These initial results will drive our forthcoming research towards creating an IRM 2.0.",,2022,10.3390/jrfm15110493,,proquest,"This paper introduces an Integrated Rating Methodology (IRM) to assess the financial performance of private companies, addressing the challenges posed by limited market data and traditional model limitations. The IRM modifies the certainty equivalent model with a shortfall approach and aligns with Black's zero-beta model and Basel agreements. An empirical application to 13,583 Italian SMEs from 2007-2014 reveals significant misallocations in credit funding, with underfunded superior-scoring companies and overfunded less deserving ones. The study suggests IRM is a promising tool for risk financing in private companies and start-ups.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:56:48.285169
72c1d3b4cb2ecc1d,How to fly to safety without overpaying for the ticket,"For most active investors treasury bonds (govs) provide diversification and thus reduce the risk of a portfolio. These features of govs become particularly desirable in times of elevated risk which materialize in the form of the flight-To-safety (FTS) phenomenon. The FTS for govs provides a shelter during market turbulence and is exceptionally beneficial for portfolio drawdown risk reduction. However, what if the unsatisfactory expected return from treasuries discourages higher bonds allocations? This research proposes a solution to this problem with Deep Target Volatility Equity-Bond Allocation (DTVEBA) that dynamically allocate portfolios between equity and treasuries. The strategy is driven by a state-of-The-Art recurrent neural network (RNN) that predicts next-day market volatility. An analysis conducted over a twelve year out-of-sample period found that with DTVEBA an investor may reduce treasury allocation by two (three) times to get the same Sharpe (Calmar) ratio and overper-forms the S&P500 index by 43% (115%). © 2023 Elsevier B.V., All rights reserved.","Kaczmarek, T.; Grobelny, P.",2023,10.18559/ebr.2023.2.738,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168003715&doi=10.18559%2Febr.2023.2.738&partnerID=40&md5=234075bf4afac4ab4e8170ed913b55f1,scopus,"This research introduces Deep Target Volatility Equity-Bond Allocation (DTVEBA), a dynamic portfolio allocation strategy between equities and treasuries. It utilizes a recurrent neural network (RNN) to predict market volatility, aiming to improve portfolio risk reduction and returns, outperforming the S&P500 index.",True,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:57:02.312582
9122c80b394b9c20,Impact of China’s Provincial Government Debt on Economic Growth and Sustainable Development,"Macroeconomic stability is the core concept of sustainable development. However, the coronavirus disease (COVID-19) pandemic has caused government debt problems worldwide. In this context, it is of practical significance to study the impact of government debt on economic growth and fluctuations. Based on panel data of 30 provinces in China from 2012 to 2019, we used the Mann–Kendall method and Kernel Density estimation to analyze the temporal and spatial evolution of China’s provincial government debt ratio and adopted a panel model and HP filtering method to study the impact of provincial government debt on economic growth and fluctuation. Our findings indicate that, during the sample period, China’s provincial government debt promoted economic growth and the regression coefficient (0.024) was significant. From different regional perspectives, the promotion effect of the central region (0.027) is higher than that of the eastern (0.020) and western regions (0.023). There is a nonlinear relationship between China’s provincial government debt and economic growth, showing an inverted “U-shaped” curve. Fluctuations in government debt aggravate economic volatility, with a coefficient of 0.009; tax burden fluctuation and population growth rate aggravate economic changes. In contrast, the optimization of the province’s industrial structure and the improvement of the opening level of provinces slow down economic fluctuations.",,2022,10.3390/su14031474,,proquest,"This study analyzes the impact of China's provincial government debt on economic growth and fluctuations using panel data from 30 provinces between 2012 and 2019. The findings suggest that government debt promoted economic growth, with a nonlinear, inverted U-shaped relationship. Debt fluctuations were found to aggravate economic volatility, while industrial structure optimization and opening-up levels helped slow it down. The study employed the Mann–Kendall method, Kernel Density estimation, a panel model, and HP filtering.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:57:11.814802
b1f77f14c23e3395,Impact of Wind Electricity Forecasts on Bidding Strategies,"The change in the generation mix from conventional electricity sources to renewables has important implications for bidding behaviour and may have an impact on prices. The main goal of this work is to discover the role played by expected wind production, together with other relevant factors, in explaining the day-ahead market price through a data panel model. The Spanish market, given the huge increase in wind generation observed in the last decade, has been chosen for this study as a paradigmatic example. The results obtained suggest that wind power forecasts are a new key determinant for supply market participants when bidding in the day-ahead market. We also provide a conservative quantification of the effect of such trading strategies on marginal prices at an hourly level for a specific year in the sample. The consequence has been an increase in marginal price to levels higher than what could be expected in a context with notable wind penetration. Therefore, the findings of this work are of interest to practitioners and regulators and support the existence of a wind risk premium embedded in electricity prices to compensate for the uncertainty of wind production.","Ballester, Cristina; Furio, Dolores",2017,10.3390/su9081318,,wos,"This study investigates the impact of wind electricity forecasts on bidding strategies in the Spanish day-ahead electricity market. Using a data panel model, it finds that wind power forecasts are a significant factor for market participants' bidding behavior, leading to higher marginal prices due to a 'wind risk premium' that compensates for production uncertainty.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:57:15.070620
7f362baf207aa52c,Implementation Tests of Financial Market Analysis by Text Mining,"In this study, we propose a new text-mining method for long-term market analysis. Using our method, we performe out-of-sample tests using monthly price data of financial markets; Japanese government bond market, Japanese stock market, and the yen-dollar market. First we extract feature vectors from monthly reports of Bank of Japan. Then, trends of each market are estimated by regression analysis using the feature vectors. As a result of comparison with support vector regression, the proposal method could forecast in higher accuracy about both the level and direction of long-term market trends. Moreover, our method showed high returns with annual rate averages as a result of the implementation test.",,2011,10.1527/tjsai.26.313,,proquest,"This study introduces a novel text-mining approach for long-term financial market analysis, specifically applied to the Japanese government bond market, stock market, and yen-dollar market. The method involves extracting feature vectors from Bank of Japan reports and using regression analysis to estimate market trends. The proposed method demonstrated higher accuracy in forecasting long-term trends compared to support vector regression and yielded high returns in implementation tests.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T11:57:38.594591
2bf3df700a0b5572,Implied Filtering Densities on the Hidden State of Stochastic Volatility,"Abstract: We formulate and analyse an inverse problem using derivative prices to obtain an implied filtering density on volatility’s hidden state. Stochastic volatility is the unobserved state in a hidden Markov model (HMM) and can be tracked using Bayesian filtering. However, derivative data can be considered as conditional expectations that are already observed in the market, and which can be used as input to an inverse problem whose solution is an implied conditional density on volatility. Our analysis relies on a specification of the martingale change of measure, which we refer to as separability. This specification has a multiplicative component that behaves like a risk premium on volatility uncertainty in the market. When applied to SPX options data, the estimated model and implied densities produce variance-swap rates that are consistent with the VIX volatility index. The implied densities are relatively stable over time and pick up some of the monthly effects that occur due to the options’ expiration, indicating that the volatility-uncertainty premium could experience cyclic effects due to the maturity date of the options. © 2015 Elsevier B.V., All rights reserved.","Fuertes, C.; Papanicolaou, A.",2014,10.1080/1350486x.2014.891357,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926101307&doi=10.1080%2F1350486X.2014.891357&partnerID=40&md5=3c8204565e0f0de1bea8624f18a14859,scopus,"This paper presents an inverse problem using derivative prices to infer an implied filtering density on the hidden state of stochastic volatility. It utilizes a hidden Markov model (HMM) and Bayesian filtering, treating derivative data as observed conditional expectations. The model, when applied to SPX options data, yields variance-swap rates consistent with the VIX index and shows stable implied densities that capture monthly effects related to option expiration, suggesting cyclic effects in the volatility-uncertainty premium.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:57:57.596263
5f39172024a62f24,Improving CAT bond pricing models via machine learning,"Enhanced machine learning methods provide an encouraging alternative to forecast asset prices by extending or generalizing the possible model specifications compared to conventional linear regression methods. Even if enhanced methods of machine learning in the literature often lead to better forecasting quality, this is not clear for small asset classes, because in small asset classes enhanced machine learning methods may potentially over-fit the in-sample data. Against this background, we compare the forecasting performance of linear regression models and enhanced machine learning methods in the market for catastrophe (CAT) bonds. We use linear regression with variable selection, penalization methods, random forests and neural networks to forecast CAT bond premia. Among the considered models, random forests exhibit the highest forecasting performance, followed by linear regression models and neural networks. © 2020 Elsevier B.V., All rights reserved.","Götze, T.; Gürtler, M.; Witowski, E.",2020,10.1057/s41260-020-00167-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089556740&doi=10.1057%2Fs41260-020-00167-0&partnerID=40&md5=43b235afab54d4b10b4bcae23c45d2d9,scopus,"This study compares the forecasting performance of linear regression models and enhanced machine learning methods (random forests, neural networks) in the market for catastrophe (CAT) bonds. Random forests showed the best forecasting performance, outperforming linear regression and neural networks.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T11:58:33.819621
8a92241a88c5f917,Improving Quality of Long-Term Bond Price Prediction Using Artificial Neural Networks,"Purpose: The aim of this paper is to propose nonlinear autoregressive neural network which can improve quality of bond price forecasting.Methodology/Approach: Due to the complex nature of market information that influence bonds, artificial intelligence could be accurate, robust and fast choice of bond price prediction method.Findings: Our results have reached a coefficient of determination higher than 95% in the training, validation and testing sets. Moreover, we proposed the nonlinear autoregressive network with external inputs using 50 year interest-rate swaps denominated in EUR and volatility index VIX as two external variables.Research Limitation/Implication: Our sample of daily prices between 4th January 2016 and 13th January 2021 (totally 1,270 trading days) suggest that both Levenberg-Marquardt and Scaled conjugate gradient learning algorithms achieved excellent results.Originality/Value of paper: Despite the fact that both learning algorithms achieved satisfying outcomes, implementation of an independent variable into the autoregressive neural network environment had no significant impact on prediction ability of the model.Category: Research paper","Verner, Robert; Tkac, Michal, Sr.; Tkac, Michal, Jr.",2021,10.12776/qip.v25i1.1532,,wos,"This paper proposes a nonlinear autoregressive neural network to improve long-term bond price forecasting, achieving a coefficient of determination over 95%. The study uses daily bond prices from 2016-2021 and explores the impact of external variables like interest-rate swaps and the VIX index, finding no significant improvement from their inclusion.",True,False,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T12:10:49.293625
a28fca24b7615f52,Improving credit risk assessment in P2P lending with explainable machine learning survival analysis,"Recent research using explainable machine learning survival analysis demonstrated its ability to identify new risk factors in the medical field. In this study, we adapted this methodology to credit risk assessment. We used a comprehensive dataset from the Estonian P2P lending platform Bondora, consisting of over 350,000 loans and 112 features with a loan volume of 915 million euros. First, we applied classical (linear) and machine learning (extreme gradient-boosted) Cox models to estimate the risk of these loans and then risk-rated them using risk stratification. For each rating category we calculated default rates, rates of return, and plotted Kaplan–Meier curves. These performance criteria revealed that the boosted Cox model outperformed both the classical Cox model and the platform’s rating. For instance, the boosted model’s highest rating category had an annual excess return of 18% and a lower default rate compared to the platform’s best rating. Second, we explained the machine learning model’s output using Shapley Additive Explanations. This analysis revealed novel nonlinear relationships (e.g., higher risk for borrowers over age 55) and interaction effects (e.g., between age and housing situation) that provide promising avenues for future research. The machine-learning model also found feature contributions aligning with existing research, such as lower default risk associated with older borrowers, females, individuals with mortgages, or those with higher education. Overall, our results reveal that explainable machine learning survival analysis excels at risk rating, profit scoring, and risk factor analysis, facilitating more precise and transparent credit risk assessments. © 2024 Elsevier B.V., All rights reserved.","Bone-Winkel, G.F.; Reichenbach, F.",2024,10.1007/s42521-024-00114-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208063837&doi=10.1007%2Fs42521-024-00114-3&partnerID=40&md5=818beb0a3f30b92b63502ce3db0e3345,scopus,"This study applies explainable machine learning survival analysis, specifically boosted Cox models and Shapley Additive Explanations, to assess credit risk in P2P lending using a large dataset from the Bondora platform. The boosted model outperformed classical methods and the platform's rating in terms of default rates and returns. The explainable AI revealed novel risk factors and confirmed existing ones, leading to more precise and transparent credit risk assessments.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:10:55.873448
2a8b4f58949f786d,Indicator variables for inflation expectations in the Euro area,"In this paper, we model the Euro area market-based inflation expectations extracted from the inflation-linked swaps, and study the macroeconomic information embedded in expected inflation. First, we estimate the Gaussian affine term structure model to decompose the forward ILS-implied inflation rate into inflation expectations and inflation risk premium at one-, two- and three-year horizons. Secondly, from a large panel of macroeconomic series we identify the most significant indicator variables for inflation expectations using the elastic net modification of the LASSO regression. Finally, we measure partial contributions of individual indicator variables to the changes in inflation expectations. Our findings reveal that across horizons considered inflation expectations are correlated to the measures of current inflation of the overall price level and price level of services, the unemployment rate, and the Euro exchange rate. The identified indicators provide a useful information about the evolution of inflation expectations with different intensities at different horizons.",,2021,10.1504/ijse.2021.114615,,proquest,"This paper models Euro area market-based inflation expectations using inflation-linked swaps. It decomposes the implied inflation rate into expectations and risk premium, identifies key macroeconomic indicator variables for inflation expectations using elastic net regression, and measures their contributions to changes in expectations. Findings show correlations with current inflation, unemployment, and exchange rates.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:11:13.651628
cf9e82b6c1d97962,Industry bubbles and unexpected consumption shocks: A cross-sectional explanation of stock returns under recursive preferences,"Assuming an environment with rational and informed agents, where investors exhibit recursive preferences and make their economic decisions embedding industry bubbles into their information sets, we study to what extent unexpected consumption shocks can proxy for revisions in expected consumption growth and, consequently, explain the cross-sectional behavior of stock returns. Our results show that unexpected consumption shocks help forecast future consumption growth, allowing the Epstein-Zin model to satisfactorily explain the equity risk premium of different anomaly portfolios on the Tokyo Stock Exchange. Furthermore, our model provides a better understanding on the dynamics of consumption and its relationship to stock returns. © 2023 Elsevier B.V., All rights reserved.","Rojo-Suárez, J.; Alonso-Conde, A.B.; Lago-Balsalobre, R.",2024,10.1016/j.iref.2023.07.086,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169004311&doi=10.1016%2Fj.iref.2023.07.086&partnerID=40&md5=d9b4ad7092203f77787e79d262f307c7,scopus,"This study investigates how unexpected consumption shocks can explain the cross-sectional behavior of stock returns within the framework of the Epstein-Zin model, incorporating industry bubbles and recursive preferences. The findings suggest that these shocks are effective predictors of future consumption growth and can account for the equity risk premium across various anomaly portfolios on the Tokyo Stock Exchange, thereby enhancing the understanding of consumption-stock return dynamics.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:11:17.463420
dcf76bf9c66b1d50,Inference in asset pricing models with a low-variance factor,"This paper concerns with the effects of including a low-variance factor in an asset pricing model. When a low-variance factor is present, the commonly applied Fama-MacBeth two-pass regression procedure is very likely to yield misleading results. Local asymptotic analysis and simulation evidence indicate that the risk premiums corresponding to all factors are very likely to be unreliably estimated. Moreover, t- and F-statistics are less likely to detect whether the risk premiums are significantly different from zero. We recommend Kleibergen's (2009)FAR statistic when there is a low-variance factor included in an asset pricing model. All rights reserved, Elsevier",,2013,10.1016/j.jbankfin.2012.11.007,,proquest,"This paper investigates the impact of a low-variance factor on asset pricing models, highlighting how it can lead to unreliable risk premium estimations and reduced statistical power in detecting significant risk premiums. It suggests using Kleibergen's FAR statistic in such scenarios.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:11:22.726549
701ad234b9ed241c,Inflation bets or deflation hedges? the changing risks of nominal bonds,"The covariance between US Treasury bond returns and stock returns has moved considerably over time. While it was slightly positive on average in the period 1953 to 2014, it was unusually high in the early 1980s and negative in the early 21st Century, particularly in the downturns of 2001 and 2007 to 2009. This paper specifies and estimates a model in which the nominal term structure of interest rates is driven by four state variables: the real interest rate, temporary and permanent components of expected inflation, and the nominalreal covariance of inflation and the real interest rate with the real economy. The last of these state variables enables the model to fit the changing covariance of bond and stock returns. In the model, a high nominal-real covariance implies a high term premium and a concave yield curve. The decline in this covariance since the early 1980s has driven down our model-implied term premium on 10-year zero-coupon nominal Treasury bonds by about two percentage points. © 2018 Elsevier B.V., All rights reserved.","Campbell, J.Y.; Sunderam, A.; Viceira, L.M.",2017,10.1561/104.00000043,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019756733&doi=10.1561%2F104.00000043&partnerID=40&md5=d1ab4f1d0502d33a087d900282246a09,scopus,"This paper models the nominal term structure of interest rates using four state variables, including real interest rate, expected inflation components, and nominal-real covariance. It explains the changing covariance between US Treasury bond and stock returns over time, noting a decline in the term premium on 10-year nominal Treasury bonds since the early 1980s.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:12:04.196734
7cad7bb9373c17b7,Integrated methodology for estimating zero-coupon yield curves: Evidence from Turkish government nominal bonds,"This study estimates the zero-coupon yield curves for Turkish government nominal bonds from February 2005 to June 2022 using the Nelson–Siegel–Svensson parametric model. We implement a weighting scheme in the objective function, where squared pricing errors are weighted by the inverse of the square root of the bond duration. This weighting scheme strikes a better balance between the short- and long-maturity bonds during the optimization process. Moreover, by employing four nonlinear optimization algorithms and three parameter initialization approaches, we aim to prevent premature convergence to local optima and improve the quality of fit. Our integrated methodology yields reasonably low in-sample root mean squared error values for price errors and offers clear guidance and a framework for researchers in constructing zero-coupon yield curves. © 2025 Elsevier B.V., All rights reserved.","Paçcı, M.Ü.; Okay, N.",2025,10.1016/j.bir.2025.05.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006716439&doi=10.1016%2Fj.bir.2025.05.003&partnerID=40&md5=89e8caa6f97eab08251fe089078164ce,scopus,This study estimates zero-coupon yield curves for Turkish government nominal bonds from February 2005 to June 2022 using the Nelson–Siegel–Svensson parametric model. It incorporates a novel weighting scheme in the objective function and employs multiple optimization techniques to enhance the accuracy and robustness of the yield curve estimation. The methodology aims to provide a reliable framework for constructing zero-coupon yield curves.,False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:12:21.920025
6b96cc3b210ad231,Intelligent Optimization Based Multi-Factor Deep Learning Stock Selection Model and Quantitative Trading Strategy,"With the rapid development of financial research theory and artificial intelligence technology, quantitative investment has gradually entered people's attention. Compared with traditional investment, the advantage of quantitative investment lies in quantification and refinement. In quantitative investment technology, quantitative stock selection is the foundation. Without good stock selection ability, the effect of quantitative investment will be greatly reduced. Therefore, this paper builds an effective multi-factor stock selection model based on intelligent optimization algorithms and deep learning and proposes corresponding trading strategies based on this. First of all, this paper selects 26 effective factors of financial indicators, technical indicators and public opinion to construct the factor database. Secondly, a Gated Recurrent Unit (GRU) neural network based on the Cuckoo Search (CS) optimization algorithm is used to build a stock selection model. Finally, a quantitative investment strategy is designed, and the proposed multi-factor deep learning stock selection model based on intelligent optimization is applied to practice to test its effectiveness. The results show that the quantitative trading strategy based on this model achieved a Sharpe ratio of 127.08%, an annualized rate of return of 40.66%, an excess return of 13.13% and a maximum drawdown rate of -17.38% during the back test period. Compared with other benchmark models, the proposed stock selection model achieved better back test performance.","Wang, Jujie; Zhuang, Zhenzhen; Feng, Liu",2022,10.3390/math10040566,,wos,"This paper proposes a multi-factor stock selection model using a GRU neural network optimized by the Cuckoo Search algorithm. The model incorporates 26 factors from financial, technical, and public opinion data. A quantitative trading strategy based on this model demonstrated strong performance in backtesting, outperforming benchmark models.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:12:28.890370
04ea9823b9217dee,Intelligent forecasting in bitcoin markets,"This paper examines the effectiveness of Artificial Intelligence (AI) in predicting Bitcoin's price movements. To achieve this, we developed two distinct trading strategies and compared their performance against each other and the traditional Buy and Hold (B&H) strategy. Over the period from January 2018 to September 2023, we found that the strategy optimized by ChatGPT 01-Preview, which integrates multiple technical indicators and sentiment analysis into a weighted composite index, delivered an exceptional total return of 944.85 %. The second strategy, that is using Extreme Gradient Boosting (XGBoost) technique achieved a total return of 189.05 %. The AI strategy's excess return of 755.8 % over the XGBoost strategy highlights the significant advantage of AI particularly in utilizing diverse data sources, such as social media, to predict Bitcoin's price trends more effectively than relying solely on economic data. Both trading strategies significantly outperformed the traditional B&H strategy, which returned 73.08 % over the same period. Furthermore, we found that AI has an advantage during periods of high Bitcoin price volatility. © 2024 Elsevier B.V., All rights reserved.","Cohen, G.; Aiche, A.",2025,10.1016/j.frl.2024.106487,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209232540&doi=10.1016%2Fj.frl.2024.106487&partnerID=40&md5=73aba1a778a03fbc9a4a154ac9f78596,scopus,"This paper investigates the efficacy of AI in predicting Bitcoin price movements, comparing two AI-driven trading strategies (one using ChatGPT 01-Preview and the other XGBoost) against the Buy and Hold strategy. The ChatGPT-optimized strategy yielded a 944.85% return, significantly outperforming XGBoost (189.05%) and Buy and Hold (73.08%), especially during volatile periods and by integrating diverse data sources like social media.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:12:37.247806
835037460463ea4b,Intraday technical trading in the foreign exchange market,"This paper examines the out-of-sample performance of intraday technical trading strategies selected using two methodologies, a genetic program and an optimized linear forecasting model. When realistic transaction costs and trading hours are taken into account, we find no evidence of excess returns to the trading rules derived with either methodology. Thus, our results are consistent with market efficiency. We do find, however, that the trading rules discover some remarkably stable patterns in the data. © 2003 Elsevier Science Ltd. All rights reserved. © 2019 Elsevier B.V., All rights reserved.","Neely, C.J.; Weller, P.A.",2003,10.1016/s0261-5606(02)00101-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037375935&doi=10.1016%2FS0261-5606%2802%2900101-8&partnerID=40&md5=23d064c24d32aa4af4ee08497bfe753f,scopus,"This paper investigates the performance of intraday technical trading strategies in the foreign exchange market, using a genetic program and an optimized linear forecasting model. After accounting for transaction costs and trading hours, no excess returns were found, suggesting market efficiency. However, the trading rules did identify stable patterns in the data.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:12:46.425943
5f6fd09a9aa772ab,Investment Portfolio Allocation and Insurance Solvency: New Evidence from Insurance Groups in the Era of Solvency II,"This study examines the effect of the investment portfolio structure on insurers’ solvency, as measured by the Solvency Capital Requirement ratio. An empirical sample of 88 EU-based insurance groups was analyzed to provide robust evidence of the portfolio’s impact on the Solvency Capital Requirement ratio from 2016 to 2022. Linear regression and supervised machine learning models, particularly extra trees regression, were used to predict the solvency ratios, with the latter outperforming the former. The investigation was supplemented with panel data analysis. Firm-specific factors, including, unit-linked and index-linked liabilities, firm size, investments in property, collective undertakings, bonds and equities, and the ratio of government bonds to corporate bonds and country-specific factors, such as life and non-life market concentration, domestic bond market development, private debt development, household spending, banking concentration, non-performing loans, and CO<inf>2</inf> emissions, were found to have an important effect on insurers’ solvency ratios. The novelty of this research lies in the investigation of the connection of solvency ratios with variables that prior studies have not yet explored, such as portfolio asset allocation, the life and non-life insurance market concentration, and unit-linked and index-linked products, via the employment of a battery of traditional and machine enhanced methods. Furthermore, it identifies the relation of solvency ratios with bond market development and investments in collective undertakings. Finally, it addresses the substantial solvency risks posed by the high banking sector concentration to insurers under Solvency II. © 2024 Elsevier B.V., All rights reserved.","Poufinas, T.; Siopi, E.",2024,10.3390/risks12120191,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213010620&doi=10.3390%2Frisks12120191&partnerID=40&md5=89d1209f8fc21c4be96e743edb1fe259,scopus,"This study investigates how investment portfolio structure affects insurance solvency, measured by the Solvency Capital Requirement (SCR) ratio, using data from 88 EU insurance groups between 2016 and 2022. It employed linear regression and machine learning models, with extra trees regression showing superior predictive performance. Key factors influencing solvency included unit-linked and index-linked liabilities, firm size, investments in property, collective undertakings, bonds, equities, and the ratio of government to corporate bonds. Country-specific factors like market concentration, bond market development, household spending, banking concentration, non-performing loans, and CO2 emissions also played a role. The research highlights novel connections between solvency ratios and variables like portfolio asset allocation, market concentration, unit-linked/index-linked products, bond market development, and investments in collective undertakings. It also addresses solvency risks associated with high banking sector concentration under Solvency II.",True,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:13:11.445673
5c62e0f1a86ccf3e,Investor attention: Can google search volumes predict stock returns?,"This paper investigates the role of investor attention in predicting future stock market returns for Brazilian stocks using Google Search Volume (GSV). We tested whether lagged variations in GSV are followed by changes in excess returns by testing 57 stocks from the Ibovespa using weekly search data from Google Brazil from 2014 to 2018. Similar to previous research on the U.S. market, we found that increases in GSV are followed by lower excess returns. Additionally, we show that the more traded a stock is, the higher the effect. This is consistent with the hypothesis that higher individual investor attention leads to lower subsequent returns, suggesting that increasing popularity causes stock prices to deviate from their fundamental value. © 2021 Elsevier B.V., All rights reserved.","Yoshinaga, C.; Rocco, F.",2020,10.15728/bbr.2020.17.5.3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100415030&doi=10.15728%2Fbbr.2020.17.5.3&partnerID=40&md5=d2051069c272e875c103600f36b929cb,scopus,"This study examines if Google Search Volume (GSV) can predict stock market returns for Brazilian stocks. Using weekly GSV data for 57 Ibovespa stocks from 2014-2018, the research found that increased GSV was followed by lower excess returns, especially for more traded stocks. This supports the idea that high investor attention can cause stock prices to deviate from their fundamental value.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:13:19.699493
a0a865e10fc2c0c7,LASSO-Type Penalties for Covariate Selection and Forecasting in Time Series,"This paper studies some forms of LASSO-type penalties in time series to reduce the dimensionality of the parameter space as well as to improve out-of-sample forecasting performance. In particular, we propose a method that we call WLadaLASSO (weighted lag adaptive LASSO), which assigns not only different weights to each coefficient but also further penalizes coefficients of higher-lagged covariates. In our Monte Carlo implementation, the WLadaLASSO is superior in terms of covariate selection, parameter estimation precision and forecasting, when compared to both LASSO and adaLASSO, especially for a higher number of candidate lags and a stronger linear dependence between predictors. Empirical studies illustrate our approach for US risk premium and US inflation forecasting with good results. Copyright © 2016 John Wiley & Sons, Ltd. © 2022 Elsevier B.V., All rights reserved.","Konzen, E.; Ziegelmann, F.A.",2016,10.1002/for.2403,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958818333&doi=10.1002%2Ffor.2403&partnerID=40&md5=2a5fa664844d07fe52cdbc5e3abfda81,scopus,"This paper introduces WLadaLASSO, a novel LASSO-type penalty for time series analysis that enhances covariate selection and forecasting by weighting coefficients and penalizing higher-lagged covariates. It outperforms standard LASSO and adaLASSO in simulations, particularly with many lags and correlated predictors. The method is demonstrated on US risk premium and inflation forecasting.",True,False,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T12:13:46.764896
ec5f21caa1e911a1,Language Models Fine-Tuning for Automatic Format Reconstruction of SEC Financial Filings,"The analysis of financial reports is a crucial task for investors and regulators, especially the mandatory annual reports (10-K) required by the SEC (Securities and Exchange Commission) that provide crucial information about a public company in the American stock market. Although SEC suggests a specific document format to standardize and simplify the analysis, in recent years, several companies have introduced their own format and organization of the contents, making human-based and automatic knowledge extraction inherently more difficult. In this research work, we investigate different Neural language models based on Transformer networks (Bidirectional recurrence-based, Autoregressive-based, and Autoencoders-based approaches) to automatically reconstruct an SEC-like format of the documents as a multi-class classification task with 18 classes at the sentence level. In particular, we propose a Bidirectional fine-tuning procedure to specialize pre-trained language models on this task. We propose and make the resulting novel transformer model, named SEC-former, publicly available to deal with this task. We evaluate SEC-former in three different scenarios: 1) in terms of topic detection performances; 2) in terms of document similarity (TF-IDF Bag-of-words and Doc2Vec) achieved with respect to original and trustable financial reports since this operation is leveraged for portfolio optimization tasks; and 3) testing the model in a real use-case scenario related to a public company that does not respect the SEC format but provides a human-supervised reference to reconstruct it.",G. Lombardo; G. Trimigno; M. Pellegrino; S. Cagnoni,2024,10.1109/access.2024.3370444,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10445214,ieeexplore,"This research explores the use of Transformer-based neural language models, specifically SEC-former, to automatically reconstruct SEC-like financial filing formats. The study investigates Bidirectional, Autoregressive, and Autoencoder approaches, fine-tuning them for a multi-class classification task at the sentence level. The model's performance is evaluated based on topic detection, document similarity, and a real-world use case involving a company with a non-standard format.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:13:54.030706
6603d19b5d0f44e3,Likelihood-based scoring rules for comparing density forecasts in tails,"We propose new scoring rules based on conditional and censored likelihood for assessing the predictive accuracy of competing density forecasts over a specific region of interest, such as the left tail in financial risk management. These scoring rules can be interpreted in terms of Kullback-Leibler divergence between weighted versions of the density forecast and the true density. Existing scoring rules based on weighted likelihood favor density forecasts with more probability mass in the given region, rendering predictive accuracy tests biased toward such densities. Using our novel likelihood-based scoring rules avoids this problem. (C) 2011 Elsevier B.V. All rights reserved.","Diks, Cees; Panchenko, Valentyn; van Dijk, Dick",2011,10.1016/j.jeconom.2011.04.001,,wos,"This paper introduces new scoring rules for comparing density forecasts, focusing on specific regions of interest like financial risk management tails. These rules are based on conditional and censored likelihood and address biases in existing methods that favor forecasts with more mass in the region of interest.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:14:00.285270
dbc0a172a842c869,Likelihood-based specification analysis of continuous-time models of the short-term interest rate,"An extensive collection of continuous-time models of the short-term interest rate is evaluated over data sets that have appeared previously in the literature. The analysis, which uses the simulated maximum likelihood procedure proposed by Durham and Gallant (2002), provides new insights regarding several previously unresolved questions. For single factor models, I find that the volatility, not the drift, is the critical component in model specification. Allowing for additional flexibility beyond a constant term in the drift provides negligible benefit. While constant drift would appear to imply that the short rate is nonstationary, in fact, stationarity is volatility-induced. The simple constant elasticity of volatility model fits weekly observations of the three-month Treasury bill rate remarkably well but is easily rejected when compared with more flexible volatility specifications over daily data. The methodology of Durham and Gallant can also be used to estimate stochastic volatility models. While adding the latent volatility component provides a large improvement in the likelihood for the physical process, it does little to improve bond-pricing performance. (C) 2003 Elsevier B.V. All rights reserved.","Durham, GB",2003,10.1016/s0304-405x(03)00207-1,,wos,"This paper evaluates continuous-time models of short-term interest rates using simulated maximum likelihood. It finds that volatility is more critical than drift for model specification and that a simple constant elasticity of volatility model fits weekly Treasury bill rate data well, but is rejected by daily data. Stochastic volatility models improve likelihood but not bond-pricing performance.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:14:25.568450
1bbee78dc006f8fe,Linear Gaussian affine term structure models with unobservable factors: Calibration and yield forecasting,"This paper provides a significant numerical evidence for out-of-sample forecasting ability of linear Gaussian interest rate models with unobservable underlying factors. We calibrate one, two and three factor linear Gaussian models using the Kalman filter oil two different bond yield data sets and compare their out-of-sample forecasting performance. One-step ahead as well as four-step ahead out-of-sample forecasts are analyzed based on the weekly data. When evaluating the one-step ahead forecasts, it is shown that a one factor model may be adequate when only the short-dated or only the long-dated yields are considered, but two and three factor models performs significantly better when the entire yield spectrum is considered. Furthermore, the results demonstrate that the predictive ability of multi-factor models remains intact far ahead out-of-sample, with accurate predictions available up to one year after the last calibration for one data set and up to three months after the last calibration for the second, more volatile data set. The experimental data denotes two different periods with different yield volatilities, and the stability of model parameters after calibration ill both the cases is deemed to be both significant and practically useful. When it comes to four-step ahead predictions, the quality of forecasts deteriorates for all models, as can be expected, but the advantage of using a multi-factor model as compared to a one factor model is still significant.In addition to the empirical study above, we also suggest a non-linear filter based on linear programming for improving the term structure matching at a given point in time. This method, when used in place of a Kalman filter update, improves the term structure fit significantly with a minimal added computational overhead. The improvement achieved with the proposed method is illustrated for out-of-sample data for both the data sets. This method call be used to model a parameterized yield curve consistently with the underlying short rate dynamics. (c) 2008 Elsevier B.V. All rights reserved.","Date, Paresh; Wang, Chieh",2009,10.1016/j.ejor.2008.01.035,,wos,"This paper investigates the out-of-sample forecasting performance of linear Gaussian interest rate models with unobservable factors. It calibrates one, two, and three-factor models using the Kalman filter on two bond yield datasets and compares their forecasting accuracy for one-step and four-step ahead predictions. The study finds that multi-factor models generally outperform one-factor models, especially when considering the entire yield spectrum. The paper also proposes a non-linear filter based on linear programming to improve term structure matching, which enhances out-of-sample fit with minimal computational cost.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T12:14:39.970850
3ea0c55e5317e324,Linear and nonlinear predictability in investment style factors: Multivariate evidence,"This paper studies the predictive performance of multivariate models at forecasting the (excess) returns of portfolios mimicking the Market, Size, Value, Momentum, and Low Volatility factors isolated in asset pricing research. We evaluate the accuracy of the point forecasts of a number of linear and regime-switching models in recursive, out-of-sample forecasting experiments. We assess the accuracy of the models using several measures of unbiasedness and predictive accuracy, and using Diebold and Mariano's approach to test whether differences in expected losses from all possible pairs of forecast models are statistically significant. We fail to find evidence that complex statistical models are uniformly more accurate than a naïve constant expected return model for factor-mimicking portfolio (excess) returns. However, we show that it is possible to build simple portfolio strategies that profit from the higher out-of-sample predictive accuracy of forecasting models with Markov switching in conditional mean coefficients. These results appear to be independent of the forecasting horizon and robust to changes in the loss function that captures the investors' objectives. © 2017 Elsevier B.V., All rights reserved.","Chincoli, F.; Guidolin, M.",2017,10.1057/s41260-017-0048-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018248298&doi=10.1057%2Fs41260-017-0048-5&partnerID=40&md5=5b890a8c51620f9b5b10a05c7bfc2bdc,scopus,"This paper investigates the predictive performance of linear and regime-switching models for forecasting excess returns of investment style factors (Market, Size, Value, Momentum, Low Volatility). While complex models did not uniformly outperform a naive model, simple strategies could profit from Markov-switching models. The results were robust across forecasting horizons and loss functions.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:14:48.400841
aa8630690d917199,"Liquidity, volume and volatility in US electricity futures: The case of Palo Verde","Previous research on liquidity has studied the relationships between liquidity, trading activity and volatility, mostly with data from US Treasury securities, stocks and foreign exchange spot markets. Liquidity in futures markets, especially electricity futures, has received little attention. However, liquidity in futures is expected to behave differently to that in spot markets because of the unique asymmetries in futures markets. Liquidity in electricity markets is of interest in countries where these markets are being deregulated. This study estimates these relationships for the Palo Verde electricity futures contract. The results show positive relations between all three pairs of key variables. © 2006 Elsevier B.V., All rights reserved.","Goss, B.",2006,10.1080/17446540500396974,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33744995794&doi=10.1080%2F17446540500396974&partnerID=40&md5=1fbf6045b943bee0079a5367dc3f2a90,scopus,"This study examines the relationships between liquidity, trading volume, and volatility in the Palo Verde electricity futures contract. It finds positive correlations between these variables, contrasting with previous research primarily focused on financial markets like Treasury securities, stocks, and foreign exchange. The research highlights the unique dynamics of futures markets, particularly in deregulating electricity sectors.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:14:54.715878
3353e1adf55ac4eb,Local lagged adapted generalized method of moments and applications,"In this work, an attempt is made for developing the local lagged adapted generalized method of moments (LLGMM). This proposed method is composed of: 1) development of the stochastic model for continuous-time dynamic process; 2) development of the discrete-time interconnected dynamic model for statistic process; 3) utilization of Euler-type discretized scheme for nonlinear and nonstationary system of stochastic differential equations; 4) development of generalized method of moment/observation equations by employing lagged adaptive expectation process; 5) introduction of the conceptual and computational parameter estimation problem; 6) formulation of the conceptual and computational state estimation scheme; and 7) definition of the conditional mean square epsilon -best sub-optimal procedure. The development of LLGMM is motivated by parameter and state estimation problems in continuous-time nonlinear and nonstationary stochastic dynamic model validation problems in biological, chemical, engineering, financial, medical, physical, and social sciences. The byproducts of LLGMM are the balance between model specification and model prescription of continuous-time dynamic process and the development of discrete-time interconnected dynamic model of local sample mean and variance statistic process (DTIDMLSMVSP). DTIDMLSMVSP is the generalization of statistic (sample mean and variance) drawn from the static dynamic population problems. Moreover, it is also an alternative approach to the GARCH (1,1) model and its many related variant models (e.g., EGARCH model, GJR GARCH model). It provides an iterative scheme for updating statistic coefficients in a system of generalized method of moment/observation equations. Furthermore, application of the LLGMM method to stochastic differential dynamic models for energy commodity price, U.S. Treasury bill yield interest rate U.S.-U.K. foreign exchange rate exhibits its unique role and scope.",,2017,10.1080/07362994.2016.1213640,,proquest,"This paper introduces the Local Lagged Adapted Generalized Method of Moments (LLGMM), a novel statistical method for parameter and state estimation in continuous-time nonlinear and nonstationary stochastic dynamic models. LLGMM involves developing stochastic and discrete-time models, utilizing Euler-type discretization, employing lagged adaptive expectation for moment equations, and formulating parameter and state estimation schemes. It is motivated by applications in various scientific fields and offers an alternative to GARCH models. The method is applied to models for energy commodity prices, U.S. Treasury bill yields, and U.S.-U.K. foreign exchange rates.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T12:15:10.855470
bb72826f453f4ef9,Long-term fiscal implications of funding assisted reproduction: a generational accounting model for Spain,"The aim of this study was to assess the lifetime economic benefits of assisted reproduction in Spain by calculating the return on this investment. We developed a generational accounting model that simulates the flow of taxes paid by the individual, minus direct government transfers received over the individual's lifetime. The difference between discounted transfers and taxes minus the cost of either IVF or artificial insemination (AI) equals the net fiscal contribution (NFC) of a child conceived through assisted reproduction. We conducted sensitivity analysis to test the robustness of our results under various macroeconomic scenarios. A child conceived through assisted reproduction would contribute €370,482 in net taxes to the Spanish Treasury and would receive €275,972 in transfers over their lifetime. Taking into account that only 75% of assisted reproduction pregnancies are successful, the NFC was estimated at €66,709 for IVF-conceived children and €67,253 for AI-conceived children. The return on investment for each euro invested was €15.98 for IVF and €18.53 for AI. The long-term NFC of a child conceived through assisted reproduction could range from €466,379 to €-9,529 (IVF) and from €466,923 to €-8,985 (AI). The return on investment would vary between €-2.28 and €111.75 (IVF), and €-2.48 and €128.66 (AI) for each euro invested. The break-even point at which the financial position would begin to favour the Spanish Treasury ranges between 29 and 41 years of age. Investment in assisted reproductive techniques may lead to positive discounted future fiscal revenue, notwithstanding its beneficial psychological effect for infertile couples in Spain.The aim of this study was to assess the lifetime economic benefits of assisted reproduction in Spain by calculating the return on this investment. We developed a generational accounting model that simulates the flow of taxes paid by the individual, minus direct government transfers received over the individual's lifetime. The difference between discounted transfers and taxes minus the cost of either IVF or artificial insemination (AI) equals the net fiscal contribution (NFC) of a child conceived through assisted reproduction. We conducted sensitivity analysis to test the robustness of our results under various macroeconomic scenarios. A child conceived through assisted reproduction would contribute €370,482 in net taxes to the Spanish Treasury and would receive €275,972 in transfers over their lifetime. Taking into account that only 75% of assisted reproduction pregnancies are successful, the NFC was estimated at €66,709 for IVF-conceived children and €67,253 for AI-conceived children. The return on investment for each euro invested was €15.98 for IVF and €18.53 for AI. The long-term NFC of a child conceived through assisted reproduction could range from €466,379 to €-9,529 (IVF) and from €466,923 to €-8,985 (AI). The return on investment would vary between €-2.28 and €111.75 (IVF), and €-2.48 and €128.66 (AI) for each euro invested. The break-even point at which the financial position would begin to favour the Spanish Treasury ranges between 29 and 41 years of age. Investment in assisted reproductive techniques may lead to positive discounted future fiscal revenue, notwithstanding its beneficial psychological effect for infertile couples in Spain.",,2015,10.1016/j.rbms.2016.04.001,,proquest,"This study uses a generational accounting model to assess the long-term fiscal implications of assisted reproduction in Spain. It estimates the net fiscal contribution (NFC) and return on investment (ROI) for children conceived through IVF and artificial insemination, finding positive fiscal returns. The break-even point for the Spanish Treasury is estimated between 29 and 41 years of age.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:15:18.778565
08bba97ee6005b87,MARKET MANIPULATION: A SURVEY,"Despite the significant attention that market manipulation has received in recent years many aspects of it are poorly understood. This article identifies from the theoretical and empirical literature what we do and do not know about market manipulation, and suggests directions for future research. We know that manipulation is possible and that it occurs in a wide variety of markets and circumstances. In contrast, we know little about how often manipulation occurs, its effects and how it responds to regulation. Suggested approaches for future research on these issues include: (1) collecting more comprehensive data sets of manipulation cases; (2) using detection controlled estimation methods to overcome sample selection and partial observability problems and (3) conducting controlled experiments. This article also constructs a novel and broad taxonomy of the different types of market manipulation and discusses approaches to defining manipulation.","Putnins, Talis J.",2012,10.1111/j.1467-6419.2011.00692.x,,wos,"This survey paper reviews the literature on market manipulation, identifying what is known and unknown about its occurrence, effects, and regulatory responses. It proposes future research directions, including data collection, advanced estimation methods, and controlled experiments, and offers a taxonomy of manipulation types.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:15:19.916566
a317ea4fb2e44a47,Machine-Learning-Based Return Predictors and the Spanning Controversy in Macro-Finance,"We propose a two-step machine learning algorithm-the Supervised Adaptive Group LASSO (SAGLasso) method-that is suitable for constructing parsimonious return predictors from a large set of macro variables. We apply this method to government bonds and a set of 917 macro variables and construct a new, transparent, and easy-to-interpret macro variable with significant out-of-sample predictive power for excess bond returns. This new macro factor, termed the SAGLasso factor, is a linear combination of merely 30 selected macro variables out of 917. Furthermore, it can be decomposed into three sublevel factors: a novel housing factor, an employment factor, and an inflation factor. Importantly, the predictive power of the SAGLasso factor is robust to bond yields, namely, the SAGLasso factor is not spanned by bond yields. Moreover, we show that the unspanned variation of the SAGLasso factor cannot be attributed to yield measurement error or macro measurement error. The SAGLasso factor therefore provides a potential resolution to the spanning controversy in the macro-finance literature.",,2023,10.1287/mnsc.2022.4386,,proquest,"This paper introduces the Supervised Adaptive Group LASSO (SAGLasso) method, a two-step machine learning algorithm for building parsimonious return predictors from a large set of macro variables. Applied to government bonds and 917 macro variables, it constructs a new, transparent, and interpretable macro factor (SAGLasso factor) with significant out-of-sample predictive power for excess bond returns. This factor, a linear combination of 30 selected variables, decomposes into housing, employment, and inflation factors. Crucially, its predictive power is robust and not spanned by bond yields, addressing the spanning controversy in macro-finance.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T12:15:50.597876
0798f3bbdbaff551,Macroeconomic Models for Monetary Policy: A Critical Review from a Finance Perspective,"We provide a critical review of macroeconomic models used for monetary policy at central banks from a finance perspective. We review the history of monetary policy modeling, survey the core monetary models used by major central banks, and construct an illustrative model for those readers who are unfamiliar with the literature. Within this framework, we highlight several important limitations of current models and methods, including the fact that local-linearization approximations omit important nonlinear dynamics, yielding biased impulse-response analysis and parameter estimates. We also propose new features for the next generation of macrofinancial policy models, including a substantial role for the financial sector, the government balance sheet, and unconventional monetary policies; heterogeneity, reallocation, and redistribution effects;the macroeconomic impact of large nonlinear risk premium dynamics; time-varying uncertainty; financial sector and systemic risks; imperfect product market and markups; and further advances in solution, estimation, and evaluation methods for dynamic quantitative structural models. © 2020 Elsevier B.V., All rights reserved.","Dou, W.W.; Lo, A.W.; Muley, A.; Uhlig, H.",2020,10.1146/annurev-financial-012820-025928,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097430621&doi=10.1146%2Fannurev-financial-012820-025928&partnerID=40&md5=6c6933942ee847d97a6763f19dbc34d1,scopus,"This paper critically reviews macroeconomic models used for monetary policy by central banks, focusing on their limitations from a finance perspective. It highlights issues with local-linearization approximations and proposes enhancements for future models, including a greater role for the financial sector, government balance sheet, unconventional policies, heterogeneity, and nonlinear risk dynamics.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:15:53.603587
3a1e5fe8c3d33851,Maximum likelihood estimation of non-affine volatility processes,"In this paper we develop a new estimation method for extracting non-affine latent stochastic volatility and risk premia from measures of model-free realized and risk-neutral integrated volatility. We estimate non-affine models with nonlinear drift and constant elasticity of variance and we compare them to the popular square-root stochastic volatility model. Our empirical findings are: (1) the square-root model is misspecified; (2) the inclusion of constant elasticity of variance and nonlinear drift captures stylized facts of volatility dynamics and (3) the square-root stochastic volatility model is explosive under the risk-neutral probability measure. All rights reserved, Elsevier",,2011,10.1016/j.jempfin.2010.10.006,,proquest,"This paper introduces a novel method for estimating non-affine latent stochastic volatility and risk premia using realized and risk-neutral integrated volatility measures. The study compares non-affine models with constant elasticity of variance and nonlinear drift against the square-root stochastic volatility model, finding the latter to be misspecified and potentially explosive under the risk-neutral measure. The proposed models better capture volatility dynamics.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:15:57.264550
5080e4a5f3be7be5,Maximum likelihood estimation of partially observed diffusion models,"This paper develops a maximum likelihood (ML) method to estimate partially observed diffusion models based on data sampled at discrete times. The method combines two techniques recently proposed in the literature in two separate steps. In the first step, the closed form approach of Ait-Sahalia (2008) is used to obtain a highly accurate approximation to the joint transition probability density of the latent and the observed states. In the second step, the efficient importance sampling technique of Richard and Zhang (2007) is used to integrate out the latent states, thereby yielding the likelihood function. Using both simulated and real data, we show that the proposed ML method works better than alternative methods. The new method does not require the underlying diffusion to have an affine structure and does not involve infill simulations. Therefore, the method has a wide range of applicability and its computational cost is moderate. (C) 2014 Elsevier B.V. All rights reserved.","Kleppe, Tore Selland; Yu, Jun; Skaug, Hans J.",2014,10.1016/j.jeconom.2014.02.002,,wos,"This paper proposes a maximum likelihood estimation method for partially observed diffusion models using discrete-time sampled data. It combines Ait-Sahalia's (2008) closed-form approximation for transition densities with Richard and Zhang's (2007) importance sampling technique to integrate out latent states and obtain the likelihood function. The method is shown to outperform alternatives on simulated and real data, offering broad applicability due to its independence from affine structures and infill simulations, with moderate computational cost.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:16:05.406068
bcd2cc2bd87fa5d9,Measuring Risk Premiums Using Financial Reports and Actuarial Disclosures,"Insurance companies increasingly augment their financial reports by releasing actuarial measures the so-called embedded value to supply information about the value of their life insurance activities. Both accounting and actuarial measures differ with respect to the timeliness of profit realisation and its reliability, and their performance in yielding information may differ. This paper asks if and how embedded values help in assessing risk premiums. We estimate multifactor market models in the spirit of Fama and French, and find that actuarial disclosures are superior to financial accounting in estimating these risk premiums. They further add information to financial reports as an estimator for growth opportunities.","Zimmermann, Jochen; Veith, Stefan; Schymczyk, Johannes",2015,10.1057/gpp.2014.17,,wos,"This paper investigates whether actuarial disclosures, specifically embedded values, improve the estimation of risk premiums compared to traditional financial reports for insurance companies. Using multifactor market models, the study finds that actuarial disclosures provide superior information for estimating risk premiums and offer additional insights into growth opportunities.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:16:09.459809
d35fd1822311585b,Measuring the Financial Value of Marketing Strategy with Excess Stock Market Return,"This paper proposes excess stock market return as a way to measure the impact of marketing strategy on firm value. First, it provides an overview of event study method. An event study examines the excess return to a firm's stock price after the release of information that is relevant to the firm's financial success. Second, it shows how excess return captures a marketing strategy's impact on firm value. It presents a model that illustrates how a marketing strategy impacts consumers, future cash flows, firm value, investor's expectations, and excess return. Third, a comparison shows that excess return stacks up well against standard marketing metrics. Excess return yields unbiased estimates, allows direct causal inference, is future oriented, includes all cash flows, accounts for opportunity costs, factors in risk, and takes into account the time value of money.",,2014,10.4018/ijrcm.2014100101,,proquest,"This paper suggests using excess stock market return to quantify the financial impact of marketing strategies on firm value. It outlines the event study method and presents a model demonstrating how marketing strategies influence consumers, future cash flows, firm value, investor expectations, and ultimately, excess returns. The paper argues that excess return is a superior metric compared to standard marketing metrics due to its unbiased estimates, causal inference capabilities, future orientation, and consideration of all cash flows, opportunity costs, risk, and the time value of money.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:16:11.111722
048eafcb2e26ee95,Minimax and Biobjective Portfolio Selection Based on Collaborative Neurodynamic Optimization,"Portfolio selection is one of the important issues in financial investments. This article is concerned with portfolio selection based on collaborative neurodynamic optimization. The classic Markowitz mean–variance (MV) framework and its variant mean conditional value-at-risk (CVaR) are formulated as minimax and biobjective portfolio selection problems. Neurodynamic approaches are then applied for solving these optimization problems. For each of the problems, multiple neural networks work collaboratively to characterize the efficient frontier by means of particle swarm optimization (PSO)-based weight optimization. Experimental results with stock data from four major markets show the performance and characteristics of the collaborative neurodynamic approaches to the portfolio optimization problems.",M. -F. Leung; J. Wang,2021,10.1109/tnnls.2019.2957105,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8948344,ieeexplore,"This article presents a neurodynamic optimization approach for portfolio selection, addressing both minimax and biobjective formulations based on Markowitz mean-variance and mean conditional value-at-risk frameworks. It utilizes collaborative neural networks and particle swarm optimization to identify efficient frontiers, with experimental validation on stock market data.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:16:13.553591
43f035f3fb505552,Misaligned expectations and bond term premium measures☆,"This paper shows that inflation expectations and those embedded in short-term interest rate expectations as reported in the Survey of Professional Forecasters show evidence of misaligned expectations. This misalignment seems to have been substantial in recent times, featuring a low correlation between inflation and the policy rate. This empirical evidence motivates an alternative explanation, based on uncertainty rather than risk, of the bond term premium measures found in the literature. This paper estimates an expectational term premium driven by misaligned short-term interest rate expectations from a behavioral DSGE model that introduces model uncertainty by assuming adaptive learning with discretionary beliefs. The estimated 10-year expectational term premium shares important features with the corresponding term premium measures obtained using no-arbitrage affine term structure models. Thus, the expectational term premium is sizable, highly persistent, mildly countercyclical, and highly correlated with those term premium measures in the most recent period studied. In short, a potential misalignment of short-term interest expectations with inflation expectations provides an important channel for explaining the bond premium lately.","Vazquez, Jesus",2025,10.1016/j.najef.2025.102442,,wos,"This paper investigates misaligned inflation and short-term interest rate expectations, particularly in recent times, suggesting this misalignment impacts bond term premium measures. It proposes an alternative explanation based on uncertainty rather than risk, estimating an expectational term premium using a behavioral DSGE model with adaptive learning and discretionary beliefs. The estimated term premium exhibits characteristics similar to those derived from no-arbitrage affine term structure models, indicating its significance, persistence, and countercyclicality, and its correlation with recent term premium measures. The misalignment of short-term interest expectations with inflation expectations is presented as a key factor in explaining recent bond premium behavior.",False,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:16:50.833059
4a7f853347d7646b,Modeling spot rate using a realized stochastic volatility model with level effect and dynamic drift,"This paper proposes a class of realized stochastic volatility model based on both various realized volatility measures and spot rate. It applies the realized stochastic volatility model (Takahashi, Omori, & Watanabe, 2009, and Koopman & Scharth, 2013) to the spot rate model with dynamic drift and level effect setups (RSVL). A jointly approximated maximum likelihood procedure is used to estimate this model. The simulation results show that the RSVL model can be consistently estimated and noise-and-jump-robust realized volatility measures improve the accuracy of the estimation. This study empirically investigates the Chinese interbank repo market with RSVL model, which manifested the advantage of taking the level effect and nonlinear drift into consideration. The noise-and-jump-robust realized volatility measures (e.g. subsample realized volatility and threshold pre-average realized volatility) decrease the volatility fitting error. The nonparametric testing suggests that the RSVL model with noise-and-jump-robust realized volatility measures has more power on forecasting excess kurtosis and fat tails and predicting dynamics of higher order autocorrelations. (C) 2017 Elsevier Inc. All rights reserved.","Li, Shaoyu; Zheng, Tingguo",2017,10.1016/j.najef.2017.03.003,,wos,"This paper introduces a realized stochastic volatility model (RSVL) for spot rates, incorporating dynamic drift and level effects. The model utilizes realized volatility measures and is estimated using a jointly approximated maximum likelihood procedure. Simulation results indicate that noise-and-jump-robust realized volatility measures enhance estimation accuracy. Empirical analysis on the Chinese interbank repo market demonstrates the RSVL model's effectiveness in capturing level effects and nonlinear drifts. The study also shows that the RSVL model with robust realized volatility measures improves volatility fitting and forecasting of excess kurtosis, fat tails, and higher-order autocorrelation dynamics.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:16:59.317515
9342a41d80edaf33,Modeling the density of US yield curve using Bayesian semiparametric dynamic Nelson-Siegel model,"This paper proposes the Bayesian semiparametric dynamic Nelson-Siegel model for estimating the density of bond yields. Specifically, we model the distribution of the yield curve factors according to an infinite Markov mixture (iMM). The model allows for time variation in the mean and covariance matrix of factors in a discrete manner, as opposed to continuous changes in these parameters such as the Time Varying Parameter (TVP) models. Estimating the number of regimes using the iMM structure endogenously leads to an adaptive process that can generate newly emerging regimes over time in response to changing economic conditions in addition to existing regimes. The potential of the proposed framework is examined using US bond yields data. The semiparametric structure of the factors can handle various forms of non-normalities including fat tails and nonlinear dependence between factors using a unified approach by generating new clusters capturing these specific characteristics. We document that modeling parameter changes in a discrete manner increases the model fit as well as forecasting performance at both short and long horizons relative to models with fixed parameters as well as the TVP model with continuous parameter changes. This is mainly due to fact that the discrete changes in parameters suit the typical low frequency monthly bond yields data characteristics better.",,2020,10.1080/07474938.2019.1690191,,proquest,"This paper introduces a Bayesian semiparametric dynamic Nelson-Siegel model using an infinite Markov mixture to estimate bond yield densities. It models discrete time variation in factor means and covariances, allowing for adaptive regime generation. The model handles non-normalities like fat tails and nonlinear dependencies. Empirical results using US bond yields show improved fit and forecasting performance compared to fixed-parameter and TVP models, attributed to the discrete parameter change approach suiting monthly yield data.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T12:17:09.293592
a4e215a81aa0f6ae,Modelling and forecasting interest rates during stages of the economic cycle: A knowledge-discovery approach,"Modelling the structure of risk-free rates and their relation to other economic and financial variables during different stages of the economic cycles has attracted much interest from both the theoretical and practical perspectives. The previous literature has emphasized the deployment of expert systems and knowledge-discovery approaches motivated by the need to address the limitations of the econometric models. However, it has failed to address the interpretability aspects and, more importantly, the need to provide methodological support that allows the deployment of such techniques in a more systematic way. This approach entails the definition of a process that includes the usual steps taken by experts to address similar problems and allows the relative merits of different techniques in relation to common goals and objectives to be gauged. This paper addresses the interpretability and the lack of methodological support by proposing a knowledge-discovery methodology that includes a minimal common number of steps to model, analyse, evaluate and deploy different non-linear techniques and models. Furthermore, the interpretability is addressed through the use of open-box techniques, such as decision trees. The proposed methodology helps to discover and describe hidden patterns, allowing for the study and characterization of economic cycles, and economic cycle stages, as well as the description of the historic relationships between interest rates and other relevant economic variables. These patterns can also be used in the forecasting of economic cycle stages, interest rates and other related variables of concern. The output of the methodology can provide actionable information for market agents, such as monetary authorities, financial institutions, and individual investors, as well as for the academic community, to increase further the knowledge and understanding of financial markets, thus enriching and complementing existing financial theories. © 2015 Elsevier B.V., All rights reserved.","Díaz, D.; Theodoulidis, B.; Dupouy, C.",2016,10.1016/j.eswa.2015.09.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945251489&doi=10.1016%2Fj.eswa.2015.09.010&partnerID=40&md5=295b07c4bfb60217b7e945fbddbf99e7,scopus,"This paper proposes a knowledge-discovery methodology to model, analyze, evaluate, and deploy non-linear techniques for understanding interest rate behavior across economic cycles. It emphasizes interpretability using open-box methods like decision trees and aims to uncover hidden patterns for forecasting economic cycles and interest rates, providing actionable insights for various market participants and academics.",True,False,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T12:17:35.272436
febabd5bf9c00b9b,Modelling and management of mortality risk: a review,"In the first part of the paper, we consider the wide range of extrapolative stochastic mortality models that have been proposed over the last 15-20 years. A number of models that we consider are framed in discrete time and place emphasis on the statistical aspects of modelling and forecasting. We discuss how these models can be evaluated, compared and contrasted. We also discuss a discrete-time market model that facilitates valuation of mortality-linked contracts with embedded options. We then review several approaches to modelling mortality in continuous time. These models tend to be simpler in nature, but make it possible to examine the potential for dynamic hedging of mortality risk. Finally, we review a range of financial instruments (traded and over-the-counter) that could be used to hedge mortality risk. Some of these, such as mortality swaps, already exist, while others anticipate future developments in the market.","Cairns, Andrew J. G.; Blake, David; Dowd, Kevin",2008,10.1080/03461230802173608,,wos,"This paper reviews stochastic mortality models, focusing on both discrete-time and continuous-time approaches. It discusses model evaluation, forecasting, and the valuation of mortality-linked contracts. The review also covers hedging strategies for mortality risk using existing and potential financial instruments.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:17:58.612191
3e25d963775485c0,Modelling financial time series with threshold nonlinearity in returns and trading volume,"This paper investigates the effect of past returns and trading volumes on the temporal behaviour of international market returns. We propose a class of nonlinear threshold time-series models with generalized autoregressive conditional heteroscedastic disturbances. Using Bayesian approach, an implementation of Markov chain Monte Carlo procedure is used to obtain estimates of unknown parameters. The proposed family of models incorporates changes in log of volumes in the sense of regime changes and asymmetric effects on the volatility functions. The results show that when differences of log volumes are involved in the system of log return and volatility models, an optimum selection can be achieved. In all the five markets considered, both mean and variance equations involve volumes in the best models selected. Our best models produce higher posterior-odds ratios than that in Gerlach et al.'s (Phys. A Statist. Mech. Appl. 2006; 360:422-444) models, indicating that our return-volume partition of regimes can offer extra gain in explaining return-volatility term structure.",,2007,10.1002/asmb.674,,proquest,"This paper proposes and implements a class of nonlinear threshold time-series models to analyze the relationship between past returns, trading volumes, and international market returns. Using a Bayesian approach with Markov chain Monte Carlo, the models incorporate regime changes and asymmetric effects related to trading volume. The findings indicate that including trading volume differences in the models improves the explanation of return-volatility dynamics across five markets.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:18:06.276123
3f8ab1bb9836d2d3,Monetary reforms and inflation expectations in Japan: Evidence from inflation-indexed bonds,"We assess the impact of news concerning recent Japanese monetary reforms on long-term inflation expectations using an arbitrage-free term structure model of nominal and real yields. Our model accounts for the value of deflation protection embedded in Japanese inflation-indexed bonds issued since 2013, which is sizable and time-varying. Our results suggest that Japanese long-term inflation expectations have remained pos-itive despite extensive spells of deflation, leaving inflation risk premia mostly negative during this period. Moreover, adjusting for deflation protection demonstrates that market responses to policy changes were not as inflationary as they appear under standard modeling procedures. Consequently, the reforms were less disappointingthan is widely perceived.(c) 2021 Elsevier B.V. All rights reserved.","Christensen, Jens H. E.; Spiegel, Mark M.",2022,10.1016/j.jeconom.2021.10.007,,wos,"This study analyzes the effect of Japanese monetary reforms on long-term inflation expectations using an arbitrage-free term structure model applied to nominal and real yields of inflation-indexed bonds issued since 2013. The model incorporates the value of deflation protection, which is significant and fluctuates over time. Findings indicate that Japanese long-term inflation expectations stayed positive even during deflationary periods, with inflation risk premia generally negative. After accounting for deflation protection, the market's reaction to policy changes was less inflationary than suggested by conventional models, implying the reforms were less disappointing than commonly believed.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:18:14.377304
25b9333c8cbad8b3,Monte Carlo Simulations for Resolving Verifiability Paradoxes in Forecast Risk Management and Corporate Treasury Applications,"Forecast risk management is central to the financial management process. This study aims to apply Monte Carlo simulation to solve three classic probabilistic paradoxes and discuss their implementation in corporate financial management. The article presents Monte Carlo simulation as an advanced tool for risk management in financial management processes. This method allows for a comprehensive risk analysis of financial forecasts, making it possible to assess potential errors in cash flow forecasts and predict the value of corporate treasury growth under various future scenarios. In the investment decision-making process, Monte Carlo simulation supports the evaluation of the effectiveness of financial projects by calculating the expected net value and identifying the risks associated with investments, allowing more informed decisions to be made in project implementation. The method is used in reducing cash flow volatility, which contributes to lowering the cost of capital and increasing the value of a company. Simulation also enables more accurate liquidity planning, including forecasting cash availability and determining appropriate financial reserves based on probability distributions. Monte Carlo also supports the management of credit and interest rate risk, enabling the simulation of the impact of various economic scenarios on a company’s financial obligations. In the context of strategic planning, the method is an extension of decision tree analysis, where subsequent decisions are made based on the results of earlier ones. Creating probabilistic models based on Monte Carlo simulations makes it possible to take into account random variables and their impact on key financial management indicators, such as free cash flow (FCF). Compared to traditional methods, Monte Carlo simulation offers a more detailed and precise approach to risk analysis and decision-making, providing companies with vital information for financial management under uncertainty. This article emphasizes that the use of Monte Carlo simulation in financial management not only enhances the effectiveness of risk management, but also supports the long-term growth of corporate value. The entire process of financial management is able to move into the future based on predicting future free cash flows discounted at the cost of capital. We used both numerical and analytical methods to solve veridical paradoxes. Veridical paradoxes are a type of paradox in which the result of the analysis is counterintuitive, but turns out to be true after careful examination. This means that although the initial reasoning may lead to a wrong conclusion, a correct mathematical or logical analysis confirms the correctness of the results. An example is Monty Hall’s problem, where the intuitive answer suggests an equal probability of success, while probabilistic analysis shows that changing the decision increases the chances of winning. We used Monte Carlo simulation as the numerical method. The following analytical methods were used: conditional probability, Bayes’ rule and Bayes’ rule with multiple conditions. We solved truth-type paradoxes and discovered why the Monty Hall problem was so widely discussed in the 1990s. We differentiated Monty Hall problems using different numbers of doors and prizes.",,2025,10.3390/ijfs13020049,,proquest,"This study applies Monte Carlo simulation to address classic probabilistic paradoxes within financial risk management and corporate treasury. It details how this method enhances financial forecast accuracy, investment decision-making, liquidity planning, and risk management (credit, interest rate). The article highlights Monte Carlo simulation's advantage over traditional methods for detailed risk analysis and strategic planning, ultimately supporting corporate value growth by enabling more informed decisions under uncertainty. It specifically uses Monte Carlo simulation to solve veridical paradoxes, such as the Monty Hall problem, employing both numerical and analytical techniques.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:18:21.129366
e148b89b6640d34c,Mortgage convexity,"Most home mortgages in the United States are fixed-rate loans with an embedded prepayment option. When long-term rates decline, the effective duration of mortgage-backed securities (MBS) falls due to heightened refinancing expectations. I show that these changes in MBS duration function as large-scale shocks to the quantity of interest rate risk that must be borne by professional bond investors. I develop a simple model in which the risk tolerance of bond investors is limited in the short run, so these fluctuations in MBS duration generate significant variation in bond risk premia. Specifically, bond risk premia are high when aggregate MBS duration is high. The model offers an explanation for why long-term rates could appear to be excessively sensitive to movements in short rates and explains how changes in MBS duration act as a positive-feedback mechanism that amplifies interest rate volatility. I find strong support for these predictions in the time series of US government bond returns. (C) 2014 Elsevier B.V. All rights reserved.","Hanson, Samuel G.",2014,10.1016/j.jfineco.2014.05.002,,wos,"This paper examines how mortgage-backed securities (MBS) duration fluctuations, driven by prepayment options and interest rate changes, impact bond risk premia. It proposes a model where limited investor risk tolerance amplifies interest rate volatility when MBS duration is high, finding empirical support in US government bond returns.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:18:43.311546
cbb317c157af2d69,Multitrend Conditional Value at Risk for Portfolio Optimization,"Trend representation has been attracting more and more attention recently in portfolio optimization (PO) via machine learning methods. It adopts concepts and phenomena from the field of empirical and behavioral finance when little prior knowledge is obtained or strict statistical assumptions cannot be guaranteed. It is used mostly in estimating the expected asset returns, but hardly in measuring risk. To fill this gap, we propose a novel multitrend conditional value at risk (MT-CVaR), which embeds multiple trends and their influences in CVaR. Besides, we propose a novel PO model with this MT-CVaR as the risk metric and then design a solving algorithm based on the interior point method to compute the portfolio. Extensive experiments on six benchmark datasets from diverse financial markets with different frequencies show that MT-CVaR achieves the state-of-the-art investing performance and risk management.",Z. -R. Lai; C. Li; X. Wu; Q. Guan; L. Fang,2024,10.1109/tnnls.2022.3183891,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9805693,ieeexplore,"This paper introduces a novel Multitrend Conditional Value at Risk (MT-CVaR) for portfolio optimization, incorporating multiple trends and their impacts into CVaR. A new portfolio optimization model using MT-CVaR as the risk metric is proposed, with a solution algorithm based on the interior point method. Experiments on benchmark datasets demonstrate MT-CVaR's effectiveness in investing performance and risk management.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:18:48.473979
f7635ac88ef7da8e,Mutual Fund Liquidity Transformation and Reverse Flight to Liquidity,"We identify fixed-income mutual funds as an important contributor to the unusually high selling pressure in liquid asset markets during the COVID-19 crisis. We show that mutual funds experienced pronounced investor outflows amplified by their liquidity transformation. In meeting redemptions, funds followed a pecking order by first selling their liquid assets, including Treasuries and high-quality corporate bonds, which generated the most concentrated selling pressure in these markets. Overall, the estimated price impact of mutual funds was sizable at a third of the increase in Treasury yields and a quarter of the increase in corporate bond yields during the COVID-19 crisis.","Ma, Yiming; Xiao, Kairong; Zeng, Yao",2022,10.1093/rfs/hhac007,,wos,"This study investigates the role of fixed-income mutual funds in exacerbating selling pressure in liquid asset markets during the COVID-19 crisis. It highlights how investor outflows, amplified by the funds' liquidity transformation, led to the sale of liquid assets like Treasuries and corporate bonds, significantly impacting yields. The paper quantifies the price impact of these sales on Treasury and corporate bond yields.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:18:52.792881
d1cbe8bbd0caa6f9,Negative option values are possible: The impact of treasury bond futures on the cash US Treasury market,"This paper uses a unique financial instrument in the U.S. Treasury market to study the price behavior of the put option embedded in the November 2009-14 callable U.S. Treasury bond. We find that, beginning in August 1993, the estimated option value was persistently negative on nearly every day for the ensuing eight months. We show that the anomalous pricing behavior arose because the underlying callable bond became the cheapest to deliver issue against U.S. Treasury bond futures contracts. Hence, this paper provides direct evidence that derivative assets can significantly distort pricing in the primary asset market.","Jordan, BD; Kuipers, DR",1997,10.1016/s0304-405x(97)00025-1,,wos,"This paper investigates the price behavior of a put option embedded in a callable U.S. Treasury bond by examining the impact of U.S. Treasury bond futures. The study found that the option's value was persistently negative for an extended period, attributed to the callable bond becoming the cheapest to deliver issue against futures contracts. This suggests that derivative assets can significantly influence pricing in the primary asset market.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:18:55.926246
eddff9ceb55591e5,Network Centrality and Managerial Market-Timing Ability,"We document that long-run excess returns following announcements of share buyback authorizations and insider purchases are a U-shaped function of firm centrality in the input-output trade-flow network. These results conform to a model of investors endowed with a large but finite capacity for analyzing firms. Additional links weaken insiders' informational advantage in peripheral firms (simple firms whose cash flows depend on few economic links), provided investors' capacity is large enough, but eventually amplify that advantage in central firms (firms with many links) as a result of investors' limited capacity. These findings shed light on the sources of managerial market-timing ability.","Evgeniou, Theodoros; Peress, Joel; Vermaelen, Theo; Yue, Ling",2022,10.1017/s0022109021000016,,wos,"This study investigates the relationship between a firm's centrality in the input-output trade-flow network and managerial market-timing ability, specifically concerning share buyback authorizations and insider purchases. The findings suggest that long-run excess returns exhibit a U-shaped pattern based on firm centrality, explained by a model where investors have limited capacity for analyzing firms. Increased network links can weaken or amplify insiders' informational advantage depending on firm centrality and investor capacity.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:19:20.420142
11848a5aa1999a9c,Neural Ordinary Differential Equation Networks for Fintech Applications Using Internet of Things,"The Internet of Things (IoT) technology is becoming increasingly pivotal in the financial services sector, with a growing number of algorithms being employed in high-frequency trading. High-frequency prediction in financial time series prediction presents a promising avenue of research. From convolutional neural networks to recurrent neural networks, deep learning have demonstrated exceptional capabilities in capturing the nonlinear characteristics of stock markets, thereby achieving high performance in stock index prediction. In this article, we employ ODE-LSTM model for high-frequency price forecasting, predicting stock price data across various time scales, including 1-, 5-, and 30-min frequencies. This approach introduces a novel concept, wherein the long short-term memory (LSTM) model is integrated with Neural ordinary differential equations (ODEs) to manage the hidden state and augment model interpretability. Over the course of 7 months, we achieved a 41.79% excess return on a simulated trading platform, with a daily average excess return of 0.30%, showcasing the commendable performance of our model and strategy.",J. Li; W. Chen; Y. Liu; J. Yang; D. Zeng; Z. Zhou,2024,10.1109/jiot.2024.3376748,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10472330,ieeexplore,"This article proposes an ODE-LSTM model, integrating Neural Ordinary Differential Equations with LSTM, for high-frequency stock price forecasting using IoT data. The model demonstrated strong performance, achieving a 41.79% excess return over 7 months on a simulated trading platform.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:19:25.927013
c828201ee931a14e,Neural network and machine learning use cases: Indian bond market predictions,"This study examines machine learning techniques to investigate how artificial intelligence (AI) affects predicting future trends in the bond market. The bond market offers a global perspective on capital costs for a business by establishing the fair value of the bond issue, which is based on multiple factors. The asset price market, which has employed machine learning (ML) and deep learning (DL) techniques to address the primary forecasting difficulty, surprisingly plays a significant role in predicting fut ure bond market returns. As an outcome, if this gap can be forecast, it can act as the bond market's data-driven long-term direction and yield additional profits. Daily securityspecific data for the 10-to-3-year Indian Treasury Bond (ITB) was gathered from 2013 to 2022 and is available in the global government bonds database. The researchers looked at how well the auto-regressive integrated moving average (ARIMA), linear regression, and deep recurrent neural network-long short-term memory (DLSTM) models could predict bond yields and returns in future bond markets. The empirical results demonstrate that the DLSTM models most fairly predict the price of government bonds over both the short and longer horizons when compared to ARIMA and linear regression. © 2024 Elsevier B.V., All rights reserved.","Antony, J.M.; Sundaram, S.",2024,10.18488/29.v11i1.3667,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195129902&doi=10.18488%2F29.V11I1.3667&partnerID=40&md5=5da04c48a69be5670e5f5b563929cbfc,scopus,"This study investigates the use of machine learning (ML) and deep learning (DL) techniques, specifically auto-regressive integrated moving average (ARIMA), linear regression, and deep recurrent neural network-long short-term memory (DLSTM) models, to predict future trends, yields, and returns in the Indian bond market. Daily data for the 10-to-3-year Indian Treasury Bond (ITB) from 2013 to 2022 was used. The DLSTM model demonstrated the most accurate predictions compared to ARIMA and linear regression.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T12:19:38.300573
c2d699d6e932c05d,Nonlinear least squares estimator for generalized diffusion processes with reflecting barriers,"In this paper, we investigate the parameter estimation problem for generalized diffusion processes with two-sided reflected barriers. The estimator is obtained using the nonlinear least squares method based on discretely observed processes. Under certain regularity conditions, we obtain consistency and establish the asymptotic normality of the proposed estimator. Our method can be readily applied to the one-sided reflected diffusion processes. Numerical results, including a two-factor financial model, show that the proposed estimator performs well with large sample sizes. The U.S. treasury rate data is used to illustrate the theoretical results.",,2025,10.1080/17442508.2024.2393257,,proquest,"This paper proposes a nonlinear least squares estimator for generalized diffusion processes with reflecting barriers, using discretely observed data. The estimator is shown to be consistent and asymptotically normal under regularity conditions. The method is applicable to one-sided reflected diffusion processes and is demonstrated with a two-factor financial model and U.S. treasury rate data, showing good performance for large sample sizes.",True,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:20:00.872404
8fbc26d4a8606d0f,Nonlinear mean reversion in the term structure of interest rates,"The expectations hypothesis implies that the yield curve provides information on the future change in the short-term interest rate. However, transaction costs exist in the financial market, which prevent investors from realizing the arbitrage opportunity, when the arbitrage does not fully cover the transaction costs. The purpose of this paper is to assess the effect of transaction costs on the predictability of the term structure by using the threshold vector error correction model, which allows for the nonlinear adjustment to the long-run equilibrium relationship. A significant amount of threshold effect is found, and the adjustment coefficients are regime-dependent. The empirical result supports the nonlinear mean reversion in the term structure of interest rates. © 2002 Elsevier Science B.V. All rights reserved. © 2004 Elsevier Science B.V., Amsterdam. All rights reserved.","Seo, B.",2003,10.1016/s0165-1889(02)00124-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038330312&doi=10.1016%2FS0165-1889%2802%2900124-0&partnerID=40&md5=9658712d925b977d5436dcf9eb640c5b,scopus,"This paper investigates the impact of transaction costs on the predictability of the term structure of interest rates. Using a threshold vector error correction model, it demonstrates nonlinear mean reversion and regime-dependent adjustment coefficients, suggesting that transaction costs influence the market's ability to predict future interest rate changes.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T12:20:41.107600
08f7e1c21ff490c4,Nonlinear predictability of stock returns using financial and economic variables,"Inspired by the linear predictability and nonlinearity found in the finance literature, this article examines the nonlinear predictability of the excess returns. The relationship between the excess returns and the predicting variables is recursively modeled by a neural-network model, which is capable of performing flexible nonlinear functional approximation. The nonlinear neural-network model is found to have better in-sample fit and out-of-sample forecasts compared to its linear counterpart. Moreover, the switching portfolio based on the recursive neural-network forecasts generates higher profits with lower risks than both the buy-and-hold market portfolio and the switching portfolio based on linear recursive forecasts. © 2017 Elsevier B.V., All rights reserved.","Qi, M.",1999,10.1080/07350015.1999.10524830,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033435319&doi=10.1080%2F07350015.1999.10524830&partnerID=40&md5=5168aa09c44a851bad6e254e59934493,scopus,"This article investigates the nonlinear predictability of stock returns using a neural network model, which demonstrates superior in-sample fit and out-of-sample forecasts compared to linear models. The study also shows that a switching portfolio based on these nonlinear forecasts yields higher profits with lower risks.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:20:55.877533
412b7080ec4282c9,Nonlinear structural estimation of corporate bond liquidity,"We estimate the term structure of corporate bond liquidity premiums using a dual estimation technique. Our estimates reveal that the term structures of the liquidity premiums were positively sloped and concave for each category of creditworthiness and in three economic epochs. As the macroeconomy transitioned from a pre-crisis to a crisis period, liquidity premiums elevated across time to maturity for both investment grade and speculative grade bonds. With the migration of the financial system from stress to relative calm, the premiums on both grades of debt declined for all maturities.",,2025,10.1007/s11156-024-01323-y,,proquest,"This study estimates the term structure of corporate bond liquidity premiums using a dual estimation technique. The findings indicate that these premiums are positively sloped and concave across different creditworthiness categories and economic periods. Liquidity premiums increased during economic crises and decreased during periods of financial stability, affecting both investment-grade and speculative-grade bonds across all maturities.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:21:00.677999
c004c1af8e9a15c6,Nonlinearity and Flight‐to‐Safety in the Risk‐Return Trade‐Off for Stocks and Bonds,"We document a highly significant, strongly nonlinear dependence of stock and bond returns on past equity market volatility as measured by the VIX. We propose a new estimator for the shape of the nonlinear forecasting relationship that exploits variation in the cross‐section of returns. The nonlinearities are mirror images for stocks and bonds, revealing flight‐to‐safety: expected returns increase for stocks when volatility increases from moderate to high levels while they decline for Treasuries. These findings provide support for dynamic asset pricing theories in which the price of risk is a nonlinear function of market volatility.",,2019,10.1111/jofi.12776,,proquest,"This study reveals a significant, nonlinear relationship between stock and bond returns and past equity market volatility (VIX). A novel estimator is introduced to capture this nonlinear forecasting relationship. The findings indicate a 'flight-to-safety' phenomenon, where stock returns rise with increasing volatility (moderate to high levels), while Treasury returns fall. This supports dynamic asset pricing theories suggesting a nonlinear price of risk dependent on market volatility.",True,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:21:32.960721
eae3754b32581403,OP73 Human and financial costs of six early years disadvantages in the UK and Bradford: a birth cohort microsimulation study,"BackgroundEarly childhood disadvantages (up to age 5) can have life-long effects on health and wellbeing. Methods of birth cohort microsimulation can capture these long-term effects and the associated public cost savings, which are hard to estimate using conventional methods (e.g. trials) because the effects may take decades to manifest. We aimed to quantify the long-term effects and public costs of six different early years disadvantages up to age 17 in the UK and Bradford, a multi-ethnic deprived local authority, using a microsimulation model based on the UK Millennium Cohort Study.MethodsUsing a cohort of 15,380 children from the Millenium Cohort Study (MCS) we model early years risk factors (conception to age 5) and subsequent outcomes up to age 17. We choose six risk factors: having a teenage mother, preterm birth, low birthweight (for gestational age), low height (age 5); disability (age 5), and school readiness (age 5). The causal effect parameters used in our microsimulation model are estimated using regressions based on Directed Acyclic Graphs (DAGs) which clearly set out our causal inference assumptions. We quantify a set of policy-relevant outcomes and their annual public costs for the UK and calibrate to Bradford based on local prevalence and population data. We assess the robustness of our findings to alternative assumptions and measures.ResultsThe public cost up to age 17 ranged between £86,058 [44,114–128,002] per 1,000 children for having a teenage mother, or £58,544 for each annual Bradford cohort to £432,920 [263,733–600,108] for school readiness or £1,872,265 for each Bradford cohort. The wellbeing impact ranged from 21 [-37–78] WELLBYs per 1,000 children for low birth weight or 20 per Bradford cohort, to 268 [245–290] for school readiness or 1,160 per Bradford cohort. Each WELLBY is valued by the UK Treasury at £13,000.ConclusionImproving school readiness yielded a larger wellbeing gain and public cost savings per child beneficiary than eliminating any of the other disadvantages we examined, but less than reducing early childhood poverty by moving families from the bottom income quintile to the next. When combined with evidence on short term effects of interventions, comparative long-term estimates of this kind may help policymakers prioritise and justify early years investments.We will report full results for the Uk and Bradford at the meeting, including a ready reckoner table of the long-term benefits and costs of reducing each childhood disadvantage.",,2025,10.1136/jech-2025-ssmabstracts.73,,proquest,"This study uses a birth cohort microsimulation model based on the UK Millennium Cohort Study to quantify the long-term effects and public costs of six early years disadvantages (teenage mother, preterm birth, low birthweight, low height, disability, school readiness) up to age 17 in the UK and Bradford. The findings indicate that improving school readiness offers the greatest wellbeing gain and public cost savings per child, though reducing early childhood poverty is even more impactful. The study suggests these long-term estimates can aid policymakers in prioritizing early years investments.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:21:56.735570
1475fced39bb3621,OPTIMAL ASSET ALLOCATION WITH MULTIVARIATE BAYESIAN DYNAMIC LINEAR MODELS,"We introduce a fast, closed-form, simulation-free method to model and forecast multiple asset returns and employ it to investigate the optimal ensemble of features to include when jointly predicting monthly stock and bond excess returns. Our approach builds on the Bayesian dynamic linear models of West and Harrison (Bayesian Forecasting and Dynamic Models (1997) Springer), and it can objectively determine, through a fully automated procedure, both the optimal set of regressors to include in the predictive system and the degree to which the model coefficients, volatilities and covariances should vary over time. When applied to a portfolio of five stock and bond returns, we find that our method leads to large forecast gains, both in statistical and economic terms. In particular, we find that relative to a standard no-predictability benchmark, the optimal combination of predictors, stochastic volatility and time-varying covariances increases the annualized certainty equivalent returns of a leverage-constrained power utility investor by more than 500 basis points.","Fisher, Jared D.; Pettenuzzo, Davide; Carvalho, Carlos M.",2020,10.1214/19-aoas1303,,wos,"This paper presents a novel, efficient method for modeling and forecasting multiple asset returns using Bayesian dynamic linear models. It automates the selection of optimal regressors, stochastic volatilities, and time-varying covariances. Applied to stock and bond returns, the method significantly improves forecast accuracy and investor returns compared to a benchmark.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T12:22:14.718238
ebf6d44f3b36d6fb,On Gaussian HJM framework for Eurodollar Futures,"One of the standard tools for the theoretical analysis of fixed income securities and their associated derivatives is the term structure model of Heath, Jarrow and Morton. In this paper the question, what specific HJM model is consistent with the observed price of an Eurodollar Futures contract? is discussed. Eurodollar Futures, apart from being the most heavily traded futures are connected to London Inter Bank Offered Rate (LIBOR) and to domestic monetary conditions. The answer to the above question will help in pricing any new derivative on Eurodollar Futures or the one that is not heavily traded. A simple tool to measure the adequacy of different HJM structures that may be used to model Eurodollar Futures price process is suggested. Moreover, the question of estimation of parameters of these models by different methods-method of realized volatility, method of maximum likelihood (ML) and a two-stage method that combines both the realized volatility and ML-is addressed. Although it sounds like a typical statistical procedure, one must be careful in applying standard statistical techniques that are not suitable under arbitrage theory, in particular, ML method. Copyright © 2010 John Wiley & Sons, Ltd. © 2011 Elsevier B.V., All rights reserved.","Raman, B.; Pozdnyakov, V.",2011,10.1002/asmb.845,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051734977&doi=10.1002%2Fasmb.845&partnerID=40&md5=0fae9ebfb31357f6d62e923e89e64c32,scopus,"This paper investigates the Gaussian Heath, Jarrow, and Morton (HJM) framework for modeling Eurodollar Futures, aiming to determine which HJM model aligns with observed Eurodollar Futures prices. It also proposes a method to assess the suitability of different HJM structures for modeling the Eurodollar Futures price process and discusses parameter estimation techniques, including the method of realized volatility, maximum likelihood (ML), and a combined two-stage approach, cautioning against the direct application of standard ML methods under arbitrage theory.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:22:28.785650
a174f9343be6fc86,On filtering in Markovian term structure models: An approximation approach,"We consider a parametrization of the Heath-Jarrow-Morton (HJM) family of term structure of interest rate models that allows a finite-dimensional Markovian representation of the stochastic dynamics. This parametrization results from letting the volatility function depend on time to maturity and on two factors: the instantaneous spot rate and one fixed-maturity forward rate. Our main purpose is an estimation methodology for which we have to model the observations under the historical probability measure. This leads us to consider as an additional third factor the market price of interest rate risk, that connects the historical and the HJM martingale measures. Assuming that the information comes from noisy observations of the fixed-maturity forward rate, the purpose is to estimate recursively, on the basis of this information, the three Markovian factors as well as the parameters in the model, in particular those in the volatility function. This leads to a nonlinear filtering problem, for the solution of which we describe an approximation methodology, based on time discretization and quantization. We prove the convergence of the approximate filters for each of the observed trajectories. © 2005 Elsevier Science B.V., Amsterdam. All rights reserved.","Chiarella, C.; Pasquali, S.; Runggaldier, W.J.",2001,10.1239/aap/1011994030,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035710457&doi=10.1239%2Faap%2F1011994030&partnerID=40&md5=354532f209d840951e8d6199d8470815,scopus,This paper proposes an approximation methodology for estimating parameters in a Markovian representation of the Heath-Jarrow-Morton (HJM) term structure of interest rate models. The approach involves time discretization and quantization to address a nonlinear filtering problem arising from noisy observations of a fixed-maturity forward rate. The goal is to recursively estimate the model's Markovian factors and volatility parameters.,False,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:22:44.842154
f54835efc01a2af6,On modeling IPO failure risk,"This paper offers a novel framework, combining firm operational risk, IPO pricing risk, and market risk, to model IPO failure risk. By analyzing nearly a thousand variables, we observe that prior IPO failure risk models have suffered from a major missing-variable problem. Evidence reveals several key new firm-level determinants, e.g., the volatility operating performance, the size of its accounts payable, pretax income to common equity, total short-term debt, and a few macroeconomic variables such as treasury bill rate, and book-to-market of the DJIA index. These findings have major economic implications. The total value loss from not predicting the imminent failure of an IPO is significantly lower with this proposed model compared to other established models. The IPO investors could have saved around $18billion over the period between 1994 and 2016 by using this model. © 2022 Elsevier B.V., All rights reserved.","Colak, G.; Fu, M.; Hasan, I.",2022,10.1016/j.econmod.2022.105790,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124421011&doi=10.1016%2Fj.econmod.2022.105790&partnerID=40&md5=f16401aacf4f384db51620d5c6763e73,scopus,"This paper introduces a new framework to model IPO failure risk by integrating operational, pricing, and market risks. It identifies key firm-level and macroeconomic determinants, suggesting that previous models suffered from missing variables. The proposed model demonstrates significant improvements in predicting IPO failures, potentially saving investors substantial amounts.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:22:59.564816
7cf8480c0b3c77f9,On some filtering problems arising in mathematical finance,"Three situations in which filtering theory is used in mathematical finance are illustrated at different levels of detail. The three problems originate from the following different works: (1) On estimating the stochastic volatility model from observed bilateral exchange rate news, by Mahieu and Schotman (1997). (2) A state space approach to estimate multi-factors CIR models of the term structure of interest rates, by Geyer and Pichler (1996). (3) Risk-minimizing hedging strategies under partial observation in pricing financial derivatives, by Fischer et al. (1996). In the first problem we propose to use a recent nonlinear filtering technique based on geometry to estimate the volatility time series from observed bilateral exchange rates. The model used here is the stochastic volatility model. The filters that we propose are known as projection filters, and a brief derivation of such filters is given. The second problem is introduced in detail, and a possible use of different filtering techniques is hinted at. In fact the filters used for this problem in (2) and part of the literature can be interpreted as projection filters and we will make some remarks on how more general and possibly more suitable projection filters can be constructed. The third problem is only presented briefly. © 1998 Elsevier Science B.V. All rights reserved. © 2018 Elsevier B.V., All rights reserved.","Brigo, D.; Hanzon, B.",1998,10.1016/s0167-6687(98)00008-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032523514&doi=10.1016%2FS0167-6687%2898%2900008-0&partnerID=40&md5=f63a02c53a85f528403e3d4c8fbf372d,scopus,"This paper illustrates three filtering problems in mathematical finance: estimating stochastic volatility from exchange rates, estimating multi-factor CIR models for interest rates, and risk-minimizing hedging strategies under partial observation. It proposes nonlinear filtering techniques, specifically projection filters, for the first problem and discusses their application to the second. The third problem is briefly presented.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T12:23:33.736024
ad0a338018943161,On the nonlinear predictability of stock returns using financial and economic variables,"In a recent article by Qi, neural networks trained by Bayesian regularization were used to predict excess returns on the S&P 500. The article concluded that the switching portfolio based on the recursive neural-network forecasts generates higher accumulated wealth with lower risks than that based on linear regression. Unfortunately, attempts to replicate the results were unsuccessful. Replicated results using the same software, approach and data detailed by Qi indicate that, in fact, the switching portfolio based on the recursive neural-network forecasts generates lower accumulated wealth with higher risks than that based on linear regression. © 2005 Elsevier Science B.V., Amsterdam. All rights reserved.","Racine, J.",2001,10.1198/073500101681019927,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035638655&doi=10.1198%2F073500101681019927&partnerID=40&md5=2f633ec6cfbf429ce536720d0c842b79,scopus,"This article attempts to replicate the findings of Qi (2005) regarding the nonlinear predictability of stock returns using neural networks. The authors found that their replication efforts were unsuccessful, and the neural network approach actually performed worse than linear regression in terms of accumulated wealth and risk.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:23:40.183363
23e04bea77ea6c28,On the significance testing of fuzzy regression applied to the CAPM: Canadian commodity futures evidence,"This paper is written with two congruent objectives. The first is to develop a framework for individual tests of significance of a fuzzy regression model by employing a simple probabilistic estimation procedure. The proposed test, based on two-phase fuzzy regression estimates, is simple and robust. The capital asset pricing model (CAPM), with induction of price limits, serves as the essential component of our analysis, due to its ability to illuminate and determine the risk premiums in a commodity futures market. The second objective is to estimate and test for the significance of the systematic risk of Canadian commodity futures and to illustrate the benefits of the significancetesting approach. Copyright © 2013 Inderscience Enterprises Ltd. © 2020 Elsevier B.V., All rights reserved.","Smimou, K.",2013,10.1504/ijams.2013.053710,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877272226&doi=10.1504%2FIJAMS.2013.053710&partnerID=40&md5=b1910293cf57fa1c92dc80288a597b51,scopus,"This paper proposes a framework for testing the significance of fuzzy regression models, applying it to the Capital Asset Pricing Model (CAPM) within the context of Canadian commodity futures. The authors aim to estimate and test the significance of systematic risk in this market, highlighting the benefits of their proposed testing approach.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:23:43.438722
106e9eb0ed46aba0,Option Pricing With Modular Neural Networks,"This paper investigates a nonparametric modular neural network (MNN) model to price the S&P-500 European call options. The modules are based on time to maturity and moneyness of the options. The option price function of interest is homogeneous of degree one with respect to the underlying index price and the strike price. When compared to an array of parametric and nonparametric models, the MNN method consistently exerts superior out-of-sample pricing performance. We conclude that modularity improves the generalization properties of standard feedforward neural network option pricing models (with and without the homogeneity hint).",N. Gradojevic; R. Gencay; D. Kukolj,2009,10.1109/tnn.2008.2011130,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4798200,ieeexplore,"This paper proposes a nonparametric modular neural network (MNN) model for pricing S&P-500 European call options, utilizing modules based on time to maturity and moneyness. The MNN model demonstrates superior out-of-sample pricing performance compared to other models, suggesting that modularity enhances generalization properties.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:23:47.128919
e89217d91d1107e3,Option price sensitivities through fuzzy numbers,"The main motivation in using fuzzy numbers in finance lies in the need for modelling the uncertainty and vagueness that are implicit in many situations. However, the fuzzy approach should not be considered as a substitute for the probabilistic approach but rather as a complementary way to describe the model peculiarities. Here, we consider, in particular, the Black and Scholes model for option pricing, and we show that the fuzzification of some key parameters enables a sensitivity analysis of the option price with respect to the risk-free interest rate, the final value of the underlying stock price, the volatility, and also better forecasts (see Thavaneswaran etaaal. (2009) for details). The sensitivities with respect to the variables of the model are represented by different letters of the Greek alphabet and they play an important role in the definition of the shape of the fuzzy option price.",,2011,10.1016/j.camwa.2010.11.024,,proquest,"This paper explores the use of fuzzy numbers to model uncertainty in option pricing, specifically within the Black-Scholes framework. It demonstrates how fuzzifying key parameters allows for a sensitivity analysis of option prices concerning interest rates, stock prices, and volatility, offering a complementary approach to probabilistic methods.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:23:53.083079
fc9d1d735105cd33,Option-Based Estimation of the Price of Coskewness and Cokurtosis Risk,"We show that the prices of risk for factors that are nonlinear in the market return can be obtained using index option prices. The price of coskewness risk corresponds to the market variance risk premium, and the price of cokurtosis risk corresponds to the market skewness risk premium. Option-based estimates of the prices of risk lead to reasonable values of the associated risk premia. An analysis of factor models with coskewness risk indicates that the new estimates of the price of risk improve the models' performance compared with regression-based estimates.",,2021,10.1017/s002210902000023x,,proquest,"This paper proposes a method to estimate the prices of coskewness and cokurtosis risk using index option prices. It demonstrates that these prices of risk can be derived from the market variance risk premium and market skewness risk premium, respectively. The study finds that option-based estimates yield reasonable risk premia and improve the performance of factor models compared to regression-based estimates.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:24:02.472393
4f8ea2176a7d1090,Option-implied skewness: Insights from ITM-options,"While the standard to calculate model-free option-implied skewness (MFIS) relies on out-of-the-money (OTM) options, we examine the empirical and economic implications of using in-the-money (ITM) options. We find that the positive short-term return predictability of OTM-based MFIS significantly reverses if ITM-options are used instead. While this reversal is inconsistent with an explanation based on skewness preferences, MFIS apparently reflects information that is not timely incorporated in stock prices due to market frictions. Based on these insights, we introduce Delta MFIS as a new measure of additional option-embedded information that significantly predicts subsequent returns beyond a large range of other option-based return predictors. (C) 2021 Elsevier B.V. All rights reserved.","Mohrschladt, Hannes; Schneider, Judith C.",2021,10.1016/j.jedc.2021.104227,,wos,"This study investigates the implications of using in-the-money (ITM) options versus out-of-the-money (OTM) options for calculating model-free option-implied skewness (MFIS). The authors find that using ITM options reverses the short-term return predictability observed with OTM-based MFIS. This suggests that MFIS captures information not immediately reflected in stock prices due to market frictions. They propose a new measure, Delta MFIS, which demonstrates significant predictive power for subsequent returns, outperforming other option-based predictors.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:24:09.506579
f2eb271685e466d0,PARAMETRIC SPECIFICATION TEST FOR NONLINEAR AUTOREGRESSIVE MODELS,"The paper considers testing parametric assumptions on the conditional mean and variance functions for nonlinear autoregressive models. To this end, we compare the kernel density estimate of the marginal density of the process with a convolution-type density estimate. It is shown that, interestingly, the latter estimate has a parametric (root n) rate of convergence, thus substantially improving the classical kernel density estimates whose rates of convergence are much inferior. Our results are confirmed by a simulation study for threshold autoregressive processes and autoregressive conditional heteroskedastic processes.","Kim, Kun Ho; Zhang, Ting; Wu, Wei Biao",2015,10.1017/s0266466614000681,,wos,"This paper proposes a parametric specification test for nonlinear autoregressive models by comparing kernel density estimates of the marginal density with a convolution-type density estimate. The convolution-type estimate achieves a parametric rate of convergence, outperforming classical kernel estimates. The method is validated through simulations on threshold autoregressive and autoregressive conditional heteroskedastic processes.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:24:18.267283
f6f95c3a073ff11d,Parameter Estimations of Heston Model Based on Consistent Extended Kalman Filter,"Heston model is widely applied to financial institutions, while there still exist difficulties in estimating the parameters and volatilities of this model. In this paper, the pseudo Maximum Likelihood Estimation and consistent extended Kalman filter (PMLE-CEKF) are implemented synchronously to estimate the Heston model. For parameter estimations, PMLE for the state equation and the measurement equation of the Heston model are conducted independently. For volatility estimations, the consistent extended Kalman filter (CEKF) algorithm is introduced to ensure the volatility to be well evaluated. Additionally, the estimation results of the Heston model are compared between PMLE-CEKF and PMLE-EKF algorithm. The numerical simulations illustrate that PMLE-CEKF algorithm works more efficiently than PMLE-EKF algorithm. Application of the PMLE-CEKF to S&P 500 shows the utility of the proposed algorithm. (C) 2017, IFAC (International Federation of Automatic Control) Hosting by Elsevier Ltd. All rights reserved.","Wang, Ximei; He, Xingkang; Zhao, Yanlong; Zuo, Zhiqiang",2017,10.1016/j.ifacol.2017.08.1850,,wos,"This paper implements a synchronous approach using Pseudo Maximum Likelihood Estimation (PMLE) and a Consistent Extended Kalman Filter (CEKF) to estimate parameters and volatilities of the Heston model, a model commonly used in financial institutions. The study compares the PMLE-CEKF algorithm with the PMLE-EKF algorithm, demonstrating the former's superior efficiency through numerical simulations and an application to the S&P 500 index.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:24:31.196178
f103b6f4fbd0108a,Portfolio Selection and Optimization through Neural Networks and Markowitz Model: A Case of Pakistan Stock Exchange Listed Companies,"This paper used artificial neural networks (ANNs) time series predictor for approximating returns of Pakistan Stock Exchange (PSX) listed 100 companies. These projected returns are then substituted into expected returns in the Markowitz’s Mean Variance (MV) portfolio Model. For comparison empirical data used is closing prices of PSX listed stocks, Karachi Inter Bank Offer Rates (KIBOR) as risk free rate and KSE-all share index as benchmark. The Portfolio returns are compared for two datasets by employing various constraints like budget, transaction costs, and turnover constraints. The value of portfolios is measured through Sharpe ratio and Information ratio. Both Sharpe and Information ratios support use of ANNs as return predictor and optimisation tool over simple MV model implemented for empirical data as well as predicted data. ANNs framework performed better in both Long and Short positions and its portfolio returns are significantly higher as compared with MV.",,2019,10.26710/reads.v5i1.354,,proquest,"This paper applies Artificial Neural Networks (ANNs) to predict stock returns for companies listed on the Pakistan Stock Exchange (PSX). These predicted returns are then integrated into the Markowitz Mean Variance (MV) portfolio optimization model. The study compares the performance of portfolios constructed using ANNs with those using the standard MV model, considering various constraints and using Sharpe and Information ratios for evaluation. The results indicate that ANNs perform better as a return predictor and optimization tool, yielding significantly higher portfolio returns.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:24:36.439224
1fbbbf6449f5cb97,"Predictability and pricing efficiency in forward and spot, developed and emerging currency markets","We study the predictability of forward and spot exchange rates of currencies of emerging and developed economies from 1994 to 2016. Our purpose is to shed light on the efficiency of currency markets and how and why it has evolved over this time. For the currencies of emerging economies, our analysis of rates of return on forward contracts finds some evidence of excess-predictability, especially in the earlier parts of the sample period, consistent with the view that this portion of the foreign exchange market has only become efficient in recent times. When we turn our attention to excess-returns computed from spot exchange rates and spot interest rates, however, we find much less predictability. In particular, over our full sample period, we find no evidence of excess-predictability, in contrast with the results reported by Hsu et al. (2016) but in agreement with Kuang et al. (2014). The different predictability of spot excess-returns and rates of return on forward contracts is a manifestation of the widespread violation of covered interest parity which emerged with the onset of the 2008 financial crisis. © 2020 Elsevier B.V., All rights reserved.","Poti, V.; Levich, R.; Conlon, T.",2020,10.1016/j.jimonfin.2020.102223,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087713148&doi=10.1016%2Fj.jimonfin.2020.102223&partnerID=40&md5=694062d698f7310f40c42b54960ef8e1,scopus,"This study investigates the predictability of forward and spot exchange rates for emerging and developed economies from 1994 to 2016 to understand currency market efficiency. It found some evidence of excess predictability in emerging market forward contracts, particularly earlier in the sample, suggesting increasing efficiency over time. Predictability was lower for spot exchange rates and spot interest rates, with no evidence of excess predictability over the full sample. This difference is attributed to violations of covered interest parity following the 2008 financial crisis.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:24:44.847249
85aad9d7cb4653db,Predicting Equity Premium: A New Momentum Indicator Selection Strategy With Machine Learning,"We propose a new momentum-determined indicator-switching (N-MDIS) strategy, harnessing the power of machine learning to enhance the accuracy of equity premium prediction. Specifically, we re-examine the regime-dependent feature of univariate predictive regression relative to the benchmark. Furthermore, we investigate the prediction mechanism of the momentum-determined indicator-switching (MDIS) strategy and validate the significance of market regime information for the MDIS. Our findings demonstrate an overwhelmingly superior ex-post forecasting performance compared with the MDIS. More notably, our empirical results substantiate that machine learning greatly aids in momentum indicator selection. The results show that the N-MDIS with machine learning generates more accurate ex-ante equity premium forecasts than both MDIS strategy and N-MDIS strategy with logistic regression, yielding statistically and economically significant results. Moreover, our new approach exhibits robust forecasting performance across a series of robustness tests.","Qu, Yong; Yuan, Ying",2025,10.1002/for.3200,,wos,"This study introduces a novel momentum-determined indicator-switching (N-MDIS) strategy that utilizes machine learning to improve equity premium prediction. The strategy re-examines regime-dependent features of predictive regressions and validates the importance of market regime information for the MDIS. The N-MDIS with machine learning demonstrates superior ex-post forecasting performance compared to existing methods, including the MDIS strategy and N-MDIS with logistic regression, providing accurate ex-ante equity premium forecasts with statistically and economically significant results. The approach also shows robust forecasting performance.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:24:56.622294
9d7d055063aef573,Predicting bank inactivity: A comparative analysis of machine learning techniques for imbalanced data,"This study compares the predictive accuracy of a set of machine learning models coupled with three resampling techniques (Random Undersampling, Random Oversampling, and Synthetic Minority Oversampling Technique) in predicting bank inactivity. Our sample includes listed banks in EU-28 member states between 2011 and 2019. We employed 23 financial ratios comprising capital adequacy, asset quality, management capability, earnings, liquidity, and sensitivity indicators. The empirical findings established that XGBoost performs exceptionally well as a classifier in predicting bank inactivity, particularly when considering a one-year time frame before the event. Furthermore, our findings indicate that random forest with Synthetic Minority Oversampling Technique demonstrates the highest predictive accuracy two years prior to inactivity, while XGBoost with Random Oversampling outperforms other methods three years in advance. Furthermore, the empirical results emphasize the significance of management capability and loan quality ratios as key factors in predicting bank inactivity. Our findings present important policy implications.HighlightsBank inactivity predictive accuracy of machine learning techniques with resampling techniques is analyzed.Data on banks in the EU-28 member states between 2011 and 2019 are used.XGBoost performs exceptionally well one year before inactivity.Random Forest with Synthetic Minority Oversampling is the best classifier two years before inactivity.XGBoost with Random Oversampling outperforms other methods three years before inactivity.",,2025,10.1007/s10479-024-06018-0,,proquest,"This study compares machine learning models (XGBoost, Random Forest) with resampling techniques to predict bank inactivity using financial ratios from EU-28 banks (2011-2019). XGBoost was best one year prior, Random Forest with SMOTE two years prior, and XGBoost with Random Oversampling three years prior. Management capability and loan quality ratios were key predictors.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:25:13.197645
992e33dba092f9dd,Predicting corporate bankruptcy using the framework of Leland-Toft: evidence from U.S.,"In this paper, we evaluate an alternative approach for bankruptcy prediction that measures the financial healthiness of firms that have coupon-paying debts. The approach is based on the framework of Leland, H. and Toft, K.B. [Optimal capital structure, endogenous bankruptcy and the term structure of credit spreads. J. Financ., 1996, 51, 987–1019], which is an extension of a widely-used model; the Black–Scholes–Merton model. Using U.S. public firms between 1995 and 2014, we show that the Leland-Toft approach is more powerful than Black–Scholes–Merton in a variety of tests. Moreover, extending popular but also contemporary corporate bankruptcy models with the probability of bankruptcy derived from the Leland-Toft model, such as Altman, E. [Financial ratios, discriminant analysis and the prediction of corporate bankruptcy. J. Financ., 1968, 23, 589–609], Ohlson, J.A. [Financial ratios and the probabilistic prediction of bankruptcy. J. Account. Res., 1980, 18, 109–131] and Campbell, J. Y., Hilscher, J. and Szilagyi, J. [In search of distress risk. J. Financ., 2008, 63, 2899–2939], yields models with improved performance. One of our tests, for example, shows that banks using these extended models, achieve superior economic performance relative to other banks. Our results are consistent under a comprehensive out-of-sample framework. © 2020 Elsevier B.V., All rights reserved.","Charalambous, C.; Martzoukos, S.H.; Taoushianis, Z.",2020,10.1080/14697688.2019.1667519,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074765488&doi=10.1080%2F14697688.2019.1667519&partnerID=40&md5=b15d939b006661b2bf31cd24674df7cd,scopus,"This paper evaluates an alternative approach for bankruptcy prediction using the Leland-Toft framework, which extends the Black-Scholes-Merton model. The study uses U.S. public firms from 1995-2014 and demonstrates that the Leland-Toft approach outperforms the Black-Scholes-Merton model. Incorporating the Leland-Toft probability of bankruptcy into existing models like Altman, Ohlson, and Campbell et al. also improves their performance. The findings are robust under out-of-sample testing, with banks using these extended models showing superior economic performance.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:25:18.133061
fd7272563aec0c21,Predicting daily oil prices: Linear and non-linear models,"In this paper, we assess the accuracy of linear and nonlinear models in predicting daily crude oil prices. Competing forecasts of crude oil prices are generated from parsimonious linear models which require no parameter estimation, as well as linear and nonlinear models. Two of the linear models that we employ exploit the informational content of oil demand and the increasing correlation between oil and equity prices and are novel to the literature. The nonlinear model that we consider is an artificial neural network. More specifically, we consider a bagged neural network, a neural network trained using the genetic algorithm as well as a neural network with fuzzy logic. We find that some of the linear models outperform the random walk in terms of out-of-sample statistical forecast accuracy. Our findings also suggest that while the buy-and-hold strategy dominates some of the models in terms of dollar payoffs and risk-adjusted returns under a long-only strategy, all the models that we consider generate higher dollar payoffs than the buy-and-hold strategy under the short-only strategy. An investor obtains the largest profits by trading based on the moving average convergence divergence which is a technical indicator.","Dbouk, Wassim; Jamali, Ibrahim",2018,10.1016/j.ribaf.2018.01.003,,wos,"This paper compares the predictive accuracy of linear and nonlinear models for daily crude oil prices. It includes novel linear models incorporating oil demand and oil-equity correlations, and nonlinear models such as bagged neural networks, genetic algorithm-trained neural networks, and fuzzy logic neural networks. Some linear models outperform the random walk, and while buy-and-hold is superior for long-only strategies, all models yield better payoffs than buy-and-hold for short-only strategies. Trading based on the moving average convergence divergence indicator yields the largest profits.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:25:21.056330
3bb815ed509032e9,Predicting risk premium under changes in the conditional distribution of stock returns,"The goal of this paper is to assess time-variation in asset returns while considering the whole conditional distribution. We use a quantile regression framework and quarterly data for the U.S., and show that the probabilistic distribution of expectations about future stock returns changes in response to variation in commonly used explanatory variables. Moreover, our results support the idea that lower quantiles are less stable than upper quantiles, thus, suggesting that asset pricing models are particularly accurate in capturing the expectations that less risk-averse agents have about future returns. © 2017 Elsevier B.V., All rights reserved.","Sousa, J.; Sousa, R.M.",2017,10.1016/j.intfin.2017.09.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030168894&doi=10.1016%2Fj.intfin.2017.09.002&partnerID=40&md5=302310729f73705e6b4efcc18f0ecccc,scopus,"This paper uses quantile regression and quarterly U.S. data to assess time-variation in asset returns by considering the entire conditional distribution. It demonstrates that the probabilistic distribution of expectations about future stock returns changes in response to variations in explanatory variables. The findings suggest that lower quantiles are less stable than upper quantiles, implying that asset pricing models are more accurate for expectations of less risk-averse agents.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:25:34.773964
cc4a38ac36cf4d52,Predicting tanker freight rates using parsimonious variables and a hybrid artificial neural network with an adaptive genetic algorithm,"Short-term prediction of tanker freight rates (TFRs) is strategically important to stakeholders in the oil shipping industry. This study develops a hybrid TFR prediction model based on an artificial neural network (ANN) and an adaptive genetic algorithm (AGA). The AGA adaptively searches satisficing network parameters such as input delay size. The ANN iteratively optimizes a prediction network considering parsimonious variables and time-lag effects as predictors. Three parsimonious variables (crude oil price, fleet productivity and bunker price) are selected by a stepwise regression of TFR variables. The article compares the performance of its hybrid model with two traditional approaches (regression and moving average), as well as with the findings of existing ANN studies. The results of our model (root mean squared error (RMSE)= 11.2 WS) are not only significantly superior to the regression approach (RMSE = 21.6 WS) and the moving average approach (RMSE = 17.5 WS), but are even slightly superior to the results of existing ANN studies (RMSE = 14.6 WS-15.8 WS).","Eslami, Payman; Jung, Kihyo; Lee, Daewon; Tjolleng, Amir",2017,10.1057/mel.2016.1,,wos,"This study proposes a hybrid model combining an Artificial Neural Network (ANN) with an Adaptive Genetic Algorithm (AGA) to predict tanker freight rates (TFRs). The model utilizes parsimonious variables like crude oil price, fleet productivity, and bunker price, and demonstrates superior performance compared to regression, moving average, and existing ANN studies.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:25:39.353261
f69c831266f5be91,Predicting the Canadian Yield Curve Using Machine Learning Techniques,"This study applies machine learning methods to predict the Canadian yield curve using a comprehensive set of macroeconomic variables. Lagged values of the yield curve and a wide array of Canadian and international macroeconomic variables are utilized across various machine learning models. Hyperparameters are estimated to minimize mispricing across government bonds with different maturities. The Group Lasso algorithm outperforms the other models studied, followed by Lasso. In addition, the majority of the models outperform the Random Walk benchmark. The feature importance analysis reveals that oil prices, bond-related factors, labor market conditions, banks’ balance sheets, and manufacturing-related factors significantly drive yield curve predictions. This study is one of the few that uses such a broad array of macroeconomic variables to examine Canadian macro-level outcomes. It provides valuable insights for policymakers and market participants, with its feature importance analysis highlighting key drivers of the yield curve.",,2025,10.3390/ijfs13030170,,proquest,"This study uses machine learning techniques, including Group Lasso and Lasso, to predict the Canadian yield curve by incorporating a wide range of macroeconomic variables. The Group Lasso model demonstrated superior performance compared to other models and a Random Walk benchmark. Key predictors identified include oil prices, bond-related factors, labor market conditions, banks' balance sheets, and manufacturing indicators. The research offers significant insights for policymakers and market participants by identifying crucial drivers of the yield curve.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T12:25:50.485862
33fcf9fe6ddb79f7,Prediction of US 30‐years‐treasury‐bonds movement and trading entry point using the robust 1DCNN‐BiLSTM‐XGBoost algorithm,"This article presents a novel algorithm that accurately predicts market trends and identifies trading entry points for US 30‐year Treasury bonds. The proposed method employs a hybrid approach, integrating a 1‐dimensional convolutional neural network (1DCNN), long‐short term memory (LSTM), and XGBoost algorithms. The 1DCNN is used to learn local and short‐term patterns, while LSTM is employed to capture both short and long‐term dependencies. Furthermore, we have implemented an algorithm that utilizes hull moving average (HMA) and simple moving average (SMA) crossover data to detect trading entry points and major trends in the market. The combination of the SMA–HMA crossover algorithm and predictions provided by the 1DCNN‐BiLSTM‐XGBoost algorithm yields exceptional results in terms of prediction accuracy and profitability. Additionally, these integrated techniques effectively filter out noise and mitigate false breakouts, which are often observed with US 30‐year Treasury bonds. In the field of financial time series prediction, the effectiveness of 1DCNN and LSTM in identifying trading entry points and market perturbations has not been comprehensively studied. Therefore, our work fills this gap by demonstrating through experiments that the proposed 1DCNN‐BiLSTM‐XGBoost algorithm, in combination with moving average crossovers, effectively reduces noise and market perturbations. This leads to the precise identification of trading entry points and accurate recognition of trend signals for US 30‐year Treasury bonds. We demonstrate through experiments that our proposed approach achieves an average root mean squared error of 0.0001 and an R‐square value of 0.9999, highlighting its promise as a method for predicting market trends and trading entry points for US 30‐year Treasury bonds.",,2024,10.1111/exsy.13459,,proquest,"This study introduces a novel algorithm combining 1DCNN, BiLSTM, and XGBoost, along with Hull Moving Average (HMA) and Simple Moving Average (SMA) crossovers, to predict US 30-year Treasury bond market trends and identify trading entry points. The approach effectively captures short and long-term patterns, filters noise, and mitigates false breakouts, achieving high prediction accuracy with an RMSE of 0.0001 and R-square of 0.9999.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T12:26:16.510606
60d9172998b8c2be,Prediction of long-term government bond yields using statistical and artificial intelligence methods,"This chapter investigates the use of different artificial intelligence and classical techniques for forecasting the monthly yield of the US 10-year Treasury bonds from a set of four economic indicators. The task is particularly challenging due to the sparseness of the data samples and the complex interactions amongst the variables. At the same time, it is of high significance because of the important and paradigmatic role played by the US market in the world economy. Four data-driven artificial intelligence approaches are considered: a manually built fuzzy logic model, a machine learned fuzzy logic model, a self-organising map model, and a multi-layer perceptron model. Their prediction accuracy is compared with that of two classical approaches: a statistical ARIMA model and an econometric error correction model. The algorithms are evaluated on a complete series of end-month US 10-year Treasury bonds yields and economic indicators from 1986:1 to 2004:12. In terms of prediction accuracy and reliability, the best results are obtained by the three parametric regression algorithms, namely the econometric, the statistical, and the multi-layer perceptron model. Due to the sparseness of the learning data samples, the manual and the automatic fuzzy logic approaches fail to follow with adequate precision the range of variations of the US 10-year Treasury bonds. For similar reasons, the self-organising map model performs unsatisfactorily. Analysis of the results indicates that the econometric model has a slight edge over the statistical and the multi-layer perceptron models. This suggests that pure data-driven induction may not fully capture the complicated mechanisms ruling the changes in interest rates. Overall, the prediction accuracy of the best models is only marginally better than the prediction accuracy of a basic one-step lag predictor. This result highlights the difficulty of the modelling task and, in general, the difficulty of building reliable predictors for financial markets. © 2014 Springer International Publishing Switzerland. © 2016 Elsevier B.V., All rights reserved.","Castellani, M.; Dos Santos, E.A.",2014,10.1007/978-3-319-01866-9_11,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958526206&doi=10.1007%2F978-3-319-01866-9_11&partnerID=40&md5=5bd2589979558156e7b4404cf22ec603,scopus,"This study compares statistical and artificial intelligence methods for forecasting US 10-year Treasury bond yields. It evaluates fuzzy logic, self-organizing maps, multi-layer perceptrons, ARIMA, and error correction models using monthly data from 1986-2004. The best performing models were parametric regression algorithms (econometric, statistical, and multi-layer perceptron), though the econometric model showed a slight advantage. The study notes the difficulty of financial market prediction, with even the best models offering only marginal improvements over a simple one-step lag predictor.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T12:26:26.718041
70093db6f3573ba2,Predictive power of Markovian models: Evidence from US recession forecasting,"This paper provides extensions to the application of Markovian models in predicting US recessions. The proposed Markovian models, including the hidden Markov and Markov models, incorporate the temporal autocorrelation of binary recession indicators in a traditional but natural way. Considering interest rates and spreads, stock prices, monetary aggregates, and output as the candidate predictors, we examine the out-of-sample performance of the Markovian models in predicting the recessions 1-12 months ahead, through rolling window experiments as well as experiments based on the fixed full training set. Our study shows that the Markovian models are superior to the probit models in detecting a recession and capturing the recession duration. But sometimes the rolling window method may affect the models' prediction reliability as it could incorporate the economy's unsystematic adjustments and erratic shocks into the forecast. In addition, the interest rate spreads and output are the most efficient predictor variables in explaining business cycles.","Tian, Ruilin; Shen, Gang",2019,10.1002/for.2579,,wos,"This paper extends the use of Markovian models (hidden Markov and Markov models) for forecasting US recessions, incorporating temporal autocorrelation of recession indicators. It compares their out-of-sample performance against probit models using interest rates, spreads, stock prices, monetary aggregates, and output as predictors. The Markovian models demonstrate superiority in recession detection and duration capture, although rolling window methods can impact reliability. Interest rate spreads and output are identified as the most effective predictors.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:26:48.942003
7b8a48fc4285407a,Predictive power of investor sentiment for Bitcoin returns: Evidence from COVID-19 pandemic,"In this paper, we examine the impact of investor sentiment on Bitcoin returns. Using a large dataset of messages discussed on social media and several financial indicators, we create a sentiment indicator based on computational text analysis and driven by the principal component analysis (PCA) method. We utilize a vector autoregressive analysis and other analytical methods to examine the sentiment index-bitcoin return nexus. Our findings reveal that the sentiment index is a strong predictor of cryptocurrency market returns in the short term. Furthermore, we confirm that during the COVID-19 pandemic, investors' sentiments significantly impacted Bitcoin returns. Our results show that the proposed sentiment index can generate excess returns for investors who utilize it as a return predictor. Our empirical findings suggest important policy implications.","Bouteska, Ahmed; Mefteh-Wali, Salma; Dang, Trung",2022,10.1016/j.techfore.2022.121999,,wos,"This study investigates the predictive power of investor sentiment on Bitcoin returns, particularly during the COVID-19 pandemic. The authors construct a sentiment indicator using social media data and financial indicators via principal component analysis (PCA) and computational text analysis. Their findings indicate that this sentiment index is a significant short-term predictor of cryptocurrency returns, with a notable impact observed during the pandemic.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:26:55.388282
5957d41f05f017e9,Predictive trading strategy for physical electricity futures,"This article presents an original predictive strategy, based on a new mid-term forecasting model, to be used for trading physical electricity futures. The forecasting model is used to predict the average spot price, which is used to estimate the Risk Premium corresponding to electricity futures trade operations with a physical delivery. A feed-forward neural network trained with the extreme learning machine algorithm is used as the initial implementation of the forecasting model. The predictive strategy and the forecasting model only need information available from electricity derivatives and spot markets at the time of negotiation. In this paper, the predictive trading strategy has been applied successfully to the Iberian Electricity Market (MIBEL). The forecasting model was applied for the six types of maturities available for monthly futures in the MIBEL, from 1 to 6 months ahead. The forecasting model was trained with MIBEL price data corresponding to 44 months and the performances of the forecasting model and of the predictive strategy were tested with data corresponding to a further 12 months. Furthermore, a simpler forecasting model and three benchmark trading strategies are also presented and evaluated using the Risk Premium in the testing period, for comparative purposes. The results prove the advantages of the predictive strategy, even using the simpler forecasting model, which showed improvements over the conventional benchmark trading strategy, evincing an interesting hedging potential for electricity futures trading. © 2020 Elsevier B.V., All rights reserved.","Monteiro, C.; Fernandez-Jimenez, L.A.; Ramírez-Rosado, I.J.",2020,10.3390/en13143555,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090803020&doi=10.3390%2Fen13143555&partnerID=40&md5=d51d140d7458a000e4c17bee11df1825,scopus,"This paper introduces a novel predictive trading strategy for physical electricity futures, utilizing a mid-term forecasting model based on a feed-forward neural network trained with the extreme learning machine algorithm. The model predicts average spot prices to estimate the Risk Premium for futures trades. The strategy, applied to the Iberian Electricity Market, uses only derivative and spot market data. Results demonstrate the strategy's advantages and hedging potential, outperforming benchmark strategies.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:27:01.131907
9e5453d40cead624,Price Forecast of Treasury Bond Market Yield: Optimize Method Based on Deep Learning Model,"Accurate forecasting of the treasury bond market is beneficial for financial institutions to formulate investment research strategies and for national managers to build a modern financial system. This paper integrates the ideas of improved multivariate time series sampling and deep learning prediction model structure optimization, and proposes an optimized deep learning model framework under the LASSO-SMLR-PCA machine learning method. Through the LASSO and SMLR methods, the multicollinearity of the multivariate time series is reduced and the variables with insignificant correlation coefficients are eliminated. Then, the PCA method is used for dimensionality reduction and reconstruction, and finally, the LSTM deep learning model with Bayesian optimized hyperparameters is used to achieve rolling time prediction of the treasury bond market yield price. The empirical results show that the optimized deep learning model performs excellently in terms of evaluation indicators for treasury bond yield price forecasting, with accurate curve fitting, efficient model structure, and stable and effective practical application.",W. Ping; Y. Hu; L. Luo,2024,10.1109/access.2024.3519438,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10806713,ieeexplore,"This paper proposes an optimized deep learning model framework, integrating multivariate time series sampling and model structure optimization, using LASSO-SMLR-PCA methods to reduce multicollinearity and dimensionality, followed by a Bayesian-optimized LSTM model for treasury bond market yield price forecasting. Empirical results demonstrate the model's excellent performance in accuracy, curve fitting, and practical application.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T12:27:42.758225
6414f68f51ea9ec4,Pricing Deflation Risk with US Treasury Yields,"We use an arbitrage-free term structure model with spanned stochastic volatility to determine the value of the deflation protection option embedded in Treasury inflation-protected securities. The model accurately prices the deflation protection option prior to the financial crisis when its value was near zero; at the peak of the crisis in late 2008 when deflationary concerns spiked sharply; and in the post-crisis period. During 2009, the average value of this option at the 5-year maturity was 41 basis points on a par-yield basis. The option value is shown to be closely linked to overall market uncertainty as measured by the VIX, especially during and after the 2008 financial crisis.","Christensen, Jens H. E.; Lopez, Jose A.; Rudebusch, Glenn D.",2016,10.1093/rof/rfv029,,wos,"This paper utilizes an arbitrage-free term structure model with spanned stochastic volatility to price the deflation protection option within US Treasury Inflation-Protected Securities. The model's accuracy is demonstrated across different periods, including pre-crisis, the peak of the 2008 financial crisis, and the post-crisis era. The value of the deflation protection option is found to be closely correlated with market uncertainty, particularly the VIX, especially around the 2008 crisis.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:27:52.368620
e8ba8666d5c2c61f,Probabilistic assessment of earthquake insurance rates for Turkey,"A probabilistic model is presented to obtain a realistic estimate of earthquake insurance rates for reinforced concrete buildings in Turkey. The model integrates information on seismic hazard and information on expected earthquake damage on engineering facilities in a systematic way, yielding to estimates of earthquake insurance premiums. In order to demonstrate the application of the proposed probabilistic method, earthquake insurance rates are computed for reinforced concrete buildings constructed in five cities located in different seismic zones of Turkey. The resulting rates are compared with the rates currently charged by the insurance companies. The earthquake insurance rates are observed to be sensitive to the assumptions on seismic hazard and damage probability matrices and to increase significantly with increasing violation of the code requirements. © Springer 2005. © 2008 Elsevier B.V., All rights reserved.","Yücemen, M.S.",2005,10.1007/s11069-004-6485-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23844495206&doi=10.1007%2Fs11069-004-6485-8&partnerID=40&md5=bc0c07df0ae19c758040a226d34e044e,scopus,"This paper presents a probabilistic model to estimate earthquake insurance rates for reinforced concrete buildings in Turkey. It combines seismic hazard data with expected earthquake damage information to calculate premiums. The model's application is demonstrated by computing rates for buildings in five Turkish cities and comparing them to current market rates. The study highlights the sensitivity of these rates to seismic hazard assumptions and damage probabilities, noting a significant increase in rates with non-compliance to building code requirements.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:27:55.253514
873e33706d751e55,Projected polynomial autoregression for prediction of stationary time series,"Polynomial autoregressions are usually considered to be unrealistic models for time series. However, this paper shows that they can successfully be used when the purpose of the time series study is to provide forecasts. A projection scheme inspired from projection pursuit regression and feedforward artificial neural networks is used in order to avoid an explosion of the number of parameters when considering a large number of lags. The estimation of the parameters of the projected polynomial autoregressions is a non-linear least-squares problem. A consistency result is proved. A simulation study shows that the naive use of the common final prediction error criterion is inappropriate to identify the best projected polynomial autoregression. An explanation of this phenomenon is given and a correction to the criterion is proposed. An important feature of the polynomial predictors introduced in this paper is their simple implementation, which allows for automatic use. This is illustrated with real data for the three-month US Treasury Bill. © 2017 Elsevier B.V., All rights reserved.","de Luna, X.",1998,10.1080/02664769822756,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032459211&doi=10.1080%2F02664769822756&partnerID=40&md5=134c8744849f235aaae3661b345458cd,scopus,"This paper proposes projected polynomial autoregressions for time series forecasting, addressing the issue of parameter explosion with a projection scheme. It demonstrates the method's applicability with real data from US Treasury Bills and suggests a corrected prediction error criterion for model selection.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T12:28:04.552773
2278b01d1d5eb032,Pseudo-True SDFs in Conditional Asset Pricing Models,"This article is motivated by the need to bridge some gap between modern asset pric-ing theory and recent developments in econometric methodology. While asset pric-ing theory enhances the use of conditional pricing models, econometric inference of conditional models can be challenging due to misspecification or weak identifica-tion. To tackle the case of misspecification, we utilize the conditional Hansen and Jagannathan (1997) (HJ) distance as studied by Gagliardini and Ronchetti (2016), but we set the focus on interpretation and estimation of the pseudo-true value defined as the argument of the minimum of this distance. While efficient Generalized Method of Moments (GMM) has no meaning for estimation of a pseudo-true value, the HJ-distance not only delivers a meaningful loss function, but also features an additional advantage for the interpretation and estimation of man-aged portfolios whose exact pricing characterizes the pseudo-true pricing kernel (stochastic discount factor (SDF)). For conditionally affine pricing kernels, we can display some managed portfolios which are well-defined independently of the pseudo-true value of the parameters, although their exact pricing is achieved by the pseudo-true SDF. For the general case of nonlinear SDFs, we propose a smooth minimum distance (SMD) estimator (Lavergne and Patilea, 2013) that avoids a focus on specific directions as in the case of managed portfolios. Albeit based on kernel smoothing, the SMD approach avoids instabilities and the resulting need of trim-ming strategies displayed by classical local GMM estimators when the density func-tion of the conditioning variables may take arbitrarily small values. In addition, the fact that SMD may allow fixed bandwidth asymptotics is helpful regarding the curse of dimensionality. In contrast with the true unknown value for a well-specified model, the estimated pseudo-true value, albeit defined in a time-invariant (uncondi-tional) way, may actually depend on the choice of the state variables that define fun-damental factors and their scaling weights. Therefore, we may not want to be overly parsimonious about the set of explanatory variables. Finally, following Antoine and Lavergne (2014), we show how SMD can be further robustified to deal with weaker identification contexts. Since SMD can be seen as a local extension of the method of jackknife GMM (Newey and Windmeijer, 2009), we characterize the Gaussian asymptotic distribution of the estimator of the pseudo-true value using classical U-statistic theorems.","Antoine, Bertille; Proulx, Kevin; Renault, Eric",2020,10.1093/jjfinec/nby017,,wos,"This paper addresses the challenge of estimating conditional asset pricing models, particularly when misspecification or weak identification is present. It focuses on the concept of the pseudo-true value, defined as the minimizer of the conditional Hansen-Jagannathan (HJ) distance. The authors propose a smooth minimum distance (SMD) estimator, which is robust to issues like small density values of conditioning variables and can handle weaker identification contexts. The SMD estimator is shown to have Gaussian asymptotic distribution properties.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:28:06.170083
cdbf8ff96c734ee2,Psychological Pathways to Fraud: Understanding and Preventing Fraud in Organizations,"In response to calls for more research on how to prevent or detect fraud (ACAP, Final Report of the Advisory Committee on the Auditing Profession, United States Department of the Treasury, Washington, DC, 2008; AICPA, SAS No. 99: Consideration of Fraud in a Financial Statement Audit, New York, NY, 2002; Carcello et al., Working Paper, University of Tennessee, Bentley University and Kennesaw State University, 2008; Wells, Journal of Accountancy, 2004), we develop a framework that identifies three psychological pathways to fraud, supported by multiple theories relating to moral intuition and disengagement, rationalization, and the role played by negative affect. The purpose of developing the framework is twofold: (1) to draw attention to important yet under-researched aspects of ethical decision-making, and (2) to increase our understanding of the psychology of committing fraud. Our framework builds on the existing fraud triangle (PCAOB, Consideration of fraud in a financial statement audit. AU Section 316, www.pcaobus.org, 2005) which is used by auditors to assess fraud risk. The fraud triangle is composed of three factors that, together, predict the likelihood of fraud within an organization: opportunity, incentive/pressure, and attitude/rationalization. We find that, when faced with the opportunity and incentive/pressure, there are three psychological pathways to fraud nestled within attitude/rationalization: (1) lack of awareness, (2) intuition coupled with rationalization, and (3) reasoning. These distinctions are important for fraud prevention because each of these paths is driven by a different psychological mechanism. This framework is useful in a number of ways. First, it identifies certain insidious situational factors in which individuals commit fraud without recognizing it. Second, it extends our knowledge of rationalization by theorizing that individuals use rationalization to avoid or reduce the negative affect that accompanies performing an unethical behavior. Negative affect is important because individuals wish to avoid it. Third, it identifies several other methods fraudsters use to reduce negative affect, each of which could serve as potential ""psychological red flags"" and helps predict future fraudulent behavior. Finally, our framework can be used as a theoretical foundation to explore several interventions designed to prevent fraud.[PUBLICATION ABSTRACT]",,2011,10.1007/s10551-011-0741-0,,proquest,"This paper proposes a framework identifying three psychological pathways to fraud within organizations, building upon the existing fraud triangle. These pathways—lack of awareness, intuition coupled with rationalization, and reasoning—are driven by different psychological mechanisms and are important for fraud prevention. The framework also highlights the role of negative affect and rationalization in ethical decision-making and suggests potential psychological red flags for predicting fraudulent behavior.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:28:10.043072
7eace64f85144252,"Public attention, sentiment and the default of Silicon Valley Bank","We assess the interplay between public attention and trading of the Silicon Valley Bank (SVB) stock before its default on March 10, 2023. Based on intra-day data in 15-min intervals, we estimate SVB market excess returns and match these with intra-day measures of investor attention based on the relative number of tweets and Google searches. Wavelet analysis reveals bilateral lead–lag patterns between both series and demonstrates that a higher level of attention led to a significant decrease in SVB returns. Thereby, the results provide evidence that Twitter sentiment and media attention ultimately fueled and accelerated the crash dynamics of Silicon Valley Bank. Economically, Twitter provides information at lower costs and higher effectiveness than newspapers and allows direct communication without potential distortions from media bias or timing lags in reporting. Hence, individuals can coordinate and communicate their run beliefs at a much faster pace, emphasizing the importance for financial stability. © 2023 Elsevier B.V., All rights reserved.","Bales, S.; Burghof, H.-P.",2024,10.1016/j.najef.2023.102026,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174724134&doi=10.1016%2Fj.najef.2023.102026&partnerID=40&md5=bbf65da12d42f5e1e21b99d3d0426f1b,scopus,"This study examines the relationship between public attention (measured by tweets and Google searches) and the trading of Silicon Valley Bank (SVB) stock leading up to its default. Using intra-day data, the analysis found that increased attention was associated with decreased SVB stock returns, suggesting that social media sentiment and media attention accelerated the bank's collapse. The authors highlight the efficiency of platforms like Twitter for rapid information dissemination and coordination of 'run beliefs', underscoring implications for financial stability.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:28:17.798360
f61f9aaf4b3a4dc3,Public debt stabilization: the relevance of policymakers' time horizons,"Policymakers are stuck in time. Political short-termism, policy myopia, policy short-sightedness, and similar words have been coined to emphasize the present-centric policy thinking. Politics tends to produce short time horizons, and as a result, policymakers often fail to use present opportunities to mitigate future harms. Focusing on fiscal and monetary strategic interactions, given different separate decision makers, our paper aims to explore the effects of policymakers' time horizons on debt stabilization. To formalize our ideas, we use the novel concept of Nonlinear-model-predictive-control Feedback Nash Equilibrium (NFNE) and find that present-centric policy thinking and decision horizons matters under several dimensions.","Di Bartolomeo, Giovanni; Di Pietro, Marco; Saltari, Enrico; Semmler, Willi",2018,10.1007/s11127-018-0584-7,,wos,"This paper investigates how policymakers' short-term perspectives affect public debt stabilization, using a novel Nonlinear-model-predictive-control Feedback Nash Equilibrium (NFNE) framework. It explores the fiscal and monetary strategic interactions and concludes that present-centric policy thinking significantly impacts debt stabilization.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:28:22.671310
3dc61fee5d9bbaf9,Purebred or hybrid?: Reproducing the volatility in term structure dynamics,"This paper investigates the ability of mixtures of affine, quadratic, and non-linear models to track the volatility in the term structure of interest rates. Term structure dynamics appear to exhibit pronounced time varying or stochastic volatility. Ahn et al. (Rev. Financial Stud. xx (2001) xxx) provide evidence suggesting that term structure models incorporating a set of quadratic factors are better able to reproduce term structure dynamics than affine models, although neither class of models is able to fully capture term structure volatility. In this study, we combine affine, quadratic and non-linear factors in order to maximize the ability of a term structure model to generate heteroskedastic volatility. We show that this combination entails a tradeoff between specification of heteroskedastic volatility and correlations among the factors. By combining factors, we are able to gauge the cost of this tradeoff. Using efficient method of moments (Gallant and Tauchen, Econometric Theory 12 (1996) 657), we find that augmenting a quadratic model with a non-linear factor results in improvement in fit over a model comprised solely of quadratic factors when the model only has to confront first and second moment dynamics. When the full dynamics are confronted, this result reverses. Since the non-linear factor is characterized by stronger dependence of volatility on the level of the factor, we conclude that flexibility in the specification of both level dependence and correlation structure of the factors are important for describing term structure dynamics. © 2003 Elsevier B.V. All rights reserved. © 2008 Elsevier B.V., All rights reserved.","Ahn, D.-H.; Dittmar, R.F.; Gallant, A.R.; Gao, B.",2003,10.1016/s0304-4076(03)00106-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346937479&doi=10.1016%2FS0304-4076%2803%2900106-4&partnerID=40&md5=ea1e59d19022c6cce6ceb6b3b72bdb0e,scopus,"This paper explores the effectiveness of combining affine, quadratic, and non-linear models to capture the volatility in term structure dynamics of interest rates. It finds that while augmenting a quadratic model with a non-linear factor improves fit for first and second moment dynamics, this advantage is lost when considering full dynamics, suggesting the importance of both level dependence and correlation structure in factor specification.",True,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:28:52.568426
771da80fe19611dc,Quasi-likelihood estimation of a threshold diffusion process,"The threshold diffusion process, first introduced by Tong (1990), is a continuous-time process satisfying a stochastic differential equation with a piecewise linear drift term and a piecewise smooth diffusion term, e.g., a piecewise constant function or a piecewise power function. We consider the problem of estimating the (drift) parameters indexing the drift term of a threshold diffusion process with continuous-time observations. Maximum likelihood estimation of the drift parameters requires prior knowledge of the functional form of the diffusion term, which is, however, often unavailable. We propose a quasi-likelihood approach for estimating the drift parameters of a two-regime threshold diffusion process that does not require prior knowledge about the functional form of the diffusion term. We show that, under mild regularity conditions, the quasi-likelihood estimators of the drift parameters are consistent. Moreover, the estimator of the threshold parameter is super consistent and weakly converges to some non-Gaussian continuous distribution. Also, the estimators of the autoregressive parameters in the drift term are jointly asymptotically normal with distribution the same as that when the threshold parameter is known. The empirical properties of the quasi-likelihood estimator are studied by simulation. We apply the threshold model to estimate the term structure of a long time series of US interest rates. The proposed approach and asymptotic results can be readily lifted to the case of a multi-regime threshold diffusion process.",,2015,10.1016/j.jeconom.2015.03.038,,proquest,"This paper proposes a quasi-likelihood estimation method for threshold diffusion processes, which are continuous-time processes with piecewise linear drift and piecewise smooth diffusion terms. The method estimates drift parameters without requiring prior knowledge of the diffusion term's functional form. The estimators are shown to be consistent, with the threshold parameter estimator being super consistent and the autoregressive parameter estimators being asymptotically normal. The approach is demonstrated through simulations and applied to US interest rate term structure estimation, with potential for extension to multi-regime processes.",True,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:29:17.889126
4ca5e1c5b38940ea,Rational overoptimism and limited liability,"Is excessive risk-taking in credit cycles driven by incentives or biased beliefs? I propose a framework suggesting that the two are actually related and, specifically, that procyclical overoptimism can arise rationally from risk-taking incentives. I show that when firms and banks have a limited liability payoff structure, they have lower incentives to pay attention to the aggregate conditions that generate risk. This leads to systematic underestimation of the accumulation of risk during economic booms and overoptimistic beliefs. As a result, agents lend and borrow excessively, further increasing downside risk. Credit cycles driven by this new “uninformed” risk-taking are consistent with existing evidence such as high credit and low-risk premia predicting a higher probability of crises and negative returns for banks. My model suggests that regulating incentives can decrease overoptimistic beliefs and thus mitigate boom-and-bust cycles. © 2024 Elsevier B.V., All rights reserved.","Gemmi, L.",2024,10.1016/j.jmoneco.2023.11.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176101825&doi=10.1016%2Fj.jmoneco.2023.11.002&partnerID=40&md5=7c53bdf5beda43dd6dbaec5930afffaf,scopus,"This paper proposes a framework where rational overoptimism arises from risk-taking incentives, particularly in the presence of limited liability for firms and banks. This leads to underestimation of aggregate risk, excessive lending/borrowing, and boom-bust cycles. The model suggests that regulating incentives can mitigate these cycles.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:29:27.216273
9f0348c54278ded0,Real-time Bayesian learning and bond return predictability,"The paper examines statistical and economic evidence of out-of-sample bond return predictability for a real-time Bayesian investor who learns about parameters, hidden states, and predictive models over time. We find some statistical evidence using information contained in forward rates. However, such statistical predictability can hardly generate any economic value for investors. Furthermore, we find that strong statistical and economic evidence of bond return predictability from fully-revised macroeconomic data vanishes when real-time macroeconomic information is used. We also show that highly levered investments in bonds can improve short-run bond return predictability. (C) 2021 Elsevier B.V. All rights reserved.","Wan, Runqing; Fulop, Andras; Li, Junye",2022,10.1016/j.jeconom.2020.04.052,,wos,"This paper investigates bond return predictability for a real-time Bayesian investor. While some statistical predictability is found using forward rates, it offers little economic value. Predictability using fully-revised macroeconomic data disappears with real-time information. Highly leveraged bond investments can enhance short-run predictability.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T12:29:50.629979
4e8eda2d3ff61821,Regime Changes and Financial Markets,"Regime-switching models can match the tendency of financial markets to often change their behavior abruptly and the phenomenon that the new behavior of financial variables often persists for several periods after such a change. Although the regimes captured by regime-switching models are identified by an econometric procedure, they often correspond to different periods in regulation, policy, and other secular changes. In empirical estimates, the means, volatilities, autocorrelations, and cross-covariances of asset returns often differ across regimes in a manner that allows regime-switching models to capture the stylized behavior of many financial series including fat tails, heteroskedasticity, skewness, and time-varying correlations. In equilibrium models, regimes in fundamental processes, such as consumption or dividend growth, strongly affect the dynamic properties of equilibrium asset prices and can induce non-linear risk-return trade-offs. Regime switches also lead to potentially large consequences for investors' optimal portfolio choice.","Ang, Andrew; Timmermann, Allan",2012,10.1146/annurev-financial-110311-101808,,wos,"This paper discusses regime-switching models and their application to financial markets. These models can capture abrupt changes in market behavior and the persistence of new behaviors. The identified regimes often align with changes in regulation, policy, or other secular shifts. Empirical estimates show differences in means, volatilities, autocorrelations, and cross-covariances of asset returns across regimes, which helps capture stylized behaviors like fat tails, heteroskedasticity, skewness, and time-varying correlations. In equilibrium models, regime switches in fundamental processes impact asset prices and investor portfolio choices.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:29:53.624062
73fcf163344c10da,Research on Financial Stock Market Prediction Based on the Hidden Quantum Markov Model,"Quantum finance, as a key application scenario of quantum computing, showcases multiple significant advantages of quantum machine learning over traditional machine learning methods. This paper first aims to overcome the limitations of the hidden quantum Markov model (HQMM) in handling continuous data and proposes an innovative method to convert continuous data into discrete-time sequence data. Second, a hybrid quantum computing model is developed to forecast stock market trends. The model was used to predict 15 stock indices from the Shanghai and Shenzhen Stock Exchanges between June 2018 and June 2021. Experimental results demonstrate that the proposed quantum model outperforms classical algorithmic models in handling higher complexity, achieving improved efficiency, reduced computation time, and superior predictive performance. This validation of quantum advantage in financial forecasting enables the practical deployment of quantum-inspired prediction models by investors and institutions in trading environments. This quantum-enhanced model empowers investors to predict market regimes (bullish/bearish/range-bound) using real-time data, enabling dynamic portfolio adjustments, optimized risk controls, and data-driven allocation shifts.",,2025,10.3390/math13152505,,proquest,"This paper proposes a hybrid quantum computing model for stock market trend forecasting, addressing limitations of the hidden quantum Markov model (HQMM) in handling continuous data by converting it to discrete-time sequences. The model was tested on 15 stock indices from Chinese exchanges and showed superior performance compared to classical models in terms of efficiency, computation time, and predictive accuracy. The authors suggest this quantum-enhanced model can be practically deployed by investors for market regime prediction and portfolio management.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:29:58.824208
a75b710a9b4a9b93,Research on RMB exchange rate forecast based on the neural network model and the Nelson–Siegel model,"This paper expands the neural network model to predict exchange rate based on the factors extracted from the Nelson–Siegel model. Based on the theory about exchange rate forecasting, interest could be used to predict the movement of exchange rate. Therefore, this paper analyzes the interest rate term structure factors based on the US and China yield curves data, then uses the Nelson–Siegel model to extract the factors of the interest rate term structure. Finally, the factors of yield curves are used as input data to of the neural network model. And the mean forecasting squared errors, mean absolute errors, mean absolute percentage errors of neural network model, Nelson–Siegel regression model, and ARIMA model are compared. The results show that the neural network model has a superior ability to explain the exchange rate fluctuations of the CNY and USD, and the prediction ability is better than the exchange rate prediction ability of the Nelson–Siegel regression model and ARIMA model.",,2020,10.1057/s41283-020-00062-3,,proquest,"This paper proposes a neural network model for RMB exchange rate forecasting, utilizing factors extracted from the Nelson-Siegel model applied to US and China yield curves. The model's predictive performance is compared against the Nelson-Siegel regression model and ARIMA, showing superior accuracy for the neural network.",True,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:30:11.330101
a83ca3456d3a610f,Reweighted Price Relative Tracking System for Automatic Portfolio Optimization,"In this paper, we propose a novel reweighted price relative tracking (RPRT) system for automatic portfolio optimization (APO). In the price prediction stage, it automatically assigns separate weights to the price relative predictions according to each asset's performance, and these weights will also be automatically updated. In the portfolio optimizing stage, a novel tracking system with a generalized increasing factor is proposed to maximize the future wealth of next period. Besides, an efficient algorithm is designed to solve the portfolio optimization objective, which is applicable to large-scale and time-limited situations. Extensive experiments on six benchmark datasets from real financial markets with diverse assets and different time spans are conducted. RPRT outperforms other state-of-the-art systems in cumulative wealth, mean excess return, annual percentage yield, and some typical risk metrics. Moreover, it can withstand considerable transaction costs and runs fast. It indicates that RPRT is an effective and efficient APO system.",Z. -R. Lai; P. -Y. Yang; L. Fang; X. Wu,2020,10.1109/tsmc.2018.2852651,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8411138,ieeexplore,"This paper introduces a novel Reweighted Price Relative Tracking (RPRT) system for automatic portfolio optimization (APO). The system features automatic weighting of price relative predictions based on asset performance and a tracking system with a generalized increasing factor to maximize future wealth. An efficient algorithm is provided for large-scale, time-limited scenarios. Experiments on benchmark datasets show RPRT outperforms existing systems in cumulative wealth, returns, and risk metrics, while being robust to transaction costs and fast.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:30:17.104576
4496875730ddc067,Risk Assessments and Risk Premiums in the Eurodollar Market,"Increasing awareness of the potential risks involved in lending to heavily indebted governments focuses attention on credit pricing in the Eurodollar market. This paper utilizes a recent survey of country-by-country risk assessments as perceived by lenders to show that a systematic relationship exists between these assessments and interest rates in the Euromarket. The relationship is derived from an underlying model described in the paper. The estimated parameters verify a number of hypotheses, providing insights on the loss rates lenders expect to incur in case of default.",,1982,10.1111/j.1540-6261.1982.tb02217.x,,proquest,"This paper investigates the relationship between lender perceptions of country-specific risks and interest rates in the Eurodollar market. It uses a survey of risk assessments to demonstrate a systematic link, derived from a theoretical model, and estimates parameters to understand expected loss rates in case of default.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:30:31.280191
6d8b6b03952fab6a,"Risk, Return and Volatility Feedback: A Bayesian Nonparametric Analysis","In this paper, we let the data speak for itself about the existence of volatility feedback and the often debated risk–return relationship. We do this by modeling the contemporaneous relationship between market excess returns and log-realized variances with a nonparametric, infinitely-ordered, mixture representation of the observables’ joint distribution. Our nonparametric estimator allows for deviation from conditional Gaussianity through non-zero, higher ordered, moments, like asymmetric, fat-tailed behavior, along with smooth, nonlinear, risk–return relationships. We use the parsimonious and relatively uninformative Bayesian Dirichlet process prior to overcoming the problem of having too many unknowns and not enough observations. Applying our Bayesian nonparametric model to more than a century’s worth of monthly US stock market returns and realized variances, we find strong, robust evidence of volatility feedback. Once volatility feedback is accounted for, we find an unambiguous positive, nonlinear, relationship between expected excess returns and expected log-realized variance. In addition to the conditional mean, volatility feedback impacts the entire joint distribution.",,2018,10.3390/jrfm11030052,,proquest,"This paper uses a Bayesian nonparametric model to analyze the relationship between market excess returns and realized variances. The model allows for non-Gaussian distributions and nonlinear risk-return relationships. The analysis of over a century of US stock market data reveals strong evidence of volatility feedback and a positive, nonlinear relationship between expected excess returns and expected log-realized variance after accounting for volatility feedback.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:30:38.734202
e46cdb87f9172c01,Risks and risk premiums of GE corn: A macromarketing framework,"It has been more than two decades since genetic engineered (GE) corn has been introduced in the United States and Spain. Despite the wide scale adoption of GE corn, (92% of planted acreage in the US in 2017), biodiversity conservation, environmental protection goals, and food safety continue to surface as potential risk factors. We developed a macromarketing framework to provide a linkage between the consumers, producers, and societal impacts of GE corn. An empirical Arbitrage Pricing Model (APT) was used to quantify risks and risk premiums associated with GE corn production. The risk premium deduced from the APT model provides a measure of intrinsic compensation for taking on higher or lower risks by participants in the marketing system. An Auto-Regressive Distributed Lag (ARDL) regression estimation of the APT model reveals that GE causes corn prices to go down due to increase supply, but significantly reduced the risk premium for producing corn in the United States. GE corn also provided environmental beneficial outcomes by reducing the use of fertilizers.",,2022,10.1016/j.ecolecon.2022.107560,,proquest,"This paper uses a macromarketing framework and an Arbitrage Pricing Model (APT) to analyze the risks and risk premiums associated with GE corn production in the US. The study found that GE corn increases supply, lowers prices, and significantly reduces the risk premium for producers, while also offering environmental benefits through reduced fertilizer use.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:30:40.968495
016ee04e5aa868c2,Robust analysis of default intensity,"The problem of robust estimation and multivariate outlier detection of the term structure of default intensity is considered. Both the multivariate Vasicek and CIR models, embedding the Kalman filter algorithm in a forward search context, are used to estimate default intensity. The focus is not on the estimation of credit models including jumps, but on the automatic detection of masked multiple outliers in multivariate time series. Both simulated and real market credit spread time series are analyzed. In order to make inference on outliers, confidence envelopes which are virtually independent of the estimated parameters are introduced. The output is not only a unique default intensity term structure curve, as often used in the financial literature, but a robust confidence interval within which default intensity is likely to stay. © 2010 Elsevier B.V. All rights reserved. © 2012 Elsevier B.V., All rights reserved.","Bellini, T.; Riani, M.",2012,10.1016/j.csda.2011.03.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052729228&doi=10.1016%2Fj.csda.2011.03.007&partnerID=40&md5=c461a0037e44f8f1bb762976a3665485,scopus,"This paper addresses robust estimation and multivariate outlier detection for the term structure of default intensity, utilizing both Vasicek and CIR models with a Kalman filter within a forward search framework. The primary focus is on identifying masked multiple outliers in multivariate time series, rather than on credit models with jumps. The study analyzes both simulated and real market credit spread data, introducing confidence envelopes for outlier inference that are largely parameter-independent. The outcome provides a robust confidence interval for default intensity, moving beyond the single default intensity term structure curve often seen in financial literature.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:30:48.244976
1a226116b97cd3c2,Robust portfolio choice with limited attention,"This paper investigates a robust portfolio selection problem with the agent’s limited attention. The agent has access to a risk-free asset and a stock in a financial market. But she does not observe perfectly the expected return rate of the stock so she has to estimate this key parameter before making decisions. Besides the general observable financial information, the agent can also acquire a news signal process whose accuracy depends on the agent’s attention. We assume that the agent pays limited attention on the signal and she does not trust her estimation model. So it is necessary to consider model ambiguity in this paper as well. The agent maximizes the expected utility of her terminal wealth under the worst-case scenario. Under this setting, we derive the robust optimal strategy explicitly. In the presence of the attention and ambiguity aversion, the myopic term of the strategy, the hedging term of the strategy and the worst-case scenario are all changed. We find that more attention makes the variance of the estimated return smaller. The numerical examples also show that a more attentive agent has a better estimation of the unobservable parameter and is more confident on her estimation. Consequently, the worst-case scenario deviates less from the reference model, which implies a higher expected return rate under the worst-case scenario, thus invests more in the stock © 2023 Elsevier B.V., All rights reserved.","Ma, Y.; Li, Z.",2023,10.3934/era.2023186,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159911737&doi=10.3934%2FERA.2023186&partnerID=40&md5=f806efd2b7aacacd69a68d507fee36c1,scopus,"This paper examines a robust portfolio selection problem where the agent has limited attention and faces model ambiguity. The agent must estimate the stock's expected return rate using observable financial information and a news signal whose accuracy depends on attention. The agent maximizes expected utility under a worst-case scenario, leading to an explicitly derived robust optimal strategy. Increased attention reduces the variance of the estimated return, improves parameter estimation, and leads to a less deviated worst-case scenario, resulting in higher investment in the stock.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:30:55.243979
9d05ef274491439a,SVM-Based Techniques for Predicting Cross-Functional Team Performance: Using Team Trust as a Predictor,"Due to the characteristics of cross-functional teams, trust is crucial for cross-functional teams to enhance performance. However, as a significant factor, trust had been neglected in previous team performance models. In this paper, we investigate whether trust can be used as a predictor of cross-functional team performance by proposing a prediction model. The inputs of the model are both team structural and contextual (SC) factors, and project process (PP) factors, which are two major sources that form team trust. The output of the model is different levels of team performance, which consists of internal performance and external performance. The support vector machine techniques are used to establish the model. Results show that prediction accuracy is high (84.95%) when using both SC and PP factors as inputs, while PP factors have better prediction accuracy than SC factors on team performance and internal performance. It is suggested that team trust can be used as a good predictor of cross-functional team performance. In practice, this paper presents a better understanding of the relationship between trust and performance in cross-functional teams, and thus, enhances practitioners' managerial skills. It also gives reference for managers to dynamically control and predict team performance during project period.",L. Zhang; X. Zhang,2015,10.1109/tem.2014.2380177,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004820,ieeexplore,"This paper proposes a prediction model using Support Vector Machine (SVM) techniques to predict cross-functional team performance. The model utilizes team structural and contextual factors, along with project process factors, as inputs to predict internal and external team performance. Results indicate high prediction accuracy (84.95%) and suggest that team trust is a significant predictor.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:31:07.810620
58ae8ef2afebef3f,Semiparametric inference in a GARCH-in-mean model,"A new semiparametric estimator for an empirical asset pricing model with general nonparametric risk-return tradeoff and GARCH-type underlying volatility is introduced. Based on the profile likelihood approach, it does not rely on any initial parametric estimator of the conditional mean function, and it is under stated conditions consistent, asymptotically normal, and efficient, i.e., it achieves the semiparametric lower bound. A sampling experiment provides finite sample comparisons with the parametric approach and the iterative semiparametric approach with parametric initial estimate of Conrad and Mammen (2008). An application to daily stock market returns suggests that the risk-return relation is indeed nonlinear. (C) 2011 Elsevier B.V. All rights reserved.","Christensen, Bent Jesper; Dahl, Christian M.; Iglesias, Emma M.",2012,10.1016/j.jeconom.2011.09.028,,wos,"This paper introduces a new semiparametric estimator for an asset pricing model that incorporates a nonparametric risk-return tradeoff and GARCH-type volatility. The estimator is shown to be consistent, asymptotically normal, and efficient. Empirical results suggest a nonlinear risk-return relationship in stock market returns.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:31:14.664880
33528ceaf46b6df9,"Sensitivity, moment conditions, and the risk-free rate in Yogo (2006)","In this paper, we show that results presented in the seminal paper by Yogo, A Consumption Based Explanation of Expected Stock Returns, cannot be replicated. We find different estimates for the parameters and we obtain values of over-identified statistics that being much larger than those in the original paper indicate rejection of the durable consumption asset pricing model. By careful inspection of Yogo's replication files, we were able to track down the inconsistency to a coding bug. The rejection of the durable model is exemplified by its inability to simultaneously explain the risk-free rate and excess stock returns. © 2018 Elsevier B.V., All rights reserved.","Borri, N.; Ragusa, G.",2017,10.1561/104.00000050,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047126663&doi=10.1561%2F104.00000050&partnerID=40&md5=2e80cdf8a809220a2a0c9a68c39d3f9e,scopus,This paper identifies a coding bug in Yogo (2006) that leads to different parameter estimates and rejection of the durable consumption asset pricing model. The corrected model fails to explain both the risk-free rate and excess stock returns simultaneously.,False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:31:19.899704
17b1f4e7dc5f6e24,Sequential Monte Carlo Methods to Train Neural Network Models,"We discuss a novel strategy for training neural networks using sequential Monte Carlo algorithms and propose a new hybrid gradient descent/sampling importance resampling algorithm (HySIR). In terms of computational time and accuracy, the hybrid SIR is a clear improvement over conventional sequential Monte Carlo techniques. The new algorithm may be viewed as a global optimization strategy that allows us to learn the probability distributions of the network weights and outputs in a sequential framework. It is well suited to applications involving on-line, nonlinear, and nongaussian signal processing. We show how the new algorithm outperforms extended Kalman filter training on several problems. In particular, we address the problem of pricing option contracts, traded in financial markets. In this context, we are able to estimate the one-step-ahead probability density functions of the options prices.",J. F. G. d. Freitas; M. Niranjan; A. H. Gee; A. Doucet,2000,10.1162/089976600300015664,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6790037,ieeexplore,"This paper introduces a novel strategy for training neural networks using sequential Monte Carlo algorithms, specifically a hybrid gradient descent/sampling importance resampling algorithm (HySIR). HySIR improves upon conventional sequential Monte Carlo methods in terms of computational time and accuracy. It functions as a global optimization strategy for learning probability distributions of network weights and outputs in a sequential framework, suitable for online, nonlinear, and non-Gaussian signal processing. The authors demonstrate its superiority over extended Kalman filter training on various problems, including option pricing in financial markets, where it estimates one-step-ahead probability density functions of option prices.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:31:40.837161
2bf6170d727cf04b,Simultaneous nonparametric inference of time series,"We consider kernel estimation of marginal densities and regression functions of stationary processes. It is shown that for a wide class of time series, with proper centering and scaling, the maximum deviations of kernel density and regression estimates are asymptotically Gumbel. Our results substantially generalize earlier ones which were obtained under independence or beta mixing assumptions. The asymptotic results can be applied to assess patterns of marginal densities or regression functions via the construction of simultaneous confidence bands for which one can perform goodness-of-fit tests. As an application, we construct simultaneous confidence bands for drift and volatility functions in a dynamic short-term rate model for the U.S. Treasury yield curve rates data. © Institute of Mathematical Statistics, 2010. © 2010 Elsevier B.V., All rights reserved.","Liu, W.; Wu, W.B.",2010,10.1214/09-aos789,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955161966&doi=10.1214%2F09-AOS789&partnerID=40&md5=ab2390352cc6b054754f9e082002cee6,scopus,"This paper presents a method for simultaneous nonparametric inference of time series, focusing on kernel estimation of marginal densities and regression functions for stationary processes. It demonstrates that for a broad range of time series, the maximum deviations of kernel estimates asymptotically follow a Gumbel distribution. These findings extend previous work by relaxing independence or beta mixing assumptions. The authors apply these asymptotic results to construct simultaneous confidence bands for assessing marginal densities and regression functions, enabling goodness-of-fit tests. An application is provided for constructing confidence bands for drift and volatility functions in a dynamic short-term rate model using U.S. Treasury yield curve rates data.",True,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:31:59.337055
b10f6a1cfc7ff655,Sovereign Default Forecasting in the Era of the COVID-19 Crisis,"The COVID-19 crisis has revealed the economic vulnerability of various countries and, thus, has instigated the systematic exploration and forecasting of sovereign default risks. Multivariate statistical and stochastic process-based sovereign default risk forecasting has a 50-year developmental history. This article describes a continuous, non-homogeneous Markov chain method as the basis for a COVID-19-related sovereign default risk forecast model. It demonstrates the estimation of sovereign probabilities of default (PDs) over a five-year horizon period with the developed model reflecting the impact of the COVID-19 crisis. The COVID-19-adopted Markov model estimates PDs for most countries, including those that are advanced with AAA and AA ratings, to suggest that no sovereign nation's economy is secure from the financial impact of the COVID-19 pandemic. The dynamics of the estimated PDs are indicative of contemporary evidence as experienced in the recent financial crisis. The empirical results of this article have policy implications for foreign investors, sovereign lenders, export finance institutions, foreign trade experts, risk management professionals, and policymakers in the field of finance. The developed model can be used to timely recognize potential problems with sovereign entities in the current COVID-19 crisis and to take appropriate mitigating actions.","Kristof, Tamas",2021,10.3390/jrfm14100494,,wos,"This article proposes a continuous, non-homogeneous Markov chain model to forecast sovereign default risks, specifically incorporating the impact of the COVID-19 crisis. The model estimates probabilities of default over a five-year horizon, suggesting no nation is immune to the pandemic's financial effects. The findings have implications for various financial stakeholders and can aid in early risk recognition and mitigation.",True,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:32:26.655924
77a29933a035a55b,Sovereign bond yield spreads and market sentiment and expectations: Empirical evidence from Euro area countries,"The paper investigates the determinants of sovereign bond yield spreads in the Euro area and extends the models commonly used in empirical analyses by focusing on the impact of market expectations and behavioral factors.Using monthly panel data for ten European countries over the period 2000-2012, the analysis adopts a pooled mean-group approach to estimate non-stationary dynamic models of spreads determinants, allowing for country heterogeneities in short-run dynamics.Results show that the behavioral indicators considered, proxies of consumer and market sentiment and expectations, strongly affect spreads behavior, especially during the crisis. Specific attention is also paid to check the robustness of the estimated effects of behavioral indicators and to assess the impact of global financial crisis on the determinants of government bond rate differentials. © 2024 Elsevier B.V., All rights reserved.","Aristei, D.; Duccio Martelli, D.",2014,10.1016/j.jeconbus.2014.08.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927125445&doi=10.1016%2Fj.jeconbus.2014.08.001&partnerID=40&md5=2e2df8bfd63ae330adfe189e467f28ee,scopus,"This paper examines the factors influencing sovereign bond yield spreads in Euro area countries, incorporating market sentiment and expectations. Using monthly panel data from 2000-2012 and a pooled mean-group approach, the study finds that behavioral indicators, representing consumer and market sentiment, significantly impact spreads, particularly during crises. The research also assesses the robustness of these effects and the influence of the global financial crisis.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:32:32.425256
cba55a63862ae3c0,Spatiotemporal adaptive neural network for long-term forecasting of financial time series,"Optimal decision-making in social settings is often based on forecasts from time series (TS) data. Recently, several approaches using deep neural networks (DNNs) such as recurrent neural networks (RNNs) have been introduced for TS forecasting and have shown promising results. However, the applicability of these approaches is being questioned for TS settings where there is a lack of quality training data and where the TS to forecast exhibit complex behaviors. Examples of such settings include financial TS forecasting, where producing accurate and consistent long-term forecasts is notoriously difficult. In this work, we investigate whether DNN-based models can be used to forecast these TS conjointly by learning a joint representation of the series instead of computing the forecast from the raw time-series representations. To this end, we make use of the dynamic factor graph (DFG) to build a multivariate autoregressive model. We investigate a common limitation of RNNs that rely on the DFG framework and propose a novel variable-length attention-based mechanism (ACTM) to address it. With ACTM, it is possible to vary the autoregressive order of a TS model over time and model a larger set of probability distributions than with previous approaches. Using this mechanism, we propose a self-supervised DNN architecture for multivariate TS forecasting that learns and takes advantage of the relationships between them. We test our model on two datasets covering 19 years of investment fund activities. Our experimental results show that the proposed approach significantly outperforms typical DNN-based and statistical models at forecasting the 21-day price trajectory. We point out how improving forecasting accuracy and knowing which forecaster to use can improve the excess return of autonomous trading strategies. © 2021 Elsevier B.V., All rights reserved.","Chatigny, P.; Patenaude, J.-M.; Wang, S.",2021,10.1016/j.ijar.2020.12.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101315001&doi=10.1016%2Fj.ijar.2020.12.002&partnerID=40&md5=eed6e32a846204aecf5b2e3a39ad4c48,scopus,"This paper proposes a novel self-supervised deep neural network architecture using a dynamic factor graph and a variable-length attention-based mechanism (ACTM) for multivariate time series forecasting. The model learns joint representations of financial time series to improve long-term forecasting accuracy, outperforming traditional DNN and statistical models on investment fund activity data for 21-day price trajectory prediction.",True,False,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T12:33:10.931075
ee3869d724e9f558,Specification tests for nonlinear dynamic models,"We propose a new adequacy test and a graphical evaluation tool for nonlinear dynamic models. The proposed techniques can be applied in any set-up where parametric conditional distribution of the data is specified and, in particular, to models involving conditional volatility, conditional higher moments, conditional quantiles, asymmetry, Value at Risk models, duration models, diffusion models, etc. Compared to other tests, the new test properly controls the nonlinear dynamic behaviour in conditional distribution and does not rely on smoothing techniques that require a choice of several tuning parameters. The test is based on a new kind of multivariate empirical process of contemporaneous and lagged probability integral transforms. We establish weak convergence of the process under parameter uncertainty and local alternatives. We justify a parametric bootstrap approximation that accounts for parameter estimation effects often ignored in practice. Monte Carlo experiments show that the test has good finite-sample size and power properties. Using the new test and graphical tools, we check the adequacy of various popular heteroscedastic models for stock exchange index data.","Kheifets, Igor L.",2015,10.1111/ectj.12040,,wos,"This paper introduces a new adequacy test and graphical tool for nonlinear dynamic models, applicable to various parametric conditional distribution models including those with conditional volatility, quantiles, and Value at Risk. The test controls for nonlinear dynamics without smoothing, uses a multivariate empirical process, and is supported by bootstrap approximation and Monte Carlo simulations. It is applied to heteroscedastic models for stock index data.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:33:32.355583
e1687aa7566d8268,Spot price forecasting for best trading strategy decision support in the Iberian electricity market,"The increasing volatility in electricity markets has reinforced the need for better trading strategies by both sellers and buyers to limit the exposure to losses. Accordingly, this paper proposes an electricity trading strategy based on a mid-term forecast of the average spot price and a risk premium analysis based on this forecast. This strategy can help traders (buyers and sellers) decide whether to trade in the futures market (of varying monthly maturity) or to wait and trade in the spot market. The forecast model consists of an Artificial Neural Network trained with the Long Short Term Memory architecture to predict the average monthly spot prices, using only market price-related data as input variables. Statistical analysis verified the correlation and dependency between variables. The forecast model was trained, validated and tested with price data from the Iberian Electricity Market (MIBEL), in particular the Spanish zone, between January 2015 and August 2019. The last year of this period was reserved for testing the performance of the proposed forecast model and trading strategy. For comparison purposes, the results of a forecasting model trained with the Extreme Learning Machine over the same period are also presented. In addition, the forecasted value of the average monthly spot price was used to perform a risk premium analysis. The results were promising, as they indicated benefits for traders adopting the proposed trading strategy, proving the potential of the forecast model and the risk premium analysis based on this forecast. © 2023 Elsevier B.V., All rights reserved.","Magalhães, B.G.; Bento, P.M.R.; Pombo, J.A.N.; Calado, M.R.A.; Mariano, S.J.P.S.",2023,10.1016/j.eswa.2023.120059,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152124621&doi=10.1016%2Fj.eswa.2023.120059&partnerID=40&md5=2fc811ab19d0dccddc007edf8afeaadc,scopus,"This paper proposes an electricity trading strategy using a Long Short Term Memory (LSTM) Artificial Neural Network to forecast average monthly spot prices in the Iberian Electricity Market (MIBEL). The strategy aims to help traders decide between trading in the futures or spot market by incorporating a risk premium analysis based on the price forecast. The model was trained and tested on data from January 2015 to August 2019, and its performance was compared to an Extreme Learning Machine model. The results suggest the proposed strategy offers benefits to traders.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:33:36.864601
e89622193853160a,Statistical-mechanical aids to calculating term-structure models,"Recent work in statistical mechanics has developed new analytical and numerical techniques to solve coupled stochastic equations. This paper describes application of the very fast simulated reannealing and path-integral methodologies to the estimation of the Brennan and Schwartz two-factor term-structure (time-dependent) model of bond prices. It is shown that these methodologies can be utilized to estimate more complicated n-factor nonlinear models. Applications to other systems are stressed. © 1990 The American Physical Society. © 2015 Elsevier B.V., All rights reserved.","Ingber, L.",1990,10.1103/physreva.42.7057,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000885755&doi=10.1103%2FPhysRevA.42.7057&partnerID=40&md5=7a538a086e652ffbed77c37d63eb9224,scopus,"This paper applies statistical mechanics techniques, specifically very fast simulated reannealing and path-integral methodologies, to estimate the Brennan and Schwartz two-factor term-structure model of bond prices. The authors demonstrate that these methods can be extended to estimate more complex n-factor nonlinear models and suggest broader applications.",True,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:34:07.524174
ff04be79fa6dba3b,Stochastic Gradient Descent in Continuous Time,"Stochastic gradient descent in continuous time (SGDCT) provides a computationally efficient method for the statistical learning of continuous-time models, which are widely used in science, engineering, and finance. The SGDCT algorithm follows a (noisy) descent direction along a continuous stream of data. SGDCT performs an online parameter update in continuous time with the parameter updates theta(t) satisfying a stochastic differential equation. We prove that lim(t ->infinity) del(g) over bar(theta(t)) = 0, where (g) over bar is is a natural objective function for the estimation of the continuous-time dynamics. The convergence proof leverages ergodicity by using an appropriate Poisson equation to help describe the evolution of the parameters for large times. For certain continuous-time problems, SGDCT has some promising advantages compared to a traditional stochastic gradient descent algorithm. This paper mainly focuses on applications in finance, such as model estimation for stocks, bonds, interest rates, and financial derivatives. SGDCT can also be used for the optimization of high-dimensional continuous time models, such as American options. As an example application, SGDCT is combined with a deep neural network to price high-dimensional American options (up to 100 dimensions).","Sirignano, Justin; Spiliopoulos, Konstantinos",2017,10.1137/17m1126825,,wos,"This paper introduces Stochastic Gradient Descent in Continuous Time (SGDCT), an efficient method for learning continuous-time models used in various fields like finance. SGDCT updates parameters in continuous time following a stochastic differential equation and is proven to converge. It offers advantages over traditional SGD for certain problems, particularly in finance for estimating models of stocks, bonds, interest rates, and derivatives. An example application combines SGDCT with deep neural networks for pricing high-dimensional American options.",True,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:36:00.580583
09fa231c7b4e23cf,Stock Selection Using Machine Learning Based on Financial Ratios,"Stock prediction has garnered considerable attention among investors, with a recent focus on the application of machine learning techniques to enhance predictive accuracy. Prior research has established the effectiveness of machine learning in forecasting stock market trends, irrespective of the analytical approach employed, be it technical, fundamental, or sentiment analysis. In the context of fiscal year-end selection, the decision may initially seem straightforward, with December 31 being the apparent choice, as discussed by B. Kamp in 2002. The primary argument for a uniform fiscal year-end centers around comparability. When assessing the financial performance of two firms with differing fiscal year-ends, substantial shifts in the business environment during non-overlapping periods can impede meaningful comparisons. Moreover, when two firms merge, the need to synchronize their annual reporting often results in shorter or longer fiscal years, complicating time series analysis. In the US S&P stock market, misaligned fiscal years lead to variations in report publication dates across different industries and market segments. Since the financial reporting dates of US S&P companies are determined independently by each listed entity, relying solely on these dates for investment decisions may prove less than entirely reliable and impact the accuracy of return prediction models. Hence, our interest lies in the synchronized fiscal year of the TW stock market, leveraging machine learning models for fundamental analysis to forecast returns. We employed four machine learning models: Random Forest (RF), Feedforward Neural Network (FNN), Gated Recurrent Unit (GRU), and Financial Graph Attention Network (FinGAT). We crafted portfolios by selecting stocks with higher predicted returns using these machine learning models. These portfolios outperformed the TW50 index benchmarks in the Taiwan stock market, demonstrating superior returns and portfolio scores. Our study’s findings underscore the advantages of using aligned financial ratios for predicting the top 20 high-return stocks in a mid-to-long-term investment context, delivering over 50% excess returns across the four models while maintaining lower risk profiles. Using the top 10 high-return stocks produced over 100% relative returns with an acceptable level of risk, highlighting the effectiveness of employing machine learning techniques based on financial ratios for stock prediction.",,2023,10.3390/math11234758,,proquest,"This study investigates the use of machine learning models (Random Forest, FNN, GRU, FinGAT) to predict stock returns in the Taiwan stock market, utilizing synchronized financial ratios. The developed portfolios based on predicted returns outperformed the TW50 index, demonstrating the effectiveness of this approach for mid-to-long-term investment and highlighting the benefits of aligned financial reporting for predictive accuracy.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:36:11.783605
0c6ee7d2bea7d19d,Stock Trading Strategy Based on Multi-Scale Deep Reinforcement Learning and Price Movement Prediction,"Owing to the dynamic and complex properties of the stock market, generating a stable and highly profitable trading strategy is a challenge. Therefore, in this paper, a novel trading strategy is proposed, grounded in multi-scale deep reinforcement learning and stock price trend prediction, with supervised learning and reinforcement learning unified within a unified framework. Our proposed method is separated into three stages. First, a novel network was designed to predict stock price movement (upward, stationary, or downward) with 67.34% accuracy by combining the strengths of Convolutional Neural Network and Long-Short Term Memory. Second, leveraging the trained trend prediction network, states were enhanced with daily and weekly stock information, and multi-scale and backbone network modules were employed for effective feature extraction. Finally, a Double Deep Q-Network algorithm based on the augmented state was adopted to learn robust trading strategies. This study also contributes to the theoretical advancement of stock price prediction models by introducing a hybrid CNN-LSTM architecture for trend prediction and a multi-scale feature extraction framework for improved decision-making. Experimental results across six U.S. financial assets demonstrate that a notable accuracy of 67.34% for price trend prediction is achieved by our Convolutional Neural Network-Long-Short Term Memory network. Furthermore, the proposed trading strategy outperforms other deep reinforcement learning algorithms, yielding an average annualized return of 44.47%. By comparison, the PPO model achieves 37.60%, the TDQN model achieves 13.38%, the DQN-vanilla model achieves 28.71%, and the DQN-pattern model achieves 22.21%. These results validate the efficacy of the approach, showcasing substantial excess returns. © 2025 Elsevier B.V., All rights reserved.","Huang, Y.; Cui, K.; Lu, X.",2025,10.1142/s0219622025500737,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012223869&doi=10.1142%2FS0219622025500737&partnerID=40&md5=d6dec46fc1f5507e9743879f2357481f,scopus,"This paper proposes a novel stock trading strategy using multi-scale deep reinforcement learning and price movement prediction. It combines CNN-LSTM for trend prediction (67.34% accuracy) and a Double Deep Q-Network for strategy learning. The strategy achieved an average annualized return of 44.47%, outperforming other deep reinforcement learning algorithms.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:36:24.816023
ab47e4e16b2d8b40,Stock market risk and return: An equilibrium approach,"Empirical evidence that expected stock returns are weakly related to volatility at the market level appears to contradict the intuition that risk and return are positively related. We investigate this issue in a general equilibrium exchange economy characterized by a regime-switching consumption process with time-varying transition probabilities between regimes. When estimated using consumption data, the model generates a complex, nonlinear and time-varying relation between expected returns and volatility, duplicating the salient features of the risk/return trade-off in the data. The results emphasize the importance of time-varying investment opportunities and highlight the perils of relying on intuition from static models.","Whitelaw, RF",2000,10.1093/rfs/13.3.521,,wos,"This paper investigates the relationship between stock market risk and return using a general equilibrium model with a regime-switching consumption process. The model generates a complex, nonlinear, and time-varying relationship between expected returns and volatility, which aligns with empirical observations and challenges static model intuitions.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:36:26.723429
6444a892b0e70ae6,Stock return predictability in emerging markets: Does the choice of predictors and models matter across countries?,"This study aims to examine return predictability in 24 emerging markets disaggregated in different regions. We propose four specifications, including a benchmark model. Then, an augmented model appropriate for each country, including a large set of potential factors, is evaluated. Furthermore, a dynamic multifactor model is investigated for all countries. Finally, we relax the symmetric hypothesis in asset return predictability based on a non-parametric non-linear approach: the projection pursuit regression model. Our study reveals three main findings. First, we reject all previous findings supporting a standard model of asset return predictability that is valuable for all countries, as we show that each country has specific domestic factors (both macroeconomic and financial) useful to predict future returns. Second, our empirical framework shows that asset return predictability might be robustly modelled based on non-linear specification based on the projection pursuit regression model. Our findings' explanatory power of out-of-sample estimations is economically relevant. Our results are useful for investors and policy-makers for portfolio diversification and regulation policies.","Hadhri, Sinda; Ftiti, Zied",2017,10.1016/j.ribaf.2017.04.057,,wos,"This study investigates stock return predictability in 24 emerging markets, exploring the impact of predictor choice and model specification. It evaluates benchmark, augmented, and dynamic multifactor models, and introduces a non-parametric non-linear approach (projection pursuit regression). Key findings indicate that domestic factors are crucial for predicting returns in each country, and that non-linear specifications offer robust and economically relevant out-of-sample predictability. The results are relevant for investors and policymakers.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:37:10.046282
d83e112304124b7e,Stock return prediction with multiple measures using neural network models,"In the field of empirical asset pricing, the challenges of high dimensionality, non-linear relationships, and interaction effects have led to the increasing popularity of machine learning (ML) methods. This study investigates the performance of ML methods when predicting different measures of stock returns from various factor models and investigates the feature importance and interaction effects among firm-specific variables and macroeconomic factors in this context. Our findings reveal that neural network models exhibit consistent performance across different stock return measures when they rely solely on firm-specific characteristic variables. However, the inclusion of macroeconomic factors from the financial market, real economic activities, and investor sentiment leads to substantial improvements in the model performance. Notably, the degree of improvement varies with the specific measures of stock returns under consideration. Furthermore, our analysis indicates that, after the inclusion of macroeconomic factors, there is a dissimilarity in model performance, variable importance, and interaction effects among macroeconomic and firm-specific variables, particularly concerning abnormal returns derived from the Fama–French three- and five-factor models compared with excess returns. This divergence is primarily attributed to the extent to which these factor models remove the variance associated with the macroeconomic variables. These findings collectively offer valuable insights into the efficacy of neural network models for stock return predictions and contribute to a deeper understanding of the intricate relationship between factor models, stock returns, and macroeconomic conditions in the domain of empirical asset pricing.",,2024,10.1186/s40854-023-00608-w,,proquest,"This study explores the use of neural network models for predicting stock returns, considering both firm-specific characteristics and macroeconomic factors. The research finds that incorporating macroeconomic data significantly enhances prediction performance, with the degree of improvement varying based on the specific stock return measure used. The study also highlights differences in model performance and variable importance when predicting abnormal returns versus excess returns, attributing this to how well factor models account for macroeconomic variance.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:37:13.853613
86f133ebdc5eaa79,Strategy and tactics in public debt management,"We examine the public debt management problem with respect to the maturity mix of new issues in a mean-variance framework. After identifying the main determinants of the long-run target (strategy), we focus on which interest rate conditions allow for a temporary deviation (tactics). The study is partly motivated by the apparent window of opportunity to issue more heavily at longer maturities given the recent historically low yields. We show that the room for long tactical positions on the long-term bond is actually narrower than predicted by rules of thumb based on Sharpe-like ratios. Once the model is augmented to embed real world features such as no price-taking and transaction costs, the scope for tactical position shrinks further. We discuss the model results and its implications in terms of the principal agent dilemma (government vs. debt manager); the paper also explores the financial stability implications arising from public debt issuance choices. All in all, our findings provide a rationale for the degree of caution often shown by many public debt managers in fulfilling their mandate. (C) 2015 Society for Policy Modeling. Published by Elsevier Inc. All rights reserved.","Dottori, Davide; Manna, Michele",2016,10.1016/j.jpolmod.2015.12.003,,wos,"This paper analyzes public debt management, focusing on the maturity mix of new debt issues within a mean-variance framework. It distinguishes between long-run strategy (target maturity mix) and short-term tactics (deviations based on interest rate conditions). The study finds that the opportunity for tactical issuance at longer maturities, despite historically low yields, is narrower than commonly assumed, especially when considering real-world factors like non-price-taking behavior and transaction costs. The findings also touch upon principal-agent issues and financial stability implications, suggesting a rationale for the cautious approach often adopted by debt managers.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:37:20.288282
fdcb4bf37f076de6,Structural break threshold VARs for predicting us recessions using the spread,"This paper proposes a model to predict recessions that accounts for non-linearity and a structural break when the spread between long- and short-term interest rates is the leading indicator. Estimation and model selection procedures allow us to estimate and identify time-varying non-linearity in a VAR. The structural break threshold VAR (SBTVAR) predicts better the timing of recessions than models with constant threshold or with only a break. Using real-time data, the SBTVAR with spread as leading indicator is able to anticipate correctly the timing of the 2001 recession. Copyright (c) 2006 John Wiley & Sons, Ltd.","Galvao, Ana Beatriz C.",2006,10.1002/jae.840,,wos,"This paper introduces a Structural Break Threshold Vector Autoregression (SBTVAR) model to predict US recessions using the interest rate spread as a leading indicator. The model accounts for non-linearity and structural breaks, outperforming simpler models in predicting recession timing. It successfully anticipated the 2001 recession using real-time data.",True,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:37:28.486563
60815d5382594062,Structure of the intact ppar-γ-rxr-α nuclear receptor complex on dna,"This article develops critical values to test the null hypothesis of a unit root against the alternative of stationarity with asymmetric adjustment. Specific attention is paid to threshold and momentum threshold autoregressive processes. The standard Dickey–Fuller tests emerge as a special case. Within a reasonable range of adjustment parameters, the power of the new tests is shown to be greater than that of the corresponding Dickey–Fuller test. The use of the tests is illustrated using the term structure of interest rates. It is shown that the movements toward the long-run equilibrium relationship are best estimated as an asymmetric process. © 1998 Taylor & Francis Group, LLC. © 2017 Elsevier B.V., All rights reserved.","Enders, W.; Granger, C.W.J.",1998,10.1080/07350015.1998.10524769,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032345729&doi=10.1080%2F07350015.1998.10524769&partnerID=40&md5=22eda4c7422cb3f9079f6babe2f18cb9,scopus,"This article develops critical values for testing unit roots against stationarity with asymmetric adjustment, focusing on threshold and momentum threshold autoregressive processes. It shows that the power of these new tests is greater than standard Dickey-Fuller tests within a certain parameter range and illustrates their use with the term structure of interest rates, suggesting asymmetric processes for movements toward long-run equilibrium.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:37:34.039344
cd2aff9d49f38305,"Supply, demand, and risk premiums in electricity markets","We model the impact of supply and demand on risk premiums in electricity futures, using daily data for 2003-2014. The model provides a satisfactory fit and allows for unspanned economic risk not embedded in futures prices. Model-implied spot risk premiums and forward biases are large, negative, highly time-varying, and exhibit plausible seasonal patterns. They differ from existing models, especially in periods of market turmoil, have not decreased in size over time, and help predict future returns. Both demand and supply have an economically significant impact on risk premiums. The risk premium associated with supply is characterized by large positive outliers. (c) 2021 Elsevier B.V. All rights reserved.","Jacobs, Kris; Li, Yu; Pirrong, Craig",2022,10.1016/j.jbankfin.2021.106390,,wos,"This study models the impact of supply and demand on risk premiums in electricity futures using daily data from 2003-2014. The model, which fits the data well, reveals unspanned economic risk and shows that model-implied spot risk premiums and forward biases are large, negative, time-varying, and have seasonal patterns. These premiums differ from existing models, especially during market turmoil, have not diminished over time, and aid in predicting future returns. Both supply and demand significantly influence risk premiums, with supply-related risk premiums exhibiting large positive outliers.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:37:36.739659
45482d9d94cd349f,THE STRUCTURE OF SKEWNESS PREFERENCES IN ASSET PRICING MODELS WITH HIGHER MOMENTS: AN EMPIRICAL TEST,"In this paper, the authors employ a nonlinear formulation to examine empirically the structural content of the three moment capital asset pricing model (CAPM). Whereas previous research focused on the coefficients of beta and co‐skewness, this paper presents empirical results on the market risk premium and elasticity coefficient components of these two coefficients. The results indicate that although the estimated coefficient of coskewness gives important information on the marginal rate of substitution between skewness and expected return, the elasticity coefficient can provide additional (albeit different) information on skewness preference that is independent of the effects of the market risk premium. This research also shows how the non‐linear formulation provides a direct linkage between the twomoment and three‐moment CAPM versions and thus provides an empirical test of the theoretical conditions under which skewness preference is consistent with the two‐moment CAPM empiricial results. Copyright © 1988, Wiley Blackwell. All rights reserved © 2016 Elsevier B.V., All rights reserved.","Sears, R.S.; Wei, K.C.J.",1988,10.1111/j.1540-6288.1988.tb00772.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0013178861&doi=10.1111%2Fj.1540-6288.1988.tb00772.x&partnerID=40&md5=4187f72b11f820ea8b517dd3bcad4551,scopus,"This paper empirically tests a nonlinear formulation of the three-moment capital asset pricing model (CAPM). It investigates the market risk premium and elasticity coefficient components of beta and co-skewness, finding that the elasticity coefficient offers independent information on skewness preference. The nonlinear formulation also links the two- and three-moment CAPM versions, allowing for an empirical test of theoretical conditions.",True,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:37:49.737104
bf72d6fc89554dd0,Technical indicators and aggregate stock returns: An updated look,"We provide updated analyses of technical indicators and aggregate stock return forecasting. We construct 105 new technical indicators as big data predictors and adopt eight advanced shrinkage methods in our forecasting analyses. Our evidence suggests that the refinements of 105 technical factors successfully overcome those of Neely et al.’s (2014) 14 technical variables to a large extent and challenge the forecasting role of Welch and Goyal's (2008) 14 popular macroeconomic variables when ENet and Lasso are used. The excellent performance of the forecasting information based on 105 technical indicators generates sufficiently high in-sample and out-of-sample R-squared values and economically sizable gains in forecasting the excess returns of the composite Standard & Poor 500 market. The corresponding evidence remains robust to changes in the business cycle, forecasting horizons, and alternative evaluation periods. © 2025 Elsevier B.V., All rights reserved.","Shi, Q.",2025,10.1016/j.mulfin.2025.100898,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216555986&doi=10.1016%2Fj.mulfin.2025.100898&partnerID=40&md5=d990bba991d2907cab343d027f7fee3d,scopus,"This study updates the analysis of technical indicators for aggregate stock return forecasting. It introduces 105 new technical indicators and employs eight advanced shrinkage methods. The findings indicate that these new indicators significantly improve forecasting compared to previous work and challenge the predictive power of popular macroeconomic variables, particularly when using ENet and Lasso. The results show high in-sample and out-of-sample R-squared values and economically significant gains in forecasting S&P 500 excess returns, with robustness across different business cycles, forecasting horizons, and evaluation periods.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:38:14.319015
f0b7502b7b81fe47,Temperature effects on crop yields in heat index insurance,"Heat can cause substantial yield losses in crop production and climate change is increasing the risk of this kind of damage. Weather index insurance can help to reduce the financial losses resulting from heat exposure. This paper introduces crop-specific payout functions based on restricted cubic splines in heat index insurance. The use of restricted cubic splines is a cutting-edge method to reflect empirically estimated temperature effects on crop yields and to estimate temperature-related yield losses. The integration of these temperature effects in payout functions facilitates insurance design and allows hourly temperatures to be used as the underlying index. An empirical analysis is used to assess heat stress effects for a panel of East German winter wheat and winter rapeseed producers, to calibrate insurance contracts accordingly and simulate the resulting risk reducing capacities. We find that the insurance scheme introduced here leads to statistically and economically significant out-of-sample risk reducing capacities for farmers, i.e. risk premiums are reduced by up to approximately 20% at the median, in comparison to the uninsured status and at the actuarially fair premium. Moreover, we highlight that policy-makers can support the cost-efficient provision of market-based weather index insurance by fostering data collection and data","Bucheli, Janic; Dalhaus, Tobias; Finger, Robert",2022,10.1016/j.foodpol.2021.102214,,wos,"This paper introduces crop-specific payout functions for heat index insurance using restricted cubic splines to model temperature effects on crop yields. An empirical analysis on East German winter wheat and rapeseed producers demonstrates the insurance's risk-reducing capacity, potentially lowering risk premiums by up to 20%. The study suggests policy support for data collection to enhance market-based weather index insurance.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:38:22.491041
187c6ade28f04d09,Term structure of risk under alternative econometric specifications,"This paper characterizes the term structure of risk measures such as value at risk (VaR) and expected shortfall under different econometric approaches including multivariate regime switching, GARCH-in-mean models with Student-t errors, two-component GARCH models and a nonparametric bootstrap. We show how to derive the risk measures for each of these models and document large variations in term structures across econometric specifications. An out-of-sample forecasting experiment applied to stock, bond and cash portfolios suggests that the best model is asset- and horizon specific but that the bootstrap and regime switching model are best overall for VaR levels of 5% and 1%, respectively. (c) 2005 Elsevier B.V. All rights reserved.","Guidolin, M; Timmermann, A",2006,10.1016/j.jeconom.2005.01.033,,wos,"This paper compares different econometric models (multivariate regime switching, GARCH-in-mean with Student-t errors, two-component GARCH, and nonparametric bootstrap) for estimating risk measures like VaR and expected shortfall. It highlights significant differences in term structures across these models and finds that the best model for out-of-sample forecasting is asset- and horizon-specific, with the bootstrap and regime switching models performing best overall for VaR at 5% and 1% levels, respectively.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:38:33.842054
d20a95b2f68502a7,Testing different forms of efficiency for Dhaka Stock Exchange,"The Efficient-Market Hypothesis (EMH) asserts that efficient markets are informationally efficient or all information (market, public or private) should reflect on stock prices. No one could earn excess profit using any kind of information in efficient market. There are three forms of efficiency in markets: strong, semi-strong and weak. We tested EMH for Dhaka Stock Exchange (DSE) for the period 2003 2005. We used the excess return market model to test the semi-strong form efficiency of DSE. Two forecasting techniques, Autoregressive Integrated Moving Average (ARIMA) and neural network, are used to test the weak form efficiency of DSE. We get excess return for many stocks listed in DSE, demonstrating that DSE is not an efficient market in semi-strong form. Besides, the DSE market index is not random and the trend could be captured by ARIMA and neural network techniques. Therefore, the DSE is also not an efficient market in weak form.",,2011,10.1504/ijfsm.2011.038325,,proquest,"This study tests the Efficient-Market Hypothesis (EMH) for the Dhaka Stock Exchange (DSE) from 2003-2005. It uses the excess return market model to assess semi-strong form efficiency and ARIMA and neural network techniques to evaluate weak form efficiency. The findings indicate that the DSE is not efficient in either the semi-strong or weak form, as excess returns were observed and market index trends could be captured by forecasting models.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:38:39.354198
00b59708a2208b21,Testing predictability and nonlinear dependence in the Indian stock market,"This paper suggests a systematic approach to studying predictability and nonlinear dependence in the context of the Indian stock market, one of the most important emerging stock markets in the world. The proposed approach considers nonlinear dependence in returns and envisages appropriate specification of both the conditional first- and second-order moments, so that final conclusions are free from any probable statistical consequences of misspecification. To this end, a number of rigorous tests are applied on the returns, based on four major daily indices of the Indian stock market. It is found that the Indian stock market is predictable, and this observed lack of efficiency is due to serial correlation, nonlinear dependence, day-of-the week effects, parameter instability, conditional heteroskedasticity (GARCH), daily-level seasonality in volatility, the short-term interest rate (in some subperiods of some indices), and some dynamics in the higher-order moments.","Sarkar, N; Mukhopadhyay, D",2005,10.1080/1540496x.2005.11052624,,wos,"This paper investigates predictability and nonlinear dependence in the Indian stock market using a systematic approach that accounts for nonlinearities in returns and specifies conditional moments. Applying rigorous tests to daily index returns, the study finds the market to be predictable due to factors like serial correlation, nonlinear dependence, day-of-the-week effects, parameter instability, GARCH, seasonality in volatility, interest rates, and higher-order moment dynamics.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:38:46.879302
74695cb70436aa62,"Testing the uncovered interest parity using traded volatility, a time-varying risk premium and heterogeneous expectations","This paper carries out an empirical investigation of an extended version of Flood and Marion's (2000, Self-fulfilling risk predictions: an application to speculative attacks. Journal of International Economics 50, 245-268) UIP model, which incorporates a nonlinear time-varying risk premium that depends on both the expected variance of the future exchange rate and the relative worldwide private holdings of domestic and foreign government bonds. A novel contribution of our paper is the use of traded currency volatility, which is directly observable in the market place, to measure expectations about the future volatility of the exchange rate. Another contribution is the explicit modelling of heterogeneous exchange rate expectations formed by forward-looking fundamentalists and backward-looking chartists. Our overall empirical evidence provides strong support for the extended nonlinear UIP model. We also investigate for the first time the role of traded volatility in the dynamic behaviour of exchange rates, and find that high currency volatility is likely to produce oscillatory and unstable exchange rate paths. © 2006 Elsevier Ltd. All rights reserved. © 2006 Elsevier B.V., All rights reserved.","Sarantis, N.",2006,10.1016/j.jimonfin.2006.08.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750960093&doi=10.1016%2Fj.jimonfin.2006.08.002&partnerID=40&md5=5f02c4167892f6e8da7fd7b515be19f7,scopus,"This paper empirically investigates an extended version of the Flood and Marion (2000) UIP model, incorporating a nonlinear time-varying risk premium and heterogeneous expectations. It uses traded currency volatility to measure expected future volatility and explicitly models expectations formed by fundamentalists and chartists. The findings strongly support the extended model and suggest that high currency volatility leads to unstable exchange rate paths.",True,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:39:35.152078
5b51de8e2d09e5f3,The American foreign exchange option in time-dependent one-dimensional diffusion model for exchange rate,"The classical Garman-Kohlhagen model for the currency exchange assumes that the domestic and foreign currency risk-free interest rates are constant and the exchange rate follows a log-normal diffusion process. In this paper we consider the general case, when exchange rate evolves according to arbitrary one-dimensional diffusion process with local volatility that is the function of time and the current exchange rate and where the domestic and foreign currency risk-free interest rates may be arbitrary continuous functions of time. First non-trivial problem we encounter in time-dependent case is the continuity in time argument of the value function of the American put option and the regularity properties of the optimal exercise boundary. We establish these properties based on systematic use of the monotonicity in volatility for the value functions of the American as well as European options with convex payoffs together with the Dynamic Programming Principle and we obtain certain type of comparison result for the value functions and corresponding exercise boundaries for the American puts with different strikes, maturities and volatilities. Starting from the latter fact that the optimal exercise boundary curve is left continuous with right-hand limits we give a mathematically rigorous and transparent derivation of the significant early exercise premium representation for the value function of the American foreign exchange put option as the sum of the European put option value function and the early exercise premium. The proof essentially relies on the particular property of the stochastic integral with respect to arbitrary continuous semimartingale over the predictable subsets of its zeros. We derive from the latter the nonlinear integral equation for the optimal exercise boundary which can be studied by numerical methods. © 2008 Springer Science+Business Media, LLC. © 2012 Elsevier B.V., All rights reserved.","Rehman, N.; Shashiashvili, M.",2009,10.1007/s00245-008-9056-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-63049128694&doi=10.1007%2Fs00245-008-9056-7&partnerID=40&md5=5005bfa644091fda79c4d7e25bdfdd4e,scopus,"This paper generalizes the Garman-Kohlhagen model for currency exchange options by allowing for time-dependent interest rates and exchange rates following a general one-dimensional diffusion process with local volatility. It rigorously derives properties of the American put option's value function and optimal exercise boundary, establishing an early exercise premium representation and a nonlinear integral equation for the boundary, suitable for numerical study.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:39:38.433257
0f494f7b776c3aa9,The Macroeconomics of Financial Speculation,"I review the literature on financial speculation driven by belief disagreements from a macroeconomics perspective. To highlight unifying themes, I develop a stylized macroeconomic model that embeds several mechanisms. With short-selling constraints, speculation can generate overvaluation and speculative bubbles. Leverage can substantially inflate speculative bubbles, and leverage limits depend on perceived downside risks. Shifts in beliefs about downside tail scenarios can explain the emergence and the collapse of leveraged speculative bubbles. Speculative bubbles are related to rational bubbles, but they match better the empirical evidence on the predictability of asset returns. Even without short-selling constraints, speculation induces procyclical asset valuation. When speculation affects the price of aggregate assets, it also influences macroeconomic outcomes such as aggregate consumption, investment, and output. Speculation in the boom years reduces asset prices, aggregate demand, and output in the subsequent recession. Macroprudential policies that restrict speculation in the boom can improve macroeconomic stability and social welfare.","Simsek, Alp",2021,10.1146/annurev-economics-092120-050543,,wos,"This paper reviews the literature on financial speculation driven by belief disagreements from a macroeconomic perspective. It presents a model where short-selling constraints and leverage can lead to speculative bubbles, affecting macroeconomic outcomes like consumption, investment, and output. The paper suggests that macroprudential policies can enhance stability by restricting speculation during boom periods.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:39:44.343917
6baa570f6471f39a,The Relationship Between Volatility and Sovereign Credit Risk in the Emerging Markets: A Nonlinear ARDL Approach,"This study investigates the short- and long-run nexus between the volatility index of VIX and sovereign credit risk represented by CDS spread in emerging markets, namely Turkey, China, Russia, Brazil, and Mexico. The emerging markets are at the center of investors' interest due to high return opportunities. The relationship between volatility and sovereign credit risk has been studied many times via linear models. However, financial series exhibit asymmetric dynamics, as volatility clustering, excess kurtosis, and others. Thus, we use nonlinear autoregressive distributed lags (NARDL) analysis to capture nonlinear relations between the volatility and the sovereign credit risks of these countries by using daily data from 04.01.2010 to 29.11.2019. The bounds test of the NARDL model confirms the cointegration between VIX and CDS spreads of the countries under study. The analysis of estimated NARDL parameters shows that negative shocks of the volatility index have a long-lasting impact on CDS spreads. Chinese CDS spread are more sensitive to VIX index changes in the short run. The effect of a decrease in volatility on Russian CDS spread is higher than the effect of an increase. Turkish and Brazilian CDS spreads are more reactive to increase in the VIX, whereas Mexican CDS is less sensitive. These findings show that investors, arbitrageurs and speculators should consider global indicators when taking a position on sovereign bonds of emerging markets.","Yigit, Fatih; Aliyev, Fuzuli",2022,10.21121/eab.1064521,,wos,"This study examines the short- and long-run relationship between the VIX volatility index and sovereign credit risk (CDS spread) in emerging markets (Turkey, China, Russia, Brazil, Mexico) using a nonlinear autoregressive distributed lags (NARDL) approach. The NARDL model captures asymmetric dynamics, revealing that negative VIX shocks have a lasting impact on CDS spreads. The study also highlights country-specific short-run sensitivities and asymmetric effects of volatility changes on CDS spreads.",True,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:39:52.774025
b81f992522b901f7,The Role of Regime Shifts in the Term Structure of Interest Rates: Further Evidence from an Emerging Market,"In this paper, we investigate the interrelationships among Turkish interest rates having different maturities by using a regime-switching vector error correction model. We find a relationship of long-run equilibrium among interest rates having various maturities. Furthermore, we conclude that term structure dynamics exhibit significant nonlinearity. A forecasting experiment also reveals that the nonlinear term structure models fare better in forecasting than other linear specifications. However, we cannot conclude that interest rate adjustments are made in an asymmetric way in the long run. Adapted from the source document.",,2012,10.2753/ree1540-496x4806s504,,proquest,"This paper examines the relationship between Turkish interest rates of different maturities using a regime-switching vector error correction model. It finds a long-run equilibrium and significant nonlinearity in term structure dynamics, with nonlinear models outperforming linear ones in forecasting. However, asymmetric long-run interest rate adjustments were not confirmed.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T12:40:10.757810
03ad76077d8f372b,The SR approach: A new estimation procedure for non-linear and non-Gaussian dynamic term structure models,This paper suggests a new approach for estimating linear and non-linear dynamic term structure models with latent factors. We impose no distributional assumptions on the factors which therefore may be non-Gaussian. The novelty of our approach is to use many observables (yields or bond prices) in the cross-section dimension. This implies that the latent factors can be determined quite accurately by a sequence of cross-section regressions. We also show how output from these regressions can be used to obtain model parameters by a two- or three-step moment-based estimation procedure.,,2015,10.1016/j.jeconom.2014.10.002,,proquest,"This paper introduces a new estimation method for dynamic term structure models, allowing for non-linear and non-Gaussian latent factors. The approach leverages a large number of observable variables (yields or bond prices) to accurately determine these factors through cross-section regressions, which are then used for parameter estimation via a moment-based procedure.",True,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:40:21.198794
9478f7d5a656d74f,The Stochastic Stationary Root Model,"We propose and study the stochastic stationary root model. The model resembles the cointegrated VAR model but is novel in that: (i) the stationary relations follow a random coefficient autoregressive process, i.e., exhibhits heavy-tailed dynamics, and (ii) the system is observed with measurement error. Unlike the cointegrated VAR model, estimation and inference for the SSR model is complicated by a lack of closed-form expressions for the likelihood function and its derivatives. To overcome this, we introduce particle filter-based approximations of the log-likelihood function, sample score, and observed Information matrix. These enable us to approximate the ML estimator via stochastic approximation and to conduct inference via the approximated observed Information matrix. We conjecture the asymptotic properties of the ML estimator and conduct a simulation study to investigate the validity of the conjecture. Model diagnostics to assess model fit are considered. Finally, we present an empirical application to the 10-year government bond rates in Germany and Greece during the period from January 1999 to February 2018.",,2018,10.3390/econometrics6030039,,proquest,"This paper introduces the stochastic stationary root (SSR) model, an extension of the cointegrated VAR model. The SSR model features random coefficient autoregressive processes for stationary relations, leading to heavy-tailed dynamics, and incorporates measurement error. Due to the lack of closed-form likelihood expressions, the authors propose particle filter-based approximations for estimation and inference, enabling a stochastic approximation of the Maximum Likelihood (ML) estimator. The study includes a simulation to validate asymptotic properties and model diagnostics. An empirical application to German and Greek government bond rates from 1999-2018 is presented.",True,True,False,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T12:40:56.947370
2d7d94f5cc68cb41,The VIX Premium,"Ex ante estimates of the volatility premium embedded in VIX futures, known as the VIX premium, fall or stay flat when ex ante measures of risk rise. This is not an artifact of mismeasurement: (i) ex ante premiums reliably predict ex post returns to VIX futures with a coefficient near one, and (ii) falling ex ante premiums predict increasing ex post market and investment risk, creating profitable trading opportunities. Falling hedging demand helps explain this behavior, as premiums and trader exposures tend to fall together when risk rises. These facts provide a puzzle for theories of why investors hedge volatility. Received January 13, 2017; editorial decision April 26, 2018 by Editor Stijn Van Nieuwerburgh. Authors have furnished an Internet Appendix, which is available on the Oxford University Press Web site next to the link to the final published paper online.","Cheng, Ing-Haw",2019,10.1093/rfs/hhy062,,wos,"This paper examines the VIX premium, which represents the ex ante estimate of the volatility premium in VIX futures. It finds that this premium declines when risk measures increase, which is not due to measurement errors. The study shows that falling VIX premiums predict future market and investment risk, offering trading opportunities. Reduced hedging demand is suggested as a reason for this phenomenon, as premiums and trader exposures decrease together during periods of heightened risk. This behavior presents a challenge to existing theories on why investors hedge volatility.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:40:59.899666
85e5ea3195695843,The advantages of CBOE credit VIXs for corporate bond investors in North America: A sectoral analysis,"This paper examines the safe-haven role of the recently introduced CBOE credit VIXs for investment-grade and high-yield corporate bonds, at both aggregate and sectoral levels. Using a time-varying quantile-based framework and daily data from June 5, 2014 to December 10, 2023, the safe-haven role of credit VIX is confirmed irrespective of bond sector. The safe-haven property of credit VIX is pronounced for high-yield bonds which embed a high credit risk-premium. This result stands when taking into account interest rate volatility, as measured by the MOVE index. A time-varying analysis shows the persistence of credit VIX as a safe-haven for all bond sectors after the COVID-19 pandemic and during a high US interest-rate regime. Corporate bond investors and traders can use these findings to refine their investment and trading decisions and offset credit risk during both normal and turbulent periods.","Iqbal, Najaf; Bouri, Elie; Ozkan, Oktay",2025,10.1016/j.ribaf.2024.102607,,wos,"This paper investigates the safe-haven properties of CBOE credit VIXs for corporate bonds in North America, finding they act as a safe haven for both investment-grade and high-yield bonds across sectors. The safe-haven effect is stronger for high-yield bonds and persists even when considering interest rate volatility and post-COVID-19 periods. The findings can aid investors in managing credit risk.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:41:06.019594
0b2ab325246838cf,The commodity risk premium and neural networks,"The paper uses linear and nonlinear predictive models to study the linkage between a set of 128 macroeconomic and financial predictors and the risk premium of commodity futures contracts. The linear models use shrinkage methods based on either naive averaging or principal components. The nonlinear models use feedforward deep neural networks (DNN) either as stand-alone or in conjunction with a long short-term memory network (LSTM). Out of the four specifications considered, the LSTM-DNN architecture best captures the risk premium, which underscores the need to estimate models that are both nonlinear and recurrent. The superior performance of the LSTM-DNN portfolio persists after accounting for transaction costs or illiquidity and is unrelated to previously-documented commodity risk factors. © 2023 Elsevier B.V., All rights reserved.","Rad, H.; Low, R.K.Y.; Miffre, J.; Faff, R.",2023,10.1016/j.jempfin.2023.101433,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174155469&doi=10.1016%2Fj.jempfin.2023.101433&partnerID=40&md5=2b9b49f1dc40abe23ee81ebf9bd51eb2,scopus,"This paper investigates the relationship between macroeconomic and financial predictors and the commodity futures risk premium using both linear and nonlinear predictive models. It finds that a combination of Long Short-Term Memory (LSTM) and Deep Neural Networks (DNN) architecture outperforms other models in capturing the risk premium, even after accounting for transaction costs and illiquidity. The study highlights the importance of nonlinear and recurrent models for this prediction task.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:41:17.973961
714f332dbdcd0c45,The contribution of wealth concentration to the subprime crisis: a quantitative estimation,"The crisis that broke out in mid-2007 was caused by the fact that the collateralised debt obligation (CDO) market had grown to a size sufficient to wreak general havoc when it suddenly collapsed. Several authors have argued that economic inequality was important to the growth of this market. This paper attempts to strengthen this argument by concentrating attention on global wealth concentration. After summarising recent evidence on the negative impact of investor demand on US bond yields in the pre-crisis period, new evidence regarding the specific contribution of high-net-worth individuals to this negative impact is presented. The paper then goes on to show how, after having helped to cause a yield problem in the major US debt markets, high-net-worth individuals (via hedge funds) continued to be a major source of the pressure on US banks to resolve this yield problem through the mass production of CDOs. Adapted from the source document.",,2014,10.1093/cje/bet061,,proquest,"This paper quantitatively estimates the contribution of global wealth concentration to the subprime crisis. It argues that high-net-worth individuals, through their demand for US bonds, lowered yields, pressuring banks to create Collateralised Debt Obligations (CDOs). The study analyzes investor demand and the role of hedge funds in this process.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:41:22.082942
f82cbd04c25cf4f6,The determinants of capitalization rates: evidence from the US real estate markets,"Purpose Establishing the strength of a novel variable-mortgage debt as a fraction of US gross domestic product (GDP)-on forecasting capitalization rates in both the US office and multifamily sectors. Design/methodology/approach The authors specifies a vector error correction model (VECM) to the data. VECM are used to address the nonstationarity issues of financial variables while maintaining the information embedded in the levels of the data, as opposed to their differences. The cap rate series used are from Green Street Advisors and represent transaction cap rates which avoids the problem of artificial smoothness found in appraisal-based cap rates. Findings Using a VECM specified with the novel variable, unemployment and past cap rates contains enough information to produce more robust forecasts than the traditional variables (return expectations and risk premiums). The method is robust both in and out of sample. Practical implications This has direct implications for governmental policy, offering a path to real estate price stability and growth through mortgage access-functions largely influenced by the Fed and the quasi-federal agencies Fannie Mae and Freddie Mac. It also offers a timely alternative to interest rate-based forecasting models, which are likely to be less useful as interest rates are to be held low for the foreseeable future. Originality/value This study offers a new and highly explanatory variable to the literature while being among the only to model either (1) transactional cap rates (versus appraisal) (2) out-of-sample data (versus in-sample) (3) without the use of the traditional variables thought to be integral to cap rate modelling (return expectations and risk premiums).","Larriva, Matt; Linneman, Peter",2022,10.1108/jpif-12-2020-0140,,wos,"This study investigates the impact of mortgage debt as a fraction of US GDP on forecasting capitalization rates in the US office and multifamily sectors. Using a vector error correction model (VECM), the research finds that this novel variable, along with unemployment and past cap rates, provides more robust forecasts than traditional variables like return expectations and risk premiums. The findings have implications for governmental policy aimed at real estate price stability and offer an alternative to interest rate-based forecasting models.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:41:32.072280
8f0d28acc3ae4ac3,The disappearance of style in the US equity market,"This article investigates the modelling of style returns in the United States and the returns to style 'tilts' based on forecasts of enhanced future style returns. We use hidden Markov model to build our forecasts for data from 1975 to 1998. We do not include more recent observations as the subsequent trend and volatility sways the analysis. Our finding that style returns are less forecastible in the late 1990s is consistent with the hypothesis that style returns are the result of anomalies rather than risk premia. The erosion of anomalous returns as public awareness of their presence is translated into strategies that arbitrage away the excess returns seems to be a hypothesis consistent with our modelling results. Reprinted by permission of Routledge, Taylor and Francis Ltd.",,2007,10.1080/09603100701217978,,proquest,"This study models US equity style returns and the effectiveness of style 'tilts' using forecasts derived from a hidden Markov model (1975-1998). The findings suggest style returns became less predictable in the late 1990s, supporting the hypothesis that these returns stem from anomalies rather than risk premia, and that increased awareness and arbitrage are eroding these anomalies.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:41:45.893607
601095a482cc9b1e,The evolution of risk premium as a measure for intra-regional equity market integration,"We estimate and test the conditional version of an international capital asset pricing model using a parsimonious multivariate GARCH process and the multivariate nonlinear least squares method. Since our approaches are fully parametric, we can recover any quantity that is a function of the first two conditional moments. Our findings strongly support using a model that includes both regional market and foreign exchange risk. However, both sources of risk are detected only when their prices are allowed to change over time. Our empirical results show clear evidence of market integration to varying degrees, explained by the US term premium and the level of market openness. Though it reaches high values during turmoil periods and exhibits an upward trend toward the end of the estimation period, the Indonesian stock market remains partially integrated into the ASEAN-5 regional market. These results suggest that diversification into Indonesian market assets continues to produce substantial profits and that asset pricing rules should reflect a state of partial integration. © 2015 Elsevier B.V., All rights reserved.","Guesmi, K.; Teulon, F.; Muzaffar, A.T.",2014,10.1016/j.irfa.2014.07.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909581587&doi=10.1016%2Fj.irfa.2014.07.003&partnerID=40&md5=6d13f0bc7cb045a3f4f9ab77d2cc3801,scopus,"This study estimates and tests a conditional international capital asset pricing model using a multivariate GARCH process and nonlinear least squares. The findings support a model incorporating both regional market and foreign exchange risk, with time-varying risk prices. The Indonesian stock market shows partial integration into the ASEAN-5 regional market, influenced by the US term premium and market openness. Diversification into Indonesian assets remains profitable, and asset pricing should reflect this partial integration.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:42:00.438279
33d0e0f20fc3942d,The expected inflation channel of government spending in the postwar U.S.,"There exist sticky price models in which the output response to a government spending change can be large if the central bank is nonresponsive to inflation. According to this ""expected inflation channel,"" government spending drives up expected inflation, which in turn, reduces the real interest rate and leads to an increase in private consumption. This paper examines whether the channel was important in the post-WWII U.S., with particular attention to the 2009 Recovery Act period. First, we show that a model calibrated to have a large output multiplier requires a large response of expected inflation to a government spending shock. Next, we show that this large response is inconsistent with structural vector autoregression evidence from the Federal Reserve's passive policy period (1959-1979). Then, we study expected inflation measures during the Recovery Act period in conjunction with a panel of professional forecaster surveys, a cross-country comparison of bond yields and fiscal policy news announcements. We show that the expected inflation response was too small to engender a large output multiplier. © 2020 Elsevier B.V., All rights reserved.","Dupor, B.; Li, R.",2015,10.1016/j.euroecorev.2014.11.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920172590&doi=10.1016%2Fj.euroecorev.2014.11.004&partnerID=40&md5=6d2a17fcdbe357ec1335ae439f96aa03,scopus,"This paper investigates the expected inflation channel of government spending in the postwar U.S., focusing on the 2009 Recovery Act period. It finds that while sticky price models suggest a large output multiplier can arise from this channel, empirical evidence from the passive policy period and the Recovery Act period indicates that the expected inflation response was too small to generate such a multiplier.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:42:04.654035
62cb6b8984b2f540,The hawk eye scan: Halyomorpha halys detection relying on aerial tele photos and neural networks,"This paper faces the challenge of monitoring the Brown Marmorated Stink Bug (H.halys) (Halyomorpha halys) within orchards, utilizing drones, and computer vision. H.halys is an invasive species originating from East Asia, that is extremely polyphagous and poses a significant threat to various crops. Our first contribution is a drone navigation protocol, which ensures risk-free drone flights in cluttered orchard environments, preserving image quality and avoiding obstacles. We then create a pioneering H.halys dataset consisting of aerial telephotos captured in the field autonomously by the drone. The dataset allows the development and evaluation for the first time of multiple ML models for H.halys detection in the field. We trained YOLOV5, YOLOV8, RETINANET, and FASTER-RCNN models using different learning methodologies, exploiting different percentages of images without the bug, and using different slicing procedures for the images. The Medium YOLOV5 model trained with all images containing a bug detects the largest number of H.halys on the testing set and overall performs the best, while RETINANET and FASTER-RCNN provide the best trade-off between precision and recall. Models vary in their ability to handle occluded H.halys and bug-free images, which are common since the presence of the bug cannot be predicted before capturing a photo. These results show promising potential for automating H.halys monitoring, despite the image complexity and the early dataset stage. Our work marks a significant step towards enhancing smart agriculture practices due to the simplicity of the data acquisition process and the off-the-shelf hardware selection. © 2025 Elsevier B.V., All rights reserved.","Palazzetti, L.; Rangarajan, A.K.; Dinca, A.; Boom, B.; Popescu, D.; Offermans, P.; Pinotti, C.M.",2024,10.1016/j.compag.2024.109365,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202194488&doi=10.1016%2Fj.compag.2024.109365&partnerID=40&md5=6343a88c6f0066b44a1045827b3a882c,scopus,"This paper introduces a drone-based system using aerial photography and machine learning (YOLOV5, YOLOV8, RETINANET, FASTER-RCNN) for detecting the invasive Brown Marmorated Stink Bug (Halyomorpha halys) in orchards. It details a drone navigation protocol, a novel dataset of aerial images, and evaluates various ML models for bug detection, with YOLOV5 showing the best detection rate and others offering good precision-recall trade-offs. The study highlights the potential for automated pest monitoring in agriculture.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:42:07.938012
029c751e000c5cf6,The impact of corporate social responsibility on financial distress: evidence from developing economy,"Purpose: This study aims to explore the role of corporate social responsibility (CSR) on the likelihood of financial distress for a sample of 139 Pakistan Stock Exchange (PSX) listed firms throughout 2008–2019. Design/methodology/approach: The dynamic generalized method of moments (GMM) estimator is used to examine the impact of CSR on financial distress. The investment in CSR is measured through a multidimensional financial approach which comprises the sum of the contribution made by the company in the form of charitable donation, employees’ welfare and research and development, while the Altman Z-score is used as an indicator of financial distress. The higher the Z-score, the lower will be the probability of financial distress. Findings: The authors find a significant positive impact of CSR on financial distress in GMM model. This finding is consistent with the shareholder view and over-investment hypothesis of CSR as management makes an investment in CSR to get personal benefits, which resultantly leads the firm toward financial distress state. Further, this positive relationship remains present for firms having strong involvement in foreign business through exports. Research limitations/implications: Like other studies, the present study is not free from limitations. First, financial firms are skipped from the sample, although literature witnesses a lot of studies highlight the financial firms’ commitment to achieving CSR goals. Second, financial distress occurs in different stages, and this study fails to establish a linkage between CSR engagement at different stages of financial distress. In the future, researchers can make valuable addition by covering these missing links in present studies. Practical implications: Findings suggest several practical implications. For policymakers, they should encourage firms to adopt more socially responsible behavior as it not only prevents them from distress but also comes with better investment behavior, minimize bankruptcies and make economies more strong and stable. Second, results suggest corporate managers emphasize socially responsible behavior as its benefits are beyond the “societal benefits” as it lessens financial distress through lower cost of debt, lesser financial constraints and reduced cost of information asymmetry, and it minimizes the cost of capital. Lastly, investors make risk premium assessments related to future earnings by determining the likelihood of financial distress in the future. Originality/value: The study extends the body of existing literature on CSR and the likelihood of financial distress in Pakistan, which is according to the best knowledge of the authors, not yet studied before. The results suggest that policymakers may pay special attention to the quality of CSR while predicting corporate financial distress. © 2021 Elsevier B.V., All rights reserved.","Farooq, M.; Noor, A.",2021,10.1108/par-10-2020-0196,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107835740&doi=10.1108%2FPAR-10-2020-0196&partnerID=40&md5=c3473de3b8ddfd1041890bffc2ae76da,scopus,"This study investigates the relationship between corporate social responsibility (CSR) and financial distress in 139 listed firms on the Pakistan Stock Exchange from 2008 to 2019. Using the dynamic generalized method of moments (GMM) estimator, the research measures CSR through charitable donations, employee welfare, and R&D, and financial distress via the Altman Z-score. The findings indicate a significant positive impact of CSR on financial distress, supporting the over-investment hypothesis where CSR investments may benefit management personally, leading to financial difficulties. This effect is stronger for firms with substantial export involvement. The study acknowledges limitations such as excluding financial firms and not differentiating CSR's impact across various stages of financial distress. Practical implications suggest policymakers should encourage CSR for economic stability and corporate managers should view CSR as a strategy to reduce debt costs, financial constraints, and information asymmetry, ultimately lowering the cost of capital. Investors can use CSR quality to assess future earnings risk.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:42:15.496377
34a2d6abf168cbd6,The impact of the Covid-19 related media coverage upon the five major developing markets,"This paper analyses the influence of the Covid-19 coverage by the social media upon the shape of the sovereign yield curves of the five major developing countries, namely Federative Republic of B razil, Russian Federation, Republic of India, People's Republic of China, and the Republic of South Africa (BRICS). The coherenc e between the level, slope, and the curvature of the sovereign yield term structures and the Covid-19 medi a coverage is found to vary between low and high ranges, depending on the phases of the pandemic. The empirical estimations of the yield-curve factors a re performed by means of the Diebold-Li modified version of the Nelson-Siegel model. The intervals of low coherence reveal the capacity of the two latent factors, level and slope, to be used for creating cross-factor diversification strategies, workable under crisis conditions, as evidenced on the example of the ongoing pandemic. Diverse coherence patterns are reported on a per-country basis, highlighting a promising potential of sovereign debt investments for designing cross-country and cross-factor fixed-income strategies, capable of hedging downside risks. © 2021 Elsevier B.V., All rights reserved.","Umar, Z.; Gubareva, M.; Sokolova, T.",2021,10.1371/journal.pone.0253791,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109009430&doi=10.1371%2Fjournal.pone.0253791&partnerID=40&md5=c2e168481e34ba89a700029c35741b67,scopus,"This paper examines how COVID-19 media coverage has affected the sovereign yield curves of five major developing countries (BRICS). It uses a modified Nelson-Siegel model to analyze the relationship between media coverage and the yield curve's level, slope, and curvature, finding that coherence varies with pandemic phases. The study suggests that latent factors like level and slope can be used for diversification and risk hedging strategies, with country-specific patterns highlighted.",True,True,False,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T12:42:36.802108
38aa2888d66b462f,The information content of a nonlinear macro-finance model for commodity prices,"State-of-the-art term structure models of commodity prices have serious difficulties extrapolating the prices of long-maturity futures contracts from short-dated contracts. This situation is problematic for valuing real commodity-linked assets. We estimate a nonlinear four-factor continuous time model of commodity price dynamics. The model nests many previous specifications. To estimate the model, we use crude oil prices and inventories. The inventory data and nonlinear price dynamics have a large impact on oil price forecasts. The additional factor in our model compared with current three-factor models has a significant impact on model-implied long-maturity futures prices. © 2021 Elsevier B.V., All rights reserved.","Khan, S.; Khokher, Z.; Simin, T.",2017,10.1093/rfs/hhw087,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027014860&doi=10.1093%2Frfs%2Fhhw087&partnerID=40&md5=9dc58ffacd4b0157ed4a02671873fc2f,scopus,"This paper estimates a nonlinear four-factor continuous time model for commodity price dynamics, addressing difficulties in extrapolating long-maturity futures prices from short-dated contracts. Using crude oil prices and inventories, the model shows that inventory data and nonlinear dynamics significantly impact oil price forecasts, and the additional factor improves long-maturity futures price implications compared to existing three-factor models.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:42:44.747828
c7b17c47b92ccbbf,The relationship between risk-neutral and actual default probabilities: the credit risk premium,"The study investigates empirically the relationship between the risk-neutral measure Q and the real-world measure P. We study the ratio between the risk-neutral and actual default intensities, which we call the coverage ratio or the relative credit risk premium. Actual default intensities are derived from rating agencies annual transition matrices, while risk-neutral default intensities are bootstrapped from CDS quotes of European corporates. We quantify the average risk premium and its changes over time. Compared to related literature, special attention is given to the effects of the recent financial and European sovereign crises. We find that average credit risk premia rose substantially and that post-crisis levels are still higher than those observed before the financial crisis. This observation is especially true for high-quality debt and if it persists, it will have an impact on corporates funding costs. The quantification and revision of risk premia contributes to the discussion of the credit spread puzzle and could give extra insights in valuation models that start from real-world estimates. Our work is furthermore important in the context of state aid assessment. The real economic value (REV) methodology, applied by the European Commission to evaluate impaired portfolios, is based on a long-term average risk premium. © 2017 Elsevier B.V., All rights reserved.","Heynderickx, W.; Cariboni, J.; Schoutens, W.; Smits, B.",2016,10.1080/00036846.2016.1150953,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966698707&doi=10.1080%2F00036846.2016.1150953&partnerID=40&md5=b10de6b3c90e0f224539f72a8ec3ddf7,scopus,"This study empirically examines the relationship between risk-neutral and actual default probabilities for European corporates, using rating agency transition matrices and CDS quotes. It quantifies the credit risk premium and finds it increased significantly after the financial and sovereign crises, particularly for high-quality debt, impacting corporate funding costs. The findings contribute to the credit spread puzzle and valuation models, and are relevant for state aid assessment.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:42:46.501358
75536a9b5e21dd96,The volatility structure of the fixed income market under the HJM framework: A nonlinear filtering approach,"The dynamics for interest rate processes within the well-known multi-factor Heath, Jarrow and Morton (HJM) specification are considered. Despite the flexibility of and the notable advances in theoretical research about the HJM model, the number of empirical studies of it is still very sparse. This paucity is principally due to the difficulties in estimating models in this class, which are not only high-dimensional, but also nonlinear and involve latent state variables. The estimation of a fairly broad class of HJM models as a nonlinear filtering problem is undertaken by adopting the local linearization filter, which is known to have some desirable statistical and numerical features, so enabling the estimation of the model via the maximum likelihood method. The estimator is then applied to the US, the UK and the Australian markets. Different two- and three-factor models are found to be the best for each market, with the factors being the level, the slope and the twist effect. The contribution of each factor towards overall variability of the interest rates and the financial reward each factor claims are found to differ considerably from one market to another. (C) 2008 Elsevier B.V. All rights reserved.","Chiarella, Carl; Hung, Hing; To, Thuy-Duong",2009,10.1016/j.csda.2008.07.036,,wos,"This paper estimates a broad class of Heath, Jarrow, and Morton (HJM) interest rate models using a nonlinear filtering approach, specifically the local linearization filter. The method is applied to US, UK, and Australian markets, identifying two- and three-factor models (level, slope, twist) as optimal. The study finds significant cross-market differences in factor contributions to interest rate variability and risk premia.",True,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:43:02.330684
2eb3060f2d35d3d1,"Tightly Coupled, Graph-Based DVL/IMU Fusion and Decoupled Mapping for SLAM-Centric Maritime Infrastructure Inspection","In this article, we address the problem of simultaneous localization and mapping (SLAM)-centric maritime infrastructure inspection [using unmanned surface vehicles (USVs)] via novel approaches in tightly-coupled, graph-based DVL/IMU fusion and decoupled mapping. As our first contribution, we formalize the preintegration of linear velocity measurements, obtained by a Doppler velocity log (DVL), in combination with angular velocity measurements, obtained by an inertial measurement unit (IMU), as binary factors encoding relative position. To evaluate state estimation improvements imparted by DVL/IMU fusion, we implement our proposed factor within a state-of-the-art, graph-based lidar-visual-inertial (LVI) SLAM system as our second contribution. Accuracy and robustness improvements are demonstrated in simulation by comparing maximum a posteriori pose estimates with and without DVL/IMU fusion against ground truth poses. As our third contribution, we propose a map generation framework for downstream inspection applications decoupled from SLAM. In our framework, volumetric data (captured by sonar, lidar, etc.) is transformed into a common world coordinate frame using extrinsic calibrations and SLAM pose estimates as input. Our framework operates over the complete set of raw volumetric data, whereas SLAM systems (both online and offline) typically operate over a subset of down-sampled volumetric data. To address the processing of additional volumetric data, we present innovations in refined pose correction and staged filtering for user-controlled denoising. We experimentally evaluate our map generation framework against the LVI SLAM system adopted for this study using real-world data and demonstrate improvements to map quality metrics important to inspection.",A. Thoms; G. Earle; N. Charron; S. Narasimhan,2023,10.1109/joe.2023.3265742,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10149804,ieeexplore,"This paper presents a novel approach for SLAM-centric maritime infrastructure inspection using unmanned surface vehicles (USVs). It introduces tightly-coupled, graph-based DVL/IMU fusion for improved state estimation and a decoupled mapping framework for downstream inspection applications. The system leverages preintegrated DVL and IMU measurements as binary factors and transforms volumetric data into a common world coordinate frame using extrinsic calibrations and SLAM pose estimates. Innovations in refined pose correction and staged filtering are also presented to enhance map quality and denoising for inspection purposes.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:43:20.311328
1ad66b307696646b,Time-varying nonlinear regression models: Nonparametric estimation and model selection,"This paper considers a general class of nonparametric time series regression models where the regression function can be time-dependent. We establish an asymptotic theory for estimates of the time-varying regression functions. For this general class of models, an important issue in practice is to address the necessity of modeling the regression function as nonlinear and time-varying. To tackle this, we propose an information criterion and prove its selection consistency property. The results are applied to the U.S. Treasury interest rate data. © 2021 Elsevier B.V., All rights reserved.","Zhang, T.; Wu, W.B.",2015,10.1214/14-aos1299,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924955365&doi=10.1214%2F14-AOS1299&partnerID=40&md5=53a8865b43b98d298f604f1e25a5a74a,scopus,This paper proposes a nonparametric estimation method and an information criterion for selecting nonlinear and time-varying regression models in time series data. The method is applied to U.S. Treasury interest rate data.,True,True,False,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T12:43:46.395981
edae0558b51d4ae1,Transaction costs and nonlinear adjustment towards equilibrium in the US treasury bill market,"This paper uses nonlinear error correction models to study yield movements in the US Treasury Bill Market. Nonlinear error correction arises because portfolio adjustment is an 'on-off' process, which occurs only when disequilibrium in the bill market is large enough to induce investors to incur the transaction costs associated with buying/selling bills. This, together with heterogeneity of transaction costs, implies that the strength of aggregate error correction depends on both the distribution of costs and the extent of disequilibrium in the market. Smooth transition models are used to describe an aggregate adjustment process which is strong when the market is distant from equilibrium, but becomes weaker as the market approaches equilibrium. Linearity tests indicate that the types of nonlinearities that would be induced by transactions costs are statistically significant, and estimated models which incororate these nonlinearities outperform their linear counterparts, both in sample and out of sample. © Blackwell Publishers 1997. © 2018 Elsevier B.V., All rights reserved.","Anderson, H.M.",1997,10.1111/1468-0084.00078,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031523523&doi=10.1111%2F1468-0084.00078&partnerID=40&md5=97619a45ee86a6fafd0456ff928a5b07,scopus,"This paper investigates yield movements in the US Treasury Bill Market using nonlinear error correction models. It posits that nonlinear adjustment occurs due to transaction costs, where portfolio adjustments are triggered only when market disequilibrium is significant enough to overcome these costs. The study employs smooth transition models to capture this aggregate adjustment process, which is stronger at greater distances from equilibrium. Empirical tests confirm the statistical significance of these nonlinearities and show that models incorporating them outperform linear models.",True,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:44:14.718259
95ecc7b2a3f725c5,Two Derivative Algorithms of Gradient Boosting Decision Tree for Silicon Content in Blast Furnace System Prediction,"The background of the present study complies with silicon content prediction in hot metal in the blast furnace system. The blast furnace system is a highly complex industrial reactor in the conventional process. The system is subject to several problems (e.g., system automation, the thermal state of the blast furnace, and the life prediction of blast furnace) that should be addressed by professionals. To determine the prediction state of the heat in the blast furnace, the silicon content in the blast furnace molten iron commonly acts as a key indicator. Based on the assumption that the blast furnace system exhibits a stable state, the accuracy of hot metal silicon is analyzed by using a range of machine learning algorithms. In the present study, two derivative algorithms of gradient boosting decision tree are adopted to develop a strong boosting predictor based on the extreme gradient boosting (XGBoost) algorithm and the light gradient boosting machine (LightGBM) algorithm for prediction. Compared with the conventional algorithms (e.g., lasso, random forest, support vector machine and gradient boosting decision tree), the prediction by using the two boosting algorithms is capable of more effectively guiding and determining the state of the blast furnace. As revealed from experimentally simulated results, the mentioned two boosting algorithms exhibit better comprehensive prediction performance than the conventional algorithms on the datasets of two practical blast furnace systems, demonstrating that the R-square of the two blast furnaces in the training set is over 0.7. The mentioned two algorithms are of certain guiding significance for exploring blast furnace problems.",S. Luo; T. Chen,2020,10.1109/access.2020.3034566,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9243945,ieeexplore,"This study proposes two gradient boosting decision tree algorithms, XGBoost and LightGBM, for predicting silicon content in blast furnace hot metal. These algorithms were compared to conventional machine learning models, showing superior prediction performance with R-square values over 0.7 on practical blast furnace datasets.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:44:27.580922
41a74f0a8de13787,U.S. vertically integrated electric utility greenhouse gas emissions and carbon risk premiums around the Paris Accord,"We study the pricing of greenhouse gas emissions of vertically integrated producers of electricity around the Paris Accord (PA). We study whether emissions are priced by financial markets, providing a market-based incentive for firms to reduce their carbon footprints and if the heightened attention on climate change post-Paris Accord (PA) impacts the size of the “carbon risk premium.” We focus on electricity generators, because they are responsible for the largest share of emissions and emissions reductions in the U.S. and are highly exposed to regulatory, physical, and stranded asset risks. We find the cost of carbon risk is reflected in the returns of vertically integrated electric utilities. The post-PA period provides the strongest evidence that carbon risk is priced. We find that equity markets provide incentives for power producers to reduce emissions, as reductions in emissions are associated with reductions in required returns on equity (increases in equity market values). The challenge for regulators is how to respond in rate cases. Lowering a utility's regulated return to reflect lower market estimates of the return on equity would dilute the market-based incentive for emissions reductions. Adding a longer-term return incentive for continued investment in emissions reductions would reinforce the market incentive.",,2024,10.1016/j.enpol.2024.114346,,proquest,"This study investigates whether financial markets price greenhouse gas emissions of vertically integrated electric utilities in the U.S. around the Paris Accord. The findings suggest that equity markets do reflect the cost of carbon risk, providing incentives for power producers to reduce emissions, with stronger evidence of this pricing in the post-Paris Accord period. The study also discusses the implications for regulators regarding rate cases and maintaining market-based incentives for emissions reductions.",False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:44:33.224482
23e3c100156433a3,US-Swiss term structures and exchange rate dynamics,"In this study, a multi-country nonlinear model is constructed to simultaneously estimate the exchange rate dynamics and the term structure of interest rates in the US and in Switzerland. The model has better empirical performance compared to the earlier well-known affine international models. Risk premiums of bond yields vary between the two countries. The estimated state variables exhibit local characteristics. These conclusions imply the potential advantages of international diversification and demonstrate the Home Bias puzzle. Exchange rate dynamics estimated by the models account for the Forward Premium Anomaly. © 2007 Elsevier Inc. All rights reserved. © 2007 Elsevier B.V., All rights reserved.","Inci, A.C.",2007,10.1016/j.gfj.2006.08.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35448929801&doi=10.1016%2Fj.gfj.2006.08.003&partnerID=40&md5=723f81983d7870a8b99c8ff115b76d95,scopus,"This study develops a multi-country nonlinear model to estimate exchange rate dynamics and term structures of interest rates in the US and Switzerland. The model outperforms existing affine international models, showing varying risk premiums and local characteristics of state variables. It also addresses the Forward Premium Anomaly in exchange rate dynamics and implies benefits of international diversification and the Home Bias puzzle.",True,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:44:41.491372
134ec5431d15e9ae,Uncertainty-Aware Portfolio Management With Risk-Sensitive Multiagent Network,"As deep neural networks (DNNs) have gained considerable attention in recent years, there have been several cases applying DNNs to portfolio management (PM). Although some researchers have experimentally demonstrated its ability to make a profit, it is still insufficient to use in real situations because existing studies have failed to answer how risky investment decisions are. Furthermore, even though the objective of PM is to maximize returns within a risk tolerance, they overlook the predictive uncertainty of DNNs in the process of risk management. To overcome these limitations, we propose a novel framework called risk-sensitive multiagent network (RSMAN), which includes risk-sensitive agents (RSAs) and a risk adaptive portfolio generator (RAPG). Standard DNNs do not understand the risks of their decision, whereas RSA can take risk-sensitive decisions by estimating market uncertainty and parameter uncertainty. Acting as a trader, this agent is trained via reinforcement learning from dynamic trading simulations to estimate the distribution of reward and via unsupervised learning to assess parameter uncertainty without labeled data. We also present an RAPG that can generate a portfolio fitting the user’s risk appetite without retraining by exploiting the estimated information from the RSAs. We tested our framework on the U.S. and Korean real financial markets to demonstrate the practicality of the RSMAN.",K. Park; H. -G. Jung; T. -S. Eom; S. -W. Lee,2024,10.1109/tnnls.2022.3174642,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9779871,ieeexplore,"This paper introduces a novel framework, Risk-Sensitive Multiagent Network (RSMAN), for portfolio management that addresses the limitations of existing deep neural network (DNN) applications by incorporating uncertainty awareness. The framework comprises risk-sensitive agents (RSAs) that estimate market and parameter uncertainty, and a risk adaptive portfolio generator (RAPG) that creates portfolios tailored to user risk appetite without retraining. The RSMAN was tested on U.S. and Korean financial markets.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:44:51.794612
a88f01f8d0739b30,Understanding index option returns,"Previous research concludes that options are mispriced based on the high average returns, CAPM alphas, and Sharpe ratios of various put selling strategies. One criticism of these conclusions is that these benchmarks are ill suited to handle the extreme statistical nature of option returns generated by nonlinear payoffs. We propose an alternative way to evaluate the statistical significance of option returns by comparing historical statistics to those generated by option pricing models. The most puzzling finding in the existing literature, the large returns to writing out-of-the-money puts, is not inconsistent (i.e., is statistically insignificant) relative to the Black-Scholes model or the Heston stochastic volatility model due to the extreme sampling uncertainty associated with put returns. This sampling problem can largely be alleviated by analyzing market-neutral portfolios such as straddles or delta-hedged returns. The returns on these portfolios can be explained by jump risk premiums and estimation risk. © 2012 Elsevier B.V., All rights reserved.","Broadie, M.; Chernov, M.; Johannes, M.",2009,10.1093/rfs/hhp032,https://www.scopus.com/inward/record.uri?eid=2-s2.0-64149102384&doi=10.1093%2Frfs%2Fhhp032&partnerID=40&md5=0ede27dbc5c9649e2874de880e44de26,scopus,This paper proposes an alternative method to evaluate the statistical significance of option returns by comparing historical statistics to those generated by option pricing models. It finds that the high average returns to writing out-of-the-money puts are not inconsistent with the Black-Scholes or Heston models due to sampling uncertainty. Market-neutral portfolios like straddles or delta-hedged returns can alleviate this sampling problem and their returns can be explained by jump risk premiums and estimation risk.,False,False,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:45:01.890204
a89249e7f1f4fe1a,Using Particle Swarm Optimization Algorithm to Calibrate the Term Structure Model,"One of the advantages of stochastic differential equations (SDE) is that they can follow a variety of different trends so that they can establish complex dynamic systems in the economic and financial fields. Although some estimation methods have been proposed to identify the unknown parameters in virtue of the results in the SDE model to speed up the process, these solutions only focus on using explicit approach to solve SDEs, and therefore they are not reliable to deal with data source merged being large and varied. Thus, this study makes progress in creating a new implicit way to fill in the gaps of accurately calibrating the unknown parameters in the SDE model. Essentially, the primary goal of the article is to generate rigid SDE simulation. Meanwhile, the particle swarm optimization method serves a purpose to search and simultaneously obtain the optimal estimation of the model unknown parameters in the complicated experiment of parameter space in an effective way. Finally, in an interest rate term structure model, it is verified that the method effectively deals with parameter estimation in the SDE model.",,2021,10.1155/2021/8893940,,proquest,This study proposes a new implicit method using the Particle Swarm Optimization algorithm to calibrate unknown parameters in a stochastic differential equation (SDE) model for interest rate term structure. The method aims to provide rigid SDE simulation and effectively estimates parameters in complex parameter spaces.,True,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:45:27.780147
e81f47b356497a31,Valuing GM technologies using real options: the case of drought tolerant wheat in Australia,"In this article we seek to estimate the value of a partially-developed crop technology from the perspective of the firm developing the technology. Firms need this value estimation to decide whether their technology will earn a sufficient return in the market to justify investing in it. However, determining the (ex-ante) value of the technology before it is commercialised is challenging as the technology is not yet in the market and hence the demand function has not yet been defined. An alternative valuation method is required. We use risk premiums, Monte Carlo simulation and real options analysis and we demonstrate this combination of valuation tools on wheat that is currently being developed in Australia to be drought tolerant. The results indicate that this drought tolerant wheat variety is likely to be adopted by farmers in most regions and has a pre-commercialisation value that justifies continued investment in its development. We also identified South Australia as a region in which the new variety would not be sufficiently valuable to farmers to see them adopt it and we consider possible explanations for this outcome.",,2018,10.1080/09537325.2018.1474194,,proquest,"This article estimates the value of a drought-tolerant wheat technology in Australia using real options analysis, Monte Carlo simulation, and risk premiums. The study suggests the technology is likely to be adopted by farmers and justifies continued investment, although adoption is unlikely in South Australia.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:45:32.907803
8ac774f2ab9a9ab4,Volatility Expectations and Returns,"We provide evidence that agents have slow-moving beliefs about stock market volatility that lead to initial underreaction to volatility shocks followed by delayed overreaction. These dynamics are mirrored in the VIX and variance risk premiums, which reflect investor expectations about volatility, and are also supported in both surveys and firm-level option prices. We embed these expectations into an asset pricing model and find that the model can account for a number of stylized facts about market returns and return volatility that are difficult to reconcile, including a weak or even negative risk-return trade-off.","Lochstoer, Lars A.; Muir, Tyler",2022,10.1111/jofi.13120,,wos,"This study investigates how slow-moving beliefs about stock market volatility influence investor reactions to volatility shocks, leading to initial underreaction and subsequent overreaction. These patterns are observed in market indicators like the VIX and variance risk premiums, and are further supported by survey data and option prices. The authors incorporate these findings into an asset pricing model, which helps explain several established market return and volatility characteristics, including a weak or negative risk-return trade-off.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:45:36.329754
339ab8be530e9be4,Volatility in equity markets and monetary policy rate uncertainty,"Asset pricing models assume the risk-free rate to be a key factor for equity prices. Hence, there should be a strong link between monetary policy rate uncertainty and equity return volatility, both in theory and data. This paper uses regression-based projections for realized variance to examine the relationship between short horizon forecasts of equity variance and proxies for monetary policy rate uncertainty. By assessing various projection models for UK, US and euro area equity indices, we show that the proxies for monetary policy rate uncertainty have a significant and positive predictive power for the equity return variance. Adding monetary policy rate uncertainty variables can significantly improve forecasting models for equity variance and volatility at weekly, monthly and even quarterly horizons. The findings imply that market views of short-term interest rate developments may indeed be embedded in equity prices and their variations. (C) 2017 The Bank of England. Published by Elsevier B.V. All rights reserved.","Kaminska, Iryna; Roberts-Sklar, Matt",2018,10.1016/j.jempfin.2017.09.008,,wos,"This paper investigates the relationship between monetary policy rate uncertainty and equity return volatility using regression-based projections for realized variance. The study finds that proxies for monetary policy rate uncertainty have significant predictive power for equity return variance across UK, US, and euro area equity indices, improving forecasting models at various horizons. The results suggest that market expectations of short-term interest rate movements are reflected in equity prices and their fluctuations.",False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:45:47.969649
139d782a705519c0,Weight of the Default Component of CDS Spreads: Avoiding Procyclicality in Credit Loss Provisioning Framework,"The current expected loss calculations have recently attracted considerable attention in the research on credit risk modeling, impairment provisioning, and financial networks’ stability. A new CDS-based approach to estimate current expected credit loss is proposed for low default portfolios, containing credit exposures to corporate issuers covered by publicly traded CDS contracts. First, a fraction of CDS spread related to a pure default compensation for different CDS maturities is assessed. Our results contrast with previous research. Second, based on the obtained historical weights of the default risk premium, a forward-looking term structure of the probabilities of default implied by the current CDS quotes is derived. The proposed approach covers both investment and noninvestment grade debt. The resulting framework is applied to a sample of corporate bonds. The developed methodology provides a useful tool, on one hand, for credit risk managers and balance-sheet preparers and, on the other hand, for regulators of financial markets as it sheds light on how procyclicality could be avoided in provisions.",,2019,10.1155/2019/7820618,,proquest,This paper proposes a new Credit Default Swap (CDS)-based approach to estimate current expected credit loss for low-default portfolios. It assesses the fraction of CDS spread related to pure default compensation and derives a forward-looking term structure of probabilities of default. The methodology aims to avoid procyclicality in provisioning and is applicable to both investment and non-investment grade debt.,False,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:45:50.999161
ec360be29d8293a3,What Drives Short Rate Dynamics? A Functional Gradient Descent Approach,"Functional gradient descent (FGD), a recent technique coming from computational statistics, is applied to the estimation of the conditional moments of the short rate process with the goal of finding the main drivers of the drift and volatility dynamics. FGD can improve the accuracy of some reasonable starting estimates obtained using classical short rate models introduced in the literature. It exploits the predictive information of an enlarged set of variables, including yields at other maturities, time, and macroeconomic indicators. Fitting this methodology to the time series of monthly US 3-month Treasury bill rates, we find that the drift dynamics react mostly in a nonlinear way to changes in macroeconomic variables, whereas volatility dynamics are subjected to time-dependent regime-switches. Finally we show the superior performance of the final predictions obtained by applying FGD in a forecasting exercise.","Audrino, Francesco",2012,10.1007/s10614-011-9310-y,,wos,"This paper applies Functional Gradient Descent (FGD), a statistical technique, to estimate the conditional moments of the short rate process, aiming to identify key drivers of drift and volatility. FGD enhances initial estimates from classical models by incorporating a wider range of variables, including other maturities' yields, time, and macroeconomic indicators. Applied to US 3-month Treasury bill rates, the study finds nonlinear responses of drift to macroeconomic changes and time-dependent regime switches in volatility. The method demonstrates superior forecasting performance.",True,True,True,gemini-2.5-flash-lite,Trine,Y,,2025-10-24T12:46:25.966675
d534573971c63b10,"What the current yield curve says, and what the future prices of energy do","Y Policymakers have always looked at the difference between the yields on long- and short-term Treasury securities as an indication of where the economy is heading. In this study, we extend the literature by examining the yield curve's ability to predict the short-term prices of crude oil and other energy products. Using linear and non-linear (parametric quantile) causality tests on daily data from 1986 to February 2020, our findings confirm that changes in the yield spread not only correlate with, but also drive the returns on crude oil, heating oil and natural gas in the short run. This short-run relationship is relatively absent from 1986 to 2003. However, since 2004, the relationship has remained quite strong, confirming that these products have been financialized. Market participants and policy makers may find our findings useful in understanding the nature of relationship between the shape of the term structure and future innovations in energy prices.","Idilbi-Bayaa, Yasmeen; Qadan, Mahmoud",2022,10.1016/j.resourpol.2021.102494,,wos,"This study investigates the predictive power of the yield curve for short-term energy prices (crude oil, heating oil, natural gas). Using causality tests on daily data from 1986-2020, the research finds that yield spread changes drive energy returns, particularly since 2004, suggesting financialization of these products. The findings are relevant for market participants and policymakers.",False,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:46:34.571577
e4b77867f689035a,Would an earlier inception of OMT by the ECB have prevented the 2012 Greek default?,"To avert further debt crises following the Greek default of 2012, the European Central Bank (ECB) adopted outright purchases of sovereign bonds as part of its monetary policy regime. This paper examines whether an earlier inception of such purchases (OMT) could have prevented the observed Greek repudiation. To account for the extraordinary circumstances surrounding the Greek default, I construct a novel model of sovereign finance in which default is political and investors’ reliance on external credit ratings gives rise to slow moving crises. Estimating the model with Greek data, I find that an earlier inception of OMT plausibly could have prevented the observed default, but the resulting counterfactual Greek state would have been so fragile that, absent any further fiscal consolidation, eventual default was effectively inevitable. Moreover, the present Greek state remains sufficiently fragile that a quick return to a predominantly private financing scheme is not advisable. © 2025 Elsevier B.V., All rights reserved.","Mäder, N.",2025,10.1016/j.iref.2025.104356,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012113857&doi=10.1016%2Fj.iref.2025.104356&partnerID=40&md5=a0410bf8f90a8a60f3cb411b27d80bd5,scopus,"This paper investigates whether the European Central Bank's Outright Monetary Transactions (OMT) could have prevented the 2012 Greek default. Using a novel political sovereign finance model estimated with Greek data, the study suggests that earlier OMT might have averted the default but would have left Greece in a fragile state, making eventual default likely without fiscal consolidation. The current Greek financial situation is also deemed fragile, advising against a rapid return to private financing.",False,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:46:47.697994
4c58711a707af646,Yield Curve Estimation Based on Government Security Prices in the Croatian Financial Market,"This article investigates the estimation of the yield curve based on government security prices using the Nelson-Siegel model in the Croatian financial market. The yield curve was estimated for samples of government securities with and without currency clauses. Since the Croatian financial market is less developed characterized by limited trading activity in government bonds, Treasury bills were also included in the analysis. To examine the difference in the estimation of yield curve parameters between a less developed and a developed market, the U.S. sample was considered. The yield curve was estimated for the full US sample and for artificially created U.S. samples corresponding to the Croatian samples of government bonds with and without currency clauses. Despite the less developed Croatian financial market, it is possible to estimate the yield curve and derive meaningful economic interpretations from the estimates.","Orlovic, Zrinka; Zoricic, Davor; Golubic, Zrinka Lovretin",2024,10.2478/zireb-2024-0016,,wos,"This study estimates the yield curve in the Croatian financial market using the Nelson-Siegel model with government securities, including those with and without currency clauses, and Treasury bills due to limited trading activity. A comparison with U.S. market data was made to understand differences between developed and less developed markets. The findings suggest that yield curve estimation and economic interpretation are feasible even in less developed markets.",False,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:47:09.569726
4feca7e995018566,Yield Curve Point Triplets in Recession Forecasting,"Several studies have highlighted the yield curve's ability to forecast economic activity. These studies use the information provided by the slope of the yield curve-i.e., pairs of short- and long-term interest rates. In this paper, we construct three models for forecasting the positive and negative deviations of real US GDP from its long-run trend over the period from 1976Q3 to 2011Q4: one that uses only pairs of interest rates and two that draw on more than two points from the yield curve. We employ two alternative forecasting methodologies: the probit model, which is commonly used in this line of literature, and the support vector machines (SVM) approach from the area of machine learning. Our results show that we can achieve a 100% out-of-sample forecasting accuracy for negative output gaps (recessions) with both methodologies and an overall accuracy (both inflationary and unemployment gaps) of 80% in the case of the best SVM model. The forecasting performance of our model strengthens the existing evidence that the yield curve can be a useful tool for gauging future economic activity. © 2021 Elsevier B.V., All rights reserved.","Gogas, P.; Papadimitriou, T.; Chrysanthidou, E.",2015,10.1111/infi.12067,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940045257&doi=10.1111%2Finfi.12067&partnerID=40&md5=9fac416464a2ce0a8f7fa26d4a7513a9,scopus,"This paper constructs three models to forecast US GDP deviations from its long-run trend, utilizing yield curve information (pairs and triplets of interest rates). It compares the commonly used probit model with the machine learning approach of Support Vector Machines (SVM). The study achieves 100% out-of-sample accuracy in forecasting recessions and 80% overall accuracy for gaps using the best SVM model, reinforcing the yield curve's utility in economic forecasting.",True,True,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:47:31.452024
416e57a3c9d256bd,Yield Spread and Economic Policy Uncertainty: Evidence from Japan,"In this paper, we adopt the nonlinear autoregressive distributed lags (NARDL) model extended by Shin et al. (2014) to investigate the relationship between the treasury yield spread and economic policy uncertainty (EPU) in Japan. This model helps us to explore the short- and long-run asymmetric reactions of explained variables through positive and negative partial sum decompositions of changes in the explanatory variable(s). In our research, the testing of the NARDL specification reveals the existence of a significant long-run asymmetric equilibrium between the yield spread and EPU in Japan. On the other hand, we find a significant positive nexus between the treasury yield spread and EPU reduction in the long run. We speculate that because of low inflation, a poor economic outlook and the low interest rate environment since 1990, financial agents are markedly sensitive to negative shocks resulting from EPU. This means that when facing a good economy, bond agents are quick to sell, especially with higher-risk long-term interest rate bonds. Meanwhile, because the Bank of Japan announced the Stock Purchasing Plan in October 2002 and from the point view of portfolio management, while the influence of a positive economic outlook dominates the negative outlook, flight from quality has no role in asset portfolio adjustment. The empirical implications are that the long history of unconventional monetary policy supports the demand for both bonds and stock markets. When taking the stock market into consideration, the correlations between the yield spread, EPU and stock market capture the full wealth effects of the low interest rate environment in Japan.","Wang, Mei-Chih; Kuo, Pao-Lan; Chen, Chan-Sheng; Chiu, Chien-Liang; Chang, Tsangyao",2020,10.3390/su12104302,,wos,"This study uses the NARDL model to analyze the relationship between the treasury yield spread and economic policy uncertainty (EPU) in Japan, finding a significant long-run asymmetric equilibrium and a positive nexus between the yield spread and EPU reduction. The authors suggest financial agents are sensitive to negative EPU shocks due to Japan's economic conditions and low interest rates, influencing bond market behavior. They also consider the impact of unconventional monetary policy and the stock market on these relationships.",True,True,False,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:47:51.447638
4f70c91263ca1522,“Intelligent” finance and treasury management: what we can expect,"Artificial intelligence poses a particular challenge in its application to finance/treasury management because most treasury functions are no longer physical processes, but rather virtual processes that are increasingly highly automated. Most finance/treasury teams are knowledge workers who make decisions and conduct analytics within often dynamic frameworks that must incorporate environmental considerations (foreign exchange rates, GDP forecasts), internal considerations (growth needs, business trends), as well as the impact of any actions on related corporate decisions which are also highly complex (e.g., hedging, investing, capital structure, liquidity levels). Artificial intelligence in finance and treasury is thus most analogous to the complexity of a human nervous system as it encompasses far more than the automation of tasks. Similar to the human nervous system, AI systems in finance/treasury must manage data quickly and accurately, including the capture and classification of data and its integration into larger datasets. At present, the AI network neural system has been gradually improved and is widely used in many fields of treasury management, such as early warning of potential financial crisis, diagnosis of financial risk, control of financial information data quality and mining of hidden financial data, information, etc.",,2020,10.1007/s00146-019-00919-6,,proquest,"This article discusses the application of Artificial Intelligence (AI) in finance and treasury management, highlighting its complexity due to the virtual and automated nature of modern treasury functions. It likens AI in this domain to the human nervous system, emphasizing the need for rapid and accurate data management, including capture, classification, and integration. The abstract notes the current use of AI in treasury management for tasks like financial crisis early warning, risk diagnosis, data quality control, and data mining.",True,False,True,gemini-2.5-flash-lite,Trine,N,,2025-10-24T12:48:13.284811
