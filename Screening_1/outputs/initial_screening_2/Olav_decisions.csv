paper_id,title,abstract,authors,year,doi,link,database,llm_summary,i1_hint,i2_hint,i3_hint,model,reviewer,decision,notes,timestamp
16b768df753f1bf5,A Chaos Analysis of the Dry Bulk Shipping Market,"Finding low-dimensional chaos is a relevant issue as it could allow short-term reliable forecasting. However, the existence of chaos in shipping freight rates remains an open and outstanding matter as previous research used methodology that can produce misleading results. Using daily data, this paper aims to unveil the nonlinear dynamics of the Baltic Dry Index that has been proposed as a measure of the shipping rates for certain raw materials. We tested for the existence of nonlinearity and low-dimensional chaos. We have also examined the chaotic dynamics throughout three sub-sampling periods, which have been determined by the volatility pattern of the series. For this purpose, from a comprehensive view we apply several metric and topological techniques, including the most suitable methods for noisy time series analysis. The proposed methodology considers the characteristics of chaotic time series, such as nonlinearity, determinism, sensitivity to initial conditions, fractal dimension and recurrence. Although there is strong evidence of a nonlinear structure, a chaotic and, therefore, deterministic behavior cannot be assumed during the whole or the three periods considered. Our findings indicate that the generalized autoregressive conditional heteroscedastic (GARCH) model and exponential GARCH (EGARCH) model explain a significant part of the nonlinear structure that is found in the dry bulk shipping freight market.","Inglada-Perez, Lucia; Coto-Millan, Pablo",2021,10.3390/math9172065,,wos,"This paper investigates the presence of low-dimensional chaos in the dry bulk shipping market using the Baltic Dry Index. While evidence suggests a nonlinear structure, chaotic and deterministic behavior could not be confirmed. The study found that GARCH and EGARCH models explain a significant portion of the nonlinearity.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:41:45.144590
32873e86106b0a11,A Class of Non-Gaussian State Space Models With Exact Likelihood Inference,"The likelihood function of a general nonlinear, non-Gaussian state space model is a high-dimensional integral with no closed-form solution. In this article, I show how to calculate the likelihood function exactly for a large class of non-Gaussian state space models that include stochastic intensity, stochastic volatility, and stochastic duration models among others. The state variables in this class follow a nonnegative stochastic process that is popular in econometrics for modeling volatility and intensities. In addition to calculating the likelihood, I also show how to perform filtering and smoothing to estimate the latent variables in the model. The procedures in this article can be used for either Bayesian or frequentist estimation of the model's unknown parameters as well as the latent state variables. Supplementary materials for this article are available online.","Creal, Drew D.",2017,10.1080/07350015.2015.1092977,,wos,"This article presents a method for exact likelihood inference in a class of non-Gaussian state space models, applicable to models with stochastic intensity, volatility, or duration. It also details procedures for filtering and smoothing to estimate latent variables, suitable for both Bayesian and frequentist estimation.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:41:50.968156
0df5735bfdc43af0,A Comparative Analysis of the Choice of Mother Wavelet Functions Affecting the Accuracy of Forecasts of Daily Balances in the Treasury Single Account,"Improving the accuracy of cash flow forecasting in the TSA is key to fulfilling government payment obligations, minimizing the cost of maintaining the cash reserve, providing the absence of outstanding debt accumulation and ensuring investment in financial instruments to obtain additional income. This study aims to improve the accuracy of traditional methods of forecasting the time series compiled from the daily remaining balances in the TSAbased on prior decomposition using a discrete wavelet transform. The paper compares the influence of selecting a mother wavelet out of 570 mother wavelet functions belonging to 10 wavelet families (Haar;Dabeshies; Symlet; Coiflet; Biorthogonal Spline; Reverse Biorthogonal Spline; Meyer; Shannon; Battle-Lemarie; and Cohen–Daubechies–Feauveau) and the decomposition level (from 1 to 8) on the forecast accuracy of time series compiled from the daily remaining balances in the TSA in comparison with the traditional forecasting method without prior timeseries decomposition. The model with prior time series decomposition based on the Reverse Biorthogonal Spline Wavelet [5.5] mother wavelet function, upon the eighth iteration, features the highest accuracy, significantly higher than that of the traditional forecasting models. The choice of the mother wavelet and the decomposition level play an important role in increasing the accuracy of forecasting the daily remaining balances in the TSA.",,2022,10.3390/economies10090213,,proquest,"This study investigates the impact of different mother wavelet functions and decomposition levels on the accuracy of forecasting daily Treasury Single Account (TSA) balances. By comparing wavelet transform-based decomposition with traditional forecasting methods, the research identifies the Reverse Biorthogonal Spline Wavelet [5.5] as yielding the highest accuracy after eight iterations, significantly outperforming traditional models. The findings highlight the importance of wavelet selection and decomposition level for improving TSA balance forecast accuracy.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:42:02.071653
ff08816516693672,"A Comparison of LSTM, GRU, and XGBoost for forecasting Morocco’s yield curve","The field of time series forecasting has grown significantly over the past several years and is now highly active. In numerous application domains, deep neural networks are exact and powerful. They are among the most popular machine learning techniques for resolving big data issues because of these factors. Historically, there have been numerous methods for accurately predicting the subsequent change in time series data. The time series forecasting problem and its mathematical underpinnings are first articulated in this study. Following that, a description of the most popular deep learning architectures used to date with success in time series forecasting is provided, emphasizing both their benefits and drawbacks. Feedforward networks, recurrent neural networks (such as Elman networks), long-and short-term memory (LSTM), and gated recurrent units (GRU) are given special consideration. Furthermore, the advantages of the XGBoost boosting tree method have shown its superiority in numerous data mining competitions in recent years. The high coefficients of the metric measures indicate that the proposed XGBoost model provides good predictive performance, according to the results. © 2024 Elsevier B.V., All rights reserved.","Jeaab, K.; Saoudi, Y.; Falloul, M.E.M.",2024,10.23939/mmc2024.03.674,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205025967&doi=10.23939%2Fmmc2024.03.674&partnerID=40&md5=a547ef809381a9bbc475c0545010746d,scopus,"This study compares LSTM, GRU, and XGBoost for forecasting Morocco's yield curve. It reviews deep learning architectures for time series forecasting and highlights XGBoost's strong predictive performance.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-28T09:42:51.143875
a2291dc0a2cab336,A Comprehensive Approach to Residual Value Analysis in the Luxury Automotive Market,"Global automotive markets have introduced new complexities, from the surge in powertrain diversity to evolving consumer purchasing habits. In the luxury car segment, residual value (RV), the car’s actual value at the end of ownership, is particularly significant. A high RV translates into lower overall ownership costs, as the car retains more of its value over time, which can boost demand as well as leasing margin. For this reason, the analysis of RV offers key insights for strategic decision-making. The present study leverages a large-scale global dataset spanning a 10-year period, capturing both internal vehicle features and three available external market conditions (CPI, unemployment rate, and 10-year bond yield). Our approach employs machine learning techniques, particularly CatBoost, achieving a mean absolute percentage error of around 5%, deemed highly acceptable within the industry. Moreover, a novel method to enhance the reliability and interpretability of RV estimations is proposed by quantifying depreciation thresholds and mitigating distortions related to sample composition via a “Standard Vehicle” concept. The approach has been validated by Ferrari S.p.A., the provider of the data, serving as a robust tool for automotive industry stakeholders.",A. Ghibellini; A. Scioletti; M. Coletto; L. Bononi; M. Gabbrielli,2025,10.1109/access.2025.3591765,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11091377,ieeexplore,"This study analyzes residual values in the luxury automotive market using a large global dataset and machine learning (CatBoost). It incorporates vehicle features and market conditions (CPI, unemployment, bond yield) to predict residual values with high accuracy (MAPE ~5%). A novel method using depreciation thresholds and a ""Standard Vehicle"" concept enhances reliability and interpretability. The approach was validated by Ferrari S.p.A.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:42:56.343773
71de55dd11e1d89d,A Hybrid Methodology Using Machine Learning Techniques and Feature Engineering Applied to Time Series for Medium- and Long-Term Energy Market Price Forecasting,"In the electricity market, the issue of contract negotiation prices between generators/traders and buyers is of particular relevance, as an accurate contract modeling leads to increased financial returns and business sustainability for the various participating agents, encouraging investments in specialized sectors for price forecasting and risk analysis. This paper presents a methodology applied in experiments on energy forward curve scenarios using a set of techniques, including Long Short-Term Memory (LSTM), Extreme Gradient Boosting (XGBoost), Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors (SARIMAX), and Feature Engineering to generate a 10-year projection of the Conventional Long-Term Price. The model validation proved to be effective, with errors of only 4.5% by Root Mean Square Error (RMSE) and slightly less than 2% by Mean Absolute Error (MAE), for a time series spanning from 7 January 2012 to 31 August 2024, in the Brazilian energy market.",,2025,10.3390/en18061387,,proquest,"This paper proposes a hybrid methodology combining Long Short-Term Memory (LSTM), Extreme Gradient Boosting (XGBoost), SARIMAX, and Feature Engineering for medium- and long-term energy market price forecasting. The model achieved low error rates (4.5% RMSE, <2% MAE) in predicting the Brazilian energy market's conventional long-term price over a 10-year projection.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:43:04.687895
7af7d2c3acf27b2d,A Markov-modulated tree-based gradient boosting model for auto-insurance risk premium pricing,"In most sub Saharan African countries,the mechanism for pricing auto-insurance policies is tariff based. This means that the key factor that influences price changes is usually based on regulation and legislative dynamics. Additionally, where ratemaking is risk based, analysis has in most cases focused on internal historical data or claims history, particularly in the sub Saharran Africa. These policy regimes have led to unfair price distortions among policyholders and have increased risk of portfolios for most insurance companies. In this study we consider geographical location risk that influence auto-insurance claim process for an insurance company. The study develops a Markov-modulated tree-based gradient boosting (MMGB) model for pricing auto-insurance premiums. The Markov-modulated tree-based gradient boosting model is a Tweedie general linear model (GLM) based pricing algorithm with a compound Poisson-Gamma distribution whose rate varies according to accident risk in a Markovian process. Thus, the study extends the existing premium pricing framework by integrating a geographical location risk factor into the main pricing framework. The study applies the model to a motor insurance data set from Ghana. The results show that the proposed method is superior to other competing models because it generates relatively fair premium predictions for the non-life auto-insurance companies, helping to mitigate more the insured risk for the firm and the industry. © 2020 Elsevier B.V., All rights reserved.","Arku, D.; Doku-Amponsah, K.; Howard, N.K.",2020,10.3233/rda-180050,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085768703&doi=10.3233%2FRDA-180050&partnerID=40&md5=43274c399c2abe9303985a4ed9506140,scopus,"This study proposes a Markov-modulated tree-based gradient boosting (MMGB) model for auto-insurance premium pricing, incorporating geographical location risk. The model, a Tweedie GLM with a compound Poisson-Gamma distribution, uses a Markovian process for accident risk and is applied to motor insurance data from Ghana. Results indicate the MMGB model provides fairer premium predictions and better risk mitigation compared to other models.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:43:11.143751
94f2194dac2bf593,A Multi-Objective Portfolio Selection Model With Fuzzy Value-at-Risk Ratio,"Considering nonstatistical uncertainties and/or insufficient historical data in security return forecasts, fuzzy set theory has been applied in the past decades to build portfolio selection models. Meanwhile, various risk measurements such as variance, entropy, and Value-at-Risk have been proposed in fuzzy environments to evaluate investment risks from different perspectives. Sharpe ratio, also known as the reward-to-variability ratio, which measures the risk premium per unit of the nonsystematic risk (asset deviation), has received great attention in modern portfolio theory. In this study, the Sharpe ratio in fuzzy environments is introduced, whereafter, a fuzzy Value-at-Risk ratio is proposed. Compared with Sharpe ratio, Value-at-Risk ratio is an index with dimensional knowledge that reflects the risk premium per unit of the systematic risk (the greatest loss under a given confidence level). On the basis of the two ratios, a multi-objective model is built to evaluate their joint impact on portfolio selection. Then, the proposed model is solved by a fuzzy simulation based multi-objective particle swarm optimization algorithm, where the global best of each iteration is determined by an improved dominance times based method. Finally, the algorithm superiority is justified via comparing with existing solvers on benchmark problems, and the model effectiveness is exemplified by using three case studies on portfolio selection.",B. Wang; Y. Li; S. Wang; J. Watada,2018,10.1109/tfuzz.2018.2842752,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8370687,ieeexplore,"This paper proposes a multi-objective portfolio selection model incorporating fuzzy Value-at-Risk ratio and Sharpe ratio. It addresses uncertainties in security return forecasts using fuzzy set theory and introduces a novel fuzzy Value-at-Risk ratio to measure systematic risk. The model is solved using a fuzzy simulation-based multi-objective particle swarm optimization algorithm, and its effectiveness is demonstrated through case studies.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:44:07.255925
d4e4962a4745e30c,A Risk-Free Discount Rate Prediction Model for Mineral Project Evaluation Using a Hybrid Discrete Wavelet Transform and Artificial Neural Network,"The discount rate input parameter of Net Present Value (NPV) in mineral project evaluation is a function of a risk-free rate and risk premium component. To obtain a reliable NPV, it is important to estimate each of these components. This study employs a hybrid approach to predict risk-free rate using Discrete Wavelet Transform and Artificial Neural Network (DWT-ANN). The DWT-ANN model was tested using London Interbank Offered Rate (LIBOR) dataset from 1986 to 2020. The results showed that Discrete Wavelet Transform-Radial Basis Function Neural Network (DWT-RBFNN) of the three different hybrid algorithms developed and applied performed best in predicting the risk-free rate. This is because it achieved the lowest root mean square error of 0.0376 and the highest correlation coefficient of 0.9995. The DWT-RBFNN model can be a useful alternative tool for predicting risk-free rate, which is a key input parameter for the determination of discount rate. © 2022 Elsevier B.V., All rights reserved.","Gyebuni, R.; Ziggah, Y.Y.; Mireku-Gyimah, D.",2022,10.1155/2022/9984679,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139564882&doi=10.1155%2F2022%2F9984679&partnerID=40&md5=1dc2d0577c07a01ce69bdf0d8deb8b22,scopus,"This study proposes a hybrid Discrete Wavelet Transform and Artificial Neural Network (DWT-ANN) model to predict the risk-free rate, a crucial component for determining the discount rate in mineral project evaluation. The DWT-RBFNN model demonstrated superior performance in predicting the risk-free rate using LIBOR data from 1986-2020, achieving a low RMSE of 0.0376 and a high correlation coefficient of 0.9995.",True,False,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-28T09:44:27.391714
339df4defc6a0a72,A Semi-Static Replication Method for Bermudan Swaptions under an Affine Multi-Factor Model,"We present a semi-static replication algorithm for Bermudan swaptions under an affine, multi-factor term structure model. In contrast to dynamic replication, which needs to be continuously updated as the market moves, a semi-static replication needs to be rebalanced on just a finite number of instances. We show that the exotic derivative can be decomposed into a portfolio of vanilla discount bond options, which mirrors its value as the market moves and can be priced in closed form. This paves the way toward the efficient numerical simulation of xVA, market, and credit risk metrics for which forward valuation is the key ingredient. The static portfolio composition is obtained by regressing the target option’s value using an interpretable, artificial neural network. Leveraging the universal approximation power of neural networks, we prove that the replication error can be arbitrarily small for a sufficiently large portfolio. A direct, a lower bound, and an upper bound estimator for the Bermudan swaption price are inferred from the replication algorithm. Additionally, closed-form error margins to the price statistics are determined. We practically study the accuracy and convergence of the method through several numerical experiments. The results indicate that the semi-static replication approaches the LSM benchmark with basis point accuracy and provides tight, efficient error bounds. For in-model simulations, the semi-static replication outperforms a traditional dynamic hedge. © 2023 Elsevier B.V., All rights reserved.","Hoencamp, J.H.; Jain, S.; Kandhai, D.",2023,10.3390/risks11100168,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175481739&doi=10.3390%2Frisks11100168&partnerID=40&md5=8b7a20248b55d866fc0a12d7eb46c0ab,scopus,"This paper introduces a semi-static replication method for Bermudan swaptions using an affine, multi-factor term structure model. Unlike dynamic replication, this method requires rebalancing only at discrete intervals. The approach decomposes the swaption into a portfolio of vanilla discount bond options, allowing for closed-form pricing. It utilizes an artificial neural network for static portfolio composition and demonstrates that replication error can be minimized with a sufficiently large portfolio. The method provides estimators for swaption prices and error margins, showing accuracy comparable to the LSM benchmark and outperforming traditional dynamic hedging in simulations.",True,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:44:33.447696
209637058300c847,A Unified Framework for Fast Large-Scale Portfolio Optimization,"We introduce a unified framework for rapid, large-scale portfolio optimization that incorporates both shrinkage and regularization techniques. This framework addresses multiple objectives, including minimum variance, mean-variance, and the maximum Sharpe ratio, and also adapts to various portfolio weight constraints. For each optimization scenario, we detail the translation into the corresponding quadratic programming (QP) problem and then integrate these solutions into a new open-source Python library. Using 50 years of return data from US mid to large-sized companies, and 33 distinct firm-specific characteristics, we utilize our framework to assess the out-of-sample monthly rebalanced portfolio performance of widely-adopted covariance matrix estimators and factor models, examining both daily and monthly returns. These estimators include the sample covariance matrix, linear and nonlinear shrinkage estimators, and factor portfolios based on Asset Pricing (AP) Trees, Principal Component Analysis (PCA), Risk Premium PCA (RP-PCA), and Instrumented PCA (IPCA). Our findings emphasize that AP-Trees and PCA-based factor models consistently outperform all other approaches in out-of-sample portfolio performance. Finally, we develop new (Formula presented.) and (Formula presented.) regularizations of factor portfolio norms which not only elevate the portfolio performance of AP-Trees and PCA-based factor models but they have a potential to reduce an excessive turnover and transaction costs often associated with these models. © 2025 Elsevier B.V., All rights reserved.","Deng, W.; Polak, P.; Safikhani, A.; Shah, R.",2024,10.1080/26941899.2023.2295539,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003413482&doi=10.1080%2F26941899.2023.2295539&partnerID=40&md5=09ebcd357e426caf6aa3db4d044e82bc,scopus,"This paper presents a unified framework for fast, large-scale portfolio optimization, integrating shrinkage and regularization techniques to handle multiple objectives (minimum variance, mean-variance, maximum Sharpe ratio) and various constraints. The framework is translated into quadratic programming problems and implemented in an open-source Python library. Empirical analysis using 50 years of US company data and 33 firm characteristics compares the out-of-sample performance of various covariance matrix estimators and factor models, finding AP-Trees and PCA-based models to be superior. New regularizations are proposed to improve performance and reduce turnover.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:44:39.111340
8467d8f718f5b82c,A class of Gaussian hybrid processes for modeling financial markets,"This paper proposes a one-factor model of financial markets using a class of Gaussian process that can be decomposed into a Brownian motion and an Ornstein-Uhlenbeck process. It is shown that this ""hybrid"" process is obtained as a continuous-time scaling limit of the differenced first-order autoregressive integrated moving average (ARIMA(1,1,1)) process. Parameter estimations using an ARIMA(1,1,1) framework and its variance ratio test show the accuracy of the proposed model. Construction of the one-factor commodity futures price model is presented as an application. A multidimensional extension of the hybrid process is also presented in the Appendix. © 2008 Springer Science+Business Media, LLC. © 2008 Elsevier B.V., All rights reserved.","Itoh, Y.",2007,10.1007/s10690-007-9058-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-41149159008&doi=10.1007%2Fs10690-007-9058-5&partnerID=40&md5=d41636f4abc62db5bad174cc42210d9d,scopus,"This paper introduces a novel one-factor financial market model based on a Gaussian process that combines Brownian motion and an Ornstein-Uhlenbeck process. The model is derived as a limit of ARIMA(1,1,1) processes and its parameters can be estimated using ARIMA frameworks and variance ratio tests. An application to commodity futures pricing is demonstrated, and a multidimensional extension is also provided.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:44:48.383705
6260d09b870d4639,A comparison of tests of nonlinear cointegration with application to the predictability of US interest rates using the term structure,"We test whether there are nonlinearities in the response of short- and long-term interest rates to the spread in interest rates, and assess the out-of-sample predictability of interest rates using linear and nonlinear models. We find strong evidence of nonlinearities in the response of interest rates to the spread. Nonlinearities are shown to result in more accurate short-horizon forecasts, especially of the spread. © 2004 International Institute of Forecasters. Published by Elsevier B.V. All rights reserved. © 2008 Elsevier B.V., All rights reserved.","Clements, M.P.; Galvão, A.B.",2004,10.1016/j.ijforecast.2003.09.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1842659918&doi=10.1016%2Fj.ijforecast.2003.09.001&partnerID=40&md5=0815b049531bbb3f1e71069935d68952,scopus,"This study investigates nonlinearities in the relationship between short- and long-term interest rates and their impact on interest rate predictability. The authors find significant nonlinearities that improve short-horizon forecasts, particularly for the interest rate spread. They compare linear and nonlinear models for out-of-sample prediction.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-28T09:45:43.487766
1d5944dc1f483123,"A constrained least square method for estimating a smooth, nonnegative forward rate sequence","We will develop an efficient method for estimating a smooth nonnegative forward rate sequence using the market price of riskless bonds. This method is an improvement of the classical Carleton-Cooper's method based on standard least square method, which often generates a non-smooth forward rate sequence and hence is not used in practice. The method to be proposed in this paper is intended to resolve this difficulty. We will impose a smoothness condition while maintaining the fitting error within an acceptable level. The resulting optimization problem is shown to be convex in the region of interest. Therefore, we can calculate a globally optimal solution very fast by standard nonlinear programming algorithms. We will demonstrate that this method generates a smooth forward rate sequence at the expense of a very small increase of fitting error. © World Scientific Publishing Company. © 2008 Elsevier B.V., All rights reserved.","Konno, H.; Ito, S.",2005,10.1142/s0219024905003293,https://www.scopus.com/inward/record.uri?eid=2-s2.0-27544457345&doi=10.1142%2FS0219024905003293&partnerID=40&md5=d4b267dbfd7198ed1b95a3f81c3e5184,scopus,"This paper proposes an improved method for estimating smooth, nonnegative forward rate sequences using market prices of riskless bonds. It addresses limitations of the classical Carleton-Cooper method by incorporating a smoothness condition into a constrained least squares approach, resulting in a convex optimization problem solvable with nonlinear programming algorithms. The method demonstrates the ability to produce smooth forward rate sequences with only a minor increase in fitting error.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:45:48.495768
3ab17db382610c1a,A dynamic target volatility strategy for asset allocation using artificial neural networks,"A challenge to developing data-driven approaches in finance and trading is the limited availability of data because periods of instability, such as during financial market crises, are relatively rare. This study applies a stability-oriented approach (SOA) based on statistical tests to compare data for the current period to a past set of data for a stable period, providing higher reliability due to a more abundant source of data. Based on an SOA, this study uses an artificial neural network (ANN), which is one of the commonly applied machine learning algorithms, for simultaneously forecasting the volatility and classifying the level of market stability. In addition, this study develops a dynamic target volatility strategy for asset allocation using an ANN to enhance the ability of a target volatility strategy that is established for automatically allocating capital between a risky asset and a risk-free cash position. In order to examine the impact of the proposed strategy, the results are compared to the buy-and-hold strategy, the static asset allocation strategy, and the conventional target volatility strategy using different volatility forecasting methodologies. An empirical case study of the proposed strategy is simulated in both the Korean and U.S. stock markets.",,2018,10.1080/0013791x.2018.1461287,,proquest,"This study proposes a dynamic target volatility strategy for asset allocation using artificial neural networks (ANNs) and a stability-oriented approach (SOA). The ANN forecasts volatility and classifies market stability, enhancing a target volatility strategy for capital allocation between risky assets and cash. The strategy's performance is compared to buy-and-hold, static allocation, and conventional target volatility strategies, with simulations in Korean and U.S. stock markets.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:45:53.895578
876edb131cd8d459,A long-short dual-mode knowledge distillation framework for empirical asset pricing models in digital financial networks,"The continuous combination of digital network technology and traditional financial services has given birth to digital financial networks, which explore massive economic data under the AI-driven models to achieve intelligent connections among financial institutions, markets, transactions, and instruments. Empirical asset pricing is a challenging task in financial analysis, which has attracted research attention. However, existing studies only focus on tackling the challenges of equity risk premium in the single stock market. Considering multiple economic linkages between the two countries, the transaction history of the US stock market as empirical knowledge is a powerful supplement to improve the prediction of equity risk premium in the China market. In this paper, we aim to fully leverage the prior information in two stock markets for empirical asset pricing models. Due to the rich financial domain knowledge, there may be various characteristic signals that partially overlap in different periods. To address these issues, we propose a framework based on long-short dual-mode knowledge distillation, termed as LSDM-KD, which incorporates US and China stock market models, and a shared characteristic signals model. The method effectively understands the relationships between assets and market behaviour, reducing reliance on expensive correlation databases and professional knowledge. Extensive experiments conducted on US and China stock market datasets demonstrate that our LSDM-KD can significantly improve the performance of empirical asset pricing.",,2024,10.1080/09540091.2024.2306970,,proquest,"This paper proposes a long-short dual-mode knowledge distillation framework (LSDM-KD) to improve empirical asset pricing models by leveraging information from both the US and China stock markets. The framework aims to capture overlapping characteristic signals and understand asset-market relationships, reducing the need for external data and expertise. Experiments show LSDM-KD significantly enhances asset pricing performance.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:45:59.527334
34e04b398fa30214,A new game-theoretical multi-objective evolutionary approach for cash-in-transit vehicle routing problem with time windows (A Real life Case),"Cash transfer from a central treasury to bank branches, which is with high security, is one of the crucial processes in the banking system. In this paper, a new multi-objective game theory-based model is developed to increase the security of cash-in-transit. For this purpose and in order to reduce the transportation costs, a bi-objective vehicle routing problem with time window is developed where the risk of transfers (including armed robbers attack and theft) and the distance traveled by vehicles are minimized. In order to better estimate the robber's performance, the probability of robber's ambush is calculated by the game theory approach, in such a way that a two-player, zero-sum game is played between the robber and the cash carrier. The probability of theft success is also estimated in the proposed approach through a multiple-criteria decision-making and in order to be further representative of real-life situations. A periodic review is also added to the proposed model to increase the cash transport security in which the previously used links would enjoy less chance of choosing in the current period. Moreover, a new multi-objective hybrid genetic algorithm incorporated with a number of new heuristics and operators is developed to tackle the proposed model. The efficiency and effectiveness of the algorithm are examined through several standard data sets, and the results indicate the effectiveness of the proposed solution algorithm. The wide applicability of our proposed approach in real-life situations is examined with a real case study as well. © 2020 Elsevier B.V., All rights reserved.","Ghannadpour, S.F.; Zandiyeh, F.",2020,10.1016/j.asoc.2020.106378,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084485975&doi=10.1016%2Fj.asoc.2020.106378&partnerID=40&md5=b4c39d98a905071477c2592dc16d0b26,scopus,"This paper introduces a novel game theory-based multi-objective evolutionary approach for the cash-in-transit vehicle routing problem with time windows. The model aims to minimize both the risk of attacks and transportation costs by incorporating game theory to estimate robber behavior and a multi-criteria decision-making approach for theft success probability. A hybrid genetic algorithm is used to solve the model, and its effectiveness is demonstrated through standard datasets and a real-life case study.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:46:04.559764
cb693ae22d0c21b2,A new optimisation approach to assess optimal asset allocation in European non-life insurance companies,"This study addresses the allocation of optimal assets in non-life insurance companies' environment with a new vision. Most previous studies are based on the maximisation of the utility function. However, in this paper, we focused on the maximisation of technical efficiency (TE). In order to validate our objective, we select a set of European non-life insurance companies (ENIC) over the period 2008-2014. In the first step, we estimate the production function characterised by the directional output distance function (DODF). In the second one, we use two metaheuristics (PSO and GA) to assess the optimal asset allocation (OAA). The empirical results show that the proportion allocated to the 'alternative investment with high-risk high-return' (AIhh) is on average lower than those found in previous studies. However, the percentage allocated to the 'risk-free assets' (RFA) is on average different from zero. This can be explained by the attention given to the competitiveness, survival and long-term profitability respecting the maximisation of TE. So, any insurance company must give more attention to the presence of different stakeholders and resolve the conflicts of interest between them.",,2021,10.1504/ijads.2021.116004,,proquest,"This study proposes a new approach to optimal asset allocation for European non-life insurance companies, focusing on maximizing technical efficiency rather than utility. It uses the directional output distance function and metaheuristics (PSO and GA) to assess asset allocation, finding a lower proportion of high-risk/high-return assets and a non-zero proportion of risk-free assets compared to previous studies. The authors emphasize the importance of considering stakeholder interests for competitiveness, survival, and long-term profitability.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:46:07.479728
2708d8ad18a0ab27,A non-Gaussian Ornstein-Uhlenbeck model for pricing wind power futures,"The recent introduction of wind power futures written on the German wind power production index has brought with it new interesting challenges in terms of modelling and pricing. Some particularities of this product are the strong seasonal component embedded in the underlying, the fact that the wind index is bounded from both above and below and also that the futures are settled against a synthetically generated spot index. Here, we consider the non-Gaussian Ornstein-Uhlenbeck type processes proposed by Barndorff-Nielsen and Shephard in the context of modelling the wind power production index. We discuss the properties of the model and estimation of the model parameters. Further, the model allows for an analytical formula for pricing wind power futures. We provide an empirical study, where the model is calibrated to 37 years of German wind power production index that is synthetically generated assuming a constant level of installed capacity. Also, based on 1 year of observed prices for wind power futures with different delivery periods, we study the market price of risk. Generally, we find a negative risk premium whose magnitude decreases as the length of the delivery period increases. To further demonstrate the benefits of our proposed model, we address the pricing of European options written on wind power futures, which can be achieved through Fourier techniques.",,2018,10.1080/1350486x.2018.1438904,,proquest,"This paper proposes a non-Gaussian Ornstein-Uhlenbeck model for pricing wind power futures, addressing the seasonality, boundedness, and synthetic settlement of the underlying index. The model's properties and parameter estimation are discussed, leading to an analytical pricing formula. An empirical study calibrates the model to German wind power production data and analyzes the market price of risk, finding a generally negative risk premium that decreases with delivery period length. The paper also demonstrates pricing European options on wind power futures using Fourier techniques.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:46:11.023764
c82f6df79c2745a6,A non-knotty inflation risk premium model,"In this article, I estimate the inflation risk premium (IRP) using a low-dimensional arbitrage-free dynamic model through a novel strategy. Instead of modelling the nominal and real yields jointly, I make assumptions about the short-term inflation rate. More specifically, I assume it follows a Gaussian process. This framework has a closed-form expression for IRP. Since inflation yields are not observed, to estimate the model parameters I approximate them by the break-even inflation rate. This approximation works well because the convexity correction is very small. I find that the estimated IRP is strongly correlated with those obtained using surveys or more complex models. Therefore, I provide an easier procedure to obtain IRP, avoiding the cumbersome estimation process of high-order models.",,2023,10.1080/00036846.2022.2111023,,proquest,"This article proposes a low-dimensional, arbitrage-free dynamic model to estimate the inflation risk premium (IRP). The model assumes a Gaussian process for the short-term inflation rate, leading to a closed-form expression for IRP. Inflation yields are approximated by break-even inflation rates for parameter estimation, a method validated by the small convexity correction. The estimated IRP shows strong correlation with survey-based and complex model results, offering a simpler alternative for IRP estimation.",False,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:46:17.519738
836eb9e2c451e5c5,A nonparametric estimator for the covariance function of functional data,"Many quantities of interest in economics and finance can be represented as partially observed functional data. Examples include structural business cycle estimation, implied volatility smile, the yield curve. Having embedded these quantities into continuous random curves, estimation of the covariance function is needed to extract factors, perform dimensionality reduction, and conduct inference on the factor scores. A series expansion for the covariance function is considered. Under summability restrictions on the absolute values of the coefficients in the series expansion, an estimation procedure that is resilient to overfitting is proposed. Under certain conditions, the rate of consistency for the resulting estimator achieves the minimax rate, allowing the observations to be weakly dependent. When the domain of the functional data is K(>1) dimensional, the absolute summability restriction of the coefficients avoids the so called curse of dimensionality. As an application, a Box-Pierce statistic to test independence of partially observed functional data is derived. Simulation results and an empirical investigation of the efficiency of the Eurodollar futures contracts on the Chicago Mercantile Exchange are included. © 2015 Elsevier B.V., All rights reserved.","Sancetta, A.",2014,10.1017/s0266466614000784,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949744301&doi=10.1017%2FS0266466614000784&partnerID=40&md5=235cb272bbf1152c694cf9b9858eb102,scopus,"This paper proposes a nonparametric estimator for the covariance function of functional data, applicable to economic and financial quantities like business cycles and implied volatility. The method uses a series expansion and is designed to prevent overfitting, achieving minimax consistency rates under certain conditions, even with weakly dependent data. It addresses the curse of dimensionality for multi-dimensional functional data and includes a Box-Pierce statistic for testing independence. The study is supported by simulations and an empirical analysis of Eurodollar futures contracts.",True,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:46:21.367683
904ecc8c3e03282b,A novel approach to Predict WTI crude spot oil price: LSTM-based feature extraction with Xgboost Regressor,"This paper presents a novel model based on LSTM to predict future prices of WTI crude oil. The WTI price forecasting utilizes data on spot gold price, US 10-year bond yield, global economic activity, and US dollar index from January 1986 to May 2023. The model's performance is assessed using measures such as MAE, MSE, RMSE, MAPE, and R2 metrics. The results generated by the proposed new model are compared to those of the existing machine and deep learning methods, and it is observed that the new model performs better than the existing models in all statistical tests. The study further examined the decision-making processes of the model using SHAP analysis and assessed the individual contribution of each feature to the model's predictions. The correlation between the US Dollar Index and Gold prices and WTI crude oil prices is evident. The SHAP research has demonstrated that the model effectively captures complicated economic linkages and enhances the accuracy of forecasts. The results of this study enhance the development of models that are capable of predicting results, even in times of significant instability, such as economic crises. Using sophisticated data analytics and AI methods would improve the efficiency of energy market oversight. © 2024 Elsevier B.V., All rights reserved.","Simsek, A.I.; Bulut, E.; Gur, Y.E.; Gültekin Tarla, E.",2024,10.1016/j.energy.2024.133102,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203530978&doi=10.1016%2Fj.energy.2024.133102&partnerID=40&md5=d9cf0148dabf51a718ea46ae4789d646,scopus,"This paper proposes a novel LSTM-based model for predicting WTI crude oil spot prices, incorporating features like gold price, US 10-year bond yield, global economic activity, and the US dollar index. The model outperforms existing methods and uses SHAP analysis to explain its predictions, highlighting the correlation between economic indicators and oil prices. The study aims to improve forecasting accuracy, especially during market instability.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:46:29.047707
d63fadf05abeb819,A novel decomposition integration model for power coal price forecasting,"Accurate prediction of steam coal prices is important for stabilizing the coal trading market and formulating coal use strategies scientifically. In this paper, a new decomposition integration model (VADM) is proposed to predict coal prices by combining the variational modal decomposition (VMD), arithmetic optimization algorithm (AOA), deep temporal convolutional network (DeepTCN), and mean impact value algorithm (MIV). Firstly, the AOA optimization algorithm is used to improve the VMD, AOA-VMD was obtained. It is used to decompose the steam coal price series. Then, the decomposed subsequences are predicted for the prediction of steam coal prices by using DeepTCN. Finally, the MIV algorithm is applied to analyze the impact of different factors on the price of steam coal. It is found that: the steam coal price sub-series decomposed by AOA-VMD are smoother and more linear compared with the original series; the errors in forecasting steam coal prices are significantly reduced after considering newly proposed factors, interest rates, such as the overnight Shanghai interbank offered rate and the six-month treasury bond yield; the MAPE, MASE and SMAPE of the VADM model all show different degrees of decline compared with benchmark models. The forecasting effect of VADM model is better than the benchmark model in terms of stability and accuracy, and can be used for short-term forecasting of coal prices. © 2023 Elsevier B.V., All rights reserved.","Wu, S.; Xia, G.; Liu, L.",2023,10.1016/j.resourpol.2022.103259,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145655492&doi=10.1016%2Fj.resourpol.2022.103259&partnerID=40&md5=227fae8fd06a4f24cab6e80ac1324e5c,scopus,"This paper proposes a novel decomposition integration model (VADM) for power coal price forecasting, combining variational modal decomposition (VMD), arithmetic optimization algorithm (AOA), deep temporal convolutional network (DeepTCN), and mean impact value algorithm (MIV). The model decomposes coal price series using AOA-VMD, predicts subsequences with DeepTCN, and analyzes factor impacts with MIV. The VADM model demonstrates improved accuracy and stability in short-term coal price forecasting compared to benchmark models, incorporating factors like interest rates.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:46:32.479547
99ce25282168bdae,A novel term structure stochastic model with adaptive correlation for trend analysis,"The prediction of underlying price continues to draw extensive attention in academic research. Based on a review of the advantages and disadvantages of different volatility models, we find that the Heston stochastic volatility model with an adaptive correlation coefficient is most suitable for analysing the Hong Kong options market. We subsequently propose a model-free implied volatility term structure formulated using options with different strikes and different maturities. The implied volatility is calculated by integrating the option price and strike price from the current time to the expiry date. Discrete points of term structure data are used to fit a term structure curve. Finally, we use the model-free implied volatility term structure as the long-run mean level of the Heston model to fully exploit the information content contained in the implied volatility term structure. We simulate the distribution of the underlying asset price based on the Heston model and constant elasticity of variance (CEV) model. The adaptive correlation Heston model provides superior results in terms of one-day-ahead prediction performance and the 79-day distribution of the underlying asset price compared with the CEV model. © 2021 Elsevier B.V., All rights reserved.","Du, J.; Lai, S.; Lai, K.K.; Zhou, S.",2021,10.1002/ijfe.2076,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089457308&doi=10.1002%2Fijfe.2076&partnerID=40&md5=0bd330a48e1a5066903ad5d67db64590,scopus,"This paper proposes a novel term structure stochastic model incorporating adaptive correlation, building upon the Heston model and a model-free implied volatility term structure. The model is applied to analyze the Hong Kong options market and demonstrates superior prediction performance compared to the Constant Elasticity of Variance (CEV) model.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:46:48.295928
5e7b8c965b7c7487,A penalized two-pass regression to predict stock returns with time-varying risk premia,"We develop a penalized two-pass regression with time-varying factor loadings. The penalization in the first pass enforces sparsity for the time-variation drivers while also maintaining compatibility with the no-arbitrage restrictions by regularizing appropriate groups of coefficients. The second pass delivers risk premia estimates to predict equity excess returns. Our Monte Carlo results and our empirical results on a large cross-sectional data set of US individual stocks show that penalization without grouping can yield to nearly all estimated time-varying models violating the no-arbitrage restrictions. Moreover, our results demonstrate that the proposed method reduces the prediction errors compared to a penalized approach without appropriate grouping or a time-invariant factor model. © 2023 Elsevier B.V., All rights reserved.","Bakalli, G.; Guerrier, S.; Scaillet, O.",2023,10.1016/j.jeconom.2022.12.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146471323&doi=10.1016%2Fj.jeconom.2022.12.004&partnerID=40&md5=3da444594d98ac1c1f1f5ec4e044ee7b,scopus,This paper proposes a penalized two-pass regression method with time-varying factor loadings to predict stock returns. The method uses penalization to enforce sparsity in time-variation drivers and maintain no-arbitrage restrictions. Empirical results show that this approach reduces prediction errors compared to other methods.,True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:46:55.344105
5c030899a8ca9d9a,A prediction model for the secure issuance scale of Chinese local government bonds,"PurposeThe purpose of this paper is to calculate the local guaranteed fiscal revenue with the local fiscal revenue of 31 provinces, and predict their guaranteed fiscal revenue in 2018 with the artificial neural network (ANN).Design/methodology/approachThe principal components analysis (PCA), particle swarm optimization (PSO) and extreme learning machine (ELM) model was designed to produce the inputs of KMV model. Then the KMV model was used for obtaining the default probabilities under different issuance scales. Data were collected from Wind Database. MATLAB 2018b and SPSS 22 were used in the field of modeling and results analysis.FindingsThis study’s findings show that PCA–PSO–ELM proposed in this research has the highest accuracy in terms of the prediction compared with ELM, back propagation neural network and auto regression. And PCA–PSO–ELM–KMV model can calculate the secure issuance scale of local government bonds effectively.Practical implicationsThe sustainability forecast in this study can help local governments effectively control the scale of debt issuance, strengthen the budget management of local debt and establish the corresponding risk warning mechanism, which could make local governments maintain good credit ratings.Originality/valueThis study sheds new light on helping local governments avoid financial risks effectively, and it is conducive to establish a debt repayment reserve system for local governments and the proper arrangement for stock debt.",,2021,10.1108/k-10-2019-0699,,proquest,"This study develops a prediction model using PCA-PSO-ELM-KMV to determine the secure issuance scale of Chinese local government bonds. The model forecasts guaranteed fiscal revenue and default probabilities, offering insights for debt management and risk mitigation.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:47:12.119418
9cd458c37bf8d94f,A quasi-maximum likelihood method for estimating the parameters of multivariate diffusions,"A quasi-maximum likelihood procedure for estimating the parameters of multi-dimensional diffusions is developed in which the transitional density is a multivariate Gaussian density with first and second moments approximating the true moments of the unknown density. For affine drift and diffusion functions, the moments are exactly those of the true transitional density and for nonlinear drift and diffusion functions the approximation is extremely good and is as effective as alternative methods based on likelihood approximations. The estimation procedure generalises to models with latent factors. A conditioning procedure is developed that allows parameter estimation in the absence of proxies. (C) 2012 Elsevier B.V. All rights reserved.","Hurn, A. S.; Lindsay, K. A.; McClelland, A. J.",2013,10.1016/j.jeconom.2012.09.002,,wos,"This paper proposes a quasi-maximum likelihood method for estimating parameters of multivariate diffusions, approximating the transitional density with a multivariate Gaussian. This method is exact for affine drift/diffusion functions and provides a good approximation for nonlinear cases, generalizing to models with latent factors and incorporating a conditioning procedure for parameter estimation without proxies.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:47:56.352618
232cb3a8af099a6f,A study of cross-industry return predictability in the Chinese stock market,"We investigate cross-industry return predictability for the Shanghai and Shenzhen stock exchanges, by constructing 6- and 26- industry portfolios. The dominance of retail investors in these markets, in conjunction with the gradual diffusion of information hypothesis provide the theoretical background that allows us to employ machine learning methods to test for cross-industry predictability. We find that Oil, Telecommunications and Finance industry portfolio returns are significant predictors of other industries. Our out-of-sample forecasting exercise shows that the OLS post-LASSO estimation outperforms a variety of benchmarks and a long–short trading strategy generates an average annual excess return of 13%. © 2022 Elsevier B.V., All rights reserved.","Ellington, M.; Stamatogiannis, M.P.; Zheng, Y.",2022,10.1016/j.irfa.2022.102249,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133314818&doi=10.1016%2Fj.irfa.2022.102249&partnerID=40&md5=f14048287a71b77d7730f86dffa447ab,scopus,"This study examines cross-industry return predictability in the Chinese stock market using machine learning on 6 and 26 industry portfolios from the Shanghai and Shenzhen stock exchanges. It finds that Oil, Telecommunications, and Finance industries predict returns in other sectors. An out-of-sample test shows OLS post-LASSO estimation is superior to benchmarks, and a long-short strategy yields a 13% annual excess return.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:47:59.912325
0c12365026f9ffcd,A tale of two coffees? Analysing interaction and futures market efficiency,"PurposeThe purpose of this paper is to assess the informational efficiency of Arabica (other milds) and Robusta coffee futures markets in terms of predicting future coffee spot prices.Design/methodology/approachFutures market efficiency is associated with the existence of a long-run equilibrium relationship between spot and future prices such that coffee futures prices are unbiased predictors of future spot prices. This study applies unit root testing to daily data for futures-spot price differentials. A range of maturities for futures contracts are considered, and the study also uses a recursive approach to consider time variation in futures market efficiency.FindingsThe other milds and Robusta futures prices tend to be unbiased predictors for their own respective spot prices. The paper further finds that other milds and Robusta futures prices are unbiased predictors of the respective Robusta and other milds spot prices. Recursive estimation suggests that the futures market efficiency associated with these cross cases has increased, though with no clear link to the implementation of the 2007 International Coffee Agreement.Originality/valueThe paper draws new insights into futures market efficiency by examining the two key types of coffee and analyses the potential interactions between them. Hitherto, no attention has been paid to futures contracts of the Robusta variety. The employment of unit root testing of spot futures coffee price differentials can be viewed as more stringent than an approach based on non-cointegration testing.",,2020,10.1108/sef-09-2019-0356,,proquest,"This paper assesses the informational efficiency of Arabica and Robusta coffee futures markets in predicting future coffee spot prices using unit root testing on futures-spot price differentials. It finds that both Arabica and Robusta futures prices are unbiased predictors of their respective spot prices, and also predict each other's spot prices. Recursive estimation indicates an increase in market efficiency over time, though not clearly linked to the 2007 International Coffee Agreement. The study offers new insights by examining both coffee types and their interactions, with a particular focus on Robusta futures.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:48:03.480448
09e7ecec6845d7fd,ADAPTIVE LASSO-TYPE ESTIMATION FOR MULTIVARIATE DIFFUSION PROCESSES,"The least absolute shrinkage and selection operator (LASSO) is a widely used statistical methodology for simultaneous estimation and variable selection. It is a shrinkage estimation method that allows one to select parsimonious models. In other words, this method estimates the redundant parameters as zero in the large samples and reduces variance of estimates. In recent years, many authors analyzed this technique from a theoretical and applied point of view. We introduce and study the adaptive LASSO problem for discretely observed multivariate diffusion processes. We prove oracle properties and also derive the asymptotic distribution of the LASSO estimator. This is a nontrivial extension of previous results by Wang and Leng (2007, Journal of the American Statistical Association, 102(479), 1039-1048) on LASSO estimation because of different rates of convergence of the estimators in the drift and diffusion coefficients. We perform simulations and real data analysis to provide some evidence on the applicability of this method.","De Gregorio, Alessandro; Iacus, Stefano M.",2012,10.1017/s0266466611000806,,wos,This paper introduces and analyzes an adaptive LASSO estimation method for discretely observed multivariate diffusion processes. It extends previous LASSO work by addressing different convergence rates for drift and diffusion coefficients and demonstrates the method's applicability through simulations and real data analysis.,True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:48:06.255924
efd37e8cd45dfd0b,ASYMMETRICAL EFFECTS OF REAL EFFECTIVE EXCHANGE RATE OF KUNA ON FOREIGN TRADE BALANCE IN REPUBLIC OF CROATIA,"This paper strives on giving an answer to whether assymetrical effects of real effective ex-change rate of kuna, J-curve and Marshall-Lerner condition exist in the Republic of Croatia. Price rigidity, capacity constraints, and adjustment costs are just some reasons why depreciation and appreciation of the real effective exchange rate do not necessarily have equally strong effect of the opposite direction on net exports. Econometric estimation by nonlinear autoregressive distributed lags method (NARDL) enables separation of positive (appreciation) from negative (depreciation) changes in the real effective exchange rate, showing that its changes do not affect net exports sym-metrically and linearly. Existence of J-curve and positive impact of the real effective exchange rate depreciation on trade balance (Marshall-Lerner condition) are confirmed, while appreciation has no significant effect. Results obtained in this paper point out that Croatia `s foreign trade bal-ance can be improved by real effective exchange rate depreciation. However, this should be taken with caution given that nominal depreciation leads to growing external indebtedness of domestic economy, affects expectations, especially those related to inflation, import prices (therefore domes-tic prices), risk premium, returns on financial assets, and there are indications that depreciation of nominal and real exchange rate in Croatia have a contractional effect on domestic economic activity, so costs of such a policy may far outweigh the benefits of currency weakening. In addition, research limitations include low variability of the real effective exchange rate (which might make it difficult to obtain econometric relevant estimates), the problem of choosing representative exchange rate measures, and the choice of proxy variables for domestic and foreign income. In the observed period, appreciation and depreciation of the real effective exchange rate were relatively mild, so the conclusions are not necessarily valid in the case of a sudden and large fluctuations in it. Compared to previous research, asymmetric effect of the real effective exchange rate is confirmed on the bal-ance of goods and services (not only balance of goods), and possible causes of nonlinearity in the response of foreign trade balance to changes in the real effective exchange rate are discussed.","Novinc, Filip",2023,10.32910/ep.74.4.4,,wos,"This paper investigates the asymmetrical effects of the real effective exchange rate (REER) of the Croatian Kuna on the foreign trade balance. Using the nonlinear autoregressive distributed lags (NARDL) method, the study confirms the existence of the J-curve and the Marshall-Lerner condition, indicating that depreciation of the REER positively impacts the trade balance, while appreciation has no significant effect. The research highlights potential benefits of REER depreciation for improving the trade balance but cautions about its negative consequences on external debt, inflation, and economic activity. Limitations include low REER variability and challenges in selecting appropriate exchange rate measures and proxy variables. The study extends previous research by confirming asymmetric effects on the balance of goods and services and discussing potential causes of nonlinearity.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:48:11.480484
302c792dbf247468,Algorithmic sign prediction and covariate selection across eleven international stock markets,"I investigate whether an expert system can be used for profitable long-term asset management. The trading strategy of the expert system needs to be based on market predictions. To this end, I generate binary predictions of the market returns by using statistical and machine-learning algorithms. The methods used include logistic regressions, regularized logistic regressions and similarity-based classification. I test the methods in a contemporary data set involving data from eleven developed markets. Both statistical and economic significance of the results are considered. As an ensemble, the results seem to indicate that there is some degree of mild predictability in the stock markets. Some of the results obtained are highly significant in the economic sense, featuring annualized excess returns of 3.1% (France), 2.9% (Netherlands) and 0.8% (United States). However, statistically significant results are seldom found. Consequently, the results do not completely invalidate the efficient-market hypothesis. © 2018 Elsevier B.V., All rights reserved.","Karhunen, M.",2019,10.1016/j.eswa.2018.07.061,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051398572&doi=10.1016%2Fj.eswa.2018.07.061&partnerID=40&md5=553aa1a74c3267f31622ea2e5a12741a,scopus,"This study explores the use of statistical and machine-learning algorithms (logistic regressions, regularized logistic regressions, similarity-based classification) to predict stock market returns across eleven international markets. The goal is to assess if an expert system can achieve profitable long-term asset management through these market predictions. While some economic significance was observed (e.g., annualized excess returns in France, Netherlands, and the US), statistically significant results were rare, leading to the conclusion that the efficient-market hypothesis is not entirely invalidated.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:48:14.976781
08de1ecc4154ae9b,An analysis of nonlinearities in term premiums and forward rates,"Previous studies often assume a linear relation between term premiums on Treasury securities and forward interest rates even though a nonlinear relation is a theoretical and an empirical possibility. To examine the relation, this paper uses a nonparametric kernel approach that permits both linear and nonlinear associations. The linear specification yields conditional expectations of term premiums that are similar to those predicted by the kernel approach only at the mean forward premiums. Generally, kernel estimation shows that the responses of expected term premiums to changes in forward premiums are time-varying and are significantly different from the constant slope coefficients produced by linear estimations. The evidence also shows that forward premiums contain much more information content for predicting future term premiums than has been found with linear estimation procedures. © 2005 Elsevier Science B.V., Amsterdam. All rights reserved.","Huang, R.D.; Lin, C.S.Y.",1996,10.1016/s0927-5398(96)00008-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030527175&doi=10.1016%2FS0927-5398%2896%2900008-4&partnerID=40&md5=2762a85a3dc870c284cd425c9e590525,scopus,"This paper uses a nonparametric kernel approach to analyze nonlinear relationships between term premiums on Treasury securities and forward interest rates. The findings indicate that nonlinearities are significant, time-varying, and that forward premiums contain more predictive information for future term premiums than previously found with linear models.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-28T09:48:28.896710
342960dee21cfd44,An application of comonotonicity theory in a stochastic life annuity framework,"A life annuity contract is an insurance instrument which pays pre-scheduled living benefits conditional on the survival of the annuitant. In order to manage the risk borne by annuity providers, one needs to take into account all sources of uncertainty that affect the value of future obligations under the contract. In this paper, we define the concept of annuity rate as the conditional expected present value random variable of future payments of the annuity, given the future dynamics of its risk factors. The annuity rate deals with the non-diversifiable systematic risk contained in the life annuity contract, and it involves mortality risk as well as investment risk. While it is plausible to assume that there is no correlation between the two risks, each affects the annuity rate through a combination of dependent random variables. In order to understand the probabilistic profile of the annuity rate, we apply comonotonicity theory to approximate its quantile function. We also derive accurate upper and lower bounds for prediction intervals for annuity rates. We use the Lee-Carter model for mortality risk and the Vasicek model for the term structure of interest rates with an annually renewable fixed-income investment policy. Different investment strategies can be handled using this framework.",,2011,10.1016/j.insmatheco.2010.11.008,,proquest,"This paper applies comonotonicity theory to a stochastic life annuity framework to manage risk. It defines the annuity rate as the conditional expected present value of future payments, considering mortality and investment risks. The Lee-Carter model for mortality and the Vasicek model for interest rates are used to approximate the quantile function of the annuity rate and derive bounds for prediction intervals.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:48:31.800422
516f97e519503054,An effective hybrid approach for forecasting currency exchange rates,"Accurately forecasting the movement of exchange rates is of interest in a variety of fields, such as international business, financial management, and monetary policy, though this is not an easy task due to dramatic fluctuations caused by political and economic events. In this study, we develop a new forecasting approach referred to as FSPSOSVR, which is able to accurately predict exchange rates by combining particle swarm optimization (PSO), random forest feature selection, and support vector regression (SVR). PSO is used to obtain the optimal SVR parameters for predicting exchange rates. Our analysis involves the monthly exchange rates from January 1971 to December 2017 of seven countries including Australia, Canada, China, the European Union, Japan, Taiwan, and the United Kingdom. The out-of-sample forecast performance of the FSPSOSVR algorithm is compared with six competing forecasting models using the mean absolute percentage error (MAPE) and root mean square error (RMSE), including random walk, exponential smoothing, autoregres-sive integrated moving average (ARIMA), seasonal ARIMA, SVR, and PSOSVR. Our empirical results show that the FSPSOSVR algorithm consistently yields excellent predictive accuracy, which compares favorably with competing models for all currencies. These findings suggest that the proposed algorithm is a promising method for the empirical forecasting of exchange rates. Finally, we show the empirical relevance of exchange rate forecasts arising from FSPSOSVR by use of foreign exchange carry trades and find that the proposed trading strategies can deliver positive excess returns of more than 3% per annum for most currencies, except for AUD and NTD. © 2021 Elsevier B.V., All rights reserved.","Shen, M.-L.; Lee, C.-F.; Liu, H.-H.; Chang, P.-Y.; Yang, C.-H.",2021,10.3390/su13052761,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102711580&doi=10.3390%2Fsu13052761&partnerID=40&md5=a7f81af1c0335702887643e103e0ea2c,scopus,"This study introduces FSPSOSVR, a hybrid forecasting approach combining particle swarm optimization (PSO), random forest feature selection, and support vector regression (SVR), to predict currency exchange rates. The model was tested on monthly exchange rates of seven countries from 1971 to 2017. FSPSOSVR demonstrated superior predictive accuracy compared to six other models (random walk, exponential smoothing, ARIMA, seasonal ARIMA, SVR, PSOSVR) based on MAPE and RMSE. The study also showed that FSPSOSVR-based trading strategies could generate positive excess returns, except for AUD and NTD.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:48:34.873118
9a0f00b75445a4f1,An end-to-end deep learning framework for the portfolio optimization with stop-loss orders,"Incorporating stop-loss orders into portfolio optimization is an effective strategy for mitigating tail risk. However, traditional methods often rely on static stop-loss rules paired with separate price prediction models, leading to amplified error propagation in the application of stop-loss orders. We propose a novel end-to-end (E2E) data-driven deep learning framework that simultaneously determines portfolio allocations and stop-loss orders for risky assets, with each asset triggering sales when its prices reach the stop-loss order. The E2E approach uses a two-branch neural network architecture that integrates historical transaction data with additional inputs such as predicted returns and price volatility. A recurrent neural network extracts feature representation vectors for each risky asset from its time series data, and two fully connected layers process the aggregated information to generate the optimal joint action of portfolio allocations and corresponding stop-loss orders. The effectiveness of the E2E solution is demonstrated by numerical experiments on four stock market datasets. The results show that the E2E strategy outperforms alternative benchmarks across multiple metrics, including cumulative returns, alpha returns, and risk-adjusted returns. The strategy exhibits superior ability to maintain robust returns and control tail risk, achieving an average excess return of 0.33% and an average Calmar ratio of 4.50 over four datasets. In addition, the introduction of stop-loss orders yields significantly positive excess returns, and the strategy maintains its advantage even when a transaction cost rate of 0.3% is included. © 2025 Elsevier B.V., All rights reserved.","Zhang, Y.; Liu, Y.; Liu, W.; Yang, X.",2025,10.1016/j.asoc.2025.113465,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009613641&doi=10.1016%2Fj.asoc.2025.113465&partnerID=40&md5=4a06f7897a7b12fe2e864719fa90d7f0,scopus,"This paper introduces an end-to-end deep learning framework for portfolio optimization that integrates stop-loss orders. The framework uses a two-branch neural network to simultaneously determine portfolio allocations and stop-loss levels, leveraging historical data, predicted returns, and volatility. Numerical experiments on stock market data show the framework outperforms benchmarks in terms of returns and risk management, even with transaction costs.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:49:59.361440
01f7b4f943c3d0b7,An estimation model for the term structure of yield spread,"An estimation model for term structure of yield spread has become an extremely important subject to evaluate securities with default risk. By Duffie and Singleton model, yield spread was explained by two factors, namely collection rate and default probability. An estimation of the collection rate is given from historical earnings data, but estimation of default probability is known to be a remaining problem. There are some approaches to express default probability. One of them is to describe it through hazard process, and the other is to represent it by risk neutral transition probability matrix of credit-rating class. Some models that use Gaussian type hazard process or Vasicek type hazard process have already constructed. An advantage of evaluation using a rating transition probability matrix is that it is easy to obtain an image of movement of the credit-rating class. We do not need to show the calculation basis of the threshold or an assumption for distribution of prospective yield spread. But the model that uses the risk neutral transition probability matrix has not established yet, because of the computational difficulty required to estimate large number of the parameters. At first, for the purposes of this article, we will estimate the term structure of credit spreads results from the possibility of future defaults. It is assumed that credit risk is specified as a discrete-state Markov chain. And we construct a model which can be used to estimate the baseline transition matrix of the credit-rating class, risk-adjusting factors, industrial drift factors, corporate drift factors and recovery ratio, from yield spreads for individual bond. This enables us to compute the implied term structure from market data. We are capable of computing the implied term structure from market date by this process. Next, we will provide a valuation model for the term structure of yield spread. © 2001 Kluwer Academic Publishers. © 2018 Elsevier B.V., All rights reserved.","Aonuma, K.; Tanabe, T.",2001,10.1023/a:1011967507050,https://www.scopus.com/inward/record.uri?eid=2-s2.0-52549092985&doi=10.1023%2FA%3A1011967507050&partnerID=40&md5=a0e6e9585c1ad15f528cb05bd29ea952,scopus,"This paper proposes a model to estimate the term structure of yield spreads, crucial for valuing securities with default risk. It builds upon the Duffie and Singleton model, incorporating factors like collection rate and default probability. The model assumes credit risk follows a discrete-state Markov chain and aims to estimate transition matrices, risk-adjusting factors, industrial and corporate drift factors, and recovery ratios from market yield spread data. This allows for the computation of implied term structures.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:50:01.657332
b4caec0083b6f9ca,An infinite hidden Markov model with GARCH for short-term interest rates,"This paper introduces a novel Bayesian time series model that combines the nonparametric features of an infinite hidden Markov model with the volatility persistence captured by the GARCH framework, to effectively model and forecast short-term interest rates. When applied to US 3-month Treasury bill rates, the GARCH-IHMM reveals both structural and persistent changes in volatility, thereby enhancing the accuracy of density forecasts compared to existing benchmark models. Out-of-sample evaluations demonstrate the superior performance of our model in density forecasts and in capturing volatility dynamics due to its adaptivity to different macroeconomic environments. © 2025 Elsevier B.V., All rights reserved.","Li, C.; Yang, Q.",2025,10.1016/j.frl.2025.107294,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002424722&doi=10.1016%2Fj.frl.2025.107294&partnerID=40&md5=5636e92e21eac3c3d6ffe3b06d97955c,scopus,"This paper proposes a new Bayesian time series model integrating an infinite hidden Markov model (IHMM) with GARCH to model and forecast short-term interest rates. Applied to US 3-month Treasury bill rates, the GARCH-IHMM captures both structural and persistent volatility changes, outperforming benchmark models in density forecasts and volatility dynamics due to its adaptability to different macroeconomic conditions.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-28T09:50:06.849296
84d299eae50d47d6,An online estimation scheme for a Hull-White model with HMM-driven parameters,"This paper considers the implementation of a mean-reverting interest rate model with Markov-modulated parameters. Hidden Markov model filtering techniques in Elliott (1994, Automatica, 30:1399-1408) and Elliott et al. (1995, Hidden Markov Models: Estimation and Control. Springer, New York) are employed to obtain optimal estimates of the model parameters via recursive filters of auxiliary quantities of the observation process. Algorithms are developed and implemented on a financial dataset of 30-day Canadian Treasury bill yields. We also provide standard errors for the model parameter estimates. Our analysis shows that within the dataset and period studied, a model with two regimes is sufficient to describe the interest rate dynamics on the basis of very small prediction errors and the Akaike information criterion. © 2007 Springer-Verlag. © 2009 Elsevier B.V., All rights reserved.","Erlwein-Sayer, C.; Mamon, R.",2009,10.1007/s10260-007-0082-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-58849132965&doi=10.1007%2Fs10260-007-0082-4&partnerID=40&md5=7f659b37a7d8a0412dc1e1436f438fff,scopus,"This paper implements a Hull-White interest rate model with parameters driven by a Hidden Markov Model (HMM). It uses HMM filtering techniques to recursively estimate model parameters and applies these algorithms to Canadian Treasury bill yield data. The study finds that a two-regime model adequately describes the interest rate dynamics within the studied dataset, supported by small prediction errors and the Akaike information criterion.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:50:22.985408
4b4d931ddca887fb,Analyst Reports and Stock Performance: Evidence From the Chinese Market,"This article applies natural language processing (NLP) to extract and quantify textual information for predicting stock performance. Utilizing an extensive dataset of Chinese analyst reports and employing a customized BERT deep learning model for Chinese text, the study categorizes the sentiment of these reports as positive, neutral, or negative. The findings highlight the predictive power of this sentiment indicator for stock volatility, excess returns, and trading volume. Specifically, analyst reports with strong positive sentiment are associated with increased excess returns and intraday volatility. Conversely, reports with strong negative sentiment also heighten volatility and trading volume but lead to a decline in future excess returns. Notably, the magnitude of the effect is more pronounced for positive sentiment reports compared to negative ones. This article contributes to the empirical literature on sentiment analysis and the stock market’s response to news, particularly within the context of the Chinese stock market. © 2025 Elsevier B.V., All rights reserved.","Liu, R.; Liang, J.; Chen, H.; Hu, Y.",2025,10.1007/s10690-025-09522-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001866318&doi=10.1007%2Fs10690-025-09522-w&partnerID=40&md5=899a1a91422d5f7e03aad92258d8bf4b,scopus,"This study uses NLP and a BERT deep learning model to analyze Chinese analyst reports, categorizing sentiment (positive, neutral, negative). The sentiment is found to predict stock volatility, excess returns, and trading volume, with positive sentiment linked to higher returns and volatility, and negative sentiment linked to higher volatility and trading volume but lower future returns. The impact of positive sentiment is stronger than negative sentiment.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:50:26.449486
c2c26c1d93079f74,Anomalies and the Expected Market Return,"We provide the first systematic evidence on the link between long‐short anomaly portfolio returns—a cornerstone of the cross‐sectional literature—and the time‐series predictability of the aggregate market excess return. Using 100 representative anomalies from the literature, we employ a variety of shrinkage techniques (including machine learning, forecast combination, and dimension reduction) to efficiently extract predictive signals in a high‐dimensional setting. We find that long‐short anomaly portfolio returns evince statistically and economically significant out‐of‐sample predictive ability for the market excess return. The predictive ability of anomaly portfolio returns appears to stem from asymmetric limits of arbitrage and overpricing correction persistence.",,2022,10.1111/jofi.13099,,proquest,"This study investigates the predictive power of long-short anomaly portfolio returns for aggregate market excess returns. The authors utilize various shrinkage techniques, including machine learning, to extract predictive signals from a large set of anomalies. They find that anomaly portfolio returns significantly predict market excess returns, attributing this to asymmetric limits of arbitrage and persistent overpricing correction.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:50:30.033301
a1a6312920cca97a,Are bond returns predictable with real-time macro data?,"We investigate the predictability of bond returns using real-time macro variables and consider the possibility of a nonlinear predictive relationship and the presence of weak factors. To address these issues, we propose a scaled sufficient forecasting (sSUFF) method and analyze its asymptotic properties. Using both the existing and the new method, we find empirically that real-time macro variables have significant forecasting power both in-sample and out-of-sample. Moreover, they generate sizable economic values, and their predictability is not spanned by the yield curve. We also observe that the forecasted bond returns are countercyclical, and the magnitude of predictability is stronger during economic recessions, which lends empirical support to well-known macro finance theories.",,2023,10.1016/j.jeconom.2022.09.008,,proquest,"This study explores the predictability of bond returns using real-time macroeconomic data, considering both linear and nonlinear relationships. A novel scaled sufficient forecasting (sSUFF) method is introduced and analyzed. Empirical results demonstrate that real-time macro variables possess significant in-sample and out-of-sample forecasting power, generating substantial economic value and not being fully explained by the yield curve. The study also finds that forecasted bond returns are countercyclical and exhibit stronger predictability during economic recessions, aligning with macro-finance theories.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-28T09:50:38.937229
1aa5adffb2e84661,Asset Pricing When 'This Time Is Different',"Recent evidence suggests that younger people update beliefs in response to aggregate shocks more than older people. We embed this generational learning bias in an equilibrium model in which agents have recursive preferences and are uncertain about exogenous aggregate dynamics. The departure from rational expectations is statistically modest, but generates high average risk premiums varying at generational frequencies, a positive relation between past returns and agents' future return forecasts, and substantial and persistent over-and undervaluation. Consistent with the model, the price-dividend ratio is empirically more sensitive to macroeconomic shocks when the fraction of young in the population is higher.","Collin-Dufresne, Pierre; Johannes, Michael; Lochstoer, Lars A.",2017,10.1093/rfs/hhw084,,wos,"This paper develops an equilibrium asset pricing model incorporating a generational learning bias, where younger individuals update beliefs more than older ones in response to aggregate shocks. This bias leads to high average risk premiums, a positive relationship between past returns and future forecasts, and significant over/undervaluation. Empirically, the price-dividend ratio is more sensitive to macroeconomic shocks when the young population fraction is higher.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:50:43.393172
fd425a361c8e4fbd,Belgian economic policy uncertainty index: Improvement through text mining,"Recently, the literature has measured economic policy uncertainty using news references, resulting in the frequently-mentioned ‘Economic Policy Uncertainty index’ (EPU). In the original setup, a news article is assumed to address policy uncertainty if it contains certain predefined keywords. We argue that the original setup is prone to measurement error, and propose an alternative methodology using text mining techniques. We compare the original method to modality annotation and support vector machines (SVM) classification in order to create an EPU index for Belgium. Validation on an out-of-sample test set speaks in favour of using an SVM classification model for constructing a news-based policy uncertainty indicator. The indicators are then used to forecast 10 macroeconomic and financial variables. The original method of measuring EPU does not have predictive power for any of these 10 variables. The SVM indicator has a higher predictive power and, notably, changes in the level of policy uncertainty during tumultuous periods of high uncertainty and risk can predict changes in the sovereign bond yield and spread, the credit default swap spread, and consumer confidence. © 2018 Elsevier B.V., All rights reserved.","Tobback, E.; Naudts, H.; Daelemans, W.; Junqué de Fortuny, E.; Martens, D.",2018,10.1016/j.ijforecast.2016.08.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85001799528&doi=10.1016%2Fj.ijforecast.2016.08.006&partnerID=40&md5=a3c133019eda083c81645c0cff02d8cb,scopus,"This paper proposes an improved method for measuring economic policy uncertainty (EPU) in Belgium using text mining and support vector machine (SVM) classification, outperforming the original keyword-based EPU index. The SVM-based EPU indicator demonstrates predictive power for macroeconomic and financial variables, particularly during periods of high uncertainty, and can predict changes in sovereign bond yield and spread, credit default swap spread, and consumer confidence.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:50:51.625534
ab700dd24895596c,Berry-Esseen inequalities for the fractional Black-Karasinski model of term structure of interest rates,"The Black-Karasinski model is a one-factor non-affine interest rate model as it describes interest rate movements driven by a single source of randomness and the drift function is a nonlinear function of the interest rate. The drift parameters represent the level and the speed of mean reversion of the interest rate. It belongs to the class of no-arbitrage models. The paper introduces some new approximate minimum contrast estimators of the mean reversion speed parameter in the model based on discretely sampled data which are efficient and studies their asymptotic distributional properties with precise rates of convergence. © 2022 Elsevier B.V., All rights reserved.","Bishwal, J.P.N.",2022,10.1515/mcma-2022-2111,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128529022&doi=10.1515%2Fmcma-2022-2111&partnerID=40&md5=0df5d76b3a1c89a37cef11986d99cff1,scopus,"This paper introduces approximate minimum contrast estimators for the mean reversion speed parameter in the fractional Black-Karasinski interest rate model, analyzing their asymptotic distributional properties and convergence rates using discretely sampled data. The model is a one-factor non-affine, no-arbitrage interest rate model.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:50:53.649250
fe830c5db5e38b8f,Beyond Stochastic Volatility and Jumps in Returns and Volatility,"While a great deal of attention has been focused on stochastic volatility in stock returns, there is strong evidence suggesting that return distributions have time-varying skewness and kurtosis as well. Under the risk-neutral measure, for example, this can be observed from variation across time in the shape of Black Scholes implied volatility smiles. This article investigates model characteristics that are consistent with variation in the shape of return distributions using a stochastic volatility model with a regime-switching feature to allow for random changes in the parameters governing volatility of volatility, leverage effect, and jump intensity. The analysis consists of two steps. First, the models are estimated using only information from observed returns and option-implied volatility. Standard model assessment tools indicate a strong preference in favor of the proposed models. Since the information from option-implied skewness and kurtosis is not used in fitting the models, it is available for diagnostic purposes. In the second step of the analysis, regressions of option-implied skewness and kurtosis on the filtered state variables (and some controls) suggest that the models have strong explanatory power for these characteristics.","Durham, Garland; Park, Yang-Ho",2013,10.1080/07350015.2013.747800,,wos,"This article proposes a stochastic volatility model with a regime-switching feature to capture time-varying skewness and kurtosis in stock returns, beyond just stochastic volatility and jumps. The model is estimated using observed returns and option-implied volatility, and its explanatory power for option-implied skewness and kurtosis is assessed.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:50:56.641091
3f98ba9b92b7d6ec,Black-litterman model with views prediction using elman recurrent neural network,"The Black-Litterman model is a portfolio model that considers investor views. The purpose of this study is to develop the Black-Litterman (BL) portfolio model with views prediction using Elman Recurrent Neural Network (ERNN) on LQ-45 stocks. The ERNN model is one of the neural network models that adjusts the input using the output feedback from the hidden layer. The BL portfolio is generated based on the capital assets pricing model (CAPM) excess return equilibrium. The data used in CAPM model must fulfill the normality assumption which is checked by using Jarque Bera test. The selected stocks for the portfolio are the member of LQ-45 stocks which meet the normality assumption and have the highest expected excess return CAPM value, those are AKRA, BBNI, INCO, and JSMR stocks. The ERNN model is employed to those stocks to obtain the views prediction. Then, the Black-Litterman portfolio is constructed by combining the ERNN views of the stock returns and the expected equilibrium return yielded by the capital assets pricing model. Three designs of relative views are considered, each design is distinguished from the percentage of each stock return prediction. The simulation shows that the best portfolio is constructed based on the design of the first views due to the most accurate views prediction. © 2021 Elsevier B.V., All rights reserved.","Wutsqa, D.U.; Pamungkas, M.A.; Subekti, R.",2021,10.13189/ujaf.2021.090609,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120813748&doi=10.13189%2Fujaf.2021.090609&partnerID=40&md5=79efd1877b35d6a272814a5d81943845,scopus,"This study develops the Black-Litterman portfolio model by incorporating investor views predicted using an Elman Recurrent Neural Network (ERNN) for LQ-45 stocks. The model combines ERNN-predicted views with equilibrium returns from the Capital Asset Pricing Model (CAPM). The best portfolio was achieved with the first design of relative views, which showed the most accurate predictions.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:51:01.833277
8624241783a1b415,Blockchain Innovation for Sustainability: Unraveling Its Market Impact Through Public Attention and Financial Performance,"This study investigates the dynamic interplay between blockchain innovation, public attention, and financial performance, with a focus on sustainability applications. Using a state-space model and the Kalman filter, it analyzes data from blockchain-related patents and Google Trends to assess their influence on the excess returns of blockchain-focused exchange-traded funds (ETFs). The findings highlight that innovation activity significantly enhances financial performance, underscoring blockchain’s role as a general-purpose technology with transformative sustainability potential. Public attention, measured through search interest, independently drives investor sentiment and market outcomes, while the interaction between innovation and public attention does not exhibit significant synergistic effects, suggesting distinct channels of influence. This study contributes to the growing stream of literature in sustainable finance, innovation management, and behavioral finance by introducing a real-time measure of blockchain innovation for sustainability into an asset pricing model, by showing that patent activity and public attention operate as separate predictors of financial returns, and by advancing methodological practice through the use of a state-space approach to capture latent innovation dynamics. The findings suggest actionable strategies: investors can track patent-based innovation and search trends as early signals of thematic-ETF performance; industry leaders can align blockchain projects with sustainability goals to unlock valuation gains; and policymakers can foster environmental, social, and governance innovation ecosystems by encouraging transparent patent disclosure and public awareness.",C. Toscano Hernandez; F. P. Appio; F. Platania,2025,10.1109/tem.2025.3598382,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11123793,ieeexplore,"This study examines how blockchain innovation, public attention, and sustainability applications affect financial performance, specifically the returns of blockchain-focused ETFs. It uses a state-space model and Kalman filter to analyze patent data and Google Trends, finding that innovation boosts financial performance and public attention influences investor sentiment independently. The research contributes to sustainable finance and innovation management by introducing a real-time measure of blockchain innovation for sustainability and demonstrating that patent activity and public attention are separate predictors of financial returns. It suggests investors monitor patent and search trends for ETF performance, industry leaders align blockchain with sustainability for valuation gains, and policymakers foster innovation ecosystems.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:51:09.241291
45ce1b29701d0093,Bond Risk Premiums with Machine Learning,"We show that machine learning methods, in particular, extreme trees and neural networks (NNs), provide strong statistical evidence in favor of bond return predictability. NN forecasts based on macroeconomic and yield information translate into economic gains that are larger than those obtained using yields alone. Interestingly, the nature of unspanned factors changes along the yield curve: stock- and labor-market-related variables are more relevant for short-term maturities, whereas output and income variables matter more for longer maturities. Finally, NN forecasts correlate with proxies for time-varying risk aversion and uncertainty, lending support to models featuring both channels.",,2021,10.1093/rfs/hhaa062,,proquest,"This study demonstrates that machine learning techniques, specifically extreme trees and neural networks (NNs), offer significant statistical evidence for predicting bond returns. NN forecasts, incorporating macroeconomic and yield data, yield greater economic benefits than those using only yield information. The study also identifies that the influence of unspanned factors varies across the yield curve, with stock and labor market factors being more significant for shorter maturities and output/income factors for longer maturities. Furthermore, NN forecasts align with measures of time-varying risk aversion and uncertainty, supporting models that include these elements.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-28T09:51:12.977382
22a2d2aa9f3427cd,Bond return predictability: Macro factors and machine learning methods,"We investigate the impact of macroeconomic variables on bond risk premia prediction via machine learning techniques. On the basis of Chinese treasury bonds from March 2006 to December 2022, we show that adding macroeconomic factors improves bond return forecasts and generates higher economic benefits to investors. This is achieved when the nonlinear relationship between macroeconomic variables and bond returns is modelled via machine learning methods. Furthermore, the importance of macroeconomic determinants changes along the yield curve. Our study sheds new light on the information contained in macroeconomic variables for treasury bond valuation and highlights the importance of utilizing appropriate machine learning methods.",,2024,10.1111/eufm.12483,,proquest,"This study explores the predictability of bond returns using macroeconomic variables and machine learning techniques on Chinese treasury bonds. It demonstrates that incorporating macroeconomic factors, especially through nonlinear modeling with machine learning, enhances return forecasts and investor benefits. The relevance of these factors varies across the yield curve.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-28T09:51:20.641593
46572bea2afbeb26,Bond risk premia in a small open economy with volatile capital flows: The case of Korea,"This paper investigates bond risk premia embedded in Korean government bonds. Unlike the U.S., Korea is a small open economy characterized by highly volatile capital flows and non-reserve currency country. My empirical findings show that among alternative predictive variables (including the macro and global liquidity factors) for one-year-ahead excess bond returns, the global liquidity factors, extracted from the panel data set of various global liquidity variables, are the only predictors that perform well across both in- and out-of-sample forecast analysis. In a similar vein, the regression analysis for the determinants of the estimated bond risk premia (with both monthly and quarterly frequencies) reveals that similar to the case of U.S. bond market, the risk premia in Korean government bonds are affected by domestic expected inflation, but more importantly, that they are affected heavily by the global liquidity variables, such as VIX, bank capital flows and the leverage of global banks. (C) 2019 Elsevier Ltd. All rights reserved.","Yun, Jaeho",2019,10.1016/j.jimonfin.2019.01.007,,wos,"This paper examines bond risk premia in Korean government bonds, highlighting Korea's status as a small open economy with volatile capital flows. Empirical findings indicate that global liquidity factors are significant predictors of excess bond returns, outperforming domestic macroeconomic variables. The study also reveals that while domestic expected inflation influences Korean bond risk premia, global liquidity variables like VIX, bank capital flows, and global bank leverage play a more substantial role.",False,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:51:25.832682
7c6e73c3fc3236c1,Buffered vector error-correction models: An application to the U.S. Treasury bond rates,"This paper extends the buffered autoregressive model to the buffered vector error-correction model (VECM). Least squares estimation and a reduced-rank estimation are discussed, and the consistency of the estimators on the delay parameter and threshold parameters is derived. We also propose a supWald test for the presence of buffer-type threshold effect. Under the null hypothesis of no threshold, the supWald test statistic converges to a function of Gaussian process. A bootstrap method is proposed to obtain the p-value for the supWald test. We investigate the effectiveness of our methods by simulation studies. We apply our model to study the monthly Federal bond rates of United States. We find the evidences of buffering regimes and the asymmetric error-correction effect. © 2022 Elsevier B.V., All rights reserved.","Lu, R.; Yu, P.L.H.",2021,10.1515/snde-2019-0047,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094603557&doi=10.1515%2Fsnde-2019-0047&partnerID=40&md5=79cc2936dd1fb0d52d32acd43219b341,scopus,"This paper introduces the buffered vector error-correction model (VECM) as an extension of the buffered autoregressive model. It covers estimation techniques, consistency of parameters, and a supWald test for threshold effects, with a bootstrap method for p-values. The model is applied to U.S. Treasury bond rates, revealing evidence of buffering regimes and asymmetric error-correction.",True,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:51:41.321428
5c1357ad22be14f6,Can Deep-Learning Models Predict Behavior of Treasury Bond Yields,"Treasury market dominates the fixed-income segment, especially in emerging economies such as India. The Treasury rates affect the investment decisions, portfolio management, capital structure, as well as corporate debt structure decisions of the firms. Even after this huge impact, the Treasury rate prediction problems are less explored. This paper investigates the efficacy of ensemble-based machine learning and deep learning models in predicting the behavior of Treasury bond yields. The work has employed six models on six Treasury index datasets with different maturity periods. The result suggests that the Recurrent Neural Network and its variants are best-suited models to work on Treasury bonds. The Gated Recurrent Unit gives the best result among all Recurrent Network variants. The deep learning models give higher accuracy for the Treasury yield with a greater maturity period. In contrast, the ensemble models perform better on the Treasury yield with a smaller maturity period. The findings call for incorporating machine learning-based prediction in the pricing and valuation of securities, investment plans, and debt structure decisions. © 2025 Elsevier B.V., All rights reserved.","Podder, D.; Mukherjee, R.; Hiremath, G.S.",2025,10.1007/s10614-025-10947-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007230849&doi=10.1007%2Fs10614-025-10947-8&partnerID=40&md5=0d918fe85ff3646830e1f74d6664d2bf,scopus,"This paper explores the use of ensemble-based machine learning and deep learning models, specifically Recurrent Neural Networks (RNNs) and their variants like Gated Recurrent Units (GRUs), for predicting Treasury bond yields in India. The study found that deep learning models, particularly GRUs, performed best for longer maturity periods, while ensemble models were more effective for shorter maturities. The findings suggest integrating machine learning for securities pricing, investment planning, and debt structure decisions.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-28T09:51:46.529709
af3aae430bbd5266,Can Voluntary Insurance ensure risk-free digital-banking in Chinese-economy: seeking attentions?,"In today’s business-world, services are carried out in a competitive manner country-wise such as China. Banking services are no different, which has resulted digital-banking. Bank Laws regulated by Central-Bank of China are characterized by evolving many factors that are often unpredictable. It faces serious pitfalls being it riskiness. Most cases, customers don’t read terms & conditions of services. Customers don’t save contract-copy. These weaknesses cause abuses. Customer faces perceived-risk. Dealing with challenges in Chinese-economy, application of Akim’s model - Voluntary Insurance (VI) can be impetus for policy-design, which can increase number of users. Welfare Analyses are used for guidance on setting insurance-price ensuring customer’s efficiency-cost so that the VI becomes appealing to parties involved. It can lead to higher number-of-users. In scenario, bank itself is an insurance-seller, the existence of adverse-selection is detected. Here estimated welfare-cost associated with inefficient-pricing created by adverse-selection is quantitatively small; however, advantageous-selection results opposite. Welfare assessment under alternative policy intervention in Chinese-economy will be vital for future-study.",,2022,10.1080/14765284.2021.1929792,,proquest,"This paper explores the potential of Voluntary Insurance (VI) to mitigate risks in China's digital banking sector. It discusses how VI, guided by Akim's model and welfare analyses, could encourage user adoption by addressing customer concerns about terms and conditions and perceived risks. The study also touches upon adverse and advantageous selection when banks act as insurance sellers, suggesting further policy interventions are needed.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:51:48.569268
1b5cd364557d4228,Can deep neural networks outperform Fama-MacBeth regression and other supervised learning approaches in stock returns prediction with asset-pricing factors?,"In asset pricing, most studies focus on finding new factors, such as macroeconomic factors or firm characteristics, to explain risk premiums. Investigating whether these factors help forecast stock returns remains active research in finance and computer science. This paper conducts an extensive comparative analysis using a large set of pricing factors. It compares out-of-sample stock-level and portfolio-level prediction performance among neural networks, the traditional Fama-MacBeth regression, and other supervised learning algorithms such as regression and tree-based algorithms. Our analysis shows the benefit of employing neural networks, and deeper neural networks enjoy marginal improvements in terms of prediction. © 2024 Elsevier B.V., All rights reserved.","Teng, H.-W.; Li, Y.-H.",2023,10.1007/s42521-023-00076-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207820403&doi=10.1007%2Fs42521-023-00076-y&partnerID=40&md5=06ef991afc31574f19f4f932a3174936,scopus,"This paper compares the stock return prediction performance of deep neural networks against traditional Fama-MacBeth regression and other supervised learning algorithms, using a large set of pricing factors. The study finds that neural networks, particularly deeper ones, offer marginal improvements in prediction accuracy.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:51:54.449377
38d168134a26a921,Can forward rates be used to improve interest rate forecasts?,"This paper evaluates the extent to which the explanatory power detected in the term structure in different markets and countries can actually be used to produce sensible forecasts of future short-term interest rates. Specifically, in spite of the forecasting connotation of the unbiasedness property of forward rates, actual evaluation of their forecasting performance has received scant attention in the literature on the term structure. This study uses monthly data for 1978-1998 on interest rates on Euro-deposits on the US dollar, yen, Deutsche mark, British pound, Spanish peseta, French franc, Italian lira and Swiss franc, comparing forecasts obtained from forward rates to those obtained from univariate autoregressions. By themselves, forward rates produce better one-step ahead forecasts, as well as better once-and-for all forecasts of 1-month interest rates over a full year horizon than those obtained from the own past of interest rates. The gain in one-step ahead forecasting disappears for longer maturities, although forward rates still produce better once-and-for all predictions of 3- and 6-month interest rates than univariate autoregressions for a number of currencies. © 2008 Elsevier B.V., All rights reserved.","Domínguez, E.; Novales, A.",2002,10.1080/09603100010007346,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036296125&doi=10.1080%2F09603100010007346&partnerID=40&md5=1ab9a64dfbdfbb88d297fb81367184c4,scopus,"This paper investigates whether forward rates can improve interest rate forecasts by comparing their performance against univariate autoregressions using monthly data from 1978-1998 for several currencies. The study finds that forward rates generally provide better one-step-ahead and one-and-for-all forecasts for short-term interest rates over a one-year horizon, although the advantage diminishes for longer maturities.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:51:56.521379
accc2a0866415987,Can gold or silver be used as a hedge against policy uncertainty and COVID-19 in the Chinese market?,"Purpose: The purpose of this study is to present evidence as to whether the use of gold or silver can be justified as an asset to hedge against policy uncertainty and COVID-19 in the Chinese market. Design/methodology/approach: By using a GARCH model with a generalized error distribution (GED), this study specifies that the gold (or silver) return is a function of a set of economic and uncertainty variables, which include volatility from interest rate innovation, a change in economic policy uncertainty (EPU), a change in geopolitical risk (GPR) and volatility due to pandemic diseases, while controlling for stock market returns, inflation rates, economic growth and the Chinese currency value. Findings: This study employs monthly data of gold and silver prices over the period from January 2002 to August 2021 to examine hedging behavior. Estimated results show that the gold return is positively correlated to the stock return and a rise in uncertainty from economic policy innovation, geopolitical risk, volatility due to US interest rate innovation as well as COVID-19 infection. This result suggests that gold cannot be used to hedge against a stock market decline, but can be used to hedge against uncertainty in general. However, the silver return only responds positively to a rise in uncertainty from the inflation rate and geopolitical risk. Evidence shows that silver returns are negatively correlated with stock returns, and display hedging characteristics. However, the evidence lacks statistically significance during the COVID-19 period, suggesting that the role of silver as a safe-haven asset against stock market turmoil is weak for this time period. Research limitations/implications: More general nonlinear specifications can be developed. The tests may include different measures of uncertainty that interact with each other or with the lagged error terms. An implication of the model is that gold can be used to hedge against a broad range of uncertainties for economic policy change, political risk and/or a pandemic. However, the use of gold as an asset to hedge against a stock downturn in Chinese market should be done with caution. Practical implications: This study has important policy implications as regards a choice in assets in formatting a portfolio to hedge against uncertainty. Specifically, this study presents empirical evidence on gold and silver return behavior and finds that gold returns respond positively to heightened uncertainty. Thus, gold is a good asset to hedge against uncertainty arising from policy innovations and infectious disease uncertainty. Social implications: This paper provides insightful information on the choice of assets toward hedging against risk in the uncertainty market conditions. It provides information to investors and policy makers to use gold price movements as a signal for detecting the arrival of uncertainty. This study also provides information for demanding a risk premium for infectious disease. Originality/value: This study empirically analyzes and verifies the role that gold serves as a safe haven asset to hedge against uncertainty in the Chinese market. This paper contributes to the literature by presenting evidence of risk/uncertainty premiums for holding gold against various sources of uncertainty such as economic policy uncertainty, geopolitical risk and equity market volatility due to US interest rate innovation and/or COVID-19. This study finds evidence that supports the use of a nonlinear specification, which demonstrates the interaction of uncertainty with the lagged change of infectious disease and helps to explain the gold/silver return behavior. Further, evidence shows that the gold return is positively correlated to the stock return. This finding contrasts with evidence in the US market. However, silver returns are negatively correlated with stock returns, but this correlation becomes insignificant during the period of COVID-19. © 2022 Elsevier B.V., All rights reserved.","Chiang, T.C.",2022,10.1108/cfri-12-2021-0232,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132296653&doi=10.1108%2FCFRI-12-2021-0232&partnerID=40&md5=4ea23791a60965a70d67bf3e4f586df2,scopus,"This study investigates whether gold or silver can serve as hedges against policy uncertainty and COVID-19 in the Chinese market using a GARCH model. Gold returns are positively correlated with stock returns and various uncertainties (economic policy, geopolitical risk, US interest rates, COVID-19), suggesting it hedges general uncertainty but not stock market declines. Silver returns are negatively correlated with stock returns and respond to inflation and geopolitical risk uncertainty, showing hedging characteristics, though this is weak during COVID-19. The findings imply gold is useful for hedging against broad uncertainties, but its use against stock downturns in China requires caution. The study highlights gold price movements as a potential signal for uncertainty and suggests a risk premium for infectious disease risk.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:52:11.721208
e47a7f4118f99119,Can negative interest rates really affect option pricing? Empirical evidence from an explicitly solvable stochastic volatility model,"The profound financial crisis generated by the collapse of Lehman Brothers and the European sovereign debt crisis in 2011 have caused negative values of government bond yields both in the USA and in the EURO area. This paper investigates whether the use of models which allow for negative interest rates can improve option pricing and implied volatility forecasting. This is done with special attention to foreign exchange and index options. To this end, we carried out an empirical analysis on the prices of call and put options on the US S&P 500 index and Eurodollar futures using a generalization of the Heston model in the stochastic interest rate framework. Specifically, the dynamics of the option’s underlying asset is described by two factors: a stochastic variance and a stochastic interest rate. The volatility is not allowed to be negative, but the interest rate is. Explicit formulas for the transition probability density function and moments are derived. These formulas are used to estimate the model parameters efficiently. Three empirical analyses are illustrated. The first two show that the use of models which allow for negative interest rates can efficiently reproduce implied volatility and forecast option prices (i.e. S&P index and foreign exchange options). The last studies how the US three-month government bond yield affects the US S&P 500 index. © 2017 Elsevier B.V., All rights reserved.","Recchioni, M.C.; Sun, Y.; Tedeschi, G.",2017,10.1080/14697688.2016.1272763,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014563788&doi=10.1080%2F14697688.2016.1272763&partnerID=40&md5=fb231c8d70605cedda83b18b1ebc4c8b,scopus,"This paper investigates the impact of negative interest rates on option pricing and implied volatility forecasting, particularly for foreign exchange and index options. Using a generalized Heston model with stochastic interest rates, the study empirically analyzes call and put options on the US S&P 500 index and Eurodollar futures. The findings suggest that models accommodating negative interest rates can effectively reproduce implied volatility and forecast option prices.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:52:21.697637
0171b0a089a6ffef,Can tree-structured classifiers add value to the investor?,"We analyse the investor welfare gain of including tree-structured classifiers’ predictions about the relative performance of stock vs. cash. The CART, bagging, and random forest methods select the VIX level and momentum, the earning bond yield level and momentum, and the detrended risk-free rate as the most important state variables to predict the outperformance of the S&P 500 vs. cash out-of-sample. These tree-structured classifiers’ predictions are used as a binary state variable to estimate optimal investor portfolios that also deliver out-of-sample higher Sharpe ratios and certainty equivalent return gains than competing portfolio strategies that exclude them. © 2019 Elsevier B.V., All rights reserved.","Laborda, R.; Laborda, J.",2017,10.1016/j.frl.2017.06.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020478167&doi=10.1016%2Fj.frl.2017.06.002&partnerID=40&md5=c5cc5c546ee15a2cf5366bb636d8e4ef,scopus,"This study investigates the benefits of using tree-structured classifiers (CART, bagging, random forest) for investor decision-making regarding stock vs. cash performance. The models identify key state variables like VIX level, momentum, earning bond yield, and detrended risk-free rate to predict S&P 500 outperformance. Incorporating these predictions into portfolio strategies results in improved Sharpe ratios and certainty equivalent returns compared to strategies that do not use them.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:52:25.273435
d47ebdc5ea5914d1,Carbon risk and return prediction: Evidence from the multi-CNN method,"This paper investigates the carbon risk and its role in stocks’ return prediction by identifying the carbon risk information implied in feature engineering. We predict the stock returns with different neural networks, construct the investment portfolio according to the predicted returns and reflect the returns of stocks with different carbon risks through the relevant evaluation of the investment portfolio. Our Multi-CNN method can best collect information on different relationship types and make full use of graph structure data to identify carbon risks. With or without carbon factor, the stock market performance of high-carbon industry is better than that of medium-carbon industry, and the performance of low-carbon industry is the worst. Moreover, our finding is consistent in both Chinese and American markets. Investment should pay attention to carbon risk and requires corresponding carbon risk premium. © 2022 Elsevier B.V., All rights reserved.","Tang, J.; Li, J.",2022,10.3389/fenvs.2022.1035809,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142006137&doi=10.3389%2Ffenvs.2022.1035809&partnerID=40&md5=6eecf37eb1d8a397e2c08fbd38faa6c7,scopus,"This paper uses a Multi-CNN method to identify carbon risk information and predict stock returns. The study finds that high-carbon industries perform better than low-carbon industries, and suggests that investment should consider carbon risk and its associated premium. The findings are consistent across Chinese and American markets.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:52:30.385295
98032144cda08a39,Central Bank Transparency and Interest Rate Volatility,"Most central banks around the world have increased their transparency in recent decades. These developments have had an impact on financial markets. Several studies have analysed the impact of central bank transparency (CBT) on variables such as inflation volatility, exchange rate volatility, or stock market volatility. One variable that has not received enough attention is interest rate volatility, despite its importance for decision-makers in both financial markets and the real economy. The study uses a panel data set of a maximum of 93 countries over the years 1998–2017. We use panel data estimators (panel fixed effects and fixed effects filter). Our main findings are that CBT helps to reduce the volatility of several interest rates (money market rates, deposit rates, savings rates, Treasury bill yields and government bond yields), while it increases the volatility of monetary policy rates. The results are also significant from an economic point of view, as a small increase in CBT can reduce the variability of treasury bill rates by up to −8.2%, deposit rates by up to −9.1%, money market rates by up to −15.8% and savings rates by up to −20.4%. © 2024 Elsevier B.V., All rights reserved.","Weber, C.S.",2024,10.1002/ijfe.3072,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212859950&doi=10.1002%2Fijfe.3072&partnerID=40&md5=709d6898cc2a2cb5f01e3dd2e10c9f3a,scopus,"This study investigates the impact of central bank transparency (CBT) on interest rate volatility using panel data from up to 93 countries between 1998 and 2017. The findings indicate that increased CBT reduces the volatility of various interest rates, including money market rates, deposit rates, savings rates, Treasury bill yields, and government bond yields. However, it paradoxically increases the volatility of monetary policy rates. The study highlights the significant economic implications of these findings, showing that even small increases in CBT can lead to substantial reductions in the variability of several key interest rates.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:52:32.993543
e093f0a24b02d3b8,Chasing the deal with the money: Measuring the required risk premium and expected abnormal returns of private equity funds to maximize their internal rate of return,"A number of scholars of private equity (“PE”) have attempted to assess the ex-post returns, or performance, of PEs by adopting an ex-post perspective of asset pricing. In doing so a set of phenomena has been recognized that is thought to be specific to the PE sector, such as “money-chasing deal phenomenon” (Gompers and Lerner, 2000) and “performance persistence” (Lerner and Schoar, 2005). However, based on their continuing use of an ex-post perspective, few scholars have paid attention to the possible extent to which these and other PE phenomena may affect expected returns from PE investments. To address this problem this article draws on an ex-ante perspective of investment decision-making in suggesting how a number of drivers and factors of PE phenomena may produce “abnormal returns”, and that each of those drivers and factors should therefore be considered in accurately assessing the required risk premium and expected abnormal returns of PE investments. In making these contributions we examined a private equity investment of a regional PE in Italy and administered a telephone questionnaire to 40 PEs in Italy and the UK and found principally that while size is the most important driver in producing abnormal returns illiquidity alone cannot explain the expected returns of PE investments (cf. Franzoni et al., 2012). Based on our findings we developed a predictive model of PE decision-making that draws on an ex-ante perspective of asset pricing and takes into account PE phenomena and abnormal returns. This model extends the work of Franzoni et al. (2012), Jegadeesh et al. (2009), and Korteweg and Sorensen (2010) who did not consider the possible influence of PE phenomena in decision-making and will also help PE managers in making better-informed decisions. © 2018 Elsevier B.V., All rights reserved.","Scarpati, F.; Ng, W.",2013,10.22495/rgcv3i3art6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939419937&doi=10.22495%2Frgcv3i3art6&partnerID=40&md5=4a3b001b9d54090bf9a7d53849498f2c,scopus,"This article uses an ex-ante perspective to examine how private equity (PE) phenomena, such as the ""money-chasing deal phenomenon"" and ""performance persistence,"" influence expected returns and risk premiums. The study surveyed 40 PEs in Italy and the UK, finding that while size is a key driver of abnormal returns, illiquidity alone does not explain expected returns. A predictive model is developed to aid PE managers in decision-making.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:52:34.873248
6e02db71875332a7,China University Online Public Opinion Risk Dataset,"With the widespread popularity of social software and self-media, online public opinion incidents in colleges and universities occur frequently and present a complicated situation. In the big data era, university students have gained a more relaxed environment in which to receive and disseminate public opinion information, enabling them to spread their opinions and insights to the Internet more rapidly, thus exacerbating the riskiness of public opinion information dissemination. We constructed CUOPO, the first risk classification dataset of China university online public opinion, and screened out 10,255 representative public opinion texts from a large number of university online public opinion information, including 3,641 risk-free and 6,614 risky texts. These risky texts cover many fields, including 1,755 college livelihood risk texts, 767 campus safety risk texts, 1,395 school order risk texts, 906 university reputation risk texts, and 1,793 advertisement risk texts. The dataset contains various information about each network opinion, including authentic labels, text information, time information, and network information. Through an in-depth study of CUOPO, we found that universities have significant risk issues in the areas of livelihood, safety, teaching order, reputation, and advertisement diversion, which require great attention from university administrators. To validate the effectiveness of the CUOPO, we conduct extensive experiments on the dataset using a series of neural network methods to provide benchmark results for predicting online public opinion risk texts. We expect that CUOPO can provide strong data support for the study of the types of online public opinion risks in colleges and universities and thus play a positive role in promoting the progress of college and university public opinion research. The dataset is available at https://github.com/TianShengLee98/CUOPO-Dataset.",S. Wang; T. Li; X. Shen; H. Zhao,2024,10.1109/access.2024.3389974,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10501775,ieeexplore,"This paper introduces CUOPO, the first dataset for classifying online public opinion risks in Chinese universities. It contains 10,255 texts categorized as risk-free or risky, with risky texts further classified into livelihood, safety, order, reputation, and advertisement risks. The dataset includes labels, text, time, and network information. Experiments using neural networks validate the dataset's effectiveness for predicting public opinion risks, highlighting significant risk areas for university administrators.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:52:38.241257
1b1a8cfbbcbec1d6,Climate Change and ESG: Focused on Green Bond Design,"We investigate about climate change, which is now emerging as a hot potato, and realize its seriousness. To respond to climate change, carbon reduction strategies are vitally important. We examine the effect of carbon reduction on ESG score improvement and show the importance of ESG management by using the concept of green spread which is a financial environmental cleanliness measure. An aim of this study is to predict numerically the impact of carbon reduction on national cleanliness and economic benefits using the methodology of CO2 emission-backed securities. In order to get attention from many people to the reduction of GHG gas, we compute risk premiums of the securities and did securitization of CO2 emissions. Then, issuing the securities will stimulate investors about the national CO2 reduction activities. Also, we expect that this study gives countries an incentive to reduce their CO2 emissions and prepare for climate change.","DONGHOON, SHIN; 최윤민; 김창기",2023,10.14251/jscm.2023.3.31,,wos,"This study investigates the impact of carbon reduction strategies on ESG score improvement and national economic benefits, using the concept of green spread and CO2 emission-backed securities. It aims to numerically predict the effects of carbon reduction and incentivize countries to reduce CO2 emissions and prepare for climate change.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:52:41.473417
1c852fb6a398bfae,Climate response uncertainty and the benefits of greenhouse gas emissions reductions,"Some recent research suggests that uncertainty about the response of the climate system to atmospheric greenhouse gas concentrations can have a disproportionately large influence on benefits estimates for climate change policies, potentially even dominating the effect of the discount rate. In this paper we conduct a series of numerical simulation experiments to investigate the quantitative significance of climate response uncertainty for economic assessments of climate change. First we characterize climate uncertainty by constructing two probability density functions-a Bayesian model-averaged and a Bayesian updated version-based on a combination of uncertainty ranges for climate sensitivity reported in the scientific literature. Next we estimate the willingness to pay of a representative agent for a range of emissions reduction policies using two simplified economic models. Our results illustrate the potential for large risk premiums in benefits estimates as suggested by the recent theoretical work on climate response uncertainty, and they show that the size and even the sign of the risk premium may depend crucially on how the posterior distribution describing the overall climate sensitivity uncertainty is constructed and on the specific shape of the damage function. © United States Environmental Protection Agency 2009. © 2017 Elsevier B.V., All rights reserved.","Newbold, S.C.; Daigneault, A.",2009,10.1007/s10640-009-9290-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76149142167&doi=10.1007%2Fs10640-009-9290-8&partnerID=40&md5=c5d868d8ee46f868b523eb5a8baa76b1,scopus,"This paper investigates the impact of climate response uncertainty on the economic benefits of greenhouse gas emissions reductions. Using numerical simulations and probability density functions for climate sensitivity, the study estimates the willingness to pay for emissions reduction policies. Results indicate that uncertainty in climate response can lead to significant risk premiums in benefits estimates, which are sensitive to the construction of the climate sensitivity distribution and the shape of the damage function.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:54:40.994676
1aca9ba06a07d5ec,"Climate, race, and the cost of capital in the municipal bond market","Both climate risk and race are factors that may affect municipal bond yields, yet each has received relatively limited empirical research attention. We analyzed > 712,000 municipal bonds representing nearly 2 trillion USD in par outstanding, focusing on credit spread or the difference between a debt issuer's interest cost to borrow and a benchmark ""risk-free"" municipal rate. The relationship between credit spread and physical climate risk is significant and slightly positive, yet the coefficient indicates no meaningful spread penalty for increased physical climate risk. We also find that racial composition (the percent of a community that is Black) explains a statistically significant and meaningful portion of municipal credit spreads, even after controlling for a variety of variables in domains such as geographic location of issuer, bond structure (e.g., bond maturity), credit rating, and non-race economic variables (e.g., per capita income). Assuming 4 trillion USD in annual outstanding par across the entire municipal market, and weighting each issuer by its percent Black, an estimated 19 basis point (bp) penalty for Black Americans sums to approximately 900 million USD annually in aggregate. Our combined findings indicate a systemic mispricing of risk in the municipal bond market, where race impacts the cost of capital, and climate does not.Both climate risk and race are factors that may affect municipal bond yields, yet each has received relatively limited empirical research attention. We analyzed > 712,000 municipal bonds representing nearly 2 trillion USD in par outstanding, focusing on credit spread or the difference between a debt issuer's interest cost to borrow and a benchmark ""risk-free"" municipal rate. The relationship between credit spread and physical climate risk is significant and slightly positive, yet the coefficient indicates no meaningful spread penalty for increased physical climate risk. We also find that racial composition (the percent of a community that is Black) explains a statistically significant and meaningful portion of municipal credit spreads, even after controlling for a variety of variables in domains such as geographic location of issuer, bond structure (e.g., bond maturity), credit rating, and non-race economic variables (e.g., per capita income). Assuming 4 trillion USD in annual outstanding par across the entire municipal market, and weighting each issuer by its percent Black, an estimated 19 basis point (bp) penalty for Black Americans sums to approximately 900 million USD annually in aggregate. Our combined findings indicate a systemic mispricing of risk in the municipal bond market, where race impacts the cost of capital, and climate does not.",,2023,10.1371/journal.pone.0288979,,proquest,"This study analyzes over 712,000 municipal bonds to investigate the impact of climate risk and race on municipal bond yields. While climate risk shows a statistically significant but not meaningfully impactful relationship with credit spreads, the racial composition of a community (specifically the percentage of Black residents) significantly affects credit spreads, even after controlling for various economic and structural factors. The findings suggest a systemic mispricing of risk in the municipal bond market, where race influences the cost of capital, but climate risk does not.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:54:42.697364
9cb31eeb7dae5c0a,"Closed-Form Expansion, Conditional Expectation, and Option Valuation","Enlightened by the theory of Watanabe [Watanabe S (1987) Analysis of Wiener functionals (Malliavin calculus) and its applications to heat kernels. Ann. Probab. 15:1-39] for analyzing generalized random variables and its further development in Yoshida [Yoshida N (1992a) Asymptotic expansions for statistics related to small diffusions. J. Japan Statist. Soc. 22: 139-159], Takahashi [Takahashi A (1995) Essays on the valuation problems of contingent claims. Ph.D. thesis, Haas School of Business, University of California, Berkeley, Takahashi A (1999) An asymptotic expansion approach to pricing contingent claims. Asia-Pacific Financial Markets 6:115-151] as well as Kunitomo and Takahashi [Kunitomo N, Takahashi A (2001) The asymptotic expansion approach to the valuation of interest rate contingent claims. Math. Finance 11(1):117-151, Kunitomo N, Takahashi A (2003) On validity of the asymptotic expansion approach in contingent claim analysis. Ann. Appl. Probab. 13(3):914-952] etc., we focus on a wide range of multivariate diffusion models and propose a general probabilistic method of small-time asymptotic expansions for approximating option price in simple closed-form up to an arbitrary order. To explicitly construct correction terms, we introduce an efficient algorithm and novel closed-form formulas for calculating conditional expectation of multiplication of iterated stochastic integrals, which are potentially useful in a wider range of topics in applied probability and stochastic modeling for operations research. The performance of our method is illustrated through various models nested in constant elasticity of variance type processes. With an application in pricing options on VIX under GARCH diffusion and its multifactor generalization to the Gatheral double lognormal stochastic volatility models, we demonstrate the versatility of our method in dealing with analytically intractable non-Levy and non-affine models. The robustness of the method is theoretically supported by justifying uniform convergence of the expansion over the whole set of parameters.","Li, Chenxu",2014,10.1287/moor.2013.0613,,wos,"This paper proposes a general probabilistic method for approximating option prices using small-time asymptotic expansions, applicable to multivariate diffusion models. It introduces an algorithm and closed-form formulas for conditional expectations of iterated stochastic integrals, demonstrating its use in various models, including options on VIX under GARCH diffusion and double lognormal stochastic volatility models. The method's robustness is theoretically supported by uniform convergence.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:54:44.425403
ae63f674b58debe4,Closed-form likelihood expansions for multivariate time-inhomogeneous diffusions,"The aim of this paper is to find approximate log-transition density functions for multivariate time-nhomogeneous diffusions in closed form. There are many empirical evidences supporting that the data generating process governing dynamics of many economics variables might vary over time because of economic climate changes or time effects. One possible way to explain the time-dependent dynamics of state variables is to model the drift or volatility terms as functions of time t as well as state variables. A way to find closed-form likelihood expansion for a multivariate time-homogeneous diffusion has been developed by Ait-Sahalia (2008). This research is built on his work and extends his results to time-inhomogeneous cases. We conduct Monte Carlo simulation studies to examine performance of the approximate transition density function when it is used to obtain ML estimates. The results reveal that our method yields a very accurate approximate likelihood function, which can be a good candidate when the true likelihood function is unavailable as is often the case. (C) 2013 Elsevier B.V. All rights reserved.","Choi, Seungmoon",2013,10.1016/j.jeconom.2011.12.007,,wos,"This paper develops closed-form approximate log-transition density functions for multivariate time-inhomogeneous diffusions, extending previous work on time-homogeneous diffusions. The authors use Monte Carlo simulations to demonstrate the accuracy of their method for Maximum Likelihood estimation, suggesting it's a viable alternative when the true likelihood function is unknown.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:54:47.777593
4d1b62d1b15ae5dc,Cointegration analysis of hazard rates and CDSs: Applications to pairs trading strategy,"This study examines the cointegration relationship between multiple credit default swap (CDS) spreads by constructing the cointegrated hazard rate model, which assumes the structure of the vector error correction model (VECM) in the drift term of hazard rate processes. We merge the cointegration nature into the framework of arbitrage-free pricing and thereby derive the theoretical spread formula of multiple cointegrated CDSs. For the estimation of hazard rate dynamics, we develop a Bayesian statistical inference method combined with the numerical ordinary differential equation solver because the theoretical CDS spread cannot be expressed in closed form. In the empirical study of Japanese corporate CDSs, we find that the overall term structures of cointegrated CDSs can be explained by a simple two-dimensional VECM of cointegrated hazard rates. Furthermore, we study the pairs trading strategy of CDSs. © 2023 Elsevier B.V., All rights reserved.","Kato, K.; Nakamura, N.",2023,10.1016/j.physa.2023.128489,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147196257&doi=10.1016%2Fj.physa.2023.128489&partnerID=40&md5=6700306a9980b3c616b999d06fa79817,scopus,"This study investigates the cointegration relationship between credit default swap (CDS) spreads using a cointegrated hazard rate model, which incorporates a vector error correction model (VECM) into hazard rate processes. The authors derive a theoretical spread formula for multiple cointegrated CDSs within an arbitrage-free pricing framework. A Bayesian inference method is used to estimate hazard rate dynamics. Empirical analysis of Japanese corporate CDSs suggests a two-dimensional VECM can explain the term structures. The study also explores a pairs trading strategy for CDSs.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:54:50.937122
06de698e287ecd19,"Collateralizable wealth, asset returns, and systemic risk: International evidence","Purpose - The purpose of this chapter is to assess the role of collateralizable wealth and systemic risk in explaining future asset returns. Methodology/approach - To test this hypothesis, the chapter uses the residuals of the trend relationship among housing wealth and labor income to predict both stock returns and government bond yields. Specifically, it shows that nonlinear deviations of housing wealth from its cointegrating relationship with labor income, hwy, forecast expected future returns. Findings - Using data for a set of industrialized countries, the chapter finds that when the housing wealth-to-income ratio falls, investors demand a higher risk premium for stocks. As for government bond returns: (i) when they are seen as a component of asset wealth, investors react in the same manner and (ii) if, however, investors perceive the increase in government bond returns as signaling a future rise in taxes or a deterioration of public finances, then they interpret the fall in the housing wealth-to-income ratio as a fall in future bond premia. Finally, this work shows that the occurrence of crisis episodes amplifies the transmission of housing market shocks to financial markets. Originality/value of chapter - These findings are novel. They also open new and challenging avenues for understanding the dynamics of the relationship between the housing sector, stock market and government bond developments, and the banking system. Copyright © 2013 by Emerald Group Publishing Limited. © 2014 Elsevier B.V., All rights reserved.","Sousa, R.M.",2010,10.1108/s1571-0386(2010)0000020006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897585761&doi=10.1108%2FS1571-0386%282010%290000020006&partnerID=40&md5=369313bd78587f6aa5380ba6033b921d,scopus,"This chapter investigates the relationship between collateralizable wealth, systemic risk, and future asset returns, using housing wealth and labor income data from industrialized countries. It finds that deviations in the housing wealth-to-income ratio predict stock returns and government bond yields, with housing market shocks amplifying transmission to financial markets during crises. The study utilizes nonlinear deviations to forecast returns.",False,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:54:53.809798
8ac5d4f4177d3e3a,Common Risk Factors in Cryptocurrency,"We find that three factors-cryptocurrency market, size, and momentum-capture the cross-sectional expected cryptocurrency returns. We consider a comprehensive list of price- and market-related return predictors in the stock market and construct their cryptocurrency counterparts. Ten cryptocurrency characteristics form successful long-short strategies that generate sizable and statistically significant excess returns, and we show that all of these strategies are accounted for by the cryptocurrency three-factor model. Lastly, we examine potential underlying mechanisms of the cryptocurrency size and momentum effects.","Liu, Yukun; Tsyvinski, Aleh; Wu, X., I",2022,10.1111/jofi.13119,,wos,"This study identifies three key factors (cryptocurrency market, size, and momentum) that explain expected returns in the cryptocurrency market. It tests ten cryptocurrency characteristics for their predictive power, finding that all are captured by the three-factor model, and explores the mechanisms behind the size and momentum effects.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:54:55.640972
12a420d5f76f8b7d,Comparing Machine Learning Models for Short-Term U.S. Treasury Yield Forecasting,"This study examines historical trends in the U.S. 10-year Treasury yield and evaluates the effectiveness of four machine learning models, linear regression, decision tree, random forest, and multi-layer perceptron (MLP) neural networks, for short-term yield forecasting. As a key benchmark in global financial markets, the 10-year Treasury yield is influenced by multiple economic factors, including core inflation, the federal funds rate, GDP growth, and the U.S. Federal government's debt growth rate. Leveraging historical data from the Federal Reserve Economic Database (FRED), this study develops predictive models to assess the impact of these factors on yield fluctuations. Empirical results indicate that the random forest model outperforms the other approaches, achieving the lowest mean squared error (MSE) and mean absolute error (MAE), alongside an R2 of 0.6073. This suggests its superior ability to capture nonlinear relationships in yield movements. The decision tree model also demonstrates competitive accuracy but is more susceptible to overfitting. Conversely, linear regression provides useful interpretability but struggles to capture complex economic interactions, leading to lower predictive accuracy. Despite its potential for handling nonlinear dependencies, the MLP model underperforms compared to the random forest, yielding an R2 of 0.5058. The findings underscore the advantages of machine learning, particularly ensemble-based methods, in short-term Treasury yield forecasting. © 2025 Elsevier B.V., All rights reserved.","Wang, M.Y.-F.; Wang, Y.-F.",2025,10.1002/cpe.70265,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105015141371&doi=10.1002%2Fcpe.70265&partnerID=40&md5=47ea3ed053791be37d1bc13f1326b377,scopus,"This study compares four machine learning models (linear regression, decision tree, random forest, MLP) for short-term U.S. 10-year Treasury yield forecasting, using historical data and economic factors. The random forest model showed the best performance, indicating the effectiveness of ensemble methods for this task.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-28T09:54:58.824837
2fff7c85c30fc367,Comparing forecasting ability of parametric and non-parametric methods: An applications with Canadian monthly interest rates,"The primary objective of this article is to compare the forecasting ability of some recent parametric and non-parametric estimation methods by using monthly Canadian interest rate data between 1964:1-1999:1. The two-factor continous time term structure model of Brennan and Schwartz was estimated where the first factor represents the short rate and the second factor the long rate using the continuous time estimation procedures developed by Bergstrom. The interest rates using the multi-variate GARCH model developed by Engle and Kroner, and two non-parametric estimation methods namely, non-parametric kernel smoothing and the artificial neural networks was modelled. For the short-term rates, it has been found that, the Bergstrom's method and the artificial neural networks model have marginally better forecasting performance than that of the linear benchmark. For the long-term rates, none of the methods produced better forecasting precision than that of the benchmark. © 2004 Elsevier Science B.V., Amsterdam. All rights reserved.","Saltoǧlu, B.",2003,10.1080/09603100110111259,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037359312&doi=10.1080%2F09603100110111259&partnerID=40&md5=0d62d541ef5b9a7cbdf61bb322a0b625,scopus,"This article compares the forecasting ability of parametric (Brennan and Schwartz, multi-variate GARCH) and non-parametric (kernel smoothing, artificial neural networks) methods using Canadian monthly interest rate data from 1964 to 1999. For short-term rates, the Brennan and Schwartz method and artificial neural networks showed slightly better forecasting performance than a linear benchmark. For long-term rates, no method outperformed the benchmark.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-28T09:55:10.016619
502c599f9f5a60b2,Comparison procedure of predicting the time to default in behavioural scoring,"The paper deals with the problem of predicting the time to default in credit behavioural scoring. This area opens a possibility of including a dynamic component in behavioural scoring modelling which enables making decisions related to limit, collection and recovery strategies, retention and attrition, as well as providing an insight into the profitability, pricing or term structure of the loan. In this paper, we compare survival analysis and neural networks in terms of modelling and results. The neural network architecture is designed such that its output is comparable to the survival analysis output. Six neural network models were created, one for each period of default. A radial basis neural network algorithm was used to test all six models. The survival model used a Cox modelling procedure. Further, different performance measures of all models were discussed since even in highly accurate scoring models, misclassification patterns appear. A systematic comparison '3 + 2 + 2' procedure is suggested to find the most effective model for a bank. Additionally, the survival analysis model is compared to neural network models according to the relative importance of different variables in predicting the time to default. Although different models can have very similar performance measures they may consist of different variables. The dataset used for the research was collected from a Croatian bank and credit customers were observed during a 12-month period. The paper emphasizes the importance of conducting a detailed comparison procedure while selecting the best model that satisfies the users' interest. © 2008 Elsevier Ltd. All rights reserved. © 2009 Elsevier B.V., All rights reserved.","Šarlija, N.; Benšíc, M.; Zekić-Sušac, M.",2009,10.1016/j.eswa.2008.11.042,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60849137198&doi=10.1016%2Fj.eswa.2008.11.042&partnerID=40&md5=c735efcf3fe984ca06b4c187f150ef0d,scopus,"This paper compares survival analysis and neural networks for predicting time to default in credit behavioral scoring. It details the methodology, including the design of neural network architectures and the use of Cox modeling for survival analysis. The study evaluates model performance using various measures and suggests a systematic comparison procedure. It also examines the relative importance of variables in prediction and uses data from a Croatian bank with a 12-month observation period.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:55:12.984828
435757a2aac6a115,Computer-aided resilience: Advanced techniques for disaster management in smart urban environments,"This research paper explores innovative contributions to the field of disaster management in smart urban environments, with a particular focus on integrating advanced computer-aided techniques, specifically GRU-CNN. Three key contributions are highlighted: (1) the development of dynamic risk assessment algorithms utilizing GRU-CNN for real-time analysis and predictive modeling, enabling proactive disaster mitigation; (2) the establishment of an integrated sensor network infrastructure for early warning systems, leveraging various sensors and GRU-CNN-based data analytics to detect and respond to potential disasters at their nascent stages; and (3) the implementation of human-centric resilience planning, utilizing GRU-CNN-based computer-aided tools to simulate disaster scenarios and engage communities in preparedness efforts. The dynamic risk assessment algorithms presented in this paper, powered by GRU-CNN, enable continuous monitoring and analysis of diverse data sources, fostering a proactive approach to disaster preparedness. The integrated sensor network architecture enhances early warning capabilities, allowing for timely responses to emerging threats through the application of GRU-CNN methodologies. Furthermore, the human-centric resilience planning approach introduces virtual simulations using GRU-CNN to model disaster scenarios, facilitating the testing and refinement of strategies in a risk-free environment. This approach not only ensures the adaptability of plans but also engages and educates communities, fostering a culture of preparedness, with GRU-CNN playing a pivotal role in the process. Through these contributions, this research paper seeks to advance the discourse on disaster management in smart urban environments, emphasizing the integration and effectiveness of GRU-CNN in enhancing resilience and response strategies. By leveraging cutting-edge GRU-CNN technologies and prioritizing community involvement, the proposed techniques aim to provide a more robust and adaptive response to the challenges posed by natural and man-made disasters in urban settings. © 2024 Elsevier B.V., All rights reserved.","Li, R.; Di, Y.; Tian, H.",2024,10.1016/j.scs.2024.105437,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192164396&doi=10.1016%2Fj.scs.2024.105437&partnerID=40&md5=5501812a88b27a6182b0b861baf448f2,scopus,"This paper presents advanced computer-aided techniques, specifically GRU-CNN, for disaster management in smart urban environments. It details three contributions: dynamic risk assessment algorithms for real-time analysis and prediction, an integrated sensor network for early warning systems using GRU-CNN data analytics, and human-centric resilience planning with GRU-CNN tools for scenario simulation and community engagement. The research aims to enhance urban resilience and response strategies through these GRU-CNN-powered methods.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:55:15.841114
d55b2643487b31d3,Computing Arbitrage-Free Yields in Multi-Factor Gaussian Shadow Rate Term Structure Models,"This paper develops an approximation to arbitrage-free bond yields in Gaussian shadow rate term structure models. In this class of models, yields are constrained to be above an effective lower bound, thus rendering standard bond pricing methods inapplicable. I propose approximating the nonlinear relationship between yields and state variables using moments of the censored normal distribution. In an empirical application, this approximation technique is accurate to within a fraction of a basis point. As I show, minimizing the yield approximation error is crucial for model estimation as even seemingly small errors can lead to economically meaningful inference biases. © 2024 Elsevier B.V., All rights reserved.","Priebsch, M.A.",2023,10.1142/s2010139223500131,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181668998&doi=10.1142%2FS2010139223500131&partnerID=40&md5=fcef3acf1d58d664f9d560482b5d0465,scopus,"This paper presents an approximation method for calculating arbitrage-free bond yields within Gaussian shadow rate term structure models. These models impose a lower bound on yields, necessitating a novel approach. The authors use moments of the censored normal distribution to approximate the nonlinear yield-state variable relationship, demonstrating its accuracy in an empirical test and highlighting the importance of minimizing approximation errors to avoid inference biases.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:55:18.681200
8126b048e769df74,Condition Prediction for Existing Educational Facilities Using Artificial Neural Networks and Regression Analysis,"Infrastructural assets such as roads, bridges, and buildings make a considerable contribution to national economies. These assets deteriorate due to aging, environmental conditions, and other external factors. Maintaining the performance of an asset in line with rational repair strategies represents a considerable challenge for decision-makers, who may not pay attention to developing adequate maintenance plans or leave the assets unmaintained. Worldwide, organizations are under pressure to ensure the sustainability of their assets. Such organizations may burden their treasury with random maintenance operations, especially with a limited budget. This research aims to develop a generalized condition assessment approach to monitor and evaluate existing facility elements. The proposed approach represents a methodology to determine the element condition index (CI). The methodology is reinforced with an artificial neural network (ANN) model to predict the element deterioration. The performance of this model was evaluated by comparing the obtained predicted CIs with ordinary least squares (OLS) regression model results to choose the most accurate prediction technique. A case study was applied to a group of wooden doors. The ANN model showed reliable results with R2 values of 0.99, 0.98, and 0.99 for training, cross-validation, and testing sets, respectively. In contrast, the OLS model R2 value was 1.00. These results show the high prediction capability of both models with an advantage to the OLS model. Applying this approach to different elements can help decision-makers develop a preventive maintenance schedule and provide the necessary funds.",,2022,10.3390/buildings12101520,,proquest,"This research proposes a generalized condition assessment approach for existing educational facilities, using Artificial Neural Networks (ANN) and Ordinary Least Squares (OLS) regression to predict element deterioration and calculate a Condition Index (CI). The ANN model demonstrated high accuracy (R2 values of 0.99 for training, cross-validation, and testing), while the OLS model achieved an R2 of 1.00, indicating a slight advantage for OLS in this case study involving wooden doors. The approach aims to aid decision-makers in developing preventive maintenance schedules and allocating funds.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:55:23.256833
66ceac4ab63fecab,Constant Proportion Portfolio Insurance Strategy in Southeast European Markets,"Background: In today's highly volatile and unpredictable market conditions, there are very few investment strategies that may offer a certain form of capital protection. The concept of portfolio insurance strategies presents an attractive investment opportunity.Objectives: The main objective of this article is to test the use of portfolio insurance strategies in Southeast European (SEE) markets. A special attention is given to modelling non-risky assets of the portfolio.Methods/Approach: Monte Carlo simulations are used to test the buy-and-hold, the constant-mix, and the constant proportion portfolio insurance (CPPI) investment strategies. A covariance discretization method is used for parameter estimation of bond returns.Results: According to the risk-adjusted return, a conservative constant mix was the best, the buy-and-hold was the second-best, and the CPPI the worst strategy in bull markets. In bear markets, the CPPI was the best in a high-volatility scenario, whereas the buy-and-hold had the same results in low- and medium-volatility conditions. In no-trend markets, the buy-and-hold was the first, the constant mix the second, and the CPPI the worst strategy. Higher transaction costs in SEE influence the efficiency of the CPPI strategy.Conclusions: Implementing the CPPI strategy in SEE could be done by combining stock markets from the region with government bond markets from Germany due to a lack of liquidity of the government bond market in SEE.",,2016,10.1515/bsrj-2016-0005,,proquest,"This article evaluates the Constant Proportion Portfolio Insurance (CPPI) strategy, along with buy-and-hold and constant-mix strategies, in Southeast European (SEE) markets using Monte Carlo simulations. It found that strategy performance varied significantly with market conditions (bull, bear, no-trend) and volatility. The study suggests that higher transaction costs in SEE markets impact CPPI efficiency and proposes combining SEE stock markets with German government bonds due to liquidity issues in SEE bond markets.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:55:25.801057
d4beb98aa69b728a,"Consumption, aggregate wealth and expected stock returns: a quantile cointegration approach","This paper empirically examines the long-run relationship between consumption, asset wealth and labor income (i.e., cay) in the United States through the lens of a quantile cointegration approach. The advantage of using this approach is that it allows for a nonlinear relationship between these variables depending on the level of consumption. We estimate the coefficients using a Phillips–Hansen type fully modified quantile estimator to correct for the presence of endogeneity in the cointegrating relationship. To test for the null of cointegration at each quantile, we apply a quantile CUSUM test. Results show that: (i) consumption is more sensitive to changes in labor income than to changes in asset wealth for the entire distribution of consumption, (ii) the elasticity of consumption with respect to labor income (asset wealth) is larger at the right (left) tail of the consumption distribution than at the left (right) tail, (iii) the series are cointegrated around the median, but not in the tails of the distribution of consumption, (iv) using the estimated cay obtained for the right (left) tail of the distribution of consumption improves the long-run (short-run) forecast ability on real excess stock returns over a risk-free rate.",,2022,10.1515/snde-2020-0059,,proquest,"This paper investigates the long-run relationship between consumption, aggregate wealth, and labor income using a quantile cointegration approach. It finds that consumption is more sensitive to labor income than asset wealth, with varying elasticities across the consumption distribution. Cointegration is observed around the median but not in the tails. The study also suggests that using estimated consumption-income ratios from different tails can improve long-run or short-run forecasts of real excess stock returns.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:55:28.905122
eed3af2446ec0a3a,"Contemporaneous threshold autoregressive models: Estimation, testing and forecasting","This paper proposes a contemporaneous smooth transition threshold autoregressive model (C-STAR) as a modification of the smooth transition threshold autoregressive model surveyed in Terasvirta [1998. Modelling economic relationships with smooth transition regressions. In: Ullah, A., Giles, D.E.A. (Eds.), Handbook of Applied Economic Statistics. Marcel Dekker, New York, pp. 507-552.], in which the regime weights depend on the ex ante probability that a latent regime-specific variable will exceed a threshold value. We argue that the contemporaneous model is well suited to rational expectations applications (and pricing exercises), in that it does not require the initial regimes to be predetermined. We investigate the properties of the model and evaluate its finite-sample maximum likelihood performance. We also propose a method to determine the number of regimes based on a modified Hansen [1992. The likelihood ratio test under nonstandard conditions: testing the Markov switching model of GNP. Journal of Applied Econometrics 7, S61-S82.] procedure. Furthermore, we construct multiple-step ahead forecasts and evaluate the forecasting performance ofthe model. Finally, an empirical application of the short term interest rate yield is presented and discussed. (c) 2006 Elsevier B.V. All rights reserved.","Dueker, Michael J.; Sola, Martin; Spagnolo, Fabio",2007,10.1016/j.jeconom.2006.10.022,,wos,"This paper introduces the Contemporaneous Smooth Transition Threshold Autoregressive (C-STAR) model, an extension of existing STAR models. The C-STAR model's regime weights are influenced by the ex ante probability of a latent regime-specific variable exceeding a threshold. This model is particularly suitable for rational expectations and pricing applications as it doesn't necessitate predetermined initial regimes. The paper examines the model's properties, evaluates its maximum likelihood performance in finite samples, and proposes a method for determining the number of regimes using a modified Hansen procedure. It also covers multi-step ahead forecasting and presents an empirical application to short-term interest rate yields.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:55:45.577437
e5a46baf011f5c8b,Corrigendum: Bond Risk Premiums with Machine Learning,"In this note we revisit the empirical results in Bianchi, Büchner, and Tamoni (2020) after correcting for using information not available at the time the forecast was made. Although we note a decrease in out-of-sample $R^2$, the revised analysis confirms that bond excess return predictability from neural networks remains statistically and economically significant.",,2021,10.1093/rfs/hhaa098,,proquest,"This corrigendum revisits empirical results on bond risk premiums, correcting for look-ahead bias. The revised analysis confirms that neural networks significantly predict bond excess returns, despite a decrease in out-of-sample R-squared.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-28T09:55:49.425621
078ff19bcec16a24,Cost-benefit analysis in a climate of change: setting social discount rates in the case of Ireland,"The global practice of Cost-Benefit Analysis (CBA), to analyse the welfare impacts of public investments, has undergone profound changes in recent years. The reforms in general practice have primarily been driven by the discussions of the implications of climate change and environmental degradation. Central to the discussion has been the social discount rate, used to value future costs and benefits in the present, and also the dual discount rates for ""environmental goods"", as goods that are of no, or of risky substitution. Official rates, in many nations, are calculated using the ""Ramsey"" formula. The literature has explored the relevant factors in this formula, but with less attention paid to the selection of the rate of future growth in consumption, or to the setting of dual discount rates in national practice guidance. Through considering the case of Ireland, this study demonstrates that the selection of growth rates in consumption, in the context of future uncertainty, requires the use of plausible scenarios, rather than historical trends or forecasts. By employing economic scenarios, alongside established values for the other factors, the main discount rate for Ireland is calculated in a range of 1.7 to 2.8 per cent. Seperately, a dual discount rate, for capital that cannot be replaced, is estimated at ≤1.3 per cent. The main discount rate is validated by comparison against discount rates found in the literature, applied in other comparable nations, and by the rate estimated from the real yield on government bonds. All four independent lines of evidence support the range estimated. This demonstrates that the Irish government's estimated discount rate, of 4.0 per cent, is not credible, and needs reduction, alongside introduction of dual discounting.",,2021,10.3934/gf.2021010,,proquest,"This study analyzes the social discount rate for Ireland, considering climate change implications. It argues for the use of plausible scenarios for future consumption growth and proposes a dual discount rate for non-replaceable capital. The calculated main discount rate ranges from 1.7% to 2.8%, and the dual rate is estimated at 1.3%. These findings challenge the Irish government's current 4.0% rate and advocate for its reduction and the adoption of dual discounting.",False,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:55:54.505298
1f799a63b4ffb64c,Credibilistic risk aversion,"In the probabilistic risk aversion approach, risks are presumed as random variables with known probability distributions. However, in some practical cases, for example, due to the absence of historical data, the inherent uncertain characteristic of risks or different subject judgements from the decision-makers, risks may be hard or not appropriate to be estimated with probability distributions. Therefore, the traditional probabilistic risk aversion theory is ineffective. Thus, in order to deal with these cases, we suggest measuring these kinds of risks as fuzzy variables, and accordingly to present an alternative risk aversion approach by employing credibility theory. In the present paper, first, the definition of credibilistic risk premium proposed by Georgescu and Kinnunen [Fuzzy Inf. Eng., 2013, 5, 399–416] is revised by taking the initial wealth into consideration, and then a general method to compute the credibilistic risk premium is provided. Secondly, regarding the risks represented with the commonly used LR fuzzy intervals, a simple calculation formula of the local credibilistic risk premium is put forward. Finally, in a global sense, several equivalent propositions for comparative risk aversion under the credibility measurement are provided. Illustrated examples are presented to show the applicability of the theoretical findings. © 2017 Elsevier B.V., All rights reserved.","Liu, Y.; Zhou, J.; Pantelous, A.A.",2017,10.1080/14697688.2016.1264617,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008225393&doi=10.1080%2F14697688.2016.1264617&partnerID=40&md5=553152a688730680f0ca8955b348c249,scopus,"This paper proposes an alternative risk aversion approach using credibility theory to measure risks represented as fuzzy variables, addressing limitations of traditional probabilistic risk aversion when probability distributions are unknown or inappropriate. It revises the definition of credibilistic risk premium, provides a general method for its computation, and offers a simple formula for LR fuzzy intervals. The paper also presents equivalent propositions for comparative risk aversion under credibility measurement and includes illustrative examples.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:55:56.665427
a8284129b0b72d35,Credit rating algorithm of corporate bonds based on Gaussian process mixture model and improved K-means,"The primary challenge in credit analysis revolves around uncovering the correlation between repayment terms and yield to maturity, constituting the interest rate term structure-an essential model for corporate credit term evaluation. Presently, interest rate term structures are predominantly examined through economic theoretical models and quantitative models. However, predicting treasury bond yields remains a challenging task for both approaches. Leveraging the clustering analysis algorithm theory and the attributes of an insurance company’s customer database, this paper enhances the K-means clustering algorithm, specifically addressing the selection of initial cluster centers in extensive sample environments. Utilizing the robust data fitting and analytical capabilities of the Gaussian process mixture model, the study applies this methodology to model and forecast Treasury yields. Additionally, the research incorporates customer credit data from a property insurance company to investigate the application of clustering algorithms in the analysis of insurance customer credit. © 2024 Elsevier B.V., All rights reserved.","Xia, W.",2023,10.61091/jcmcc117-14,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184140053&doi=10.61091%2Fjcmcc117-14&partnerID=40&md5=db4b7e03f54cd2d82f22b94ad0140e56,scopus,"This paper proposes an enhanced K-means clustering algorithm for selecting initial cluster centers in large datasets, applied to a Gaussian process mixture model for forecasting Treasury yields. It also explores the use of clustering algorithms for analyzing insurance customer credit data.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:56:00.361248
a4d80f5bda510793,Crude oil price forecasting: a biogeography-based optimization approach,"The importance of crude oil in the world economy has made it imperative for efficient models to be designed for predicting future prices. This paper proposes an alternative approach based on a time series and biogeography-based optimization (BMMR-BBO) for the estimation of the West Texas Intermediate (WTI) crude oil price. To evaluate the forecasting ability of the presented model, we compared its performance with those of time series functions. The results of the experiment showed that BMMR-BBO performed better than the other methods and is a fairly good option for crude oil price prediction. The proposed model can be useful in the formulation of policies related to international crude oil price estimations, development plans, and industrial production.","Dehghani, Hesam; Zangeneh, Mahsa",2018,10.1080/15567249.2018.1501121,,wos,"This paper proposes a new model, BMMR-BBO, which combines time series analysis with biogeography-based optimization to forecast West Texas Intermediate (WTI) crude oil prices. The model outperformed traditional time series methods in experiments and is suggested for policy formulation related to oil prices.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:56:04.296987
2bd7bf35284a9fc0,Daily Stock Returns Characteristics and Forecastability,"While stock prices and economic activity are interrelated in a nation, they are not coincident with each other. Stock prices are a leading economic indicator of the United States of America's (U.S.A. 's) economy. An economic variable that influences stock market prices is interest rates through an inverse relationship. The changes in stock prices (or stock returns) are generally caused by the demand for stocks. This paper reports on a study that investigates the underlying spectral and time-frequency characteristics of daily Standard and Poor's (S&P) 500, Dow Jones Industrial Average (DJIA), and National Association of Securities Dealers Automated Quotations (NASDAQ) composite stock returns, and changes in interest rate (namely, inverted 3-month Treasury bill). The study thereafter compared these findings with those obtained in a previous study by Joseph et al, which focused on monthly stock returns and interest rate data. Subsequent to studying stock returns and changes in interest rate that showed relatively similar spectral and frequency-time characteristics, this study investigated the forecastability of stock returns (in S&P 500, DJIA, and NASDAQ composite) by inverted interest rate (in 3-month Treasury bills) over prediction horizons of five and 30 days with the forecasting period covering the last 13 years. The measures of forecast accuracy used were root mean square error and correlation. The forecasts were favorable in all cases even with simpler neural network models. (c) 2017 The Authors. Published by Elsevier B.V.","Joseph, Anthony; Larrain, Maurice; Turner, Claude",2017,10.1016/j.procs.2017.09.033,,wos,"This study analyzes the spectral and time-frequency characteristics of daily stock returns (S&P 500, DJIA, NASDAQ) and changes in interest rates (3-month Treasury bill). It compares these findings with a previous study on monthly data. The research then investigates the forecastability of stock returns using interest rate changes over 5 and 30-day horizons, employing neural network models and finding favorable forecasts.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:56:42.849429
e1aea4fb54dfddef,De-noising option prices with the wavelet method,"Financial time series are known to carry noise. Hence, techniques to de-noise such data deserve great attention. Wavelet analysis is widely used in science and engineering to de-noise data. In this paper we show, through the use of Monte Carlo simulations, the power of the wavelet method in the de-noising of option price data. We also find that the estimation of risk-neutral density functions and out-of-sample price forecasting is significantly improved after noise is removed using the wavelet method. (C) 2012 Elsevier B.V. All rights reserved.","Haven, Emmanuel; Liu, Xiaoquan; Shen, Liya",2012,10.1016/j.ejor.2012.04.020,,wos,"This paper demonstrates the effectiveness of the wavelet method for de-noising option price data, showing improvements in risk-neutral density estimation and out-of-sample price forecasting through Monte Carlo simulations.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:56:45.337648
4e00c304d3b4af2d,Deep Learning Model for Stock Excess Return Prediction Based on Nonlinear Random Matrix and Esg Factor,"Aiming at the problem that the traditional model has low accuracy in describing stock excess return, in order to further analyze the change law of stock excess, based on the nonlinear random matrix and esg factor theory, the traditional learning model is analyzed, and the corresponding optimized deep learning model is obtained by introducing the single ring theorem and statistical data. Through the analysis and research of related indexes, the change rules of different indexes are obtained, and the optimization model is used to calculate and forecast the excess return of stocks. The results show that the statistics and spectral radius show typical local linear variation with the increase of eigenvalue. The corresponding statistics show a trend of gradual increase. The corresponding spectral radius has a decreasing variation law, and the two curves have obvious symmetry at some eigenvalues. It can be seen from the change curves under different factors that the change trend of the yield curve is mainly affected by the investment factor, while the change rule of the specific value of the yield curve is controlled by the profit factor. This shows that the two factors have the same influence on the stock excess return. The influence of optimized deep learning model on stock excess index has typical linear characteristics, which can be divided into linear increase and linear decline according to different change rules. The basic type has the greatest influence, while the corresponding pattern analysis type has the least influence. Finally, the method of experimental verification is used to verify the stock excess data, and the results show that the optimized deep learning model can better characterize the experimental results. Therefore, the optimized deep learning model based on nonlinear random matrix and esg factor can carry out targeted analysis of different types of stock returns, thus improving research ideas and calculation methods for the application of deep learning model in different fields.",,2022,10.1155/2022/5239493,,proquest,"This study proposes an optimized deep learning model for stock excess return prediction, incorporating nonlinear random matrix theory and ESG factors. The model aims to improve accuracy over traditional methods by analyzing index changes and utilizing the single ring theorem. Results indicate that investment and profit factors influence yield curve trends and specific values, respectively. The deep learning model exhibits linear characteristics in its influence on stock excess returns, with the basic type having the most significant impact. Experimental verification confirms the model's effectiveness in characterizing stock excess data.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:56:52.241302
bfb98861daf4485c,Deep Learning and Machine Learning Insights Into the Global Economic Drivers of the Bitcoin Price,"This study examines the connection between Bitcoin and global factors, including the VIX, the oil price, the US dollar index, the gold price, and interest rates estimated using the Federal funds rate and treasury securities rate, for forecasting analysis. Deep learning methodologies, including LSTM, GRU, CNN, and TFT, with machine learning algorithms such as XGBoost, LightGBM, and SVR, were employed to identify the optimal prediction model for the Bitcoin price. The findings indicate that the TFT model is the most successful predictive approach, with the gold price identified as the most relevant component in determining the Bitcoin price. After the gold indicator, the US dollar index was a substantial factor in the explanation of the Bitcoin price. The TFT model also included regulatory decisions and global events. It was estimated that the Bitcoin price was significantly influenced by the COVID-19 pandemic. After that, global climate events and China mining ban strongly affected the Bitcoin price. These findings indicate that regulatory decisions and global events determine the Bitcoin price in addition to macroeconomic factors. The VAR analysis was employed as a robustness check. The results indicate that gold and oil prices have a strong negative influence on Bitcoin, particularly in the long term. The paper has significant policy implications for investors, portfolio managers, and scholars.","Kose, Nezir; Gur, Yunus Emre; Unal, Emre",2025,10.1002/for.3258,,wos,"This study investigates the relationship between Bitcoin prices and global economic factors (VIX, oil price, US dollar index, gold price, interest rates) using deep learning (LSTM, GRU, CNN, TFT) and machine learning (XGBoost, LightGBM, SVR) models. The TFT model proved most effective, identifying gold price and US dollar index as key drivers. The study also highlights the impact of regulatory decisions, global events (COVID-19, climate events, China mining ban), and macroeconomic factors on Bitcoin prices, with VAR analysis confirming a long-term negative influence from gold and oil prices.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:56:56.121270
beadf3ba10095213,Deep Sequence Modeling: Development and Applications in Asset Pricing,"The authors predict asset returns and measure risk premiums using a prominent technique from artificial intelligence: deep sequence modeling. Because asset returns often exhibit sequential dependence that may not be effectively captured by conventional time-series models, sequence modeling offers a promising path with its data-driven approach and superior performance. In this article, the authors first overview the development of deep sequence models, introduce their applications in asset pricing, and discuss their advantages and limitations. They then perform a comparative analysis of these methods using data on US equities. They demonstrate how sequence modeling benefits investors in general through incorporating complex historical path dependence and that long short-term memory–based models tend to have the best out-of-sample performance. © 2022 Elsevier B.V., All rights reserved.","Cong, L.W.; Tang, K.; Wang, J.; Zhang, Y.",2021,10.3905/jfds.2020.1.053,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126541141&doi=10.3905%2Fjfds.2020.1.053&partnerID=40&md5=bc886caceb37d23d08acc78479f0ddd8,scopus,"This article explores the application of deep sequence modeling, a technique from artificial intelligence, to predict asset returns and measure risk premiums. The authors highlight the advantages of sequence modeling over conventional time-series models for capturing sequential dependence in asset returns. They provide an overview of deep sequence models, discuss their applications and limitations in asset pricing, and conduct a comparative analysis using US equity data. The study demonstrates that long short-term memory (LSTM)-based models generally perform best out-of-sample.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:56:58.833233
827ebc1f4234f963,Deep-learning models for forecasting financial risk premia and their interpretations,"The measurement of financial risk premia, the amount that a risky asset will outperform a risk-free one, is an important problem in asset pricing. The noisiness and non-stationarity of asset returns makes the estimation of risk premia using machine learning (ML) techniques challenging. In this work, we develop ML models that solve the problems associated with risk premia forecasting by separating risk premia prediction into two independent tasks, a time series model and a cross-sectional model, and using neural networks with skip connections to enable their deep neural network training. These models are tested robustly with different metrics, and we observe that our models outperform several existing standard ML models. A known issue with ML models is their ‘black box’ nature, i.e. their opaqueness to interpretability. We interpret these deep neural networks using local approximation-based techniques that provide explanations for our model's predictions. © 2023 Elsevier B.V., All rights reserved.","Lo, A.W.; Singh, M.",2023,10.1080/14697688.2023.2203844,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163601916&doi=10.1080%2F14697688.2023.2203844&partnerID=40&md5=95327ef8a4d77eea1380fa58a81d947c,scopus,"This paper proposes deep learning models for forecasting financial risk premia, addressing challenges of noise and non-stationarity by separating prediction into time series and cross-sectional tasks. The models utilize neural networks with skip connections and are shown to outperform existing ML models. Interpretability of these 'black box' models is achieved through local approximation techniques.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:57:07.233383
0f6fe1c92923c038,"Demand for Information, Uncertainty, and the Response of US Treasury Securities to News","We use clickstream data to show that investors' demand for information about macroeconomic factors affecting the path of future interest rates is a measure of their uncertainty about this path. In particular, an increase in information demand ahead of influential economic announcements affecting investors' beliefs about future interest rates predicts a stronger reaction of U.S. Treasury note yields to these announcements, as it should if information demand positively covaries with uncertainty. This relationship does not vanish after using standard measures of uncertainty as predictors, suggesting that clickstream data contain unique information about investors' uncertainty.","Benamar, Hedi; Foucault, Thierry; Vega, Clara",2021,10.1093/rfs/hhaa072,,wos,"This study utilizes clickstream data to demonstrate that increased investor demand for information regarding macroeconomic factors influencing future interest rates serves as a proxy for uncertainty about those rates. Higher information demand preceding significant economic announcements correlates with a more pronounced reaction in U.S. Treasury note yields, consistent with information demand reflecting uncertainty. This finding persists even when controlling for conventional uncertainty measures, indicating that clickstream data offers distinct insights into investor uncertainty.",False,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:57:09.729216
96b3bcf64aff81eb,Density estimation for nonlinear parametric models with conditional heteroscedasticity,"This article studies density and parameter estimation problems for nonlinear parametric models with conditional heteroscedasticity. We propose a simple density estimate that is particularly useful for studying the stationary density of nonlinear time series models. Under a general dependence structure, we establish the root it consistency of the proposed density estimate. For parameter estimation, a Bahadur type representation is obtained for the conditional maximum likelihood estimate. The parameter estimate is shown to be asymptotically efficient in the sense that its limiting variance attains the Cramer-Rao lower bound. The performance of our density estimate is studied by simulations. (C) 2009 Elsevier B.V. All rights reserved.","Zhao, Zhibiao",2010,10.1016/j.jeconom.2009.09.013,,wos,"This article proposes a density estimation method for nonlinear parametric models with conditional heteroscedasticity, useful for analyzing stationary densities of nonlinear time series. It establishes root-n consistency for the density estimate and obtains a Bahadur-type representation for the conditional maximum likelihood estimate, proving its asymptotic efficiency. Simulation studies are included.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:58:07.585516
e2d80cda8686c72a,Detecting Multiple Structural Breaks in Systems of Linear Regression Equations With Integrated and Stationary Regressors,"In this paper, we propose a two‐step procedure based on the group LASSO estimator in combination with a backward elimination algorithm to detect multiple structural breaks in linear regressions with multivariate responses. Applying the two‐step estimator, we jointly detect the number and location of structural breaks and provide consistent estimates of the coefficients. Our framework is flexible enough to allow for a mix of integrated and stationary regressors, as well as deterministic terms. Using simulation experiments, we show that the proposed two‐step estimator performs competitively against the likelihood‐based approach in finite samples. However, the two‐step estimator is computationally much more efficient. An economic application to the identification of structural breaks in the term structure of interest rates illustrates this methodology.",,2025,10.1111/obes.12666,,proquest,"This paper introduces a two-step method using group LASSO and backward elimination to identify multiple structural breaks in linear regression models with multivariate responses. The method can handle both integrated and stationary regressors and deterministic terms, jointly determining the number and location of breaks while estimating coefficients consistently. Simulations show it is computationally efficient and competitive with likelihood-based methods. An application to the term structure of interest rates demonstrates its use.",True,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:58:10.409003
03db3ea92636934e,Development of hybrid weighted networks of RNN and DBN for facilitating the secure information system in cyber security using meta-heuristic improvement,"As communication and information technologies are integral to everyone’s daily activities, the significance of cybersecurity has become more pronounced due to the growing vulnerability of these technologies to cyber threats. Traditional cyber security systems use various preventive measures to secure the information and trust authentication methods are used to provide the essential security measure against cyber attacks. These methods are efficient and are also equipped to perform in real-world scenarios. However, the conventional cyber security system does not provide essential security against all types of cyber attacks as they are nature-distributed for controlling the systems. Securing these Distributed Control Systems is highly significant for providing a secure and risk-free operation of the connected systems from cyber attacks and other threats. Therefore, a novel method of risk prediction and risk mitigation is developed using the heuristic-based Hybrid Deep Weighted Networks for protecting the data in the information system. The recommended work is based on risk analysis and a cyber security framework built around the information technology security system. This model aimed to design the cyber security system by mitigating all the threats in that particular information system. The main aim is to predict the risk level and mitigate the security threats completely from the system. To achieve this, initially, the data are gathered from different sources and given to the HDWN. The HDWN is developed by combining the Deep Belief Network and a Recurrent Neural Network. These two networks help to predict the risk values of the threat. To attain the enhanced results, the parameters in this model are optimized by using a hybrid algorithm known as African Vultures with Water Wave Optimization, which is developed by combining the Water Wave Optimization algorithm with the African Vulture Optimization Algorithm. Another intention of this model is to mitigate the threat present in the system. Based on the predicted risk value, the system generates warning signals to alert the admin to block the communication. Thus, the threat and risk from the system are predicted and mitigated without interrupting the system’s performance. Finally, the performance validation is performed on the developed model by comparing it with diverse approaches, and the results demonstrate that the proposed model provides impressive outcomes in ensuring data security. © 2025 Elsevier B.V., All rights reserved.","Lakshman Naik, R.L.; Jain, S.; Bairam, M.",2025,10.1007/s11276-025-03955-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003692475&doi=10.1007%2Fs11276-025-03955-x&partnerID=40&md5=d4fbb63b13dd15cc3ea6db266c72f329,scopus,"This paper proposes a novel risk prediction and mitigation method for cybersecurity using a Hybrid Deep Weighted Network (HDWN), which combines Recurrent Neural Networks (RNN) and Deep Belief Networks (DBN). The model is optimized using a hybrid meta-heuristic algorithm (African Vultures with Water Wave Optimization) to predict risk levels and mitigate threats in information systems without interrupting performance. The authors claim their model achieves impressive results in ensuring data security compared to other approaches.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:58:13.649466
e24f1b7a54c54554,"Digital twin-based cyber-physical manufacturing systems, extended reality metaverse enterprise and production management algorithms, and Internet of Things financial and labor market technologies in generative artificial intelligence economics","Research background: Generative artificial intelligence (AI) and machine learning algorithms support industrial Internet of Things (IoT)-based big data and enterprise asset management in multiphysics simulation environments by industrial big data processing, modeling, and monitoring, enabling business organizational and managerial practices. Machine learning-based decision support and edge generative AI sensing systems can reduce persistent labor shortages and job vacancies and power productivity growth and labor market dynamics, shaping career pathways and facilitating occupational transitions by skill gap identification and laborintensive manufacturing job automation by path planning and spatial cognition algorithms, furthering theoretical implications for management sciences. Generative AI fintech, machine learning algorithms, and behavioral analytics can assist multi-layered payment and transaction processing screening with regard to authorized push payment, account takeover, and synthetic identity frauds, flagging suspicious activities and combating economic crimes by rigorous verification processes. Purpose of the article: We show that edge device management functionalities of cloud industrial IoT and virtual robotic simulation technologies configure plant production and route planning processes across cyber-physical production and industrial automation systems in multi-cloud immersive 3D environments, leading to tangible business outcomes by reinforcement learning and convolutional neural networks. Labor-augmenting automation and generative AI technologies can impact employment participation, increase wage and wealth inequality, and lead to potential job displacement and massive labor market disruptions. The deep learning capabilities of generative AI fintech in terms of adaptive behavioral analytics and credit scoring mechanisms can enhance financial transaction behaviors and algorithmic trading returns, identify fraudulent payment transactions swiftly, and improve financial forecasts, leading to customized investment recommendations and well-informed financial decisions. Methods: Machine learning-based study selection process and text mining systematic review management software and tools leveraged include Abstrackr, CADIMA, Colandr, DistillerSR, EPPI-Reviewer, JBI SUMARI, METAGEAR package for R, SluRp, and SWIFT-Active Screener. Such reference management systems are harnessed for methodologically rigorous evidence synthesis, study selection and characteristic extraction, predictive document classification, machine learning-based citation and record screening, bias assessment, article retrieval automation, and document classification and prioritization.Findings & value added: Industrial IoT and 3D augmented reality technologies can create business value by streamlining virtual product and remote asset management across extended reality-based navigation and robotic autonomous systems in smart factory environments by generative AI and machine learning algorithms, articulating business organizational level and theory of management implications. 3D simulation and operational modeling tools can execute and complete complex cognitive task-oriented and knowledge economy jobs, producing first-rate quality outputs swiftly while leading to unemployment spells, labor market disruptions, job displacement losses, and reduced earnings by machine learning clustering and spatial cognition algorithms.Generative AI decentralized finance, interoperable blockchain networks, cash flow management tools, and asset tokenization can mitigate fraud risks, enable digital fund and crypto investing servicing, and automate treasury operations by integrating real-time payment capabilities, routing and configurable workflows, and lending and payment technologies.","Lazaroiu, George; Gedeon, Tom; Rogalska, Elzbieta; Valaskova, Katarina; Nagy, Marek; Musa, Hussam; Zvarikova, Katarina; Poliak, Milos; Horak, Jakub; Cretoiu, Raluca Ionela; Krulicky, Tomas; Ionescu, Luminita; Popa, Catalin; Hurloiu, Lacramioara Rodica; Nistor, Filip; Avram, Laurentia Georgeta; Braga, Viorica",2024,10.24136/oc.3183,,wos,"This article explores the integration of generative AI, machine learning, digital twins, extended reality, and IoT within manufacturing and financial systems. It discusses how these technologies can optimize production, manage assets, and improve financial operations, including fraud detection and algorithmic trading. The paper also addresses the potential impacts on the labor market, such as job displacement and skill gaps, and outlines the systematic review methods used to analyze relevant literature.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:58:34.646618
69d40c22a0ca58a8,Dimension Reduction via Penalized GLMs for Non-Gaussian Response: Application to Stock Market Volatility,"We fit U.S. stock market volatilities on macroeconomic and financial market indicators and some industry level financial ratios. Stock market volatility is non-Gaussian distributed. It can be approximated by an inverse Gaussian (IG) distribution or it can be transformed by Box–Cox transformation to a Gaussian distribution. Hence, we used a Box–Cox transformed Gaussian LASSO model and an IG GLM LASSO model as dimension reduction techniques and we attempted to identify some common indicators to help us forecast stock market volatility. Via simulation, we validated the use of four models, i.e., a univariate Box–Cox transformation Gaussian LASSO model, a three-phase iterative grid search Box–Cox transformation Gaussian LASSO model, and both canonical link and optimal link IG GLM LASSO models. The latter two models assume an approximately IG distributed response. Using these four models in an empirical study, we identified three macroeconomic indicators that could help us forecast stock market volatility. These are the credit spread between the U.S. Aaa corporate bond yield and the 10-year treasury yield, the total outstanding non-revolving consumer credit, and the total outstanding non-financial corporate bonds.",,2021,10.3390/jrfm14120583,,proquest,"This study applies dimension reduction techniques using penalized Generalized Linear Models (GLMs) to forecast U.S. stock market volatility, which is non-Gaussian. The authors compare Box-Cox transformed Gaussian LASSO models with Inverse Gaussian (IG) GLM LASSO models, validating their performance through simulations and an empirical study. They identify three key macroeconomic indicators (credit spread, consumer credit, corporate bonds) that can help forecast stock market volatility.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:58:38.137343
8d2944af2307c048,Distressed debt prices and recovery rate estimation,"This paper has two purposes. First, it uses distressed debt prices to estimate recovery rates at default. In this regard, estimates are obtained for three recovery rate models: recovery of face value, recovery of Treasury, and recovery of market value. We show that identifying the economic default date, as distinct from the recorded default date, is crucial to obtaining unbiased estimates. The economic default date is defined to be the first date when the market prices the firm's debt as if it has defaulted. An implication is that the standard industry practice of using 30-day post default prices to compute recovery rate yields biased estimates. Second, we construct and estimate a distressed debt pricing model. We use this model to implicitly estimate the parameters of the embedded recovery rate process and to price distressed debt. Our distressed debt pricing model fits market prices well, with an average pricing error of less than one basis point.","Guo, Xin; Jarrow, Robert A.; Lin, Haizhi",2008,10.1007/s11147-009-9029-2,,wos,"This paper estimates recovery rates at default using distressed debt prices and proposes three models for recovery rates: recovery of face value, Treasury, and market value. It highlights the importance of identifying the economic default date over the recorded one for unbiased estimates and suggests that standard industry practices using post-default prices are biased. The study also develops and estimates a distressed debt pricing model, which implicitly estimates recovery rate parameters and prices distressed debt, showing a high degree of accuracy.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:58:40.201284
a913bfc8378c3d14,Do asymmetric and nonlinear adjustments explain the forward premium anomaly?,"This paper explores some of the asymmetries and nonlinearities in an attempt to throw light on the forward premium anomaly, where spot exchange rate returns are typically found to be negatively correlated with the lagged interest rate differential and lead to an apparent rejection of Uncovered Interest Parity (UIP). The approach in this paper is motivated by some recent theoretical literature on the limits to speculation and hysteresis. The paper estimates Logistic Smooth Transition Dynamic Regression (LSTR) models with a variety of transition variables, including the lagged forward premium, monetary and income fundamentals and also variables associated with time varying risk premium, including the conditional variances of some fundamentals. Results are reported for nine different currencies. Many of the estimated LSTR models provide evidence for the existence of an outer regime that is consistent with UIP. Estimation of the standard forward premium regression on observations falling in these regimes across the sample is moderately supportive of UIP holding in the outer regime. A simulation experiment also suggests that an LSTR dgp can produce data consistent with the anomaly. However, parameter estimation issues leads to considerable uncertainty with the estimated transition functions and hence imprecise definitions of regimes. The results are an interesting step in the direction of understanding the nonlinear dimension of the problem without fully resolving the anomaly.",,2006,10.1016/j.jimonfin.2005.10.002,,proquest,"This paper investigates the forward premium anomaly, where spot exchange rate returns are negatively correlated with lagged interest rate differentials, potentially rejecting Uncovered Interest Parity (UIP). Using Logistic Smooth Transition Dynamic Regression (LSTR) models, the study explores asymmetries and nonlinearities, incorporating variables like lagged forward premium, monetary fundamentals, and risk premium indicators. While some results suggest UIP holds in specific 'outer regimes,' the anomaly is not fully resolved due to estimation uncertainties. The findings indicate that nonlinear dynamics might contribute to the anomaly but require further investigation.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:58:45.352993
fff1e1b48fd13413,Do local and global factors impact the emerging markets' sovereign yield curves? Evidence from a data‐rich environment,"This paper investigates the relation between yield curve and macroeconomic factors for 10 emerging sovereign bond markets using the sample from January 2006 to April 2019. To this end, the diffusion indices obtained under four categories (global variables, inflation, domestic financial variables, and economic activity) are incorporated by estimating dynamic panel data regressions together with the yield curve factors. Besides, in order to capture dynamic interaction between yield curve and macroeconomic/financial factors, a panel vector autoregressive (VAR) analysis based on the system generalized method of moments (GMM) approach is utilized. Empirical results suggest that the level factor responds to shocks originated from inflation, domestic financial variables, and global variables. Furthermore, the slope factor is affected by shocks in global variables, and the curvature factor appears to be influenced by domestic financial variables. We also show that macroeconomic/financial factors captures significant predictive information over yield curve factors by running individual country factor‐augmented predictive regressions and variable selection algorithms such ridge regression, least absolute shrinkage operator (LASSO), and Elastic Net. Our findings have important implications for policymakers and fund managers by explaining the underlying forces of movements in the yield curve and forecasting accurately dynamics of yield curve factors.",,2021,10.1002/for.2763,,proquest,"This paper examines the relationship between sovereign yield curves and macroeconomic factors in 10 emerging markets from 2006 to 2019. Using dynamic panel data regressions and a panel VAR approach, it finds that inflation, domestic financial variables, and global variables impact the level factor of the yield curve. Global variables affect the slope factor, and domestic financial variables influence the curvature factor. The study also demonstrates that macroeconomic/financial factors provide significant predictive information for yield curve factors through factor-augmented predictive regressions and machine learning techniques like ridge regression, LASSO, and Elastic Net.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:58:59.617716
d98996151ddb63ed,Does Real Estate Determine REIT Bond Risk Premia?,"This study is the first to examine the real estate-specific determinants of REIT bond risk premia. Using a dataset of 33,857 U.S. REIT bond yield spreads and 24 explanatory variables, we predict REIT bond yield spreads with a non-parametric artificial neural network algorithm and interpret the model’s predictions using the explainable machine learning method Accumulated Local Effect Plots (ALE). We report evidence of a direct real estate factor for U.S. REIT bond yield spreads proxied by real estate market total return and REIT property type. In addition, we find a property-type diversification risk premium for REIT bonds, indicating that there is no economic benefit in the form of lower cost of bond debt for most property-type diversification at the REIT-level. We argue that this is due to higher management and valuation complexity of diversified REIT portfolios. This study’s findings have relevant implications for REIT portfolio strategy and REIT capital structure decisions, as we show that specialized REITs generally have lower bond debt costs compared to diversified REITs. Moreover, a better understanding of the drivers influencing REIT bond risk premia helps investors to effectively manage bond portfolio risks. © 2025 Elsevier B.V., All rights reserved.","Kozak, J.; Nagl, C.; Nagl, M.; Beracha, E.; Schaefers, W.",2025,10.1007/s11146-025-10018-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007860488&doi=10.1007%2Fs11146-025-10018-7&partnerID=40&md5=745049e004467b65713fb8985dcc9bcd,scopus,"This study investigates the real estate determinants of REIT bond risk premia using a dataset of 33,857 U.S. REIT bond yield spreads. It employs a non-parametric artificial neural network and Accumulated Local Effect Plots (ALE) to analyze the data. The findings suggest that real estate market total return and REIT property type directly influence REIT bond yield spreads. Additionally, a property-type diversification risk premium exists, implying limited debt cost benefits for diversified REITs due to increased complexity. The study concludes that specialized REITs tend to have lower bond debt costs than diversified REITs, offering insights for REIT portfolio strategy and capital structure decisions.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:59:03.785284
9d67744e2a06c457,"Dopamine D2 −141C Ins/Del and Taq1A polymorphisms, body mass index, and prediction error brain response","The prediction error model is a widely used paradigm that is conceptually based on neuronal dopamine function. However, whether dopamine receptor gene alleles contribute to human neuroimaging prediction error results is uncertain. Recent research implicated the dopamine D2 receptor in behavior response during a prediction error paradigm and we expected that polymorphisms of that receptor would contribute to prediction error brain response. In this study, healthy female participants in the early follicular phase of the menstrual cycle underwent a taste prediction error paradigm during functional magnetic resonance imaging. Participants were also genotyped for dopamine receptor polymorphisms. Our data suggest that the dopamine D2 receptor −141C Ins/Del and Taq1A polymorphisms together with body mass index selectively explain putamen prediction error response. This was true using a region of interest analysis as well as for a whole-brain analysis (FWE corrected). Polymorphisms for dopamine D1 or D4 receptors, dopamine transporter, or COMT did not significantly contribute to prediction error activation. The prediction error model is a computational reward-learning paradigm that is important in psychiatric research and has been associated with dopamine. The results from this study indicate that dopamine D2 receptor polymorphisms together with body mass index are important determinants to include in research that tests prediction error response of the brain. Psychiatric disorders are frequently associated with elevated or reduced body weight. Adding BMI to genetic information in brain-imaging studies that use reward and the prediction error paradigm may be important to increase validity and reliability of results.",,2018,10.1038/s41398-018-0147-1,,proquest,"This study investigated the relationship between dopamine D2 receptor gene polymorphisms (D2 -141C Ins/Del and Taq1A), body mass index (BMI), and brain response to prediction errors in healthy female participants using fMRI during a taste prediction error paradigm. The findings suggest that D2 polymorphisms and BMI collectively predict putamen prediction error response, highlighting their importance in brain imaging studies of reward and prediction error. Other dopamine-related gene polymorphisms did not show significant contributions.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:59:05.705335
9702a4f5490bd692,"Economic costs of Fusarium Head Blight, scab and deoxynivalenol","Fusarium Head Blight (FHB) has led to major economic costs for wheat and barley producers. Grain products and feed grain contaminated with deoxynivalenol (DON) (commonly known as vomitoxin) are subject to Food and Drug Administration advisory limits and as a result end-users place restrictions on their use. This has led to steep price discounts, as well as higher risks for producers and grain merchandisers. Varietal research has led to development of varieties that are resistant or moderately resistant to FHB. Studies indicate combinations of genetic resistance, fungicides and some management practices (combine settings, tillage practices, etc.) can be used to decrease economic costs due to FHB. The purpose of this study was to estimate the economic costs of scab. To do so we developed several economic models, analysed extensive data and conducted surveys of wheat flour millers, barley maltsters, and grain handlers. A detailed assessment of costs indicates the most important costs accrued by the wheat and barley industries were the risk premium paid to induce adoption of DON reducing technologies and the value of yield forgone. These were followed by the direct costs of fungicide, added shipping costs, testing and segregation and discounts.",,2018,10.3920/wmj2017.2204,,proquest,"This study estimates the economic costs of Fusarium Head Blight (FHB) in wheat and barley, focusing on the impact of deoxynivalenol (DON) contamination. It analyzes the costs associated with price discounts, risk premiums, yield losses, fungicide use, and segregation. The research suggests that combining genetic resistance, fungicides, and management practices can mitigate these economic impacts. The study utilized economic models, extensive data analysis, and surveys of industry stakeholders.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:59:07.537337
10e22f6f446668c0,Economic forces and seasonality in secirity returns,"This article finds strong seasonal behavior in the innovations for three Canadian macroeconomic variables (industrial production, unexpected inflation and GDP). An APT model is estimated as a restricted nonlinear multivariate regression system using seven macroeconomic variables, various residual market factor (RMF) proxies, and the returns on fifty size related portfolios of equities that traded on the Toronto Stock Exchange (TSE). As in Chen, Roll and Ross (1986), five macrofactors (lagged industrial production, lagged GDP, term structure, unexpected inflation, and risk premium) have significantly priced risk premia. The risk premia are insignificant for RMF based on two value weighted indices, and marginally significant (0.10 level) for the RMF based on an equally weighted index, which is somewhat consistent with McElroy and Burmeister (1988) and Brown and Otsuki (1989). The significance of the RMF risk premium appears not to be robust to whether portfolios or individual securities are used in the estimations. The significance of the estimated risk premia for the macrofactors also appear not to be robust to the number of portfolios (equations) used in the estimations. Unlike the risk premia estimates for the RMF, those for the other macrofactors are generally unaffected by the inclusion of a January dummy. This implies that the January seasonal remains a market phenomenon that requires further study. © 1992 Kluwer Academic Publishers. © 2007 Elsevier B.V., All rights reserved.","Kryzanowski, L.; Zhang, H.",1992,10.1007/bf00586436,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039122166&doi=10.1007%2FBF00586436&partnerID=40&md5=7c54072676b2de26142e5bce6c6f16dc,scopus,"This study investigates seasonal patterns in Canadian macroeconomic variables and their impact on security returns. Using an APT model with seven macroeconomic variables and equity portfolio returns from the Toronto Stock Exchange, it identifies five macrofactors with significant risk premia. The study also examines the role of residual market factors (RMF) and finds that the January seasonal effect appears to be a market-wide phenomenon.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:59:08.858048
6af02ce8367645c4,Effectiveness of family policy in Russia: Evidence-based approach,"Family policy in Russia, is based on a “narrow” demographic interpretation that neglects policy effectiveness and impact of state support on fertility indicators. This gap can be addressed using the evidence-based approach, which embraces both the influence of public policies on fertility, and human capital. The paper discusses the theoretical underpinnings of policy based on the Becker-Barreau and Baldrin-Jones concepts. We show the importance of incorporating “Big Data” into family policy analysis to address the problem of data completeness and analytical information for family policy needs. We rely on A. Sagradov’s ideas about quantitative determination of population reproduction patterns with non-demographic processes, including institutional changes and transformation of economic mechanisms of family policy. We estimated the demographic result per unit of budget expenditures in Russia (based on empirical data from EMISS and the Federal Treasury for 86 regions from 2011 to 2021, with a breakdown by months). The “random forest” method is used to identify the key factors influencing the results of the machine learning model, and to demonstrate the significance of parameters for assessing the socio-economic effectiveness of family policy in Russia. The research findings indirectly confirm the pro-natalist nature of family policy in Russia, the effectiveness of which is ensured by economic mechanisms of direct cash payments to the population. The paper concludes with a discussion of the prospects for using an evidence-based approach to family policy in Russia. © 2021 Elsevier B.V., All rights reserved.","Kapoguzov, E.A.; Chupin, R.I.",2021,10.18522/2073-6606-2021-19-3-20-36,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119446410&doi=10.18522%2F2073-6606-2021-19-3-20-36&partnerID=40&md5=2ee5f6c1dca99c380322ca57d563bd35,scopus,"This paper examines the effectiveness of Russian family policy using an evidence-based approach, incorporating theoretical concepts and empirical data. It highlights the need for ""Big Data"" and quantitative analysis, utilizing the ""random forest"" method to identify key factors influencing policy outcomes. The findings suggest that Russia's family policy is pro-natalist and effective through direct cash payments.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:59:12.001105
0657bf71f73e0954,Efficient MCMC estimation of some elliptical copula regression models through scale mixtures of normals,"This paper proposes an efficient estimation method for some elliptical copula regression models by expressing both copula density and marginal density functions as scale mixtures of normals (SMN). Implementing these models using the SMN is novel and allows efficient estimation via Bayesian methods. An innovative algorithm for the case of complex semicontinuous margins is also presented. We utilize the facts that copulas are invariant to the location and scale of the margins; all elliptical distributions have the same correlation structure; and some densities can be represented by the SMN. Two simulation studies, one on continuous margins and the other on semicontinuous margins, highlight the favorable performance of the proposed methods. Two empirical studies, one on the US excess returns and one on the Thai wage earnings, further illustrate the applicability of the proposals. © 2019 Elsevier B.V., All rights reserved.","Wichitaksorn, N.; Gerlach, R.; Choy, S.T.B.",2019,10.1002/asmb.2410,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053731638&doi=10.1002%2Fasmb.2410&partnerID=40&md5=f78b5cba81fbb9780f1904c40f0be3e3,scopus,This paper introduces an efficient Bayesian estimation method for elliptical copula regression models by representing both copula and marginal densities as scale mixtures of normals (SMN). It includes a novel algorithm for semicontinuous margins and demonstrates favorable performance through simulation and empirical studies on US excess returns and Thai wage earnings.,True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:59:23.369269
bf8c440f1d08b343,"Elliptical Capital Asset Pricing Models: Formulation, Diagnostics, Case Study with Chilean Data, and Economic Rationale","The capital asset pricing model (CAPM) is often based on the Gaussianity or normality assumption. However, such an assumption is frequently violated in practical situations. In this paper, we introduce the symmetric CAPM considering distributions with lighter or heavier tails than the normal distribution. These distributions are symmetric and belong to the family of elliptical distributions. We pay special attention to the family members related to the normal, power-exponential, and Student-t cases, with the power-exponential distribution being particularly considered, as it has not been explored widely. Based on these cases, the expectation-maximization algorithm can be used to facilitate the estimation of model parameters utilizing the maximum likelihood method. In addition, we derive the leverage and local influence methods to carry out diagnostics in the symmetric CAPM. We conduct a detailed case study to apply the obtained results estimating the systematic risk of the financial assets of a Chilean company with real data. We employ the Akaike information criterion to conclude that the studied models provide better results than the CAPM under Gaussianity.",,2023,10.3390/math11061394,,proquest,"This paper proposes symmetric Capital Asset Pricing Models (CAPM) that accommodate elliptical distributions, which have lighter or heavier tails than the normal distribution. It focuses on power-exponential and Student-t distributions, using the expectation-maximization algorithm for parameter estimation and leverage/local influence methods for diagnostics. A case study with Chilean financial data demonstrates that these models outperform the standard Gaussian CAPM.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:59:26.640719
fd197540678bf733,"Emerging markets’ resource booms and busts, borrowing risk and regime change","Resource booms create real sector and credit expansion in resource-rich countries; however it can also give rise to over-borrowing. If a resource bust hits, the affected economies contract through declining export revenues, and possibly face increased default risk. Thus, both booms and busts can create macroeconomic instability. Using a dynamic growth model, we model regime changes which reveal nonlinear effects of debt on the economy, depending on the level of leveraging. We find a change from stable to unstable dynamics if the external debt to capital ratio rises above a certain threshold. Risk premia are introduced highlighting the state-dependent borrowing costs for the dynamic paths. We study excess debt and over-leveraging as deviations from sustainable ratios. We find country-specific risk premia that are likely associated with booms and busts for the given debt assessment. Our empirical estimates suggest that certain oil-exporting countries are at a heightened risk for debt crises. © 2017 Elsevier B.V., All rights reserved.","Nyambuu, U.; Semmler, W.",2017,10.1016/j.strueco.2017.02.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014406799&doi=10.1016%2Fj.strueco.2017.02.001&partnerID=40&md5=93c2c5125aae33db8872245de3e2f76d,scopus,"This paper uses a dynamic growth model to explore how resource booms and busts in emerging markets affect borrowing risk and can lead to regime changes in economic dynamics. It finds that exceeding a certain external debt to capital ratio threshold can destabilize the economy, and introduces risk premia to account for state-dependent borrowing costs. The study suggests that certain oil-exporting countries face a heightened risk of debt crises.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:59:28.745299
8132ab240b3a2bd6,Enhancing Customer Service Efficiency: Automating Responses to Customer Queries using Natural Language Processing and Radial Basis Function Neural Network (RBFNN),"The insurance sector is progressively embracing Artificial Intelligence (AI) and Natural Language Processing (NLP) to enhance customer service and streamline operations. This research offers a comprehensive framework that seamlessly integrates a chatbot system, empowered by advanced natural language processing techniques, with a Radial Basis Function Neural Network (RBFNN). In this approach, a chatbot serves as the primary user interface, facilitating easy and efficient communication regarding insurance policies and claims. Natural language processing algorithms are employed to interpret user queries, extract vital information, and generate structured responses. However, the key innovation lies in the application of the RBFNN, which is adapted to model and predict various aspects of insurance documents, including policy terms, premium calculations, and claim procedures. The RBFNN's ability to capture intricate data patterns significantly enhances the accuracy and efficiency of document generation. The combined approach accelerates document creation, reduces errors, and enhances the overall customer experience in the insurance domain. The performance of the proposed technique is evaluated in the Python platform and compared with existing approaches. Empirical results and comparisons with traditional methods illustrate the advantages in terms of accuracy and efficiency. The RBFNN achieved an average accuracyof99% across the five folds. This means that the RBFNN was able to correctly generate insurance documents for 99% of the customers in the test set, on average. © 2024 Elsevier B.V., All rights reserved.","Kolambe, S.A.; Kaur, P.",2024,10.62441/nano-ntp.v20is6.64,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203091771&doi=10.62441%2Fnano-ntp.v20iS6.64&partnerID=40&md5=7bed1d5ec424827f558477f89ed58e15,scopus,"This research introduces a framework for automating customer service in the insurance sector by integrating a chatbot with NLP and a Radial Basis Function Neural Network (RBFNN). The NLP interprets queries, and the RBFNN models insurance documents (policies, premiums, claims) to enhance accuracy and efficiency in document generation. The system achieved 99% accuracy in generating insurance documents.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:59:31.265093
d1f11092e4cf4435,Enhancing stock timing predictions based on multimodal architecture: Leveraging large language models (LLMs) for text quality improvement,"This study aims to enhance stock timing predictions by leveraging large language models (LLMs), specifically GPT-4, to filter and analyze online investor comment data. Recognizing challenges such as variable comment quality, redundancy, and authenticity issues, we propose a multimodal architecture that integrates filtered comment data with stock price dynamics and technical indicators. Using data from nine Chinese banks, we compare four filtering models and demonstrate that employing GPT-4 significantly improves financial metrics like profit-loss ratio, win rate, and excess return rate. The multimodal architecture outperforms baseline models by effectively preprocessing comment data and combining it with quantitative financial data. While focused on Chinese banks, the approach can be adapted to broader markets by modifying the prompts of large language models. Our findings highlight the potential of LLMs in financial forecasting and provide more reliable decision support for investors. © 2025 Elsevier B.V., All rights reserved.","Chen, M.; Tang, Y.; Qi, Q.; Dai, H.; Lin, Y.; Ling, C.; Li, T.",2025,10.1371/journal.pone.0326034,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008471640&doi=10.1371%2Fjournal.pone.0326034&partnerID=40&md5=98a5d95e4c7ae2a55e9367ab9130232b,scopus,"This study enhances stock timing predictions by using GPT-4 to filter and analyze online investor comments, integrating this data with stock prices and technical indicators. The multimodal architecture, tested on Chinese banks, significantly improved financial metrics compared to baseline models, demonstrating the value of LLMs in financial forecasting.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:59:34.377291
02d7fb5a3d6611e6,Equilibrium forward curves for commodities,"We develop an equilibrium model of the term structure of forward prices for storable commodities. As a consequence of a nonnegativity constraint on inventory, the spot commodity has an embedded timing option that is absent in forward contracts. This option's value changes over time due to both endogenous inventory and exogenous transitory shocks to supply and demand. Our model makes predictions about Volatilities of forward prices at different horizons and shows how conditional violations of the Samuelson effect occur. We extend the model to incorporate a permanent second factor and calibrate the model to crude oil futures data.","Routledge, BR; Seppi, DJ; Spatt, CS",2000,10.1111/0022-1082.00248,,wos,"This paper presents an equilibrium model for commodity forward prices, considering inventory non-negativity and its impact on timing options. It analyzes volatility and the Samuelson effect, extends the model with a second factor, and calibrates it to crude oil futures data.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:59:37.553050
e533d86ac723242f,Equilibrium modeling of asset prices: Rationality versus rules of thumb,"General equilibrium models with representative agents have proved to be inadequate descriptions of U.S. financial data. I present a model with heterogeneous agents, optimizers, and nonoptimizers that exhibits high stock-price volatility and mimics empirical regularities found in U.S. consumption, stock return, and three-month treasury-bill return data. The simulation and estimation of the model are performed using a new technique called “backsolving,” which is of independent interest to researchers attempting to solve nonlinear, stochastic models. © 1990 American Statistical Association. © 2016 Elsevier B.V., All rights reserved.","Ingram, B.F.",1990,10.1080/07350015.1990.10509781,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910958135&doi=10.1080%2F07350015.1990.10509781&partnerID=40&md5=c2de1a714026f6f119f9fc2871c0c81a,scopus,"This paper presents a heterogeneous agent model that better explains U.S. financial data than traditional representative agent models. It incorporates both optimizing and non-optimizing agents to capture high stock-price volatility and empirical regularities in consumption and returns. The study also introduces a novel 'backsolving' technique for solving nonlinear, stochastic models.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:59:39.537211
64f45a55163d6b93,Equity Return Dispersion and Stock Market Volatility: Evidence from Multivariate Linear and Nonlinear Causality Tests,"We employ bivariate and multivariate nonlinear causality tests to document causality from equity return dispersion to stock market volatility and excess returns, even after controlling for the state of the economy. Expansionary (contractionary) market states are associated with a low (high) level of equity return dispersion, indicating asymmetries in the relationship between return dispersion and economic conditions. Our findings indicate that both return dispersion and business conditions are valid joint forecasters of stock market volatility and excess returns and that return dispersion possesses incremental information regarding future stock return dynamics beyond that which can be explained by the state of the economy.",,2019,10.3390/su11020351,,proquest,"This study uses nonlinear causality tests to show that equity return dispersion influences stock market volatility and excess returns. It finds that market conditions affect return dispersion asymmetrically and that both return dispersion and economic conditions can predict stock market volatility and excess returns, with return dispersion offering additional predictive information.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:59:41.625249
f3a9a0a79144a932,"Equity market valuation, systematic risk and monetary policy","This study examines the relationship between equity market valuation and risk indicators that portend economic downswings. The indicators are implied options volatility, Treasury-Eurodollar (TED) spread and exchange rate. While implied volatility captures market risk in that it reflects the fear factor embedded in the price of an option, TED spread reflects the default risk premium that is priced into a key short-term credit instrument. Equity markets often show a tendency to reflect the incidence of these risk factors. And because they provide valuable information about the health of the economy, many have argued that equity market valuation be taken into account in the formulation of monetary policy. Results of this study not only show a statistically significant inverse relationship between the stock market and these risk factors, but also evidence of a cointegration. In a variance decomposition of the series, we find that equity valuation is a major contributor to the forecast error variances of each of the risk indicators, a finding that lends tacit support to the argument that risk indicators associated with the equity market be considered in monetary policy decisions.","Obi, Pat; Dubihlela, Job; Choi, Jeong-Gil",2012,10.1080/00036846.2011.579065,,wos,"This study investigates the link between equity market valuation and risk indicators (implied options volatility, TED spread, exchange rate) that signal economic downturns. It finds a significant inverse relationship and cointegration between stock market and these risk factors. The findings suggest that equity market valuation significantly contributes to the forecast error variances of risk indicators, supporting its consideration in monetary policy.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:59:44.048832
b186ed887412ad85,"Equity risk premiums (ERP): Determinants, estimation and implications - A post-crisis update","The risk premium is a fundamental and critical component in portfolio management, corporate finance and valuation. Given its importance, it is surprising that more attention has not been paid in practical terms to estimation issues. In this paper, we began by looking at the determinants of equity risk premiums including macro economic volatility, investor risk aversion and behavioral components. We then looked at the three basic approaches used to estimate equity risk premiums - the survey approach, where investors or managers are asked to provide estimates of the equity risk premium for the future, the historical return approach, where the premium is based upon how well equities have done in the past and the implied approach, where we use future cash flows or observed bond default spreads to estimate the current equity risk premium. The premiums we estimate can vary widely across approaches, and we considered two questions towards the end of the paper. The first is why the numbers vary across approaches and the second is how to choose the right number to use in analysis. For the latter question, we argued that the choice of a premium will depend upon the forecast period, whether your believe markets are efficient and whether you are required to be market neutral in your analysis. © 2009 New York University Salomon Center and Wiley Periodicals, Inc. © 2012 Elsevier B.V., All rights reserved.","Damodaran, A.",2009,10.1111/j.1468-0416.2009.00151.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72149127876&doi=10.1111%2Fj.1468-0416.2009.00151.x&partnerID=40&md5=c449d6a2cfde5f6af31db937ef4a7b0b,scopus,"This paper examines the determinants, estimation methods (survey, historical, implied), and implications of equity risk premiums (ERP). It discusses why estimates vary across methods and how to choose an appropriate ERP based on the forecast period, market efficiency beliefs, and neutrality requirements.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:59:48.681115
85e46487d751331e,Estimating and testing non-affine option pricing models with a large unbalanced panel of options,"In this paper, we considerjoint estimation of objective and risk-neutral parameters for stochastic volatility option pricing models using both stock and option prices. A common strategy simplifies the task by limiting the analysis to just one option per date. We first discuss its drawbacks on the basis of model interpretation, estimation results and pricing exercises. We then turn the attention to a more flexible approach, that successfully exploits the wealth of information contained in large heterogeneous panels of options, and we apply it to actual S&P 500 index and index call options data. Our approach breaks the stochastic singularity between contemporaneous option prices by assuming that every observation is affected by measurement error, essentially recasting the problem as a non-linear filtering one. The resulting likelihood function is evaluated using a Monte Carlo Importance Sampling (MC-IS) strategy, combined with a Particle Filter algorithm. The results provide useful intuitions on the directions that should be followed to extend the model, in particular by allowing jumps or regime switching in the volatility process.","Ferriani, Fabrizio; Pastorello, Sergio",2012,10.1111/j.1368-423x.2012.00372.x,,wos,"This paper proposes a method for jointly estimating parameters for stochastic volatility option pricing models using both stock and option prices. It addresses the limitations of simpler approaches that use only one option per date and instead utilizes large, heterogeneous panels of options. The method treats option price observations as having measurement errors, reframing the problem as a non-linear filtering task. A Particle Filter and Monte Carlo Importance Sampling are used to evaluate the likelihood function. The results offer insights for extending the model, such as incorporating jumps or regime switching in volatility.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T09:59:52.760983
e3fc14003bf395e3,Estimation of Parameters in Mean-Reverting Stochastic Systems,"Stochastic differential equation (SDE) is a very important mathematical tool to describe complex systems in which noise plays an important role. SDE models have been widely used to study the dynamic properties of various nonlinear systems in biology, engineering, finance, and economics, as well as physical sciences. Since a SDE can generate unlimited numbers of trajectories, it is difficult to estimatemodel parameters based on experimental observationswhichmay represent only one trajectory of the stochastic model. Although substantial research efforts have been made to develop effective methods, it is still a challenge to infer unknown parameters in SDE models from observations that may have large variations. Using an interest rate model as a test problem, in this work we use the Bayesian inference and Markov Chain Monte Carlo method to estimate unknown parameters in SDE models.","Tian, Tianhai; Zhou, Yanli; Wu, Yonghong; Ge, Xiangyu",2014,10.1155/2014/317059,,wos,"This paper proposes using Bayesian inference and Markov Chain Monte Carlo methods to estimate parameters in mean-reverting stochastic systems, particularly when dealing with limited observational data from a single trajectory. The authors test their approach on an interest rate model.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-28T10:00:41.472975
fd910f31509a0f0d,European options in a nonlinear incomplete market model with default,"We study the superhedging prices and the associated superhedging strategies for European options in a nonlinear incomplete market model with default. The underlying market model consists of one risk-free asset and one risky asset, whose price may admit a jump at the default time. The portfolio processes follow nonlinear dynamics with a nonlinear driver f. By using a dynamic programming approach, we first provide a dual formulation of the seller's (superhedging) price for the European option as the supremum, over a suitable set of equivalent probability measures Q ∈ Q, of the fevaluation/expectation under Q of the payoff. We also establish a characterization of the seller's (superhedging) price as the initial value of the minimal supersolution of a constrained backward stochastic differential equation with default. Moreover, we provide some properties of the terminal profit made by the seller, and some results related to replication and no-arbitrage issues. Our results rely on first establishing a nonlinear optional and a nonlinear predictable decomposition for processes which are Ef-strong supermartingales under Q for all Q ∈ Q. © 2020 Elsevier B.V., All rights reserved.","Grigorova, M.; Quenez, M.-C.; Sulem, A.",2020,10.1137/20m1318018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091823497&doi=10.1137%2F20M1318018&partnerID=40&md5=2cdc28d979b26bd118cd3f70ddad0322,scopus,"This paper investigates superhedging prices and strategies for European options within a nonlinear, incomplete market model that incorporates default risk. The model features a risk-free asset and a risky asset that can jump upon default, with nonlinear dynamics driven by a function 'f'. Using dynamic programming, the authors derive a dual formulation for the seller's superhedging price as the supremum of expected payoffs under various equivalent probability measures. They also characterize this price as the initial value of a minimal supersolution to a constrained backward stochastic differential equation with default. The study includes analysis of terminal profits, replication, and no-arbitrage conditions, building upon nonlinear optional and predictable decomposition theorems for specific types of supermartingales.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:00:44.152887
cedfa0a421c3207f,Evaluation of the influence of surface structure on tribological properties of the solid–liquid interface: Analytical and experimental assessment,"Nowadays, multiple strategies have been suggested to improve product efficiency on the basis of risk free techniques. Owing to the vast applications of the structured surface in many industrial sectors, considerable attention has been paid by many scholars to improve the surface performance aspects by studying their solid–liquid interface characterizations. In the current study, a new technique is introduced to manufacture a functional surface with water repellency feature through grinding process. Based on the solid–liquid interface evaluations of the modified surface, by structuring the surface, the contact angle increases from 35° (untreated surface) to 127° for structured surface. Therefore, the reinforced surface showed hydrophobicity with a 263% improvement for a scratch area fraction more than 75%. Formation of air pads trapped in the vacant space of the scratches was the main cause to enhance contact angle for structured surface. Since air molecules have very low inclination to chemical bonding with water molecules, the water droplet did not penetrate the scratches space and contact angle enhanced. A new analytical model for predicting the solid–liquid surface property was simultaneously developed by interaction of the kinematics of the grinding technique with single diamond and the basic wettability theory of the Cassie-Baxter. Therefore, the effect of all input parameters on the output results was studied in terms of achieving the best solid–liquid interaction performance through expanded model. The optimal values were calculated and the proposed structured surface was manufactured during the grinding process. According to the value of the static contact angle measured during experimental tests (127°) and its analytical value (123°), less than 4% error was observed between the results, which showed the high accuracy of the equation expanded on the present work. © 2023 Elsevier B.V., All rights reserved.","Musavi, S.H.; Razfar, M.; Domiri Ganji, D.D.",2023,10.1016/j.molliq.2023.122967,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170069308&doi=10.1016%2Fj.molliq.2023.122967&partnerID=40&md5=f2510f436420d6846dc300fecee68675,scopus,"This study introduces a new technique using a grinding process to create a functional, water-repellent surface. The structured surface demonstrated increased hydrophobicity, with a contact angle improving from 35° to 127°. This enhancement is attributed to air pads trapped in the surface's scratches. An analytical model was developed to predict solid-liquid surface properties based on the grinding kinematics and Cassie-Baxter wettability theory, showing good agreement with experimental results.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:00:46.185076
6753cb49c5eeba27,Evolutionary-based return forecasting with nonlinear STAR models: evidence from the Eurozone peripheral stock markets,"Traditional linear regression and time-series models often fail to produce accurate forecasts due to inherent nonlinearities and structural instabilities, which characterize financial markets and challenge the Efficient Market Hypothesis. Machine learning techniques are becoming widespread tools for return forecasting as they are capable of dealing efficiently with nonlinear modeling. An evolutionary programming approach based on genetic algorithms is introduced in order to estimate and fine-tune the parameters of the STAR-class models, as opposed to conventional techniques. Using a hybrid method we employ trading rules that generate excess returns for the Eurozone southern periphery stock markets, over a long out-of-sample period after the introduction of the Euro common currency. Our results may have important implications for market efficiency and predictability. Investment or trading strategies based on the proposed approach may allow market agents to earn higher returns.",,2018,10.1007/s10479-015-2078-z,,proquest,"This paper proposes an evolutionary programming approach using genetic algorithms to estimate nonlinear STAR models for stock market return forecasting. The authors demonstrate that this hybrid method generates excess returns in Eurozone peripheral stock markets, challenging the Efficient Market Hypothesis and suggesting potential for higher investment returns.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:00:49.105014
ac8f1b6ca53d639a,Evolving Fuzzy-GARCH Approach for Financial Volatility Modeling and Forecasting,"Volatility modeling and forecasting play a key role in asset allocation, risk management, derivatives pricing and policy making. The purpose of this paper is to develop an evolving fuzzy-GARCH modeling approach for stock market asset returns forecasting. The method addresses GARCH volatility modeling within the framwork of evolving fuzzy systems. This hybrid methodology aims to account for time-varying volatility, from GARCH approach, as well as volatility clustering and nonlinear time series identification, from evolving fuzzy systems, which use time-varying data streams to continuously and simultaneously adapt the structure and functionality of fuzzy models. The motivation is to improve model performance as new data is input through gradual model construction, inducing model adaptation and refinement without catastrophic forgetting while keeping current model useful. An empirical application includes the forecasting of S&P 500 and Ibovespa indexes by the evolving fuzzy-GARCH against traditional GARCH-family models and a fuzzy GJR-GARCH methodology. The results indicate the high potential of the evolving fuzzy-GARCH model to forecast stock returns volatility, which outperforms GARCH-type models and showed comparable forecasts with fuzzy GJR-GARCH methodology.","Maciel, Leandro; Gomide, Fernando; Ballini, Rosangela",2016,10.1007/s10614-015-9535-2,,wos,"This paper proposes an evolving fuzzy-GARCH model for stock market volatility forecasting, combining GARCH's time-varying volatility with evolving fuzzy systems' ability to adapt to nonlinear time series and data streams. The model aims to improve forecasting by continuously adapting without forgetting past information. Empirical results on S&P 500 and Ibovespa indexes suggest the evolving fuzzy-GARCH model outperforms traditional GARCH models and is comparable to fuzzy GJR-GARCH.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:00:54.904942
04e2d697edac7893,Examination and Modification of Multi-Factor Model in Explaining Stock Excess Return with Hybrid Approach in Empirical Study of Chinese Stock Market,"To search significant variables which can illustrate the abnormal return of stock price, this research is generally based on the Fama-French five-factor model to develop a multi-factor model. We evaluated the existing factors in the empirical study of Chinese stock market and examined for new factors to extend the model by OLS and ridge regression model. With data from 2007 to 2018, the regression analysis was conducted on 1097 stocks separately in the market with computer simulation based on Python. Moreover, we conducted research on factor cyclical pattern via chi-square test and developed a corresponding trading strategy with trend analysis. For the results, we found that except market risk premium, each industry corresponds differently to the rest of six risk factors. The factor cyclical pattern can be used to predict the direction of seven risk factors and a simple moving average approach based on the relationships between risk factors and each industry was conducted in back-test which suggested that SMB (size premium), CMA (investment growth premium), CRMHL (momentum premium), and AMLH (asset turnover premium) can gain positive return.",,2019,10.3390/jrfm12020091,,proquest,"This study extends the Fama-French five-factor model to explain stock excess returns in the Chinese stock market using OLS and ridge regression. It identifies significant factors, analyzes their cyclical patterns, and develops a trading strategy based on trend analysis, finding that SMB, CMA, CRMHL, and AMLH can yield positive returns.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:03:22.249905
bee113618fd74452,Examining the myths of connected and autonomous vehicles: Analysing the pathway to a driverless mobility paradigm,"Connected and autonomous vehicles (CAVs) could become the most powerful mobility intervention in the history of human race; possibly greater than the conception of the wheel itself or the shift from horse-carriages to automobiles. Despite CAVs' likely traffic safety, economic, environmental, social inclusion and network performance benefits their full-scale implementation may not be as predictable, uncomplicated, acceptable and risk-free as it is often communicated by a large share of automotive industries, policy-makers and transport experts. Framing an 'unproven', 'disruptive' and 'life-changing' intervention, primarily based on its competitive advantages over today's conventional automobile technologies, may create misconceptions, overreaching expectations and room for errors that societies need to be cautious about. This article 'tests' eleven myths referring to an overly optimistic CAVs' development and adoption timeline. This approach highlights unresolved issues that need to be addressed before an inescapable CAV-based mobility paradigm transition takes place and provides relevant policy recommendations on how to achieve that. © 2020 Elsevier B.V., All rights reserved.","Nikitas, A.; Tchouamou Njoya, E.T.; Dani, S.",2019,10.1504/ijatm.2019.098513,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063507634&doi=10.1504%2FIJATM.2019.098513&partnerID=40&md5=7076e763a81cf64f2fa696878081f9af,scopus,"This article examines optimistic myths surrounding the development and adoption of connected and autonomous vehicles (CAVs). It challenges the predictable, uncomplicated, acceptable, and risk-free implementation often communicated by industry and policymakers. By testing eleven myths about CAV timelines, the article highlights unresolved issues and offers policy recommendations for a transition to a CAV-based mobility paradigm.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:03:23.545193
5405d96dfae218a6,Experience rating of risk premium for Esscher premium principle,"In this article, a new method is introduced under the Bayesian framework to derive the credibility estimator of risk premiums based on the Esscher premium principle. This new estimator offers desirable statistical properties, making it more useful and practical compared to existing estimators. Additionally, Bayesian models for policy portfolios are established, and empirical Bayes methods are employed to estimate the structural parameters. The empirical Bayesian estimation of risk premiums is also discussed in detail. The convergence rate and goodness of the proposed estimators are verified through simulations. The results demonstrate the effectiveness and accuracy of the new estimator and its superior performance compared to other existing methods. Finally, an empirical analysis is conducted using real insurance data, which further confirms the applicability and reliability of the proposed credibility estimator and its superiority in practical insurance applications. © 2024 Elsevier B.V., All rights reserved.","Zhang, Y.; Wen, L.",2024,10.1080/03610926.2023.2286192,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178100660&doi=10.1080%2F03610926.2023.2286192&partnerID=40&md5=41e426440e05e0182b9e7d5f4d9127c3,scopus,"This article introduces a new Bayesian framework for deriving a credibility estimator of risk premiums based on the Esscher premium principle. It establishes Bayesian models for policy portfolios, employs empirical Bayes methods for parameter estimation, and discusses empirical Bayesian estimation of risk premiums. Simulations verify the estimators' convergence and goodness, showing superior performance compared to existing methods. An empirical analysis using real insurance data confirms the practical applicability and reliability of the proposed estimator.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:03:25.392671
c6c0fac67ecede1f,Exploration of Stock Portfolio Investment Construction Using Deep Learning Neural Network,"To study the intelligent and efficient stock portfolio in China’s financial market, based on the relevant theories such as deep learning (DL) neural network (NN) and stock portfolio, this study selects 111 stable stocks from the constituent stocks of the China Security Index (CSI) 300 from January 1, 2018, to December 31, 2021, as the research samples. Then, it analyzes these research samples and imports the relevant data of 111 stocks into the DL NN model. The corresponding prediction results of stock prices are obtained. Finally, the stock portfolio model based on DL NN is compared with the data results of the Shanghai Stock Exchange (SSE) 50 Index and CSI 500 Index. The results show that the closing prices of the selected 111 stocks are relatively stable and fluctuate up and down around the horizontal axis, and the positive and negative returns are relatively balanced, roughly between −5% and 5%. There is a phenomenon of fluctuation aggregation to a certain extent. Comparing the prediction results of different models reveals that the prediction results of model c are closest to the actual stock price trend. Comparing the relevant returns of the proposed stock portfolio with other stocks uncovers that the annualized return of the stock portfolio based on the DL NN model is 47.44%. The sharp ratio is 1.52, the maximum pullback is 18.15%, the monthly excess return is 3.11%, and the information ratio is 0.82. Compared with other indexes, the proposed stock portfolio shows the best results. Therefore, the proposal of the stock portfolio based on DL NN provides a theoretical basis for the development of the financial field in the future.",,2022,10.1155/2022/7957097,,proquest,"This study constructs a stock portfolio using a deep learning neural network model with 111 stable stocks from the China Security Index (CSI) 300 between 2018 and 2021. The model's predictions were compared to actual stock prices and benchmark indices (SSE 50 Index and CSI 500 Index). The deep learning-based portfolio achieved an annualized return of 47.44% with a Sharpe ratio of 1.52, outperforming other indices.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:03:30.201315
17dfde09cbd8f47e,Exploring External Influences on Cryptocurrency Prices: Using A Multi-Analytical Approach,"Cryptocurrencies have experienced exponential growth within the last decade, with market capitalization hovering above the one-trillion-dollar mark since 2022. One area of concern for current and potential crypto users and investors is their unprecedented price volatility. As cryptos become interlinked with the regulated financial system, questions emerge regarding the possibility of linkages of their prices to the external environments. Financial and macroeconomic factors of inflation, economic growth, interest rates, currency exchange rates, equity market returns, corporate bond yields, gold and oil prices are examined against the cryptocurrency returns. This study encompasses a multi-analytical approach, firstly with the empirical tests of Spearman’s correlational analysis to discover the most pertinent relationships, followed by the PCA analysis to reduce redundancy. The predictive regression model of the Granger Causality test, a vector autoregression (VAR) time series forecasting method, is applied to examine whether the highly effective factors Granger cause the crypto price movements. The Machine Learning Random Forest Regression is also applied where a nuanced understanding of the external factors affecting cryptos prices is gained. The findings of this study pertain to more recent times when the pandemic crisis has subsided and stable economies are in place. The results examined four major cryptos of Bitcoin, Binance Coin, Ripple and Tether, where most behaviours suggest that users and investors are willing to take on riskier assets during periods of economic growth, a strong equity market complements crypto demands and gold and oil are good substitutes for cryptos. Tether, a stablecoin, was the least impacted by external factors and behaved similarly to a fiat currency. This investigation into external factors will empower cryptocurrency users and investors with valuable insights into the crypto price mechanisms, enabling them to refine their investing and portfolio diversification strategies.",,2025,10.32479/ijefi.19455,,proquest,"This study investigates the influence of financial and macroeconomic factors (inflation, economic growth, interest rates, currency exchange rates, equity market returns, corporate bond yields, gold and oil prices) on the prices of four major cryptocurrencies (Bitcoin, Binance Coin, Ripple, and Tether). It employs a multi-analytical approach including Spearman's correlational analysis, PCA, Granger Causality tests (VAR), and Random Forest Regression. Findings suggest that during economic growth and strong equity markets, investors are more willing to invest in riskier assets like cryptocurrencies, and gold/oil act as substitutes. Tether, a stablecoin, showed behavior similar to fiat currency, being less impacted by external factors. The study aims to provide insights for cryptocurrency users and investors to refine their strategies.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:03:35.297424
3da66af54c1d1abc,Exploring the role of artificial intelligence in orthopedic medical education: A narrative review,"Artificial intelligence (AI) is transforming orthopedic medical education by enhancing diagnostic accuracy, surgical training, and personalized learning. This narrative review explores AI's applications, including machine learning (ML) and computer vision for interpreting imaging studies, virtual reality (VR) and augmented reality (AR) for immersive surgical simulations, and natural language processing (NLP) for streamlining clinical workflows. AI-powered tools offer objective feedback, adaptive learning modules, and risk-free environments for skill acquisition, bridging gaps in traditional training methods. However, challenges such as data privacy, algorithmic bias, and the need for robust validation remain. Ethical considerations, including patient trust and trainee over-reliance on AI, must also be addressed. Despite these barriers, AI democratizes access to high-quality education, particularly in resource-limited settings, through cloud-based platforms and mobile applications. The future of AI in orthopedics is promising, with advancements in predictive analytics, robotic-assisted surgery, and haptic feedback technologies poised to further revolutionize training. Collaborative efforts among educators, clinicians, and developers are essential to ensure responsible integration. This review highlights AI's potential to reshape orthopedic education while emphasizing the importance of preserving the mentor-trainee relationship and fostering evidence-based adoption.Artificial intelligence (AI) is transforming orthopedic medical education by enhancing diagnostic accuracy, surgical training, and personalized learning. This narrative review explores AI's applications, including machine learning (ML) and computer vision for interpreting imaging studies, virtual reality (VR) and augmented reality (AR) for immersive surgical simulations, and natural language processing (NLP) for streamlining clinical workflows. AI-powered tools offer objective feedback, adaptive learning modules, and risk-free environments for skill acquisition, bridging gaps in traditional training methods. However, challenges such as data privacy, algorithmic bias, and the need for robust validation remain. Ethical considerations, including patient trust and trainee over-reliance on AI, must also be addressed. Despite these barriers, AI democratizes access to high-quality education, particularly in resource-limited settings, through cloud-based platforms and mobile applications. The future of AI in orthopedics is promising, with advancements in predictive analytics, robotic-assisted surgery, and haptic feedback technologies poised to further revolutionize training. Collaborative efforts among educators, clinicians, and developers are essential to ensure responsible integration. This review highlights AI's potential to reshape orthopedic education while emphasizing the importance of preserving the mentor-trainee relationship and fostering evidence-based adoption.",,2025,10.1016/j.jcot.2025.103100,,proquest,"This narrative review examines the impact of artificial intelligence (AI) on orthopedic medical education, detailing applications in diagnostics, surgical training, and personalized learning through technologies like machine learning, computer vision, VR, AR, and NLP. It discusses benefits such as objective feedback and accessible training, alongside challenges like data privacy and ethical concerns. The review emphasizes AI's potential to democratize education and revolutionize training, advocating for responsible integration and collaboration.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:03:38.129450
05792d551d45f496,Factor Investing Based on Musharakah Principle,"Shariah stock investing has become a widely discussed topic in financial industry as part of today's investment strategy. The strategy primarily applies market capitalization allocations. However, some researchers have argued that market capitalization weighting is inherently flawed and have advocated replacing market capitalization allocations with factor allocations. In this paper, we discuss the rationale for factor investing based on Musharakah principle. The essential elements or factors of Musharakah principle such as business sector, management capability, profitability growth and capital efficiency are embedded in the Shariah-compliant stock. We then transform these factors into indexation for better analysis and performance measurement. Investment universe for this research covers Malaysian stocks for the period of January 2009 to December 2013. We found out that these factor indexes have historically earned excess returns over market capitalization weighted indexes and experienced higher Sharpe Ratios.","Simon, Shahril; Omar, Mohd; Lazam, Norazliani Md; Amin, Mohd Nazrul Mohd",2015,10.1063/1.4932468,,wos,"This paper proposes a factor investing strategy based on the Musharakah principle for Shariah-compliant stocks. It identifies key factors like business sector, management capability, profitability growth, and capital efficiency, transforms them into indexation, and empirically tests their performance against market capitalization-weighted indexes using Malaysian stocks from 2009-2013. The findings suggest that factor indexes historically outperformed market capitalization indexes and had higher Sharpe Ratios.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:03:40.528879
4b1a93d999b867fb,Filtering of a Discrete-Time HMM-Driven Multivariate Ornstein-Uhlenbeck Model With Application to Forecasting Market Liquidity Regimes,"This paper investigates the modeling of risk due to market and funding liquidity by capturing the joint dynamics of three time series: the treasury-Eurodollar spread, the VIX, and a metric derived from the S&P 500 spread. We propose a two-regime mean-reverting model for explaining the behaviour of three time series, which mirror liquidity levels for financial markets. An expectation-maximisation algorithm in conjunction with multivariate filters is employed to construct optimal parameter estimates of the proposed model. The selection of the modeling set-up is justified by balancing the best-fit criterion and model complexity. The model performance is demonstrated on historical market data, and a descriptive analysis of the different liquidity measures shows the presence of clear high and low states.",A. Tenyakov; R. Mamon; M. Davison,2016,10.1109/jstsp.2016.2549499,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7445144,ieeexplore,"This paper proposes a two-regime mean-reverting model to capture the joint dynamics of the treasury-Eurodollar spread, VIX, and S&P 500 spread, reflecting market and funding liquidity. An expectation-maximization algorithm and multivariate filters are used for parameter estimation. The model's performance is demonstrated on historical data, showing distinct high and low liquidity states.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:03:45.209356
a7bce1b51fda5973,"Financial asset returns, direction-of-change forecasting, and volatility dynamics","We consider three sets of phenomena that feature prominently in the financial economics literature: (1) conditional mean dependence (or lack thereof) in asset returns, (2) dependence (and hence forecastability) in asset return signs, and (3) dependence (and hence forecastability) in asset return volatilities. We show that they are very much interrelated and explore the relationships in detail. Among other things, we show that (1) volatility dependence produces sign dependence, so long as expected returns are nonzero, so that one should expect sign dependence, given the overwhelming evidence of volatility dependence; (2) it is statistically possible to have sign dependence without conditional mean dependence; (3) sign dependence is not likely to be found via analysis of sign autocorrelations, runs tests, or traditional market timing tests because of the special nonlinear nature of sign dependence, so that traditional market timing tests are best viewed as tests for sign dependence arising from variation in expected returns rather than from variation in volatility or higher moments; (4) sign dependence is not likely to be found in very high-frequency (e.g., daily) or very low-frequency (e.g., annual) returns; instead, it is more likely to be found at intermediate return horizons; and (5) the link between volatility dependence and sign dependence remains intact in conditionally non-Gaussian environments, for example, with time-varying conditional skewness and/or kurtosis.","Christoffersen, Peter F.; Diebold, Francis X.",2006,10.1287/mnsc.1060.0520,,wos,"This paper investigates the interrelationships between conditional mean dependence in asset returns, dependence in asset return signs (direction-of-change forecasting), and dependence in asset return volatilities. It demonstrates that volatility dependence can lead to sign dependence, especially when expected returns are non-zero. The study also highlights that sign dependence might not be easily detected using traditional methods like sign autocorrelations or market timing tests due to its nonlinear nature, and it is more likely to be found at intermediate return horizons. The link between volatility and sign dependence persists even in non-Gaussian environments.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:03:47.233428
50461e7a8d89df57,Financial attention and the demand for information,"This study tracks the daily traffic on the leading financial websites in the US, and uses the attention paid to these websites as a proxy for retailers’ aggregate demand for information. We determine that attention to financial websites is positively correlated with uncertainty and negatively associated with investor sentiment. Furthermore, market shocks drive attention to financial websites, and heightened attention predicts an increase in the following trading day's volatility. Consistent with the information arrival hypothesis, the search for information is higher on Mondays and Tuesdays and lower on weekends. As some retail investors are noise traders, attention to financial websites has a positive effect on volatility and increases trading volume. Finally, using 5-min intraday data, we construct a daily-implied risk aversion proxy and provide evidence supporting the theoretical contention that risk-averse agents gather information as a hedge against uncertainty. However, our findings do not support the avoidance of information theories. © 2019 Elsevier B.V., All rights reserved.","Qadan, M.; Zoua'bi, M.",2019,10.1016/j.socec.2019.101450,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069670415&doi=10.1016%2Fj.socec.2019.101450&partnerID=40&md5=a6e61eecd3f0a101c774cad8ea7d88e8,scopus,"This study uses daily traffic on US financial websites as a proxy for retailer demand for information. It finds that attention to these sites correlates positively with uncertainty and negatively with investor sentiment, and is driven by market shocks. Increased attention predicts higher subsequent volatility. Information seeking is higher early in the week, consistent with the information arrival hypothesis. Noise traders' attention increases volatility and trading volume. The study also constructs a daily-implied risk aversion proxy, supporting the idea that risk-averse agents seek information as a hedge against uncertainty, but not theories of information avoidance.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:03:48.913021
f0af41484ecdd8f7,Flexible inflation targeting and stock market volatility: Evidence from emerging market economies,"Since the recent financial crisis, inflation targeting has been considered as one of the causes of the authorities’ unresponsiveness to the buildup of financial instability. Related research has either emphasized the effects of the central bank instrument and/or outcome or exclusively the impact of a unique central bank institutional characteristic, but never all of them as a core part of a unified framework. We fulfill this gap by providing evidence on whether the stock market volatility from Flexible Inflation Targeting (FIT) countries is less than volatility from non-FIT countries. Using data on a large set of emerging and employing causal inference methods, we examine the impact of FIT on stock market volatility. The results demonstrate that FIT is effective in containing volatility. By anchoring market operators’ expectations, FIT shapes the risk premium, which compensates for inflation uncertainty by lowering its main component, i.e. uncertainty, hence eventually bringing down the stock market volatility. © 2023 Elsevier B.V., All rights reserved.","Dridi, I.; Boughrara, A.",2023,10.1016/j.econmod.2023.106420,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165012437&doi=10.1016%2Fj.econmod.2023.106420&partnerID=40&md5=aefbb3defb36a70add814f54d2335234,scopus,"This study investigates whether Flexible Inflation Targeting (FIT) reduces stock market volatility in emerging economies. Using causal inference methods on a large dataset, the research finds that FIT is effective in containing volatility by anchoring expectations and reducing uncertainty, thereby lowering the risk premium.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:03:51.049029
3d1b3ebf3347f4b5,Fluctuations and Forecasting of Carbon Price Based on A Hybrid Ensemble Learning GARCH-LSTM-Based Approach: A Case of Five Carbon Trading Markets in China,"Carbon trading risk management and policy making require accurate forecasting of carbon trading prices. Based on the sample of China’s carbon emission trading pilot market, this paper firstly uses the Augmented Dickey–Fuller test and Autoregressive conditional heteroscedasticity model to test the stationarity and autocorrelation of carbon trading price returns, uses the Generalized Autoregressive Conditional Heteroscedasticity family model to analyze the persistence, risk and asymmetry of carbon trading price return fluctuations, and then proposes a hybrid prediction model neural network (generalized autoregressive conditional heteroscedasticity–long short-term memory network) due to the shortcomings of GARCH models in carbon price fluctuation analysis and prediction. The model is used to predict the carbon trading price. The results show that the carbon trading pilots have different degrees of volatility aggregation characteristics and the volatility persistence is long, among which only the Shanghai and Beijing carbon trading markets have risk premiums. The other pilot returns have no correlation with risks, and the fluctuations of carbon trading prices and returns are asymmetrical. The prediction results of different models show that the root mean square error (RMSE) of Hubei, Shenzhen and Shanghai carbon trading pilots based on the GARCH-LSTM model is significantly lower than that of the single GARCH model, and the RMSE values are reduced by 0.0006, 0.2993 and 0.0151, respectively. The RMSE in the three pilot markets improved by 0.0007, 0.3011 and 0.0157, respectively, compared to the standalone LSTM model. At the same time, compared with the single model, the GARCH-LSTM model significantly increased the R^2 value in Hubei (0.2000), Shenzhen (0.7607), Shanghai (0.0542) and Beijing (0.0595). Therefore, compared with other models, the GARCH-LSTM model can significantly improve the prediction accuracy of carbon price and provide a new idea for scientifically predicting the fluctuation of financial time series such as carbon price.",,2024,10.3390/su16041588,,proquest,"This paper proposes a hybrid GARCH-LSTM model to forecast carbon trading prices in five Chinese markets. The study analyzes price return volatility, risk premiums, and asymmetry. Results indicate that the GARCH-LSTM model significantly improves prediction accuracy compared to single GARCH or LSTM models, with notable improvements in RMSE and R^2 values for Hubei, Shenzhen, Shanghai, and Beijing markets.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:03:53.881539
14951138294d4fff,Forecasting Chinese Stock Market Volatility With Volatilities in Bond Markets,"In this paper, we investigate whether the bond markets contain important information that can improve the accuracy of stock market volatility forecasts in China. We use realized volatility (RV) implemented by different maturity treasury bond futures contracts to predict the Chinese stock market volatility. Our work is based on the heterogeneous autoregressive (HAR) framework. Empirical results show that the volatility of treasury bond contracts with longer maturities (especially 10 years) has the best effect on predicting the Chinese stock market volatility, both in sample and out of sample. Two machine learning methods, the scaled principal component analysis (SPCA) and the least absolute shrinkage and selection operator (lasso), are also more effective than the HAR benchmark model's prediction. Finally, mean–variance investors can achieve substantial economic gains by allocating their investment portfolios based on volatility forecasts after introducing treasury bond futures volatility.",,2025,10.1002/for.3215,,proquest,"This study explores the predictive power of Chinese bond market volatilities for the Chinese stock market volatility. Using a heterogeneous autoregressive (HAR) framework and machine learning methods (SPCA, lasso), it finds that longer-maturity treasury bond futures volatility (especially 10-year) significantly improves stock market volatility forecasts. The findings suggest that mean-variance investors can benefit economically from incorporating these bond volatility forecasts into their portfolio allocation strategies.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:04:06.593520
08fc23dd1562d6e4,Forecasting ETF Performance: A Comparative Study of Deep Learning Models and the Fama-French Three-Factor Model,"The global financial landscape has witnessed a significant shift towards Exchange-Traded Funds (ETFs), with their market capitalization surpassing USD 10 trillion in 2023, due to advantages such as low management fees, high liquidity, and broad market exposure. As ETFs become increasingly central to investment strategies, accurately forecasting their performance has become crucial. This study addresses this need by comparing the efficacy of deep learning models against the traditional Fama-French three-factor model in predicting daily ETF returns. The methodology employs eight artificial neural network architectures, including ANN, LSTM, GRU, CNN, and their variants, implemented in Python and applied to data ranging from 2010 to 2020, while also exploring the impact of additional factors on forecast accuracy. Empirical results reveal that LSTM and the Fama-French three-factor model exhibit a superior performance in ETF return prediction. This study contributes to the literature on financial forecasting and offers practical insights into investment decision making. By leveraging advanced artificial intelligence techniques, this study aims to enhance the toolkit available for ETF performance analysis, potentially improving investment strategies in this dynamic market segment.",,2024,10.3390/math12193158,,proquest,"This study compares deep learning models (ANN, LSTM, GRU, CNN, and variants) with the Fama-French three-factor model for forecasting daily ETF returns from 2010-2020. LSTM and the Fama-French model showed superior performance. The research aims to improve ETF performance analysis and investment strategies.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:04:11.137544
8fb8a050541580b5,Forecasting Yield Curves with Survey Information,"In recent years, affine term structure models have provided alternatives to the expectations hypothesis and have become very popular in the finance literature. In particular, the widely accepted dynamic Nelson-Siegel model employs ingenious measures of the level, slope, and curvature of the yield curve that captured the attention of Francis and Hua. They supplement the dynamic Nelson-Siegel model with the Federal Reserve's Survey of Professional Forecasters data. Because these data utilize information from dozens of professional forecasters who study numerous macroeconomic variables, the author's wanted to see if this information-rich supplementary data could be used to improve the interest rate forecasting models for out-of-sample forecasts for Treasury bond maturities ranging from three months to 10 years that extend from three months to one year into the future.","Francis, Jack Clark; Hua, Jian",2012,10.3905/jpm.2012.38.3.149,,wos,"This study enhances the dynamic Nelson-Siegel model by incorporating the Federal Reserve's Survey of Professional Forecasters data to improve out-of-sample interest rate forecasts for Treasury bonds. The goal is to leverage the information-rich survey data to predict bond yields for maturities ranging from three months to 10 years, with forecasts extending one year into the future.",False,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:09:45.164135
fd8a58d650cc3e18,Forecasting and trading credit default swap indices using a deep learning model integrating Merton and LSTMs,"Using macroeconomic and financial conditions to forecast credit default swap (CDS) spreads is a challenging task. In this paper, we propose the Merton-LSTM model, a modified LSTM model formed by integrating with the Merton determinants model, to forecast the CDS indices. We provide the rigorous math behind the Merton-LSTM model, which demonstrates that by leveraging the nonlinear learning ability of LSTM with increased model capacity, the Merton-LSTM model is expected to learn the inherent association between the Merton determinants and CDS spreads. Further, the Merton-LSTM model is compared with the machine learning models LSTM, gated recurrent unit (GRU), multilayer perceptron network (MLP), support vector machine (SVM) and a typical sto-chastic series model in forecasting the two most liquid five-year CDS indices, North America High Yield index (CDX.NA.HY) and North America Investment Grade index (CDX.NA.IG) through the root mean squared error (RMSE) and the Diebold-Mariano test. The comparison results show that the RMSEs of the Merton-LSTM model are the lowest (6.2570-27.2000 for CDX.NA.HY and 1.3168-6.4772 for CDX.NA.IG) compared to other competitive models. The superiority of the Merton-LSTM model in forecasting performance is highlighted in long-term prediction even with a forecasting horizon extended to 28 days. Simulated trading with different thresholds and horizons is conducted in this study. We find that the Merton-LSTM trading strategy yields the highest annualized Sharpe ratios and lowest maximum losses at most thresholds and horizons, highlighting the economic significance of the proposed model.","Mao, Weifang; Zhu, Huiming; Wu, Hao; Lu, Yijie; Wang, Haidong",2023,10.1016/j.eswa.2022.119012,,wos,"This paper introduces the Merton-LSTM model, a hybrid deep learning approach combining the Merton determinants model with LSTMs, to forecast Credit Default Swap (CDS) indices. The model demonstrates superior forecasting accuracy compared to traditional machine learning and time series models, particularly for longer prediction horizons. Furthermore, a simulated trading strategy based on the Merton-LSTM model shows promising economic significance with high Sharpe ratios and low maximum losses.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:09:48.874595
f924407706d3a0c2,Forecasting benchmarks of long-term stock returns via machine learning,"Recent advances in pension product development seem to favour alternatives to the risk free asset often used in the financial theory as a performance standard for measuring the value generated by an investment or a reference point for determining the value of a financial instrument. To this end, in this paper, we apply the simplest machine learning technique, namely, a fully nonparametric smoother with the covariates and the smoothing parameter chosen by cross-validation to forecast stock returns in excess of different benchmarks, including the short-term interest rate, long-term interest rate, earnings-by-price ratio, and the inflation. We find that, net-of-inflation, the combined earnings-by-price and long-short rate spread form our best-performing two-dimensional set of predictors for future annual stock returns. This is a crucial conclusion for actuarial applications that aim to provide real-income forecasts for pensioners.",,2019,10.1007/s10479-019-03338-4,,proquest,"This paper uses a nonparametric smoother with cross-validation to forecast stock returns in excess of various benchmarks, including interest rates and inflation. The best predictors for future annual stock returns were found to be the earnings-by-price ratio and the long-short rate spread, net of inflation. This finding is relevant for actuarial applications providing real-income forecasts.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:09:57.211104
9169d6d7b0d2309e,Forecasting carbon emissions future prices using the machine learning methods,"Due to the uncertainty surrounding the coupling and decoupling of natural gas, oil, and energy commodity futures prices, the current study seeks to investigate the interactions between energy commodity futures, oil price futures, and carbon emission futures from a forecasting perspective with implications for environmental sustainability. We employed daily data on natural gas futures prices, crude oil futures prices, carbon futures prices, and Dow Jones energy commodity futures prices from January 2018 to October 2021. For empirical analysis, we applied machine learning tools including traditional multiple linear regression (MLR), artificial neural network (ANN), support vector regression (SVR), and long short-term memory (LSTM). The machine learning analysis provides two key findings. First, the nonlinear frameworks outperform linear models in developing the relationships between future oil prices (crude oil and heating oil) and carbon emission futures prices. Second, the machine learning findings establish that when oil prices and natural gas prices display extreme movement, carbon emission futures prices react nonlinearly. Understanding the nonlinear dynamics of extreme movements can help policymakers design climate and environmental policies, as well as adjust natural gas and oil futures prices. We discuss important implications to sustainable development goals mainly SDG 7 and SDG 12.","Shahzad, Umer; Sengupta, Tuhin; Rao, Amar; Cui, Lianbiao",2024,10.1007/s10479-023-05188-7,,wos,"This study forecasts carbon emission futures prices by examining the interactions with natural gas, oil, and energy commodity futures prices using machine learning methods (MLR, ANN, SVR, LSTM). The findings indicate that nonlinear models outperform linear ones, and carbon emission futures prices react nonlinearly to extreme movements in oil and natural gas prices. The research has implications for environmental sustainability, climate policies, and sustainable development goals (SDG 7 and SDG 12).",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:09:58.882077
fa405ec7825ac3fb,Forecasting government bond spreads with heuristic models: evidence from the Eurozone periphery,"This study investigates the predictability of European long-term government bond spreads through the application of heuristic and metaheuristic support vector regression (SVR) hybrid structures. Genetic, krill herd and sine–cosine algorithms are applied to the parameterization process of the SVR and locally weighted SVR (LSVR) methods. The inputs of the SVR models are selected from a large pool of linear and non-linear individual predictors. The statistical performance of the main models is evaluated against a random walk, an Autoregressive Moving Average, the best individual prediction model and the traditional SVR and LSVR structures. All models are applied to forecast daily and weekly government bond spreads of Greece, Ireland, Italy, Portugal and Spain over the sample period 2000–2017. The results show that the sine–cosine LSVR is outperforming its counterparts in terms of statistical accuracy, while metaheuristic approaches seem to benefit the parameterization process more than the heuristic ones.",,2019,10.1007/s10479-018-2808-0,,proquest,"This study explores the predictability of European government bond spreads using hybrid Support Vector Regression (SVR) models enhanced with genetic, krill herd, and sine-cosine algorithms. The models were tested on daily and weekly data for Greece, Ireland, Italy, Portugal, and Spain from 2000-2017. The sine-cosine LSVR model demonstrated superior statistical accuracy, suggesting that metaheuristic approaches improve parameterization.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:10:02.978889
1317143c8a3a95de,Forecasting interest rate swap spreads using domestic and international risk factors: evidence from linear and non-linear models,"This paper explores the ability of factor models to predict the dynamics of US and UK interest rate swap spreads within a linear and a non-linear framework. We reject linearity for the US and UK swap spreads in favour of a regime-switching smooth transition vector autoregressive (STVAR) model, where the switching between regimes is controlled by the slope of the US term structure of interest rates. We compare the ability of the STVAR model to predict swap spreads with that of a non-linear nearest-neighbours model as well as that of linear AR and VAR models. We find some evidence that the non-linear models predict better than the linear ones. At short horizons, the nearest-neighbours (NN) model predicts better than the STVAR model US swap spreads in periods of increasing risk conditions and UK swap spreads in periods of decreasing risk conditions. At long horizons, the STVAR model increases its forecasting ability over the linear models, whereas the NN model does not outperform the rest of the models. Copyright John Wiley & Sons. Reproduced with permission. An electronic version of this article is available online at http://www.interscience.wiley.com",,2007,10.1002/for.1048,,proquest,"This paper investigates the prediction of US and UK interest rate swap spreads using linear and non-linear factor models. A regime-switching smooth transition vector autoregressive (STVAR) model, triggered by the US term structure slope, is found to be superior to linear AR and VAR models, especially at longer horizons. A non-linear nearest-neighbours (NN) model shows better short-term prediction for US swap spreads during increasing risk and UK swap spreads during decreasing risk.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:10:04.987049
4ac39d877a1b7094,Forecasting macroeconomy based on the term structure of credit spreads: Evidence from China,"This article establishes an original methodology to forecast macroeconomy based on the term structure of credit spreads. It combines the traditional Svensson model with genetic algorithms to obtain the interest rate term structures of government bonds and corporate bonds and calculates credit spreads as their differences. And this article defines three factors of the term structure of credit spreads: level, slope and curvature. Based on these three factors and several macroeconomic variables, VAR models are developed and tested to forecast macroeconomic variables. The empirical results confirm that VAR models can predict the changes of China's macroeconomy well, which indicates that the term structure of credit spreads contains information of future changes of macroeconomic variables. We believe this result has significant implications for macroeconomy policy-makers. © 2013 Copyright Taylor and Francis Group, LLC. © 2013 Elsevier B.V., All rights reserved.","Zhou, R.; Wang, X.; Tong, G.",2013,10.1080/13504851.2013.806778,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883325066&doi=10.1080%2F13504851.2013.806778&partnerID=40&md5=eafa30b532af4735b80691807774aefd,scopus,"This study develops a novel method to forecast the macroeconomy using the term structure of credit spreads. It integrates the Svensson model with genetic algorithms to derive term structures for government and corporate bonds, calculating credit spreads. Three factors (level, slope, curvature) from these spreads, along with macroeconomic variables, are used in VAR models for forecasting. Empirical results show these models effectively predict China's macroeconomic changes, suggesting credit spread term structures hold predictive information for policymakers.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:10:15.018893
4eb235a93e4df864,Forecasting market trends with neural networks,"Neural networks are just one of the many technologies that are giving businesses a competitive edge.  Neural networks (NN) are a branch of artificial intelligence which has generated considerable interest across many disciplines during the past few years.  An NN is a nonlinear type of model which receives its inspiration from the neural architecture of the human brain.  Three sample neural networks are presented:  1.  real estate assessment, 2.  credit application evaluation, and 3.  Treasury Bill rate forecasting.",,1999,none,,proquest,"This article discusses the application of neural networks (NNs) as a competitive business tool, drawing inspiration from the human brain's neural architecture. It presents three sample NN applications: real estate assessment, credit application evaluation, and Treasury Bill rate forecasting.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-28T10:10:37.666853
cb1a4b16413ff22b,"Forecasting oil prices with penalized regressions, variance risk premia and Google data","This paper investigates whether augmenting models with the variance risk premium (VRP) and Google search data improves the quality of the forecasts for real oil prices. We considered a time sample of monthly data from 2007 to 2019 that includes several episodes of high volatility in the oil market. Our evidence shows that penalized regressions provided the best forecasting performances across most of the forecasting horizons. Moreover, we found that models using the VRP as an additional predictor performed best for forecasts up to 6–12 months ahead forecasts, while models using Google data as an additional predictor performed better for longer-term forecasts up to 12–24 months ahead. However, we found that the differences in forecasting performances were not statistically different for most models, and only the Principal Component Regression (PCR) and the Partial least squares (PLS) regression were consistently excluded from the set of best forecasting models. These results also held after a set of robustness checks that considered model specifications using a wider set of influential variables, a Hierarchical Vector Auto-Regression model estimated with the LASSO, and a set of forecasting models using a simplified specification for Google Trends data. © 2024 Elsevier B.V., All rights reserved.","Lycheva, M.; Mironenkov, A.; Kurbatskii, A.; Fantazzini, D.",2022,10.22394/1993-7601-2022-68-28-49,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148285877&doi=10.22394%2F1993-7601-2022-68-28-49&partnerID=40&md5=96233f97d9b1ad6c3495359610f59ea2,scopus,"This study explores the forecasting accuracy of real oil prices using penalized regressions, incorporating the variance risk premium (VRP) and Google search data. The findings suggest that penalized regressions generally offer the best performance, with VRP being beneficial for short-to-medium term forecasts (6-12 months) and Google data for longer-term forecasts (12-24 months). However, the performance differences were often not statistically significant, with Principal Component Regression (PCR) and Partial Least Squares (PLS) regression consistently underperforming.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:10:41.515039
6af741a2238a82fe,"Forecasting realized volatility: HAR against Principal Components Combining, neural networks and GARCH","This paper examines whether nonlinear models, like Principal Components Combining, neural networks and GARCH are more accurate on realized volatility forecasting than the Heterogeneous Autoregressive (HAR) model. The answer is no. The realized volatility property of persistence is too important to leave out of a realized volatility forecasting model. However, the Principal Components Combining model is ranked very close to HAR. Analysis is implemented in seven US financial markets: spot equity, spot foreign exchange rates, exchange traded funds, equity index futures, US Treasury bonds futures, energy futures, and commodities options. © 2016 Elsevier B.V., All rights reserved.","Vortelinos, D.I.",2017,10.1016/j.ribaf.2015.01.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925263796&doi=10.1016%2Fj.ribaf.2015.01.004&partnerID=40&md5=221cb713631134e38331665696f3cf0c,scopus,"This paper compares the forecasting accuracy of nonlinear models (Principal Components Combining, neural networks, GARCH) against the Heterogeneous Autoregressive (HAR) model for realized volatility. The study finds that the HAR model, which captures the persistence property of realized volatility, outperforms the nonlinear models. The analysis covers seven US financial markets. The Principal Components Combining model showed performance close to HAR.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:10:45.362446
c973994bdef4a8d4,Forecasting sovereign risk in the Euro area via machine learning,"We test the usefulness of machine learning (ML) for the valuation and pricing of sovereign risk in the Euro area along two important dimensions: i) its predictive accuracy compared with traditional econometric methods, and ii) its assessment of the main economic factors underlying market perceptions of sovereign risk.We find that ML techniques can capture the dynamics inherent in the market valuation of country risk far more efficiently than traditional econometric models, both in the cross‐section and in the time series. Moreover, we show that public sentiment about financial news, redenomination fears and the degree of hawkishness/dovishness expressed in the ECB president's speeches are major contributors to sovereign bond spreads. We also confirm that macroeconomic and global financial factors affect sovereign risk assessment and the corresponding formation of sovereign spreads.",,2023,10.1002/for.2938,,proquest,"This study evaluates the effectiveness of machine learning (ML) for valuing and pricing sovereign risk in the Euro area. It compares ML's predictive accuracy to traditional econometric methods and identifies key economic factors influencing market perceptions of sovereign risk. The findings indicate that ML models outperform traditional methods in capturing the dynamics of country risk valuation and highlight the significant impact of public sentiment, redenomination fears, ECB president's speeches, macroeconomic factors, and global financial conditions on sovereign bond spreads.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:10:57.171102
13bf1e3982d4d0a1,Forecasting stock prices changes using long-short term memory neural network with symbolic genetic programming,"This study introduces an augmented Long-Short Term Memory (LSTM) neural network architecture, integrating Symbolic Genetic Programming (SGP), with the objective of forecasting cross-sectional price returns across a comprehensive dataset comprising 4500 listed stocks in the Chinese market over the period from 2014 to 2022. Using the S&P Alpha Pool Dataset for China as basic input, this architecture incorporates data augmentation and feature extraction techniques. The result of this study demonstrates significant improvements in Rank Information coefficient (Rank IC) and IC information ratio (ICIR) by 1128% and 5360% respectively when it is applied to fundamental indicators. For technical indicators, the hybrid model achieves a 206% increase in Rank IC and an impressive surge of 2752% in ICIR. Furthermore, the proposed hybrid SGP-LSTM model outperforms major Chinese stock indexes, generating average annualized excess returns of 31.00%, 24.48%, and 16.38% compared to the CSI 300 index, CSI 500 index, and the average portfolio, respectively. These findings highlight the effectiveness of SGP-LSTM model in improving the accuracy of cross-sectional stock return predictions and provide valuable insights for fund managers, traders, and financial analysts.",,2024,10.1038/s41598-023-50783-0,,proquest,"This study proposes a hybrid Symbolic Genetic Programming-Long-Short Term Memory (SGP-LSTM) neural network model for forecasting stock price returns in the Chinese market. The model integrates data augmentation and feature extraction, demonstrating significant improvements in prediction accuracy (Rank IC and ICIR) for both fundamental and technical indicators. The SGP-LSTM model also outperformed major Chinese stock indexes, generating substantial annualized excess returns.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:11:01.754651
547e3e2a7c69aaf4,Forecasting the European Monetary Union equity risk premium with regression trees,"This paper investigates whether classification and regression trees ensemble algorithms such as bagging, random forests and boosting improve on traditional parametric models for forecasting the equity risk premium. In particular, we work with European Monetary Union (EMU) data for the period from its foundation in 2000 to 2020. The paper first compares the monthly out-of-sample forecasting ability of multiple economic and technical variables using univariate linear regression models and regression tree techniques. The results obtained suggest that regression trees do not show better forecasting ability than a first-order autoregressive benchmark model and univariate linear regressions. The paper then analyses asset allocation strategies with regression trees and checks whether these can select the best economic pre-dictors to form dynamic portfolios composed of two assets: a risk-free asset and an equity index. The results indicate that trading strategies built with two or three economic predictors selected with boosting and random forest algorithms can generate economic value for a risk-averse investor with a quadratic utility function. © 2022 Elsevier B.V., All rights reserved.","Cortés-Sánchez, D.; Soriano-Felipe, P.",2022,10.21314/jor.2022.035,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138497197&doi=10.21314%2FJOR.2022.035&partnerID=40&md5=1675b21b61b962828b2cd173adb7b029,scopus,"This paper explores the use of regression tree ensemble algorithms (bagging, random forests, boosting) for forecasting the European Monetary Union's equity risk premium from 2000-2020. It compares their forecasting ability against traditional linear models and analyzes asset allocation strategies. While regression trees did not outperform linear models in forecasting, strategies using predictors selected by boosting and random forest algorithms showed economic value for risk-averse investors.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:11:04.002705
db8470eac8d63910,Forecasting the Volatility of US Oil and Gas Firms With Machine Learning,"Forecasting the realized volatility of oil and gas firms is of interest to investors and practitioners trading on the energy spot and derivative markets. In this paper, we assess whether several machine learning (ML) techniques can offer superior forecasts compared to HAR models for predicting realized volatility at the firm level. Moreover, we investigate whether economically motivated variables and technical indicators contain valuable information for forecasting firm volatility beyond those contained in various volatility factors previously identified in the literature. Our results demonstrate that certain ML techniques provide superior forecasting accuracy compared to the benchmark model. Additionally, we identify variables such as the 1‐month treasury bill and the aggregate VIX index as significant drivers of realized firm volatility in the oil and gas industry.",,2025,10.1002/for.3245,,proquest,"This paper evaluates the effectiveness of machine learning (ML) techniques in forecasting the realized volatility of US oil and gas firms, comparing them against traditional HAR models. It also explores the predictive power of economically motivated variables and technical indicators. The study finds that certain ML methods outperform the benchmark and identifies the 1-month treasury bill and aggregate VIX index as significant predictors of firm volatility.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:11:10.474763
84691e655de528a2,Forecasting the equity risk premium: The role of technical indicators,"Academic research relies extensively on macroeconomic variables to forecast the U.S. equity risk premium, with relatively little attention paid to the technical indicators widely employed by practitioners. Our paper fills this gap by comparing the predictive ability of technical indicators with that of macroeconomic variables. Technical indicators display statistically and economically significant in-sample and out-of-sample predictive power, matching or exceeding that of macroeconomic variables. Furthermore, technical indicators and macroeconomic variables provide complementary information over the business cycle: technical indicators better detect the typical decline in the equity risk premium near business-cycle peaks, whereas macroeconomic variables more readily pick up the typical rise in the equity risk premium near cyclical troughs. Consistent with this behavior, we show that combining information from both technical indicators and macroeconomic variables significantly improves equity risk premium forecasts versus using either type of information alone. Overall, the substantial countercyclical fluctuations in the equity risk premium appear well captured by the combined information in technical indicators and macroeconomic variables. © 2014 Elsevier B.V., All rights reserved.","Neely, C.J.; Rapach, D.E.; Tu, J.; Zhou, G.",2014,10.1287/mnsc.2013.1838,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897701069&doi=10.1287%2Fmnsc.2013.1838&partnerID=40&md5=e863e6e4d1820309d93770b620ce4674,scopus,"This paper investigates the predictive power of technical indicators for the U.S. equity risk premium, comparing them to macroeconomic variables. The findings indicate that technical indicators offer significant in-sample and out-of-sample predictive capabilities, comparable to or better than macroeconomic variables. The study also reveals that combining both types of indicators enhances forecasting accuracy, particularly in capturing cyclical fluctuations of the equity risk premium.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:11:13.074828
43806516a814d135,Forecasting the stock risk premium: A new statistical constraint,"We develop a new statistical constraint to improve the stock return forecasting performance of predictive models. This constraint uses a new objective function that combines the Huber loss function with the Ridge penalty. Out‐of‐sample results indicate that our constraint improves the predictive ability of the univariate models. The constrained univariate models significantly outperform the historical average benchmark model assuming no predictability. The forecast improvement based on the new constraint is also evident for multivariate information methods including forecast combination and diffusion index. The model is capable of capturing time‐varying risk which serves as the potential economic explanation of the improved return predictability. Our results are robust to different evaluation subsamples, validation sample lengths, and different risk aversion coefficients.",,2023,10.1002/for.2984,,proquest,"This paper introduces a novel statistical constraint for stock return forecasting, combining Huber loss and Ridge penalty. The constraint enhances the predictive performance of univariate and multivariate models, outperforming historical averages and capturing time-varying risk. Results are robust across various evaluation settings.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:11:24.202363
c0db7e92ec868ec7,Foreign Residency Rights and Corporate Bond Yield Spreads,"We investigate the effect of ultimate controllers' foreign residency rights on corporate bond yield spreads. Using data on Chinese listed firms from 2010 to 2021, this study reveals a positive association between foreign residency rights and corporate bond yield spreads. The positive relationship is robust across different measures of foreign residency rights, estimation methods and after considering potential endogeneity issues. We propose two potential channels for the positive effect of foreign residency rights on corporate bond yield spreads: increasing firms' earnings management and taking more risk. Further, stronger external governance and internal governance alleviate the positive effects of foreign residency rights on corporate bond yield spreads. Additional analyses indicate that ultimate controllers with overseas residency rights are associated with more bond covenants, higher bank loan spreads and shorter loan maturity. Overall, our results indicate that corporate bondholders are fully aware of the expropriation risk created by controllers' foreign residency rights. Thus, investors and regulators in emerging markets should pay attention to controllers' foreign residency rights. © 2025 Elsevier B.V., All rights reserved.","Su, Z.-Q.; Zhu, Y.; Jin, H.; Wu, M.",2025,10.1002/ijfe.70029,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012620539&doi=10.1002%2Fijfe.70029&partnerID=40&md5=e5ab7afd2b6ee1f120a929042e541067,scopus,"This study examines how the foreign residency rights of ultimate controllers affect corporate bond yield spreads in Chinese listed firms from 2010-2021. It finds a positive association, suggesting increased earnings management and risk-taking. Stronger governance mitigates this effect. The findings highlight that bondholders recognize the expropriation risk associated with these rights, suggesting attention from investors and regulators in emerging markets.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:11:27.298533
a2c19207a6e258b2,Foreign exchange risk and risk exposure in the Japanese stock market,"Purpose - Whether stock returns are linked to exchange rate changes and whether foreign exchange risk is priced in a domestic context are less conclusive and thus still subject to a great debate. The purpose of this paper is to provide new empirical evidence on these two inter-related issues, which are critical to investors and corporate risk management. Design/methodology/approach - This paper applies two different econometric approaches: Nonlinear Seemingly Unrelated Regression (NLSUR) via Hansen's Generalized Method of Moment (GMM) and multivariate GARCH in mean (MGARCH-M) to examine the exchange rate exposure and its pricing. Findings - Using industry data for Japan, similar to previous studies, foreign exchange risk is not priced based on the test of an unconditional two-factor asset pricing model. However, strong evidence of time-varying foreign exchange risk premium and significant exchange rate betas are obtained based on the tests of conditional asset pricing models using MGARCH-M approach where both conditional first and second moments of industry returns and risk factors are estimated simultaneously. Research limitations/implications - The strong empirical evidence found in this study implies that corporate currency hedging not only results in more stable cash flows for a firm, but also reduces its cost of capital, and hence is justifiable. Originality/value - This paper conducts an in-depth investigation regarding the exchange rate exposure and its pricing by utilizing two different econometric approaches: NLSUR via Hansen's GMM and MGARCH-M. In doing so, a more reliable conclusion about the exchange rate exposure and its pricing can be drawn.",,2010,10.1108/03074351011042991,,proquest,"This paper investigates the relationship between stock returns and exchange rate changes in the Japanese market using two econometric approaches: Nonlinear Seemingly Unrelated Regression (NLSUR) via Hansen's Generalized Method of Moment (GMM) and multivariate GARCH in mean (MGARCH-M). While unconditional models show no pricing of foreign exchange risk, conditional models reveal time-varying risk premiums and significant exchange rate betas. The findings suggest that corporate currency hedging can stabilize cash flows and reduce the cost of capital.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:11:31.034722
5c50a6dc234cc7e9,Front-Page News: The Effect of News Positioning on Financial Markets,"This paper estimates the effect of news positioning on the speed of price discovery, using exogenous variation in prominent (front-page) positioning of news articles on the Bloomberg terminal. Front-page articles see 240% higher trading volume and 176% larger absolute excess returns during the first 10 minutes after publication than equally important non-front-page articles. Overall, the information in front-page articles is fully incorporated into prices within an hour of publication. The response to non-front-page information of similar importance eventually converges but takes more than two days to be fully reflected in prices.","Fedyk, Anastassia",2024,10.1111/jofi.13287,,wos,"This study investigates how the placement of news (front-page vs. non-front-page) on the Bloomberg terminal impacts the speed of price discovery in financial markets. Front-page news leads to significantly higher trading volume and immediate price changes, with information fully incorporated into prices within an hour. Non-front-page news of similar importance takes over two days to be fully reflected.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:11:34.122718
11fabc255e24d79e,Functional shocks to inflation expectations and real interest rates and their macroeconomic effects,"This paper applies a recently developed method (Inoue and Rossi, 2021) to estimate functional inflation expectations and ex-ante real interest rate shocks, and then examines their macroeconomic effects in the context of a Functional Vector Autoregressive model with exogenous variables (Functional VARX). Monthly data from January 1998 to May 2023 for the US, the UK and the euro area are used for the analysis. The estimated impulse responses show significant effects of the functional shocks on both inflation and output. In addition, threshold functional local projections indicate that the effects are nonlinear and depend on central bank credibility. Further, inflation expectations shocks have similar effects to supply (demand) ones when they are driven by long-term (short-term) changes. In the presence of an inverted (steepening) real interest rate term structure, the effects are inflationary (deflationary) and expansionary (recessionary). Finally, the responses of inflation, output and the policy rate are driven primarily by the slope and curvature factors of the term structure shocks, which contain important information not captured by traditional scalar shocks.",,2024,10.1007/s10290-024-00538-4,,proquest,"This paper estimates functional inflation expectations and real interest rate shocks using a Functional VARX model with US, UK, and euro area data from 1998-2023. It finds significant macroeconomic effects of these shocks on inflation and output, with nonlinearities dependent on central bank credibility. The study highlights the importance of term structure factors in driving these responses.",True,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:11:41.138793
b6874ce0d132be04,"Funding shortages, expectations, and forward rate risk premium",This paper estimates term risk premium and expected future spot rates embedded in Treasury forward rates to study the impact of short-term funding shortages on these quantities. Our approach is consistent with dynamic equilibrium models and avoids the arbitrage-free dynamic inconsistency problems exhibited by traditional methods. We find that short-term funding shortages in money markets affect both expectations of spot rates and forward rate risk premium for all maturity forward rates. The leverage ratio of intermediaries (primary dealers) significantly affects term risk premium but not expectations of future spot rates. Yield curve inversion has no impact on the forward rate curve's evolution.,"Jarrow, Robert; Lamichhane, Sujan",2022,10.1080/14697688.2022.2057352,,wos,"This paper estimates the term risk premium and expected future spot rates from Treasury forward rates, investigating the influence of short-term funding shortages. The methodology aligns with dynamic equilibrium models, circumventing arbitrage-free dynamic inconsistency issues. Findings indicate that funding shortages impact both expectations and risk premiums across all maturities. The leverage ratio of primary dealers influences term risk premium but not future spot rate expectations. Yield curve inversions do not affect the forward rate curve's progression.",False,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:11:44.378953
b391e6771a6ea657,G-PINNs: A Bayesian-Optimized GRU-Enhanced Physics-Informed Neural Network for Advancing Short Rate Model Predictions,"Interest rate modeling plays a crucial role in financial risk management, derivative pricing, and economic forecasting. To address the challenges of capturing complex stochastic dynamics, this study proposes a novel Bayesian-Optimized GRU-Enhanced Physics-Informed Neural Network (G-PINNs) architecture, integrated with the Hull–White (HW) short-rate model, to improve the prediction accuracy of yield forecasting, zero-coupon bond (ZCB) pricing, and option pricing. The proposed framework effectively models time dependent variations and stochastic behavior in interest rate dynamics by leveraging Gated Recurrent Units (GRU) for sequential pattern recognition and Physics Informed Neural Networks (PINNs) to enforce financial constraints through partial differential equations (PDEs) of the HW model. For empirical validation, US treasury yield data from April 2020 to March 2025 is utilized. To achieve the best optimal hyperparameters to enhance both predictive accuracy and training efficiency, Bayesian Optimization (BO) is employed for hyperparameter tuning. The proposed model outperforms Vanilla PINNs as evidenced by higher R2 values and reduced error metrics (MAE, MSE, RMSE, Max & Min error, MSLE, Huber loss, MedAE) in yield prediction, ZCB pricing, and option pricing, as indicated by the numerical results. Furthermore, the results are statistically validated through the paired t-test, which confirms that the G-PINNs model's performance improvement is significant and not a consequence of random variation. Also, 5-fold cross-validation is performed to ensure robust and unbiased model evaluation across different data splits. © 2025 Elsevier B.V., All rights reserved.","Rani, I.; Kumar Verma, C.K.",2025,10.1016/j.enganabound.2025.106396,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011519011&doi=10.1016%2Fj.enganabound.2025.106396&partnerID=40&md5=f568c88dee94315797af2ea970c71c42,scopus,"This study introduces G-PINNs, a Bayesian-Optimized GRU-Enhanced Physics-Informed Neural Network, to improve short-rate model predictions for yield forecasting, zero-coupon bond pricing, and option pricing. The model integrates GRU for sequential data and PINNs to enforce financial constraints from the Hull-White model. Using US treasury yield data, G-PINNs demonstrated superior performance over Vanilla PINNs, with statistically significant improvements confirmed by paired t-tests and cross-validation.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-28T10:11:57.362991
703481e0f3994ee0,"Good volatility, bad volatility, and the cross section of cryptocurrency returns","This paper examines the predictability of realized volatility measures (RVM), especially the realized signed jumps (RSJ), on future volatility and returns. We confirm the existence of volatility persistence and future volatility is more strongly related to the volatility of past positive returns than to that of negative returns in the cryptocurrency market. RSJ-sorted cryptocurrency portfolios yield statistically and economically significant differences in the subsequent portfolio returns. After controlling for cryptocurrency market characteristics and existing risk factors, the differences remain significant. The investor attention explains the predictability of realized jump risk in future cryptocurrency returns.","Zhang, Zehua; Zhao, Ran",2023,10.1016/j.irfa.2023.102712,,wos,"This paper investigates the predictive power of realized volatility measures, particularly realized signed jumps (RSJ), on future cryptocurrency returns and volatility. It finds that future volatility is more influenced by past positive returns than negative ones, and RSJ-sorted portfolios show significant return differences. Investor attention is identified as a factor explaining this predictability.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:11:59.802588
f274dfa7538c8887,Government Bonds of the CIS Countries: Integration Dynamics of Debt Markets in the Context of External Instability,"The paper examines development specifics of government bond markets in the CIS countries. The sample includes Russia, Kazakhstan, Uzbekistan and Azerbaijan, since only these countries, among the CIS members, possess enough government bonds included in the global debt market. The relevance of the study is due to the increasing financial uncertainty, which attracts attention to relatively reliable means of public debt; the need to understand the functioning of debt markets against the background of anti -Russian sanctions and the increasing influence of the State. The aim of the work is to empirically verify the connectivity, integration and predictability of the government bond markets of Russia, Kazakhstan, Uzbekistan and Azerbaijan. Empirical data include daily refinancing rates of national central banks, indices of total government bond yields, G -spreads of international bonds of the countries in relation to the conditionally risk -free US bond yield curve for 2019-2023. The effects of market development features are divided into local, regional and global, such as the reaction to COVID-19 and anti -Russian sanctions after 2022. We use the following methods: dynamics analysis, correlation, factor and regression analysis. The novelty of the research lies in introducing new empirical data into scientific discourse, testing a methodology that allows us to assess the interaction of monetary policies and the functioning of government bond markets, common features and differences in the behavior of these markets before and after the imposition of sanctions against the Russian financial system. We conclude that the integration of the considered markets within the CIS is violated, which poses risks to the effective economic development of the region. We consider the relatively developed and integrated, but poorly predictable markets of Russia and Kazakhstan. Unlike Russia, Kazakhstan has more connectivity regarding its monetary policy, government bond yields and risks. The yield of Azerbaijan's government bonds is influenced by a more developed market of Kazakhstan, especially in terms of risk assessment, but the market itself is developed poorly. Uzbekistan's market is even less integrated and developed.","Romashkina, G. f.; Andrianov, K. v.; Yukhtanova, Yu. A.",2024,10.15838/esc.2024.2.92.8,,wos,"This study analyzes the government bond markets of Russia, Kazakhstan, Uzbekistan, and Azerbaijan, focusing on their integration and predictability amidst external instability. Using daily data from 2019-2023, including refinancing rates, bond yield indices, and G-spreads, the research employs correlation, factor, and regression analyses to assess market connectivity. Findings indicate a violation of integration within the CIS region, with Russia and Kazakhstan exhibiting relatively developed but poorly predictable markets. Kazakhstan's monetary policy shows greater connectivity than Russia's. Azerbaijan's bond yields are influenced by Kazakhstan's market, though Azerbaijan's market is less developed. Uzbekistan's market is the least integrated and developed. The study highlights the impact of events like COVID-19 and anti-Russian sanctions on these markets.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:12:02.226988
d88500794b020572,Governmental Investment Impacts on the Construction Sector Considering the Liquidity Trap,"Considering the liquidity trap is critical as a primary step for a complete understanding of public investment's impacts on the financial supply and demand within the construction industry during deflationary periods. However, minimal research has been conducted to formulate efficient models that can quantify optimal governmental investments. To bridge the gap, an integrated model of the investment savings-liquidity preference money supply (IS-LM) curve and the dynamic stochastic general equilibrium (DSGE) analysis was developed to investigate the balance of supply and demand during deflation status in addition to the associated spending adjustment mechanisms. The most recent data were analyzed, and the deep parameters were obtained using Bayesian estimation via the Markov chain Monte Carlo (MCMC) technique. The analysis result showed that public investment within economies in a deflationary state, which is in a liquidity trap, are expected to crowd out private investment. Also, due to the issuance of government bonds during deflation, the effect of public investment in this situation is more significant than that during inflation. Therefore, decision makers can use the proposed model to manage and quantify the highway construction and maintenance sector's governmental annual optimal investment. © 2021 Elsevier B.V., All rights reserved.","Alshboul, O.; Shehadeh, A.; Hamedat, O.",2022,10.1061/(asce)me.1943-5479.0001003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122233790&doi=10.1061%2F%28ASCE%29ME.1943-5479.0001003&partnerID=40&md5=f82449215e4cf8df3ce0d0b1678d527e,scopus,This study develops an integrated IS-LM and DSGE model to analyze the impact of governmental investment on the construction sector during a liquidity trap. It found that public investment can crowd out private investment and has a more significant effect during deflation than inflation. The model can be used to quantify optimal annual governmental investment in highway construction and maintenance.,True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:12:04.986874
82eadd829f8e6a8b,Habit formation and macroeconomic models of the term structure of interest rates,"This paper introduces a new class of nonaffine models of the term structure of interest rates that is supported by an economy with habit formation. Distinguishing features of the model are that the interest rate dynamics are nonlinear, interest rates depend on lagged monetary and consumption shocks, and the price of risk is not a constant multiple of interest rate volatility. We find that habit persistence can help reproduce the nonlinearity of the spot rate process, the documented deviations from the expectations hypothesis, the persistence of the conditional volatility of interest rates, and the lead-lag relationship between interest rates and monetary aggregates.","Buraschi, Andrea; Jiltsov, Alexei",2007,10.1111/j.1540-6261.2007.01299.x,,wos,"This paper presents a new class of non-affine models for the term structure of interest rates, incorporating habit formation in the economy. Key features include nonlinear interest rate dynamics, dependence on lagged monetary and consumption shocks, and a price of risk not directly proportional to interest rate volatility. The model successfully explains the nonlinearity of the spot rate process, deviations from the expectations hypothesis, persistent conditional volatility, and the lead-lag relationship between interest rates and monetary aggregates.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:12:09.179178
03efc18d2b3b0920,Health and Quality Risk Assessment of Bottled Water,"The risk and quality assessment paper is dedicated to estimation of impact (for human health) of bottled drinking-water package (especially PET one). The investigation is concentrated on using one integral method for different risk types (factors connected with potential carcinogenicity: concentrations of antimony, formaldehyde, diethylhexylphthalate; and organoleptic factors: turbidity, colour, and pH). We imply the nature of organoleptic (quality assessment) factors close to risk ones because their indirect influence on polluting power of chemicals can be amplifying. The acceptable risk levels for these types are fixed as 10-1 and 10-5 respectively. The research is based on Russian and International (principally, American) scientific researches and standards. The calculation of risk metric is proposed to be estimated in dimensionless number (hazard quotient – HQ). HQ can be transformed in probabilistic numbers in conversion to events per million (Risk Index – RI and risk of olfactory-reflectory impact factors, Integral Index of Water Risk). In the article we used the idea of “chronic daily intake” (CDI) as an acceptable risk-free measure of factors of potential carcinogenicity, which is an adequate evaluation of permissible concentration. 5 brands of Russian bottled water were analyzed, it turned out that one of them had an exceeded acceptable level of risk.",,2019,10.1088/1755-1315/272/2/022142,,proquest,"This paper assesses the health and quality risks associated with bottled water packaging, particularly PET. It uses an integrated method to evaluate carcinogenicity factors (antimony, formaldehyde, diethylhexylphthalate) and organoleptic factors (turbidity, color, pH), proposing a hazard quotient (HQ) for risk calculation. Analysis of five Russian bottled water brands revealed one exceeded acceptable risk levels.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:12:12.242752
24c473804012a63d,Heterogeneous beliefs with information processing capacity constraints and asset pricing in a monetary economy,"This paper proposes a monetary model to explore the influences of heterogeneous beliefs and information processing capacity constraints on the dynamics of asset prices. The capacity constraints not only influence the estimations of the capacity constrained investor, but also generate persistent disagreements among the investors. The model implies that reducing the levels of capacity constraints can alleviate the influences of heterogeneous beliefs and helps to stabilize financial markets. The model also reveals that introducing heterogeneous beliefs about both real and nominal sectors not only leads the stock with low monetary policy exposure to have significantly higher average return than the stock with high monetary policy exposure, but also can explain the mixed results about the relationship between the volatility and the risk premium of the aggregate stock market.","Wang, Hailong; Hu, Duni",2024,10.1016/j.najef.2024.102143,,wos,This paper develops a monetary model to investigate how heterogeneous beliefs and constraints on information processing capacity affect asset prices. It suggests that these constraints create lasting disagreements among investors and that easing them can stabilize markets. The model also shows that incorporating heterogeneous beliefs about both real and nominal sectors can explain differences in stock returns based on monetary policy exposure and reconcile mixed findings on the relationship between stock market volatility and risk premium.,False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:12:14.442409
7263c110b7556c6d,"How Are Interbank and Sovereign Debt Markets Linked? Evidence from 14 OECD Countries, the Euro Area and Russia","The paper explores causal linkages between interbank and sovereign bond markets in 14 OECD countries, the Euro area and Russia during the 2008-2009 crisis and post-crisis period. The analysis has been carried out for individual countries and in a multivariate framework. It enables to identify systemically important countries in both markets. The USA, Switzerland, Australia, South Korea and Russia are of particular significance in the interbank lending market. Switzerland, the UK, Poland, Australia and Canada play a pivotal role in the public debt market. The analysis under the multivariate framework reveals substantial heterogeneity in the network structure of both markets. Only 12% of causal relationships coincide, which may fuel financial contagion. Volatility spillovers underlie the causal linkages. They are estimated by means of dynamic volatility indices based on rolling correlation matrices and help identify the transformation of the international banking turmoil into the sovereign debt crisis.","Stolbov, Mikhail",2014,10.2298/pan1403331s,,wos,"This paper investigates the causal links between interbank and sovereign bond markets in 14 OECD countries, the Euro area, and Russia, focusing on the 2008-2009 crisis and post-crisis periods. It identifies systemically important countries in both markets and reveals significant heterogeneity in network structures, with only 12% of causal relationships coinciding. Volatility spillovers are identified as the basis for these causal linkages, explaining the transmission of international banking turmoil to sovereign debt crises.",True,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:12:19.202504
585f44403016e47b,How do zero-coupon inflation swaps predict inflation rates in the euro area? Evidence of efficiency and accuracy on 1-year contracts,"This paper examines the risk-neutral efficient market hypothesis for inflation swap markets in the euro area from 2005.10 to 2014.07. Overall, we conclude that 1-year zero-coupon inflation swap rates are unbiased predictors of inflation rates. Further, there is no empirical evidence of an inflation risk premium and the assumption of rationality seems to hold. Definitely, these inferences encourage the reading of inflation expectations embedded in short-term inflation swaps. Additionally, we compare the predictive ability of inflation swaps with other measures of inflation expectations. The in-sample results show that, in contrast with surveys, market-based measures are able to accurately forecast inflation rates. In turn, based on an out-of-sample analysis, a straightforward econometric model dominates other sources. Therefore, a combined analysis that uses different sources contributes to a more robust view of future inflation rates.","Ribeiro, Pedro Pires; Curto, Jose Dias",2018,10.1007/s00181-017-1268-8,,wos,"This paper investigates the efficiency and accuracy of 1-year zero-coupon inflation swaps in predicting euro area inflation rates from 2005 to 2014. It finds that these swap rates are unbiased predictors, with no evidence of an inflation risk premium, suggesting rationality in the market. The study also compares inflation swaps with other expectation measures, concluding that market-based measures, particularly through a simple econometric model, offer accurate inflation forecasts.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:12:21.298935
e43070a782da8f80,How good are analyst forecasts of oil prices?,"Even though there is a wide consensus that having good oil price forecasts is very valuable for many agents in the economy, results have not been fully satisfactory and there is an ongoing effort to improve their accuracy. Research has explored many different modeling approaches including time series, regressions, and artificial intelligence, among others. Also, many different sources of input data have been used like spot and futures prices, product spreads, and micro and macro variables. This paper explores how useful analyst expected price data are for forecasting when appropriate measures are taken to account for their sparse nature and high volatility. It proposes a multifactor stochastic pricing model, with time-varying risk premiums calibrated with filtered futures and analyst forecasts using a Kalman Filter. The forecasting model is applied to ten years of oil prices and analyst forecasts, from NYMEX and Bloomberg, respectively. Results are very encouraging showing that the model forecasts are much better than the no-change forecasts, commonly used as a benchmark, and better than those from the widely used Bloomberg's Consensus Expected Price Model. We conclude that analyst forecasts are a valuable source of input data that should be considered in future forecasting models.",,2021,10.1016/j.eneco.2021.105500,,proquest,"This paper investigates the usefulness of analyst forecasts for predicting oil prices. It proposes a multifactor stochastic pricing model incorporating a Kalman Filter to handle sparse and volatile analyst data. The model, tested on ten years of oil price data, demonstrates superior forecasting accuracy compared to benchmark models, suggesting analyst forecasts are a valuable input for future models.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:12:23.330964
4ad7c5f96fa212c7,How informative are variance risk premium and implied volatility for Value-at-Risk prediction? International evidence,"The aim of this paper is to examine the information embedded in the implied volatility index and the variance risk premium in terms of quantifying market risk for developed and emerging stock markets. The backtesting results indicate that incorporating the relative variance risk premium into the GARCH model, greatly enhances the forecasts of one-day-ahead Value-at-Risk (VaR) for a long trading position in developed markets, while the standard GARCH is the most relevant specification in capturing risk in emerging markets. Results are found to be robust against distressed financial markets and alternative measures of the variance risk premium. Moreover, the empirical evidence shows that the superior performance of these models cannot completely reduce the scope of implied volatility as a risk management tool. Including implied volatility into the GARCH model incurs substantial savings in terms of efficient regulatory capital provisions. (C) 2019 Board of Trustees of the University of Illinois. Published by Elsevier Inc. All rights reserved.","Slim, Skander; Dahmene, Meriam; Boughrara, Adel",2020,10.1016/j.qref.2019.08.006,,wos,"This paper investigates the predictive power of implied volatility and variance risk premium for Value-at-Risk (VaR) in developed and emerging stock markets. It finds that the variance risk premium improves VaR forecasts in developed markets when incorporated into a GARCH model, while the standard GARCH model is sufficient for emerging markets. Implied volatility also proves valuable for risk management and efficient regulatory capital provisions.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:12:32.298920
6a24a503b3d75e11,"Human health risk assessment for exposure to BTEXN in an urban aquifer using deterministic and probabilistic methods: A case study of Chennai city, India","The aquifer in Tondiarpet, Chennai, had been severely contaminated with petroleum fuels due to an underground pipeline leakage. Groundwater samples were analyzed quarterly for priority pollutants such as benzene, toluene, ethylbenzene, xylenes, and naphthalene (BTEXN) using purge and trap gas chromatography and mass spectrometer from 2016 to 2018. The maximum concentrations of BTEXN in groundwater at the site were found to be greater than the permissible limits significantly. Among the five sampling locations (MW1, MW2, MW3, MW4, and MW5), mean BTEXN levels were found to be higher near MW2, confirming the source location of petroleum leakage. Human health risk assessment was carried out using deterministic and probabilistic methods for exposure to BTEXN by oral and dermal exposure pathways. Risk analysis indicated that mean cancer and non-cancer risks were many times higher than the allowable limits of 1E-06 and 1 respectively in all age groups (children, teens, and adults), implying the adverse health effects. Oral exposure is predominately contributing (60-80%) to the total health risk in comparison to the dermal exposure route. Variability and uncertainty were addressed using the Monte Carlo simulations and the resultant minimum, maximum, 5th, 95th, and mean percentile risks were predicted. Under the random exposure conditions to BTEXN, it was estimated that the risk would become unacceptable for >98.7% of the exposed population. Based on the sensitivity analysis, exposure duration, and ingestion rate are the crucial variables contributing significantly to the health risk. As part of the risk management, preliminary remediation goals for the study site were estimated, which require >99% removal of the BTEXN contamination for risk-free exposures. It is suggested that the residents of Tondiarpet shouldn't utilize the contaminated groundwater mainly for oral ingestion to lower the cancer incidence related to exposure to BTEXN.The aquifer in Tondiarpet, Chennai, had been severely contaminated with petroleum fuels due to an underground pipeline leakage. Groundwater samples were analyzed quarterly for priority pollutants such as benzene, toluene, ethylbenzene, xylenes, and naphthalene (BTEXN) using purge and trap gas chromatography and mass spectrometer from 2016 to 2018. The maximum concentrations of BTEXN in groundwater at the site were found to be greater than the permissible limits significantly. Among the five sampling locations (MW1, MW2, MW3, MW4, and MW5), mean BTEXN levels were found to be higher near MW2, confirming the source location of petroleum leakage. Human health risk assessment was carried out using deterministic and probabilistic methods for exposure to BTEXN by oral and dermal exposure pathways. Risk analysis indicated that mean cancer and non-cancer risks were many times higher than the allowable limits of 1E-06 and 1 respectively in all age groups (children, teens, and adults), implying the adverse health effects. Oral exposure is predominately contributing (60-80%) to the total health risk in comparison to the dermal exposure route. Variability and uncertainty were addressed using the Monte Carlo simulations and the resultant minimum, maximum, 5th, 95th, and mean percentile risks were predicted. Under the random exposure conditions to BTEXN, it was estimated that the risk would become unacceptable for >98.7% of the exposed population. Based on the sensitivity analysis, exposure duration, and ingestion rate are the crucial variables contributing significantly to the health risk. As part of the risk management, preliminary remediation goals for the study site were estimated, which require >99% removal of the BTEXN contamination for risk-free exposures. It is suggested that the residents of Tondiarpet shouldn't utilize the contaminated groundwater mainly for oral ingestion to lower the cancer incidence related to exposure to BTEXN.",,2020,10.1016/j.envpol.2020.114814,,proquest,"This study assesses the human health risks associated with BTEXN contamination in an urban aquifer in Chennai, India, resulting from a petroleum pipeline leak. Groundwater analysis revealed BTEXN concentrations exceeding permissible limits. Both deterministic and probabilistic methods were employed to evaluate cancer and non-cancer risks from oral and dermal exposure across different age groups. The findings indicate significantly elevated risks, with oral exposure being the primary contributor. Monte Carlo simulations were used to address uncertainty, predicting unacceptable risks for over 98.7% of the exposed population under random exposure conditions. Sensitivity analysis identified exposure duration and ingestion rate as key risk factors. Remediation goals suggest over 99% BTEXN removal is needed for safe exposure. Residents are advised against using the contaminated groundwater for oral ingestion.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:12:33.866988
53eaa29638130e5b,Implied volatility surface predictability: The case of commodity markets,"Recent literature seek to forecast implied volatility derived from equity, index, foreign exchange, and interest rate options using latent factor and parametric frameworks. Motivated by increased public attention borne out of the financialization of futures markets in the early 2000s, we investigate if these extant models can uncover predictable patterns in the implied volatility surfaces of the most actively traded commodity options between 2006 and 2016. Adopting a rolling out-of-sample forecasting framework that addresses the common multiple comparisons problem, we establish that, for energy and precious metals options, explicitly modeling the term structure of implied volatility using the Nelson-Siegel factors produces the most accurate forecasts. © 2019 Elsevier B.V., All rights reserved.","Kearney, F.; Shang, H.L.; Sheenan, L.",2019,10.1016/j.jbankfin.2019.105657,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072749898&doi=10.1016%2Fj.jbankfin.2019.105657&partnerID=40&md5=725a9ef86c2c5b30331c554ee665baeb,scopus,"This study investigates the predictability of implied volatility surfaces in commodity markets (energy and precious metals options) between 2006 and 2016. It adapts existing latent factor and parametric frameworks from equity and FX markets, finding that modeling the term structure of implied volatility using Nelson-Siegel factors yields the most accurate out-of-sample forecasts.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:12:37.514928
ffa96bdfcb8e2559,Improving the Accuracy of Forecasting the TSA Daily Budgetary Fund Balance Based on Wavelet Packet Transforms,"Improving the accuracy of cash flow forecasting in the TSA is the key to fulfilling government payment obligations, minimizing the cost of maintaining the cash reserve, providing the absence of outstanding debt accumulation, and ensuring investment in various financial instruments to obtain additional income. The article describes a method for improving the accuracy of forecasting a time series composed of daily budgetary fund balances in the TSA, based on its preliminary decomposition using a discrete wavelet packet transform of the Daubechies family. This makes it possible to increase the accuracy of traditional forecasting methods from 80% to more than 96%. The decomposition level varied from one to eight to minimize the mean absolute error and improve the forecasting accuracy. Calculations of statistical tests for adequacy confirm the effectiveness of the proposed method for improving forecasting accuracy. The scientific novelty of the proposed method for improving the forecasting accuracy of time series from daily budgetary fund balances in the TSA lies in proving the need for preliminary timeseries decomposition and subsequent construction of forecasts for the obtained parts, resulting in high forecasting accuracy. The result differs significantly from traditional econometric methods (ARIMA/SARIMA), characterized by a much lower accuracy (50–80%) and a decrease in forecasting accuracy with an increase in the forecast horizon. This article is novel, as it forms a new approach to solving the problem of increasing the efficiency of using budgetary funds, associated with improving the accuracy of forecasting daily budgetary fund balance in the TSA.",,2022,10.3390/joitmc8030107,,proquest,This article proposes a method to improve the accuracy of forecasting the TSA daily budgetary fund balance by using wavelet packet transforms for time series decomposition before applying traditional forecasting methods. This approach significantly increases accuracy compared to standard econometric models like ARIMA/SARIMA.,True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:12:40.891004
17c70911a878167b,Incorporating Markov decision process on genetic algorithms to formulate trading strategies for stock markets,"With the arrival of low interest rates, investors entered the stock market to seek higher returns. However, the stock market proved volatile, and only rarely could investors gain excess returns when trading in real time. Most investors use technical indicators to time the market. However the use of technical indicators is associated with problems, such as indicator selection, use of conflicting versus similar indicators. Investors thus have difficulty relying on technical indicators to make stock market investment decisions. This research combines Markov decision process and genetic algorithms to propose a new analytical framework and develop a decision support system for devising stock trading strategies. This investigation uses the prediction characteristics and real-time analysis capabilities of the Markov decision process to make timing decisions. The stock selection and capital allocation employ string encoding to express different investment strategies for genetic algorithms. The parallel search capabilities of genetic algorithms are applied to identify the best investment strategy. Additionally, when investors lack sufficient money and stock, the architecture of this study can complete the transaction via credit transactions. The experiments confirm that the model presented in this research can yield higher rewards than other benchmarks. © 2017 Elsevier B.V., All rights reserved.","Chang, Y.-H.; Lee, M.-S.",2017,10.1016/j.asoc.2016.09.016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994403308&doi=10.1016%2Fj.asoc.2016.09.016&partnerID=40&md5=7f450fb3552c013c98635d476b9fe7e5,scopus,"This research proposes a new framework combining Markov decision process and genetic algorithms to develop a decision support system for stock trading strategies. It uses the Markov decision process for timing decisions and genetic algorithms for stock selection and capital allocation, aiming to identify the best investment strategy and achieve higher rewards than benchmarks.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:12:52.971188
5355b131ba74d9f1,Incorporating Research Reports and Market Sentiment for Stock Excess Return Prediction: A Case of Mainland China,"The prediction of stock excess returns is an important research topic for quantitative trading, and stock price prediction based on machine learning is receiving more and more attention. This article takes the data of Chinese A-shares from July 2014 to September 2017 as the research object, and proposes a method of stock excess return forecasting that combines research reports and investor sentiment. The proposed method measures individual stocks released by analysts, separates the two indicators of research report attention and rating sentiment, calculates investor sentiment based on external market factors, and uses the LSTM model to represent the time series characteristics of stocks. The results show that (1) the accuracy and F1 evaluation indicators are used, and the proposed algorithm is better than the benchmark algorithm. (2) The performance of deep learning LSTM algorithm is better than traditional machine learning algorithm SVM. (3) Investor sentiment as the initial hidden state of the model can improve the accuracy of the algorithm. (4) The attention of the split research report takes the two indicators of investor sentiment and price as the input of the model, which can effectively improve the performance of the model.",,2020,10.1155/2020/8894757,,proquest,"This study proposes a method for forecasting stock excess returns in Mainland China by combining analyst research reports and investor sentiment, using an LSTM model. The results indicate that this combined approach outperforms benchmark algorithms and traditional machine learning models like SVM, with specific components like investor sentiment as the initial hidden state and split research report attention improving model performance.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:13:27.290572
049416f83ae200ea,"Indian equity options: Smile, risk premiums, and efficiency","We study the pricing of equity options in India which is one of the world's largest options markets. Our findings are supportive of market efficiency: A parsimonious smile-adjusted Black model fits option prices well, and the implied volatility (IV) has incremental predictive power for future volatility. However, the risk premium embedded in IV for Single Stock Options appears to be higher than in other markets. The study suggests that even a very liquid market with substantial participation of global institutional investors can have structural features that lead to systematic departures from the behavior of a fully rational market while being microefficient.","Jain, Sonali; Varma, Jayanth R.; Agarwalla, Sobhesh Kumar",2019,10.1002/fut.21971,,wos,"This study examines Indian equity options, finding evidence of market efficiency with a Black model fitting prices well and implied volatility predicting future volatility. However, it notes a potentially higher risk premium in Indian Single Stock Options compared to other markets, suggesting that even liquid markets can exhibit systematic deviations from rational behavior.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:14:29.490719
f02729ef603cf0e9,Inference on co-integration parameters in heteroskedastic vector autoregressions,"We consider estimation and hypothesis testing on the coefficients of the co-integrating relations and the adjustment coefficients in vector autoregressions driven by shocks which display both conditional and unconditional heteroskedasticity of a quite general and unknown form. We show that the conventional results in Johansen (1996) for the maximum likelihood estimators and associated likelihood ratio tests derived under homoskedasticity do not in general hold under heteroskedasticity. As a result, standard confidence intervals and hypothesis tests on these coefficients are potentially unreliable. Solutions based on Wald tests (using a sandwich estimator of the variance matrix) and on the use of the wild bootstrap are discussed. These do not require the practitioner to specify a parametric model for volatility. We establish the conditions under which these methods are asymptotically valid. A Monte Carlo simulation study demonstrates that significant improvements in finite sample size can be obtained by the bootstrap over the corresponding asymptotic tests in both heteroskedastic and homoskedastic environments. An application to the term structure of interest rates in the US illustrates the difference between standard and bootstrap inferences regarding hypotheses on the co-integrating vectors and adjustment coefficients. (C) 2015 Elsevier B.V. All rights reserved.","Boswijk, H. Peter; Cavaliere, Giuseppe; Rahbek, Anders; Taylor, A. M. Robert",2016,10.1016/j.jeconom.2015.07.005,,wos,"This paper investigates the estimation and hypothesis testing of co-integration parameters in vector autoregressions with heteroskedasticity. It demonstrates that standard methods assuming homoskedasticity are unreliable under heteroskedasticity and proposes solutions using Wald tests and wild bootstrap, which do not require specifying a parametric volatility model. A simulation study shows bootstrap improvements, and an application to the US term structure of interest rates illustrates differences in inference.",False,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:14:31.506466
07ccec1141e89737,Institutional investor attention and stock market volatility and liquidity: international evidence,"In this paper, we examine the influence of the daily institutional investor attention to particular stocks on stock volatility and liquidity. The institutional investor attention is measured from the number of times that users of Bloomberg terminal, who are mostly institutional investors, search for or read articles on a specific stock. Relying on a large international dataset of approximately a million daily observations over the period 2011-2020 from nine countries (Canada, France, Germany, Japan, Russia, South Korea, Switzerland, the UK, and the US), we find that this recent measure of institutional investor attention has a strong positive effect on stock volatility and liquidity. Confirmed by a battery of robustness tests, our findings suggest that this continuous barometer of attention by institutional investors can be used by financial practitioners to predict future stock volatility and liquidity.","El Ouadghiri, Imane; Erragragui, Elias; Jaballah, Jamil; Peillex, Jonathan",2022,10.1080/00036846.2022.2036689,,wos,"This study investigates how institutional investor attention, measured by Bloomberg terminal searches, impacts stock market volatility and liquidity across nine countries from 2011-2020. The findings indicate a significant positive relationship, suggesting this attention metric can predict future stock volatility and liquidity.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:14:32.954725
c98db5fb581ab79d,Integrated prediction of green bond return under the dual risks of climate change and energy crisis,"Prediction of bond return is a classic problem in financial area, providing an important basis for portfolio construction and risk management. The sustainable investment attribute of green bonds has been favored by investors, so that green bonds have become an important component for major asset allocation. However, due to the specific investment focus of green bonds, investors' return expectations are influenced not only by traditional corporate bond factors, but also by related factors such as climate change and energy transition. Against the backdrop of increasingly severe climate risks and the global energy crisis, this paper analyses the volatility characteristics of China's green bonds at multiple time scales, and introduces exogenous variables such as returns of the alternative financial assets, climate risks and returns of energy markets for prediction. Based on the LSTM model, the volatility of green bond yield at different time scales is separately predicted using optimal exogenous variable before integration. It is found that the new integrated prediction model can significantly improve the forecasting performance compared to traditional single LSTM models and simple decomposition-integrated models. Further, both climate risks and energy markets variables have a significant improvement effect on predicting green bond in low-frequency item, while energy markets variables also have a better predictive effect on trend items. Building on the use of only LSTM model, it could be further enhanced by integrating more algorithms to select the best single model for each component, further improve the prediction accuracy and provide a more effective quantitative tool for investment decision-making and risk management in related fields.","Nie, Qimiao; Chen, Siying; Chen, Yiming; Hu, Yiguo",2023,10.3389/fenvs.2023.1336867,,wos,"This paper develops an integrated prediction model for green bond returns, incorporating climate change and energy crisis risks. Using an LSTM model, it analyzes volatility at multiple time scales and integrates exogenous variables like alternative asset returns, climate risks, and energy market returns. The model shows improved forecasting performance compared to traditional methods, with climate and energy variables significantly impacting low-frequency and trend components.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:14:36.683097
3de04c8e7553d522,Interest Rate Based on The Lie Group SO(3) in the Evidence of Chaos,"This paper aims to test the structure of interest rates during the period from 1 September 1981 to 28 December 2020 by using Lie algebras and groups. The selected period experienced substantial events impacting interest rates, such as the economic crisis, the military intervention of the USA in Iraq, and the COVID-19 pandemic, in which economies were in lockdown. These conditions caused the interest rate to have a nonlinear structure, chaotic behavior, and outliers. Under these conditions, an alternative method is proposed to test the random and nonlinear structure of interest rates to be evolved by a stochastic differential equation captured on a curved state space based on Lie algebras and group. Then, parameter estimates of this equation were obtained by OLS, NLS, and GMM estimators (hereafter, LieNLS, LieOLS, and LieGMM, respectively). Therefore, the interest rates that possess nonlinear structures and/or chaotic behaviors or outliers were tested with LieNLS, LieOLS, and LieGMM. We compared our LieNLS, LieOLS, and LieGMM results with the traditional OLS, NLS, and GMM methods, and the results favor the improvement achieved by the proposed LieNLS, LieOLS, and LieGMM in terms of the RMSE and MAE in the out-of-sample forecasts. Lastly, the Lie algebras with NLS estimators exhibited the lowest RMSE and MAE followed by the Lie algebras with GMM, and the Lie algebras with OLS, respectively.",,2022,10.3390/math10213998,,proquest,"This paper investigates the nonlinear and chaotic structure of interest rates from 1981 to 2020 using Lie algebras and groups. It proposes an alternative method to test random and nonlinear structures, evolving interest rates with a stochastic differential equation on a curved state space. The study compares parameter estimates from LieNLS, LieOLS, and LieGMM with traditional OLS, NLS, and GMM, finding improved out-of-sample forecast accuracy with the Lie-based methods, particularly LieNLS.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:14:39.322825
9e2b52f34baf3156,Interest Rate Model With Investor Attitude and Text Mining,"This paper develops and estimates an interest rate model with investor attitude factors, which are extracted by a text mining method. First, we consider two contrastive attitudes (optimistic versus conservative) towards uncertainties about Brownian motions driving economy, develop an interest rate model, and obtain an empirical framework of the economy consisting of permanent and transitory factors. Second, we apply the framework to a bond market under extremely low interest rate environment in recent years, and show that our three-factor model with level, steepening and flattening factors based on different investor attitudes is capable of explaining the yield curve in the Japanese government bond (JGB) markets. Third, text mining of a large text base of daily financial news reports enables us to distinguish between steepening and flattening factors, and from these textual data we can identify events and economic conditions that are associated with the steepening and flattening factors. We then estimate the yield curve and three factors with frequencies of relevant word groups chosen from textual data in addition to observed interest rates. Finally, we show that the estimated three factors, extracted only from the bond market data, are able to explain the movement in stock markets, in particular Nikkei 225 index.",S. Nakatani; K. G. Nishimura; T. Saito; A. Takahashi,2020,10.1109/access.2020.2992477,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9086462,ieeexplore,"This paper develops an interest rate model incorporating investor attitudes derived from text mining. It proposes a three-factor model (level, steepening, flattening) to explain the Japanese government bond yield curve, distinguishing between optimistic and conservative investor attitudes. Text mining of financial news helps identify factors associated with specific economic conditions. The model's factors, derived from bond market data, also show explanatory power for stock market movements, specifically the Nikkei 225 index.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-28T10:14:55.010400
ae189095c8b70524,Interest rate prediction: a neuro-hybrid approach with data preprocessing,"The following research implements a differential evolution-based fuzzy-type clustering method with a fuzzy inference neural network after input preprocessing with regression analysis in order to predict future interest rates, particularly 3-month T-bill rates. The empirical results of the proposed model is compared against nonparametric models, such as locally weighted regression and least squares support vector machines, along with two linear benchmark models, the autoregressive model and the random walk model. The root mean square error is reported for comparison.","Mehdiyev, Nijat; Enke, David",2014,10.1080/03081079.2014.883386,,wos,"This study proposes a neuro-hybrid model using differential evolution-based fuzzy-type clustering and a fuzzy inference neural network, preceded by regression analysis, to forecast 3-month T-bill interest rates. The model's performance is evaluated against nonparametric and linear benchmark models using root mean square error.",True,False,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-28T10:15:11.418704
2f9cd73a4b3f546d,Interest rate spreads as predictors of German inflation and business cycles,"We have studied the comparative performance of a number of interest rate spreads as predictors of the German inflation and business cycle in the post-Bretton Woods era. The two-regime Markov-switch model that we used as a nonlinear filter allows the dynamic behavior of the economy to vary between expansions and recessions in terms of duration and volatility. We found that the bank term structure, the public term structure, and the spread based on the call rate predicted all recessions with a comfortable lead, although they lagged some of the recoveries by a few months. The bank-public spread generates a series of false signals, and missed completely the upturn in the mid-1970s, but detected the last two recoveries with an average lead of nearly 12 months. The source of the predictive power of interest rate spreads lies in the information they contain not only about monetary policy, but also about an assortment of general macroeconomic shocks. The filter probabilities from three of the interest rate differentials also foreshadowed the long swings in the German inflation rate remarkably well, with a lead time of 2-4 years without any false signals. © International Institute of Forecasters. © 2017 Elsevier B.V., All rights reserved.","Ivanova, D.; Lahiri, K.; Seitz, F.",2000,10.1016/s0169-2070(99)00029-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0010061067&doi=10.1016%2FS0169-2070%2899%2900029-1&partnerID=40&md5=7b914d1774858e720fa00fff034fe499,scopus,"This study investigates the predictive power of various interest rate spreads for German inflation and business cycles in the post-Bretton Woods era. Using a two-regime Markov-switch model, the research indicates that certain spreads (bank term structure, public term structure, call rate spread) effectively predict recessions, while others show mixed results for recoveries. The predictive ability is attributed to the information these spreads hold about monetary policy and macroeconomic shocks. Additionally, some interest rate differentials accurately foreshadowed long swings in German inflation.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:15:16.546859
ad94029bc09772d2,Introduction to m-m processes,"In this paper, we introduce a new type of nonlinear model, called the min-max model, and analyze its properties for a pair of series. The stability conditions of this system are given for a nonlinearly integrated bivariate series. Under these stability conditions, the difference between the two series exhibits threshold-type nonlinearity. It is possible to construct a threshold error correction model from the min-max processes. Neglected nonlinearity tests are applied, both to the univariate series and to the bivariate system, in order to detect nonlinearity, and it turns out that the tests using the bivariate series have better power. We apply the min-max model to U.S. Treasury bills and commercial paper interest rates. The spread of these interest rates shows threshold-type nonlinearity, and this model outperforms a linear model in terms of its predictability for out-of-sample data. © 2005 Elsevier B.V. All rights reserved. © 2008 Elsevier B.V., All rights reserved.","Granger, C.W.J.; Hyung, N.",2006,10.1016/j.jeconom.2004.09.013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-28244490968&doi=10.1016%2Fj.jeconom.2004.09.013&partnerID=40&md5=9e94f88585e6c3c302b12e0c1abef8a6,scopus,"This paper introduces a novel min-max nonlinear model for analyzing pairs of series, focusing on stability conditions and threshold-type nonlinearity in the difference between series. It demonstrates that this model can be used to construct threshold error correction models and that nonlinearity tests are more powerful when applied to bivariate series. The model is applied to U.S. Treasury bills and commercial paper interest rates, showing its effectiveness in predicting out-of-sample data and outperforming linear models.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:15:42.578429
9172933e44b166cd,Investigation of the Financial Stability of S&P 500 Using Realized Volatility and Stock Returns Distribution,"In this work, the financial data of 377 stocks of Standard & Poor’s 500 Index (S&P 500) from the years 1998–2012 with a 250-day time window were investigated by measuring realized stock returns and realized volatility. We examined the normal distribution and frequency distribution for both daily stock returns and volatility. We also determined the beta-coefficient and correlation among the stocks for 15 years and found that, during the crisis period, the beta-coefficient between the market index and stock’s prices and correlation among stock’s prices increased remarkably and decreased during the non-crisis period. We compared the stock volatility and stock returns for specific time periods i.e., non-crisis, before crisis and during crisis year in detail and found that the distribution behaviors of stock return prices has a better long-term effect that allows predictions of near-future market behavior than realized volatility of stock returns. Our detailed statistical analysis provides a valuable guideline for both researchers and market participants because it provides a significantly clearer comparison of the strengths and weaknesses of the two methods.",,2018,10.3390/jrfm11020022,,proquest,"This study investigates the financial stability of the S&P 500 by analyzing realized stock returns and volatility for 377 stocks from 1998-2012. It examines stock return and volatility distributions, beta-coefficients, and correlations, finding that beta and correlation increase during crisis periods. The research suggests stock return price distributions are better predictors of near-future market behavior than realized volatility.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:15:45.306324
533240a828b01136,Investor Attention and Stock Returns,"We propose an investor attention index based on proxies in the literature and find that it predicts the stock market risk premium significantly, both in sample and out of sample, whereas every proxy individually has little predictive power. The index is extracted using partial least squares, but the results are similar by the scaled principal component analysis. Moreover, the index can deliver sizable economic gains for mean-variance investors in asset allocation. The predictive power of the investor attention index stems primarily from the reversal of temporary price pressure and from the stronger forecasting ability for high-variance stocks.",,2022,10.1017/s0022109021000090,,proquest,"This study develops an investor attention index using proxies from existing literature. The index, derived through partial least squares or scaled principal component analysis, significantly predicts the stock market risk premium, outperforming individual proxies. It also offers economic benefits for investors in asset allocation, primarily due to the reversal of temporary price pressure and improved forecasting for high-variance stocks.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:15:47.650230
85c477dfff28cf7e,Investor Sentiment and Bond Risk Premia: Evidence from China,"This article shows the statistical signi?cance of a set of variables related to market sentiment and uses them to predict the risk premium embedded in China's sovereign bonds. We construct a composite index of market-wide investor sentiment as a linear combination of proxies for a degree of market participation and risk appetite of investors. Further, we show that these sentiment-related factors can be summarized in a single-return forecasting factor, similar in a spirit of Cochrane and Piazzesi (2005). Our empirical results show that this sentiment factor has predictive power beyond that contained in the yield curve and macroeconomic variables, and this predictability is robust for out-of-sample testing. In addition, the predictive power of the sentiment factor shows relevance during the 2008 global financial crisis, indicating that the forecasting ability of investor sentiment is mainly derived by a sentiment-induced flight-to-quality.","Lee, Kiryoung; Kim, Minki",2019,10.1080/1540496x.2018.1466276,,wos,"This study constructs a composite index of investor sentiment in China's sovereign bond market, combining proxies for market participation and risk appetite. This sentiment index is shown to predict bond risk premia, outperforming traditional yield curve and macroeconomic variables, particularly during the 2008 financial crisis, suggesting a flight-to-quality effect.",False,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:16:10.650437
95a3a3ccfd16d50a,Investor attention and cryptocurrency: Evidence from the Bitcoin market,"[...]investor attention had been applied in traditional financial markets, i.e., stock market and FX market, and proved to be an influential factor in certain markets.The empirical results may shed lights on investors in Bitcoin market to focus more on the variations in behavioral variable; Second, existing studies mainly focused on the linear connections between Bitcoin market and investor attention, failing to comprehensively explore the non-linear connections between the two.[...]current research may be incomplete in explaining the relationships between investor attention and Bitcoin market.The results for out-of-sample predictions further illustrate the importance of investor attention in Bitcoin market and will surely guide the investors to forecast the Bitcoin return with the investor attention.[...]the empirical results add evidence on the in-sample and out-of-sample analysis; Fourth, based on the empirical results of out-of-sample predictions for Bitcoin return, we construct several simple portfolios including Bitcoin asset and risk-free asset to further explore the usefulness of investor attention in Bitcoin portfolio management based on the framework of asset allocation.[...]Neves [42] suggested that investment attractiveness had a prominent role in Bitcoin price formation, while other researchers [3, 34, 43, 44] argued the stock market, exchange rate, gold, oil, Economic Policy Uncertainty (EPU), and the Geopolitical Risk Index, etc.",,2021,10.1371/journal.pone.0246331,,proquest,"This study investigates the impact of investor attention on the Bitcoin market, extending previous research by exploring non-linear relationships and utilizing investor attention for out-of-sample predictions and portfolio management. The findings suggest that investor attention is a significant factor in forecasting Bitcoin returns and can guide asset allocation strategies.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:16:13.762011
d7eb87f6f4ac7e58,Investor attention and stock market volatility,"We investigate, in a theoretical framework, the joint role played by investors' attention to news and learning uncertainty in determining asset prices. The model provides two main predictions. First, stock return variance and risk premia increase with both attention and uncertainty. Second, this increasing relationship is quadratic. We empirically test these two predictions, and we show that the data lend support to the increasing relationship. The evidence for a quadratic relationship is mixed. Overall, our study shows theoretically and empirically that both attention and uncertainty are key determinants of asset prices. © 2019 Elsevier B.V., All rights reserved.","Andrei, D.; Hasler, M.",2015,10.1093/rfs/hhu059,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939258280&doi=10.1093%2Frfs%2Fhhu059&partnerID=40&md5=6ea0f026b737f00de5f512be2b375874,scopus,"This study theoretically and empirically investigates how investor attention to news and learning uncertainty jointly influence asset prices. It predicts that stock return variance and risk premia increase with both attention and uncertainty, with a potentially quadratic relationship. The empirical tests support the increasing relationship but provide mixed evidence for the quadratic aspect.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:16:15.698410
54f9c8bc1b19bffb,Investor attention using the Google search volume index – impact on stock returns,"PurposeThe purpose of this paper is to investigate whether the investor attention using the Google search volume index (GSVI) can be used to forecast stock returns. The authors also find the answer to whether the “price pressure hypothesis” would hold true for the Indian stock market.Design/methodology/approachThe authors employ a more recent fully balanced panel data for the period from July 2012 to Jun 2017 (260 weeks) of observations for companies of NIFTY 50 of the National Stock Exchange in the Indian stock market. The authors are motivated by Tetlock (2007) and Bijl et al. (2016) to employ regression approach of econometric estimation.FindingsThe authors find that high Google search volumes lead to positive returns. More precisely, the high Google search volumes predict positive and significant returns in the subsequent fourth and fifth weeks. The GSVI performs as an useful predictor of the direction as well as the magnitude of the excess returns. The higher quantiles of the GSVI have corresponding higher excess returns. The authors notice that the domestic investor searches are correlated with higher excess returns than the worldwide investor searches. The findings imply that the signals from the search volume data could be of help in the construction of profitable trading strategies.Originality/valueTo the best of the authors knowledge, no paper has examined the relationship between Google search intensity and stock-trading behavior in the Indian stock market. The authors use a more recent data for the period from 2012 to 2017 to investigate whether search query data on company names can be used to predict weekly stock returns for individual firms. This study complements the prior studies by investigating the relationship between search intensity and stock-trading behavior in the Indian stock market.",,2019,10.1108/rbf-04-2018-0033,,proquest,"This study investigates the predictive power of Google Search Volume Index (GSVI) for stock returns in the Indian stock market. Using a balanced panel data from July 2012 to June 2017 for NIFTY 50 companies, the authors found that higher GSVI predicts positive and significant stock returns in the subsequent fourth and fifth weeks. The GSVI is shown to be a useful predictor of both the direction and magnitude of excess returns, with higher quantiles of GSVI corresponding to higher excess returns. Domestic investor searches were found to be more correlated with higher excess returns than worldwide searches. The findings suggest that search volume data can aid in constructing profitable trading strategies.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:16:19.698095
3e4b27043ebffa04,Investors' and Central Bank's Uncertainty Embedded in Index Options,"Shocks to equity options' implied volatility are followed by persistently lower short-term rates. Shocks to puts' over calls' out-of-the-money implied volatilities (P/C) are followed by persistently higher rates. Stock and Treasury bond implied volatilities, which measure market and policy uncertainty, are countercyclical, while P/C, which measures downside risk, is procyclical. An equilibrium model in which investors and the central bank learn about composite regimes of economic and policy variables explains these dynamics, linking them to a learning-based, forward-looking Taylor rule. Survey data support our model's predictions on the effect of uncertainty on the level and fluctuations of implied volatilities.","David, Alexander; Veronesi, Pietro",2014,10.1093/rfs/hhu024,,wos,"This paper investigates how investor and central bank uncertainty, as reflected in equity options' implied volatility, influences short-term interest rates. It finds that shocks to implied volatility lead to lower rates, while shocks to the ratio of put to call implied volatilities (P/C) lead to higher rates. The study proposes an equilibrium model incorporating learning about economic and policy variables to explain these dynamics, supported by survey data on uncertainty's impact on implied volatilities.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:17:44.338402
1a7da81a013d9178,Is There an On-the-Run Premium in TIPS?,"In the U.S. Treasury market, the most recently issued, or so-called on-the-run, security typically trades at a price above those of more seasoned but otherwise comparable securities. This difference is known as the on-the-run premium. In this paper, yield spreads between pairs of Treasury Inflation-Protected Securities (TIPS) with both matching and nearly-matching maturities but of separate vintages are analyzed. Adjusting for differences in conventional liquidity premiums, values of embedded deflation options, and coupon rates, the results show a small, insignificant premium on recently issued TIPS, which leads us to conclude that there is no on-the-run premium in the TIPS market.","Christensen, Jens H. E.; Lopez, Jose A.; Shultz, Patrick J.",2020,10.1142/s201013922050007x,,wos,"This paper analyzes yield spreads between Treasury Inflation-Protected Securities (TIPS) of different vintages. After adjusting for liquidity premiums, deflation options, and coupon rates, the study finds a small, insignificant premium on recently issued TIPS, concluding that there is no on-the-run premium in the TIPS market.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:17:46.594052
cc24ac6a38116c55,Is nonlinear drift implied by the short end of the term structure?,"Nonlinear drift models of the short rate are estimated using data on the short end of the term structure, where the cross-sectional relation is obtained by an analytical approximation. The findings reveal that (i) nonlinear physical drift is not implied unless it is strongly affected by cross-sectional dimensions of the data; (ii) nonlinear risk-neutral drift that allows for fast mean reversion for high rates is desirable to explain and predict observed patterns of yield spreads; and (iii) for higher frequency data from which transitory shocks are removed, (ii) still remains valid although the nonlinearity is somewhat reduced. Reprinted by permission of Oxford University Press",,2008,10.1093/rfs/hhm072,,proquest,"This study estimates nonlinear drift models for the short rate using data from the short end of the term structure. It finds that nonlinear risk-neutral drift is necessary to explain yield spread patterns, especially when allowing for fast mean reversion at high rates. While nonlinearity is somewhat reduced with higher frequency data, the model remains valid.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:17:53.802203
465719be2fd4d237,Is the short rate drift actually nonlinear?,"Ait-Sahalia (1996) and Stanton (1997) use nonparametric estimators applied to short-term interest rate data to conclude that the drift function contains important nonlinearities. We study the finite-sample properties of their estimators by applying them to simulated sample paths of a square-root diffusion. Although the drift function is linear, both estimators suggest nonlinearities of the type and magnitude reported in Ait-Sahalia (1996) and Stanton (1997). Combined with the results of a weighted least squares estimator, this evidence implies that nonlinearity of the short rate drift is not a robust stylized fact.","Chapman, DA; Pearson, ND",2000,10.1111/0022-1082.00208,,wos,"This paper investigates the robustness of nonlinearities in the short rate drift, as suggested by previous nonparametric estimators. By applying these estimators to simulated data from a linear drift model, the authors demonstrate that the estimators can falsely detect nonlinearities. This suggests that the nonlinearity of the short rate drift may not be a reliable characteristic.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:17:57.538279
ea00aca5b04457b7,Is the term structure nonlinear? A semiparametric investigation,A semiparametric error correction model (ECM) is estimated using US term structure data. We use 5 and 10 year interest rates to predict short-term (1 month to 12 month) interest rates. It is found that the semiparametric ECM model predicts better than the popular linear ECM. These results provide further evidence of nonlinearity in the term structure.,"Bachmeier, L; Li, Q",2002,10.1080/13504850110053275,,wos,"This study estimates a semiparametric error correction model (ECM) using US term structure data, predicting short-term interest rates with 5 and 10-year rates. The semiparametric ECM outperforms the linear ECM, suggesting nonlinearity in the term structure.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:18:20.394411
1931a2bd3a13f6eb,Joint Estimation of Factor Sensitivities and Risk Premia for the Arbitrage Pricing Theory,"The APT is represented as a multivariate regression model with across‐equations restrictions. Both observed and unobserved (latent) macroeconomic factors are included, thus generalizing and unifying two previous strands of literature. Large portfolios representing unobserved factors are treated as endogenous, and nonlinear 3SLS estimates are shown to differ sharply from estimates that ignore this endogeneity. Using monthly stock returns and six factors, we cannot reject January effects. The following results are invariant with respect to the inclusion of January effects: we reject the CAPM in favor of the APT; however, we cannot reject the APT restrictions on the linear factor model. 1988 The American Finance Association © 2016 Elsevier B.V., All rights reserved.","Burmeister, E.; McElroy, M.B.",1988,10.1111/j.1540-6261.1988.tb04603.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977711619&doi=10.1111%2Fj.1540-6261.1988.tb04603.x&partnerID=40&md5=e622a3c5536f6ad1f7416c41a68d2843,scopus,"This paper proposes a multivariate regression model for the Arbitrage Pricing Theory (APT) that incorporates both observed and unobserved macroeconomic factors. It addresses the endogeneity of factors representing large portfolios and compares nonlinear 3SLS estimates with those that ignore endogeneity. The study finds no significant January effects in monthly stock returns and rejects the Capital Asset Pricing Model (CAPM) in favor of the APT, while not rejecting the APT's linear factor model restrictions.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:18:21.658234
f4e21d42f7de9c14,Jumps and time-varying correlations in daily foreign exchange rates,"This paper extends the multivariate latent factor ARCH model approach of Diebold and Nerlove (Journal of Applied Econometrics 4 (1989) 1) as a parsimonious alternative that pays particular attention to time series properties of daily foreign exchange rates such as jumps and to changing volatilities in both the common and country-specific factors. Using seven major daily dollar exchange rates from January 1 1992 to December 31 1996, this paper finds evidence of significant time-varying correlations and the country-specific variances. Consistent with the finding of Alexius and Sellin (1997) (A latent factor model of European exchange rate risk premia. Manuscript, The Economic Research Institute, Stockholm School of Economics), the two factor model appears to be a reasonable description of the major exchange rates. © 2001 Elsevier Science Ltd. © 2005 Elsevier Science B.V., Amsterdam. All rights reserved.","Chang, K.-H.; Kim, M.-J.",2001,10.1016/s0261-5606(01)00007-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0041825343&doi=10.1016%2FS0261-5606%2801%2900007-9&partnerID=40&md5=fcf9dd0391f4978033d3a52d2a2777fc,scopus,"This paper extends the multivariate latent factor ARCH model to analyze daily foreign exchange rates, focusing on jumps and time-varying volatilities in common and country-specific factors. The study uses seven major dollar exchange rates from 1992-1996 and finds evidence of significant time-varying correlations and country-specific variances, suggesting a two-factor model is a reasonable description of these exchange rates.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:18:22.898000
6379bcb62a5fb903,Kriging of financial term-structures,"Due to the lack of reliable market information, building financial term-structures may be associated with a significant degree of uncertainty. In this paper, we propose a new term-structure interpolation method that extends classical spline techniques by additionally allowing for quantification of uncertainty. The proposed method is based on a generalization of kriging models with linear equality constraints (market-fit conditions) and shape-preserving conditions such as monotonicity or positivity (no-arbitrage conditions). We define the most likely curve and show how to build confidence bands. The Gaussian process covariance hyper-parameters under the construction constraints are estimated using cross-validation techniques. Based on observed market quotes at different dates, we demonstrate the efficiency of the method by building curves together with confidence intervals for term-structures of OIS discount rates, of zero-coupon swaps rates and of CDS implied default probabilities. We also show how to construct interest-rate surfaces or default probability surfaces by considering time (quotation dates) as an additional dimension.",,2016,10.1016/j.ejor.2016.05.057,,proquest,"This paper introduces a novel method for interpolating financial term-structures, extending spline techniques with kriging models to quantify uncertainty. It incorporates market-fit, monotonicity, and positivity constraints, estimates Gaussian process hyperparameters via cross-validation, and demonstrates its efficiency in building confidence intervals for OIS discount rates, swap rates, and CDS implied default probabilities. The method can also construct interest-rate and default probability surfaces by including time as an additional dimension.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:18:26.522388
fa741ee39590c1c0,Large data sets and machine learning: Applications to statistical arbitrage,"Machine learning algorithms and big data are transforming all industries including the finance and portfolio management sectors. While these techniques, such as Deep Belief Networks or Random Forests, are becoming more and more popular on the market, the academic literature is relatively sparse. Through a series of applications involving hundreds of variables/predictors and stocks, this article presents some of the state-of-the-art techniques and how they can be implemented to manage a long-short portfolio. Numerous practical and empirical issues are developed. One of the main questions beyond big data use is the value of information. Does an increase in the number of predictors improve the portfolio performance? Which features are the most important? A large number of predictors means, potentially, a high level of noise. How do the algorithms manage this? This article develops an application using a 22-year trading period, up to 300 U.S. large caps and around 600 predictors. The empirical results underline the ability of these techniques to generate useful trading signals for portfolios with important turnovers and short holding periods (one or five days). Positive excess returns are reported between 1993 and 2008. They are strongly reduced after accounting for transaction costs and traditional risk factors. When these machine learning tools were readily available in the market, excess returns turned into the negative in most recent times. Results also show that adding features is far from being a guarantee to boost the alpha of the portfolio. © 2019 Elsevier B.V., All rights reserved.","Huck, N.",2019,10.1016/j.ejor.2019.04.013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064846164&doi=10.1016%2Fj.ejor.2019.04.013&partnerID=40&md5=352bd849257f78318868f7b1b38825dd,scopus,"This article explores the application of machine learning (ML) and big data techniques to statistical arbitrage in portfolio management. It demonstrates the implementation of advanced ML methods like Deep Belief Networks and Random Forests using a large dataset spanning 22 years, hundreds of variables, and numerous stocks. The study investigates the impact of predictor quantity on portfolio performance and addresses issues of noise and feature importance. Empirical results show that while ML can generate trading signals, excess returns are significantly reduced by transaction costs and risk factors, and even turn negative in recent periods. The study also highlights that increasing the number of features does not necessarily improve portfolio alpha.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:18:29.026140
0da0f4c7d8aadf7f,Learning Forecast-Efficient Yield Curve Factor Decompositions with Neural Networks,"Most factor-based forecasting models for the term structure of interest rates depend on a fixed number of factor loading functions that have to be specified in advance. In this study, we relax this assumption by building a yield curve forecasting model that learns new factor decompositions directly from data for an arbitrary number of factors, combining a Gaussian linear state-space model with a neural network that generates smooth yield curve factor loadings. In order to control the model complexity, we define prior distributions with a shrinkage effect over the model parameters, and we present how to obtain computationally efficient maximum a posteriori numerical estimates using the Kalman filter and automatic differentiation. An evaluation of the model’s performance on 14 years of historical data of the Brazilian yield curve shows that the proposed technique was able to obtain better overall out-of-sample forecasts than traditional approaches, such as the dynamic Nelson and Siegel model and its extensions. © 2022 Elsevier B.V., All rights reserved.","Kauffmann, P.C.; Takada, H.H.; Terada, A.T.; Stern, J.M.",2022,10.3390/econometrics10020015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128238593&doi=10.3390%2Feconometrics10020015&partnerID=40&md5=065080cdb62c05c804b818ccfed43282,scopus,This study proposes a novel yield curve forecasting model that learns factor decompositions directly from data using a combination of a Gaussian linear state-space model and a neural network. It addresses the limitation of fixed factor loading functions in traditional models. The model incorporates prior distributions for complexity control and uses efficient estimation techniques. Empirical evaluation on Brazilian yield curve data demonstrates superior out-of-sample forecasting performance compared to traditional methods.,True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-28T10:18:43.913550
d29dd67ecedbed99,Learning and forecasts about option returns through the volatility risk premium,"We use learning in an equilibrium model to explain the puzzling predictive power of the volatility risk premium (VRP) for option returns. In the model, a representative agent follows a rational Bayesian learning process in an economy under incomplete information with the objective of pricing options. We show that learning induces dynamic differences between probability measures P and Q, which produces predictability patterns from the VRP for option returns. The forecasting features of the VRP for option returns, obtained through our model, exhibit the same behaviour as those observed in an empirical analysis with S&P 500 index options. (C) 2017 Elsevier B.V. All rights reserved.","Bernales, Alejandro; Chen, Louisa; Valenzuela, Marcela",2017,10.1016/j.jedc.2017.06.007,,wos,"This paper develops an equilibrium model where rational Bayesian learning explains the predictive power of the volatility risk premium (VRP) for option returns. The model demonstrates how learning creates dynamic differences between probability measures P and Q, leading to VRP-based predictability in option returns, consistent with empirical findings from S&P 500 index options.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:18:45.122155
0bd417484ae81bb2,Likelihood inference for dynamic linear models with Markov switching parameters: on the efficiency of the Kim filter,"The Kim filter (KF) approximation is widely used for the likelihood calculation of dynamic linear models with Markov regime-switching parameters. However, despite its popularity, its approximation error has not yet been examined rigorously. Therefore, this study investigates the reliability of the KF approximation for maximum likelihood (ML) and Bayesian estimations. To measure the approximation error, we compare the outcomes of the KF method with those of the auxiliary particle filter (APF). The APF is a numerical method that requires a longer computing time, but its numerical error can be sufficiently minimized by increasing simulation size. According to our extensive simulation and empirical studies, the likelihood values obtained from the KF approximation are practically identical to those of the APF. Furthermore, we show that the KF method is reliable, particularly when regimes are persistent and sample size is small. From the Bayesian perspective, we show that the KF method improves the efficiency of posterior simulation. This study contributes to the literature by providing evidence to justify the use of the KF method in both ML and Bayesian estimations.","Kim, Young Min; Kang, Kyu Ho",2019,10.1080/07474938.2018.1514027,,wos,"This study evaluates the accuracy of the Kim filter (KF) approximation for dynamic linear models with Markov switching parameters, comparing it to the auxiliary particle filter (APF). The findings indicate that the KF approximation is highly reliable, producing likelihood values virtually identical to the APF, especially when regimes are persistent and sample sizes are small. The research also demonstrates that the KF method enhances the efficiency of posterior simulations in Bayesian estimations, thus justifying its use in both maximum likelihood and Bayesian contexts.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:18:46.970180
23cd22560c926a1e,Limited information-processing capacity and asymmetric stock correlations,"Through an orthogonalized impulse-response analysis, I studied the relationship between the variance risk premium, market variance and stock correlations in the French stock market from September 2002 through September 2006, using high-frequency data-based measures. Variance risk premium is estimated using realized variances and index options-implied variances and used as a state vector to proxy investors perceived uncertainty. I found that a shock to variance risk premium causes long-lasting increases in the market variance pointing to the limitedness of investors information-processing capacity. At the same time, the shock generates consecutive increases in realized correlations between individual stocks and the market portfolio. I propose this as a possible explanation for the asymmetric/counter-cyclic behaviour of stock correlations. © 2019 Elsevier B.V., All rights reserved.","Ceylan, O.",2015,10.1080/14697688.2013.808374,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929133348&doi=10.1080%2F14697688.2013.808374&partnerID=40&md5=4cdf28a6de9803f2ca275da5d8c0647a,scopus,"This study investigates the link between the variance risk premium, market variance, and stock correlations in the French stock market using high-frequency data. The findings suggest that shocks to the variance risk premium lead to prolonged increases in market variance and consecutive rises in stock-market correlations, potentially explaining the asymmetric behavior of stock correlations and highlighting limitations in investors' information processing capacity.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:18:48.770105
1c7ef051f405988d,Linear regression versus backpropagation networks to predict: Quarterly stock market excess returns,"This paper compares a linear model to predict quarterly stock market excess returns to several backpropagation networks. Research findings suggest that quarterly stock market returns are to some extent predictable, but only marginal attention has been paid to possible nonlinearities in the return generating process. The paper discusses input selection, elaborates on how to generate out-of-sample predictions to estimate generalization performance, motivates the choice for a particular network, examines backpropagation training, and evaluates network performance. The out-of-sample predictions are used to calculate several performance metrics, and to determine added value when applying a straightforward tactical asset allocation policy. A nonparametric test is selected to evaluate generalization behavior, and sensitivity analysis examines the selected network's qualitative behavior. Strong nonlinear effects appear to be absent, but the proposed backpropagation network generates an asset allocation policy that outperforms the linear model. © 1996 Kluwer Academic Publishers. © 2018 Elsevier B.V., All rights reserved.","Hiemstra, Y.",1996,10.1007/bf00115692,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0043266837&doi=10.1007%2FBF00115692&partnerID=40&md5=44c0893012d125f91ad203a8be14cb41,scopus,"This paper compares linear models with backpropagation networks for predicting quarterly stock market excess returns. While strong nonlinear effects were not found, the backpropagation network outperformed the linear model in an asset allocation policy. The study discusses input selection, out-of-sample prediction, network training, and performance evaluation.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:21:03.898950
b7b3d42a192ae0ce,Linear-price term structure models,"We characterize the term structure models in which the zero-coupon prices are linear functions of underlying factors. These models are called Linear-price Term Structure Models (LTSM). We provide two types of LTSM where the observable factors predict regimes which are not observed by the investor. These hidden regimes are represented by a Markov chain, which features either an exogenous, or an endogenous dynamics. We illustrate the possible term structure patterns, their evolutions, in particular their ability to stay close to a zero lower bound. © 2013 Elsevier B.V. © 2023 Elsevier B.V., All rights reserved.","Gouriéroux, C.; Monfort, A.",2013,10.1016/j.jempfin.2013.07.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883523553&doi=10.1016%2Fj.jempfin.2013.07.004&partnerID=40&md5=e34fe3bdc2f523b56b4b64e14ceb551c,scopus,"This paper introduces Linear-price Term Structure Models (LTSM) where zero-coupon prices are linear functions of underlying factors. It presents two types of LTSM with unobserved Markov chain regimes (exogenous or endogenous dynamics) and illustrates term structure patterns, their evolution, and their ability to approach a zero lower bound.",False,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:21:06.506454
28798acb3ba870d9,Local lagged adapted generalized method of moments: An innovative estimation and forecasting approach and its applications,"In this work, an attempt is made to apply the Local Lagged Adapted Generalized Method of Moments (LLGMM) to estimate state and parameters in stochastic differential dynamic models. The development of LLGMM is motivated by parameter and state estimation problems in continuous-time nonlinear and non-stationary stochastic dynamic model validation problems in biological, chemical, engineering, energy commodity markets, financial, medical, military, physical sciences and social sciences. The byproducts of this innovative approach (LLGMM) are the balance between model specification and model prescription of continuous-time dynamic process and the development of discrete-time interconnected dynamic model of local sample mean and variance statistic process (DTIDMLSMVSP). Moreover, LLGMM is a dynamic non-parametric method. The DTIDMLSMVSP is an alternative approach to the GARCH(1,1) model, and it provides an iterative scheme for updating statistic coefficients in a system of generalized method of moment/observation equations. Furthermore, applications of LLGMM to energy commodities price, U.S. Treasury Bill interest rate and the U.S.-U.K. foreign exchange rate data strongly exhibit its unique role, scope and performance, in particular, in forecasting and confidence-interval problems in applied statistics. © 2019 Elsevier B.V., All rights reserved.","Otunuga, O.M.; Ladde, G.S.; Ladde, N.G.",2019,10.1515/jtse-2016-0024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060695794&doi=10.1515%2Fjtse-2016-0024&partnerID=40&md5=40c6e24628aa2141e4d8b7306439ce99,scopus,"This paper introduces the Local Lagged Adapted Generalized Method of Moments (LLGMM), a novel non-parametric approach for estimating states and parameters in continuous-time nonlinear and non-stationary stochastic dynamic models. LLGMM is presented as an alternative to GARCH(1,1) for modeling local sample mean and variance statistics and offers an iterative scheme for updating coefficients. The method's effectiveness is demonstrated through applications in energy commodities prices, U.S. Treasury Bill interest rates, and U.S.-U.K. foreign exchange rates, particularly highlighting its performance in forecasting and confidence interval estimation.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-28T10:21:51.683272
b86b59095cc0f715,Long memory affine term structure models,"We develop a Gaussian discrete time essentially affine term structure model with long, memory state variables. This feature reconciles the strong persistence observed in nominal yields and inflation with the theoretical implications of affine models, especially for long maturities. We characterize in closed form the dynamic and cross-sectional implications of long memory for our model. We explain how long memory can naturally arise within the term structure of interest rates, providing a theoretical underpinning for our model. Despite the infinite-dimensional structure that long memory implies, we show how to cast the model in state space and estimate it by maximum likelihood. An empirical application of our model is presented. (C) 2015 Elsevier B.V. All rights reserved.","Golinski, Adam; Zaffaroni, Paolo",2016,10.1016/j.jeconom.2015.09.006,,wos,"This paper presents a Gaussian discrete time essentially affine term structure model incorporating long memory state variables. This model addresses the strong persistence in yields and inflation, aligning with affine model theory for long maturities. It provides closed-form solutions for the dynamic and cross-sectional impacts of long memory, explaining its natural emergence in interest rates. The model is cast in state space for maximum likelihood estimation, despite its infinite-dimensional nature, and includes an empirical application.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:22:05.547042
2097fbf52b7eeb1d,Long-range facility planning based on dynamic programming for optimum combined cost and probability paths,"Dynamic programming (DP)-based planning algorithms have been shown to be valuable tools since they provide a basis for sampling, enumeration, and optimization of options for long-range deployment of facilities. Previous applications of DP to optimize pipeline long-range facility planning problems based on either the least-cost path for the facility or the most-probable path for noncost constraints have been documented in the literature. Such applications, however, are faced with a challenge in selecting the optimum facility deployment path, as the least-cost path does not always necessarily coincide with the most-probable path. As a result, the selection of a path that combines both features has to be achieved through a subjective compromise and in a rather arbitrary manner. In the present paper, two new DP methods have been developed which are based on the concept of combining cost and probability to give a single-objective probability-adjusted cost. One method incorporated the probability of each arc in the DP architecture using a variation of the Black-Scholes partial differential equation. The solution of the resulting equation gave a probability-adjusted arc cost dependent on the year (or stage) the cost incurred, the overall probability of all constraints associated with this arc, and the risk-free rate. The other method was based on simply dividing the present value of each arc cost by its probability to give a single probability-adjusted cost. Both approaches were applied to a complex DP architecture composed of 10 stages and 10 different options at each stage in which all options were available at every stage in a directed manner. The optimum paths from the new approaches were compared to the least-cost options, and most-probable options, and were found to combine the two features. Finally, all options from all methods were found to lie on a Pareto front obtained from a multiobjective genetic algorithm. © 2010 ASCE. © 2011 Elsevier B.V., All rights reserved.","Botros, K.K.; Tchir, W.J.; Henderson, J.F.; Chmilar, B.",2010,10.1061/(asce)ps.1949-1204.0000052,https://www.scopus.com/inward/record.uri?eid=2-s2.0-82355181547&doi=10.1061%2F%28ASCE%29PS.1949-1204.0000052&partnerID=40&md5=3cd46f312ce4b7a4fd1f3059e79dc975,scopus,"This paper presents two novel dynamic programming (DP) methods for long-range facility planning that combine cost and probability into a single objective. These methods address the limitation of previous DP approaches that focused solely on least-cost or most-probable paths. The first method integrates arc probabilities into the DP framework using a Black-Scholes equation variation, yielding a probability-adjusted arc cost. The second method calculates probability-adjusted arc cost by dividing arc cost by its probability. Both methods were tested on a complex DP architecture and demonstrated the ability to find paths that balance cost and probability, outperforming methods that consider only one factor. The results were also compared to a multiobjective genetic algorithm, showing the optimized paths lie on the Pareto front.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:22:10.931370
478dbbeb2d89f44e,Long/Short Equity Risk Premia Parity Portfolios via Implicit Factors in Regularized Covariance Regression,"A robust time series basis decomposition and non-stationary trend extraction technique, known as Empirical Mode Decomposition (EMD), will be combined with Regularised Covariance Regression (RCR) to produce a novel covariance forecasting technique. EMD is designed for multiscale and adaptive time-frequency decomposition in nonstationary time series. EMD-RCR generates multi-time-frequency resolution adaptive forecasting models of predictive covariance forecasts for a universe of selected asset returns. This provides a unique method to obtain predictive covariance regression structures for the short-, mid-, and long-time-scale portfolio dynamics. EMD isolates structures in a frequency-hierarchical fashion (with automated sorting of structures through EMD-MDLP available) which allows this multi-time-frequency covariance forecasting framework that uses the structures isolated using EMD (referred to as IMFs: Intrinsic Mode Functions) as the explanatory variables in the RCR framework. Having developed these techniques, a case study is used for exposition for active portfolio asset management. The case study is based on a dynamic long/short equity and risk premia parity (or risk parity) portfolio-of-portfolios investment strategy using the 11 sectors dividing the 505 stocks of the S&P 500. Each of the 11 sector indices is constructed using a market capitalisation ratio of the companies within the respective sector. The portfolio will be reweighted monthly based on the covariance structure forecast using covariance regression, in which covariance regression factors will be obtained at multiple time-frequency scales endogenously from the ETF asset price returns from each sector. At the end of each month, the covariance is forecast for the next month or investment horizon. This is done using low-, mid-, and high-frequency IMFs isolated using EMD from the 11 sector indices over the previous year. The IMFs isolated from the 11 sector indices over the previous year are fitted against the daily logarithmic returns in the RCR model to make multi-frequency covariance forecasts. We construct long/short equity and risk premia parity portfolios using each different covariance forecast and review the results. The performance of the portfolios will be measured using multiple performance measures (the most relevant being risk-related measures with risk premia parity in focus) and contrasted against multiple benchmark portfolios using several well-known portfolio optimisation techniques such as PCA and multivariate GARCH extensions. This paper promotes what we term “implicit factor” extraction, empirical market factors, and RCR in portfolio optimisation, horizon-specific active portfolio optimisation, long/short equity portfolios, and risk parity portfolios.",C. Van Jaarsveldt; G. W. Peters; M. Ames; M. Chantler,2024,10.1109/access.2024.3444479,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10637332,ieeexplore,"This paper introduces a novel covariance forecasting technique by combining Empirical Mode Decomposition (EMD) with Regularised Covariance Regression (RCR). The EMD-RCR method is used to generate adaptive forecasting models for asset returns across different time scales. A case study demonstrates this technique for a dynamic long/short equity and risk premia parity portfolio strategy using S&P 500 sector indices. The portfolio is rebalanced monthly based on covariance forecasts derived from EMD-isolated intrinsic mode functions (IMFs) and RCR. The performance of these portfolios is evaluated against benchmarks and other optimization techniques, highlighting the use of ""implicit factors"" and RCR in portfolio optimization.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:22:13.451332
d6f098222fbb7a8d,Machine learning for US cross-industry return predictability under information uncertainty,"This paper investigates the association between industry information uncertainty and cross-industry return predictability using machine learning in a general predictive regression framework. We show that controlling for post-selection inference and performing multiple tests improves the in-sample predictive performance of cross-industry return predictability in industries characterized by high uncertainty. Ordinary least squares post-least absolute shrinkage and selection operator models incorporating lagged industry information uncertainty for the financial and commodity industries are critical to improving prediction performance. Furthermore, in-sample industry return forecasts establish heterogeneous predictability over US industries, in which excess returns are more predictable in sectors with medium or low uncertainty. © 2023 Elsevier B.V., All rights reserved.","Awijen, H.; Ben-Zaied, Y.; Ben Lahouel, B.; Khlifi, F.",2023,10.1016/j.ribaf.2023.101893,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147577512&doi=10.1016%2Fj.ribaf.2023.101893&partnerID=40&md5=982b6c1f6cf5e817b5b4628e63b106a5,scopus,"This paper explores how industry information uncertainty affects cross-industry return predictability in the US using machine learning. It finds that controlling for statistical inference issues and using models like OLS-LASSO with lagged uncertainty improves in-sample prediction, especially in financial and commodity industries. Predictability varies across industries, with medium or low uncertainty sectors showing more predictable excess returns.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:22:18.595327
a4339a705a7fa1f2,Machine learning models and cost-sensitive decision trees for bond rating prediction,"Since the outbreak of the financial crisis, the major global credit rating agencies have implemented significant changes to their methodologies to assess the sovereign credit risk. Therefore, bond rating prediction has become an interesting potential for investors and financial institutions. Previous research studies in this field have applied traditional statistical methods to develop models which provide prediction accuracy. However, no overall distinguished methods have been used in predicting bond ratings. Moreover, recent studies have suggested ensembles of classifiers that may be used in bond rating prediction. This article proposes an improved machine learning aimed to predict the rating of financial bonds. We empirically compare the classifiers ranging from logistic regression and discriminant analysis to nonparametric classifiers, such as support vector machine, neural networks, the cost-sensitive decision tree algorithm and deep neural networks. Three real-world bond rating data sets were selected to check the effectiveness and the viability of the set of the classifiers. The experimental results confirm that data mining methods can represent an alternative to the traditional prediction models of bond rating. © 2020 Elsevier B.V., All rights reserved.","Ben Jabeur, S.B.; Saadaoui, A.; Sghaier, A.; Aloui, R.",2020,10.1080/01605682.2019.1581405,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064715076&doi=10.1080%2F01605682.2019.1581405&partnerID=40&md5=e3791d5826922a1e07d61ea236e0eb79,scopus,"This article proposes an improved machine learning approach for predicting financial bond ratings, comparing various classifiers including logistic regression, support vector machines, neural networks, cost-sensitive decision trees, and deep neural networks. The study uses three real-world bond rating datasets to evaluate the effectiveness of these methods, suggesting that data mining techniques can serve as an alternative to traditional prediction models.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:22:25.755338
592ae86c1ecb9d0b,Macro Factors and Bond Returns in China,"As a central issue in macro-finance studies, the spanning hypothesis has always been the focus of research. Previous studies have focused on whether this hypothesis holds true in developed markets, while paying little attention to that in emerging markets. Because of their unique monetary systems, governments in most emerging markets play a key role in bond returns. This study identifies macroeconomic factors for forecasting excess returns in emerging government bond markets under spanning hypothesis. We find that in previous research, government intervention factors employed in excess returns forecasting have no additional predictive ability, as they are already incorporated in current yields. Using dynamic factor analysis, we find that macroeconomic information, including pure macroeconomic activities and financial factors, has robust incremental predictive power for in-sample and out-of-sample bond excess returns. © 2022 Elsevier B.V., All rights reserved.","Li, X.; Yang, B.; Su, Y.; Qi, Y.; An, Y.",2022,10.1080/1540496x.2021.1941860,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109852085&doi=10.1080%2F1540496X.2021.1941860&partnerID=40&md5=4b6119268539038cc29dbe316dc7106a,scopus,"This study investigates the spanning hypothesis in emerging government bond markets, identifying macroeconomic factors that forecast excess returns. It finds that while government intervention factors are already incorporated in current yields, macroeconomic activities and financial factors demonstrate robust incremental predictive power for bond excess returns, both in-sample and out-of-sample.",False,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:22:39.203376
135c3081d1b818b4,Macroeconomic Attention and Announcement Risk Premia,"We construct macroeconomic attention indexes (MAI), which are new measures of attention to different macroeconomic risks, including unemployment and monetary policy. Individual MAI tend to increase around related announcements and following changes in related fundamentals. Further, bad news raises attention more than good news. For unemployment and FOMC, attention predicts announcement risk premiums and implied volatility changes with large economic magnitudes. Our findings support theories of endogenous attention and announcement risk premiums, while demonstrating future research directions, including that announcements can raise new concerns. Macroeconomic announcements are important not only for contents and timing but also for attention.Authors have furnished an Internet Appendix, which is available on the Oxford University Press Web site next to the link to the final published paper online.",,2022,10.1093/rfs/hhac011,,proquest,"This paper introduces macroeconomic attention indexes (MAI) to measure attention to macroeconomic risks like unemployment and monetary policy. These indexes increase around relevant announcements and fundamental changes, with bad news having a greater impact than good news. The study finds that attention predicts announcement risk premiums and implied volatility, supporting theories of endogenous attention and announcement risk premiums. The authors suggest that macroeconomic announcements influence not only content and timing but also attention, potentially raising new concerns.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:22:41.915491
f8d976b8b744af6c,"Macroeconomic attention, economic policy uncertainty, and stock volatility predictability","This study adopts the newly constructed macroeconomic attention indices (MAI) and category-specific economic policy uncertainty (EPU) indices to predict stock volatility. Principal component analysis (PCA), scaled PCA (sPCA), and partial least squares (PLS) are used to extract the principal components from indicators. The results show that the combination of MAI and EPU indices can obtain additional information for predicting stock market volatility. In addition, the comprehensive index containing all indicator information (F-t(All)) has the strongest short-term forecasting ability, whereas the MAI show the most substantial forecasting ability in long-term forecasting.","Ma, Feng; Guo, Yangli; Chevallier, Julien; Huang, Dengshi",2022,10.1016/j.irfa.2022.102339,,wos,"This study uses macroeconomic attention indices (MAI) and economic policy uncertainty (EPU) indices to predict stock volatility. It employs Principal Component Analysis (PCA), scaled PCA (sPCA), and Partial Least Squares (PLS) for component extraction. The findings indicate that combining MAI and EPU improves stock market volatility prediction. A comprehensive index (F-t(All)) shows strong short-term forecasting ability, while MAI demonstrates superior long-term forecasting power.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:22:50.436463
61421c153a5054b3,Macroeconomic impact on the risk management of offshore wind farms,"The present study aims to develop a risk model to analyse offshore wind projects based on operational and macroeconomic data. The study investigates the underlying parameters defining the project-specific risk premium attached to an offshore wind project. These parameters are modelled as stochastic variables, and a probabilistic financial analysis is conducted using Monte Carlo Simulation. To calculate an interest rate based on operational characteristics, the present study assumes that two net present value equations yielding the same result can be written where certainty equivalent cash flows discounted at the risk-free rate and b) expected cash flows discounted at the cost of capital. The project-related risk is then estimated by solving the resulting equation for the unknown cost of capital. The macroeconomic factors are also considered as they impact the uncertainty associated with revenue and operating expenditure. The model developed to calculate the cost of capital is validated by comparing it with the data obtained for publicly traded renewable energy companies worldwide. Finally, the developed model is demonstrated for a fictitious ageing offshore wind farm under different economic circumstances. The parametric study is conducted on the effect of critical project-specific parameters such as the number of offshore wind turbines, the life extension duration, and the degree of uncertainty related to cash flows. © 2023 Elsevier B.V., All rights reserved.","Yeter, B.; Garbatov, Y.; Brennan, F.; Kolios, A.",2023,10.1016/j.oceaneng.2023.115224,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163853313&doi=10.1016%2Fj.oceaneng.2023.115224&partnerID=40&md5=e3ba1273ef2d4c7cb232dbc4702c5d8a,scopus,"This study develops a risk model for offshore wind projects using operational and macroeconomic data. It models project-specific risk parameters as stochastic variables and uses Monte Carlo Simulation for financial analysis. The model calculates the cost of capital by equating certainty equivalent cash flows discounted at the risk-free rate with expected cash flows discounted at the cost of capital. Macroeconomic factors influencing revenue and operating expenditure uncertainty are considered. The model is validated against data from global renewable energy companies and demonstrated on a hypothetical offshore wind farm under various economic scenarios, analyzing the impact of parameters like turbine count, life extension, and cash flow uncertainty.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:22:53.067373
44be03948484da4f,Man versus Machine Learning Revisited,"Binsbergen, Han, and Lopez-Lira (2023) predict analysts' forecast errors using a random forest model. A strategy that trades against this model's predictions earns a monthly alpha of 1.54% ($ t $-value = 5.84). This estimate represents a large improvement over studies using classical statistical methods. We attribute the difference to a look-ahead bias. Removing the bias erases the alpha. Linear models yield as accurate forecasts and superior trading profits. Neither alternative machine learning models nor combinations thereof resurrect the predictability. We discuss the state of research into the term structure of analysts' forecasts and its causal relationship with returns.","Zhang, Yingguang; Zhu, Yandi; Linnainmaa, Juhani T.",2025,10.1093/rfs/hhaf066,,wos,"This study re-examines the use of machine learning (ML) in predicting analysts' forecast errors, finding that a random forest model initially suggested a profitable trading strategy. However, upon closer inspection, this profitability was attributed to a look-ahead bias. After correcting for this bias, the alpha disappeared. The study concludes that linear models are equally accurate for forecasting and yield superior trading profits, and that other ML models do not revive the predictability. The research also touches upon the term structure of analysts' forecasts and its link to returns.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:23:00.771433
5b3ffa4249af3ca0,Market Efficiency and Equity Risk Premium Predictability,"This work examines equity risk premium predictability in periods of market efficiency and market inefficiency. Efficiency is measured by the return's degree of multifractality, calculated from the multifractal detrended fluctuation analysis method. For the S&P 500 index during the 1951–2022 period, the results show that market efficiency varies over time, with recurrent periods of statistically significant inefficiency (multifractality). Moments of inefficiency are associated with (i) a higher level of financial uncertainty—financial uncertainty Granger causes the degree of multifractality (inefficiency), (ii) a greater variability in the patterns of dependence of returns and also (iii) with periods of more relevant volatility clusters. In times of market inefficiency (efficiency), the use of financial (technical) indicators shows statistically significant in-sample and out-of-sample accuracy for equity risk premium prediction. When the market is efficient (inefficient), the use of financial (technical) indicators should be avoided due to the degradation of their predictive capacity. To build accurate and statistically significant predictions of the risk premium, thus enhancing decision-making processes, investors should monitor the informational efficiency status of the market before selecting financial and technical indicators as predictive variables. Finally, during market inefficiency periods, there is a greater polarisation of investors' opinions, with increased attention to fundamental variables for risk premium prediction, leading to a breakdown in price trend patterns, explaining the worst predictive capacity of technical indicators. © 2025 Elsevier B.V., All rights reserved.","Maciel, L.; da Silva, R.F.",2025,10.1002/ijfe.3058,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207528014&doi=10.1002%2Fijfe.3058&partnerID=40&md5=1ffdd4485e71ae8d511b671b1fd70f5e,scopus,"This study investigates the predictability of the equity risk premium by analyzing market efficiency, measured through multifractality using the multifractal detrended fluctuation analysis method on S&P 500 index data from 1951-2022. The findings indicate that market efficiency fluctuates over time, with periods of significant inefficiency characterized by higher financial uncertainty, greater return dependency variability, and increased volatility clustering. During inefficient markets, financial indicators are more accurate for predicting the equity risk premium, while technical indicators are more effective during efficient markets. The study suggests investors monitor market efficiency to optimize their selection of predictive indicators and improve decision-making. Investor opinion polarization and a focus on fundamental variables during inefficiency periods contribute to the breakdown of price trend patterns and reduced predictive capacity of technical indicators.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:23:03.395619
8e03ef7073e5ba57,Markov switching models in empirical finance,"I review the burgeoning literature on applications of Markov regime switching models in empirical finance. In particular, distinct attention is devoted to the ability of Markov Switching models to fit the data, filter unknown regimes and states on the basis of the data, to allow a powerful tool to test hypotheses formulated in light of financial theories, and to their forecasting performance with reference to both point and density predictions. The review covers papers concerning a multiplicity of subfields in financial economics, ranging from empirical analyses of stock returns, the term structure of default-free interest rates, the dynamics of exchange rates, as well as the joint process of stock and bond returns. Copyright © 2011 by Emerald Group Publishing Limited. © 2013 Elsevier B.V., All rights reserved.","Guidolin, M.",2011,10.1108/s0731-9053(2011)000027b004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872420543&doi=10.1108%2FS0731-9053%282011%29000027B004&partnerID=40&md5=f6c892890a65c56195475aa0f9c3021c,scopus,"This paper reviews the application of Markov regime switching models in empirical finance, focusing on their ability to fit data, filter regimes, test hypotheses, and their forecasting performance for stock returns, interest rates, exchange rates, and joint stock/bond returns.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:23:12.811656
53a26b3792f39f00,Maximum Likelihood Estimation in Markov Regime-Switching Models With Covariate-Dependent Transition Probabilities,"This paper considers maximum likelihood (ML) estimation in a large class of models with hidden Markov regimes. We investigate consistency of the ML estimator and local asymptotic normality for the models under general conditions, which allow for autoregressive dynamics in the observable process, Markov regime sequences with covariate-dependent transition matrices, and possible model misspecification. A Monte Carlo study examines the finite-sample properties of the ML estimator in correctly specified and misspecified models. An empirical application is also discussed.","Pouzo, Demian; Psaradakis, Zacharias; Sola, Martin",2022,10.3982/ecta17249,,wos,"This paper focuses on maximum likelihood estimation for Markov regime-switching models, particularly when transition probabilities depend on covariates. It examines the theoretical properties of the estimator, including consistency and local asymptotic normality, under various conditions such as autoregressive dynamics and potential model misspecification. The study is supported by a Monte Carlo simulation to assess finite-sample performance and includes an empirical application.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:23:16.243526
6e7428a05a102988,"Measurement error, skewness, and risk analysis: Coping with the long tail of the distribution","Probabilistic risk analyses often construct multistage chance trees to estimate the joint probability of compound events. If random measurement error is associated with some or all of the estimates, we show that resulting estimates of joint probability may be highly skewed. Joint probability estimates based on the analysis of multistage chance trees are more likely than not to be below the true probability of adverse events, but will sometimes substantially overestimate them. In contexts such as insurance markets for environmental risks, skewed distributions of risk estimates amplify the ""winner's curse"" so that the estimated risk premium for low-probability events is likely to be lower than the normative value. Skewness may result even in unbiased estimators of expected value from simple lotteries, if measurement error is associated with both the probability and pay-off terms. Further, skewness may occur even if the error associated with these two estimates is symmetrically distributed. Under certain circumstances, skewed estimates of expected value may result in risk-neutral decisionmakers exhibiting a tendency to choose a certainty equivalent over a lottery of equal expected value, or vice versa. We show that when distributions of estimates of expected value are positively skewed, under certain circumstances it will be optimal to choose lotteries with nominal values lower than the value of apparently superior certainty equivalents. Extending the previous work of Goodman (1960), we provide an exact formula for the skewness of products. © 2008 Elsevier B.V., All rights reserved.","Mumpower, J.L.; McClelland, G.",2002,10.1111/0272-4332.00027,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036093310&doi=10.1111%2F0272-4332.00027&partnerID=40&md5=e208b86abcf5d91cfe313e6fa5828657,scopus,"This paper discusses how measurement error in probabilistic risk analyses can lead to skewed estimates of joint probabilities, particularly for low-probability events. It explains that these skewed estimates can affect decision-making in contexts like insurance markets and can even cause risk-neutral individuals to make suboptimal choices. The authors provide a formula for calculating the skewness of products, extending previous work.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:23:19.467419
97f84ada8039939e,Measuring ESG risk premia with contingent claims,"We propose a contingent claims approach for estimating ESG risk premia from market information and market participants' decisions. To this end, we infer the asset value dynamics via the structural model of Merton [1974, On the Pricing of Corporate Debt: The Risk Structure of Interest Rates. Journal of Finance 29: 449-470.] for a large panel of S&P 500 firms using an estimation algorithm that utilizes the information embedded in stock market prices, CDS spreads, and default probabilities. We find a statistically significant relationship between the ESG score and the volatility and drift terms of the asset value process, suggesting that ESG factors are structurally connected to the value of the firm. We establish a mapping between ESG scores and the cost of equity and debt as implied by firm's contingent claims, and derive estimates of the ESG risk premium across different ESG and leverage profiles. In addition, we break down the ESG risk premia by industry, and demonstrate how practitioners can adjust the weighed average cost of capital of ESG laggard firms for valuation and decision making purposes.","Michopoulos, Ioannis; Bougias, Alexandros; Episcopos, Athanasios; Livanis, Efstratios",2025,10.1080/1351847x.2024.2394550,,wos,"This paper proposes a contingent claims approach to estimate ESG risk premia using market data and participant decisions. It applies Merton's structural model to S&P 500 firms, incorporating stock prices, CDS spreads, and default probabilities. The study finds a link between ESG scores and asset value dynamics, implying ESG factors influence firm value. It derives ESG risk premia and shows how to adjust the cost of capital for ESG laggard firms.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:23:22.115753
50f2b2d275365f3b,Measuring Economic Uncertainty in China†,"This study develops a new economic uncertainty (EU) index based on Chinese newspapers to address the media coverage bias of existing measures. We investigate how the EU affects China’s macroeconomy. Our results suggest that the EU reduces aggregate output. We find that uncertainty predicts fluctuations in economic activity and actual economic activity also predicts EU, but nonlinearly. Furthermore, we show that uncertainty in the United States leads to uncertainty in China, implying that negative EU on the Chinese economy is coming from the U.S. Finally, we conduct some asset-pricing tests, showing that EU can predict stock returns and commands risk premium. Our results are helpful for both researchers and policymakers to stabilize the economy and financial markets in China. © 2022 Elsevier B.V., All rights reserved.","Pan, W.-F.; Wang, X.; Wang, S.",2022,10.1080/1540496x.2021.1873764,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100533968&doi=10.1080%2F1540496X.2021.1873764&partnerID=40&md5=0e68f4835371abb9317f7969560367ff,scopus,"This study introduces a new economic uncertainty (EU) index for China, derived from newspaper coverage, to mitigate biases in existing measures. It examines the impact of this EU index on China's macroeconomy, finding that increased uncertainty reduces aggregate output. The research also reveals that uncertainty predicts economic activity, and vice versa, with a nonlinear relationship. Additionally, it demonstrates a spillover effect from US uncertainty to China, suggesting external factors influence China's economic uncertainty. The study further explores the predictive power of EU on stock returns and its associated risk premium, offering insights for economic and financial market stabilization.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:23:24.203478
a0ae28c3f6db31b0,Measuring contagion effects between crude oil and Chinese stock market sectors,"The role of cross-market linkages in the occurrence of tail events in stock and energy markets has not yet been fully understood in the contagion literature. This paper investigates the contagion from oil prices to Chinese stock sectors by considering differences between extreme positive returns and extreme negative returns. We compute time-varying cut-offs by employing a generalized Pareto distribution (GPD) function to estimate excess returns. We then use a multinomial logit (MNL) model to examine the probability of Chinese stock sector co-exceedances associated with oil price exceedances. Our results indicate that, compared to common domestic factors, the contagion between oil price and stock sectors is relatively weak, but never negligible. We argue that faced with volatile oil prices during turbulent periods, the existence of any contagion weakens the benefits of portfolio diversification related to oil and Chinese stock sector investment. Based on our findings, investors holding a portfolio of oil and Chinese sector stocks should pay special attention to the extreme changes in crude oil prices and adopt hedging measures to protect their portfolio from extreme shocks to oil markets. © 2018 Elsevier B.V., All rights reserved.","Fang, S.; Egan, P.",2018,10.1016/j.qref.2017.11.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034079456&doi=10.1016%2Fj.qref.2017.11.010&partnerID=40&md5=d7590a7aff8c4cc31d0bc45a65327a35,scopus,"This paper investigates the contagion effects between crude oil prices and Chinese stock market sectors, distinguishing between extreme positive and negative returns. Using a generalized Pareto distribution (GPD) to estimate excess returns and a multinomial logit (MNL) model to analyze co-exceedances, the study finds that contagion is relatively weak but present. The authors suggest that investors should be aware of extreme oil price changes and consider hedging strategies.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:23:28.274945
19c588a05711d2d3,Measuring the Level and Uncertainty of Trend Inflation,"Firmly anchored inflation expectations are widely viewed as playing a central role for the conduct of monetary policy. This paper presents estimates of trend inflation, based on information contained in monthly data on realized inflation, survey expectations, and the term structure of interest rates. In order to assess whether inflation expectations are anchored, a timevarying volatility of trend shocks is estimated as well. While there is some commonality in inflation- and survey-based estimates of trend inflation, yield-based trend estimates embed a highly persistent component orthogonal to trend inflation. Trimmed-mean inflation rates and survey forecasts are most indicative of trend inflation.","Mertens, Elmar",2016,10.1162/rest_a_00549,,wos,"This paper estimates trend inflation using monthly data on realized inflation, survey expectations, and the term structure of interest rates. It also estimates time-varying volatility of trend shocks to assess inflation expectations anchoring. Trimmed-mean inflation rates and survey forecasts are found to be most indicative of trend inflation, while yield-based estimates include a persistent component orthogonal to trend inflation.",False,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:23:30.547667
4d23cc600c835687,Measuring the natural rate of interest,"The natural rate of interest-the real interest rate consistent with output equaling its natural rate and stable inflation-plays a central role in macroeconomic theory and monetary policy. Estimation of the natural rate of interest, however, has received little attention. We apply the Kalman filter to estimate jointly time-varying natural rates of interest and output and trend growth. We find a close link between the natural rate of interest and the trend growth rate, as predicted by theory. Estimates of the natural rate of interest, however, are very imprecise and subject to considerable real-time measurement error.","Laubach, T; Williams, JC",2003,10.1162/003465303772815934,,wos,"This paper estimates the natural rate of interest, defined as the real interest rate consistent with output at its natural rate and stable inflation, using the Kalman filter to jointly estimate time-varying natural rates of interest and output and trend growth. The study finds a link between the natural rate of interest and trend growth, but notes that estimates are imprecise and subject to measurement error.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:23:34.099613
014d516a2e0c45b7,"Measuring the performance of government bond portfolios with index-based level, slope, and curvature factors","This paper introduces a three-factor interest rate risk model to improve the measurement of active bond fund performance. Traditional models assume a linear relationship between risk exposure and expected returns, leading to biases. By incorporating level, slope, and curvature factors derived from Treasury index returns, the proposed model better captures the nonlinear nature of bond returns. Empirical tests on passive and active US government bond portfolios confirm its accuracy in estimating passive style returns and active alpha. The study also provides the first performance analysis of fixed-income separate accounts, revealing their economic significance and superior value-added performance over mutual funds. © 2025 Elsevier B.V., All rights reserved.","Rohleder, M.",2025,10.1002/rfe.70024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014890422&doi=10.1002%2Frfe.70024&partnerID=40&md5=6803ce0a2bde3094c0a0d12da1441a18,scopus,"This paper proposes a three-factor interest rate risk model (level, slope, curvature) using Treasury index returns to enhance the measurement of active bond fund performance, addressing limitations of traditional linear models. Empirical tests on US government bond portfolios validate its accuracy in estimating passive returns and active alpha, and reveal superior performance of fixed-income separate accounts over mutual funds.",True,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:23:38.547906
e60825a4816e6116,Medical Extended Reality for Radiology Education and Training,"Medical extended reality (MXR), encompassing augmented reality, virtual reality, and mixed reality (MR), presents a novel paradigm in radiology training by offering immersive, interactive, and realistic learning experiences in health care. Although traditional educational tools in the field of radiology are essential, it is necessary to capitalize on the innovative and emerging educational applications of extended reality (XR) technologies. At the most basic level of learning anatomy, XR has been extensively used with an emphasis on its superiority over conventional learning methods, especially in spatial understanding and recall. For imaging interpretation, XR has fostered the concepts of virtual reading rooms by enabling collaborative learning environments and enhancing image analysis and understanding. Moreover, image-guided interventions in interventional radiology have witnessed an uptick in XR utilization, illustrating its effectiveness in procedural training and skill acquisition for medical students and residents in a safe and risk-free environment. However, there remain several challenges and limitations for XR in radiology education, including technological, economic, and ergonomic challenges and and integration into existing curricula. This review explores the transformative potential of MXR in radiology education and training along with insights on the future of XR in radiology education, forecasting advancements in immersive simulations, artificial intelligence integration for personalized learning, and the potential of cloud-based XR platforms for remote and collaborative training. In summation, MXR's burgeoning role in reshaping radiology education offers a safer, scalable, and more efficient training model that aligns with the dynamic healthcare landscape.Medical extended reality (MXR), encompassing augmented reality, virtual reality, and mixed reality (MR), presents a novel paradigm in radiology training by offering immersive, interactive, and realistic learning experiences in health care. Although traditional educational tools in the field of radiology are essential, it is necessary to capitalize on the innovative and emerging educational applications of extended reality (XR) technologies. At the most basic level of learning anatomy, XR has been extensively used with an emphasis on its superiority over conventional learning methods, especially in spatial understanding and recall. For imaging interpretation, XR has fostered the concepts of virtual reading rooms by enabling collaborative learning environments and enhancing image analysis and understanding. Moreover, image-guided interventions in interventional radiology have witnessed an uptick in XR utilization, illustrating its effectiveness in procedural training and skill acquisition for medical students and residents in a safe and risk-free environment. However, there remain several challenges and limitations for XR in radiology education, including technological, economic, and ergonomic challenges and and integration into existing curricula. This review explores the transformative potential of MXR in radiology education and training along with insights on the future of XR in radiology education, forecasting advancements in immersive simulations, artificial intelligence integration for personalized learning, and the potential of cloud-based XR platforms for remote and collaborative training. In summation, MXR's burgeoning role in reshaping radiology education offers a safer, scalable, and more efficient training model that aligns with the dynamic healthcare landscape.",,2024,10.1016/j.jacr.2024.05.006,,proquest,"This review explores the use of Medical Extended Reality (MXR), including AR, VR, and MR, in radiology education and training. It highlights MXR's potential for anatomy learning, imaging interpretation, and interventional radiology procedures, while also acknowledging challenges like technological and integration issues. The review forecasts future advancements such as AI integration and cloud-based platforms for remote training, suggesting MXR offers a safer, scalable, and efficient training model.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:23:40.299440
465b71f382fbc310,Merchant Commodity Storage and Term-Structure Model Error,"Merchant operations involves valuing and hedging the cash flows of commodity- and energy-conversion assets as real options based on stochastic models that inevitably embed model error. In this paper we quantify how empirically calibrated model errors concerning the futures term structure affect the valuation and hedging of natural gas storage. We find that even small model errors-on the order of 1%-2% of the empirical futures price variance-can have a disproportionate impact on storage valuation and hedging. In particular, theoretically equivalent hedging strategies have very different sensitivities to model error, with one natural strategy exhibiting potentially catastrophic performance in the presence of small model errors. We propose effective approaches to mitigate the negative effect of futures term-structure model error on hedging, also taking into account futures contract illiquidity, and provide theoretical justification for some of these approaches. Beyond commodity storage, our analysis has relevance for other real and financial options that depend on futures term-structure dynamics, as well as for inventory, production, and capacity investment policies that rely on demand-forecast term structures.","Secomandi, Nicola; Lai, Guoming; Margot, Francois; Scheller-Wolf, Alan; Seppi, Duane J.",2015,10.1287/msom.2015.0518,,wos,"This paper quantifies the impact of empirically calibrated model errors in futures term structure on the valuation and hedging of natural gas storage. It finds that even small errors can significantly affect storage valuation and hedging, leading to potentially catastrophic performance for certain hedging strategies. The study proposes mitigation approaches and discusses broader relevance for other options and investment policies.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:23:42.291879
b883d647a8f20108,"Model misspecification, the equilibrium natural interest rate, and the equity premium","This paper analyzes the natural rate of interest and the equity premium in a nonlinear model where agents are uncertain over both future technology growth and the future course of monetary policy. I show that model uncertainty, and notably uncertainty on the future course of monetary policy, can give rise to a sizable precautionary savings motive. This result is potentially problematic for both the estimation of the natural rate and its use as a policy indicator. Monetary uncertainty can also contribute to amplify the equity premium, and to account for its apparent, positive link with inflation. © 2009 The Ohio State University. © 2012 Elsevier B.V., All rights reserved.","Tristani, O.",2009,10.1111/j.1538-4616.2009.00263.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349421148&doi=10.1111%2Fj.1538-4616.2009.00263.x&partnerID=40&md5=e79ed1d77fb7696770f3d1293d03fc42,scopus,"This paper examines the natural rate of interest and the equity premium within a nonlinear model that incorporates uncertainty about future technological growth and monetary policy. The study highlights how model uncertainty, particularly concerning monetary policy, can stimulate precautionary savings, potentially complicating the estimation and policy relevance of the natural rate. Additionally, monetary uncertainty is shown to amplify the equity premium and explain its correlation with inflation.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:23:45.443656
6e6b601cc5aa10eb,Model of corporate bond spread based on improved neural network,"The reasons for credit spreads can be divided into enterprise-specific non-systematic risks and widespread macroeconomic systemic risks. The previous traditional research mainly focused on the perspective of unsystematic risk. However, at present, more and more scholars are beginning to focus on systemic risks. Based on the neural network algorithm, this paper constructs an improved neural network-based corporate bond spread model to explore the impact of macro systemic risks on credit spreads. Based on the multi-factor no-arbitrage model, the linear relationship between the credit spread and the risk premium of each factor is obtained. At the same time, based on previous research results and observations of the current market reality, this paper identifies five important macroeconomic factors: actual economic output factors, inflation factors, stock market volatility factors, stock market return factors and inter-bank funding factors. The research results show that the model constructed in this paper has excellent performance.","Luo, Qiaoshun; Liu, Xinping",2021,10.3233/jifs-189497,,wos,"This paper proposes an improved neural network model to analyze corporate bond spreads, focusing on the impact of macroeconomic systemic risks. It incorporates five key macroeconomic factors and demonstrates the model's strong performance.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:23:47.611174
0e4008aea258c83e,Modeling Health Data Using Machine Learning Techniques Applied to Financial Management Predictions,"Health management has steadily improved in performance and accuracy using IT technology. Hospitals and health institutions hold an enormous number of data in their software applications, which can be used with Big Data methodologies to extract useful information. One of the most challenging aspects of health institutional management is financial management; billing prediction is a key aspect to maintain a predictable service level for patients, avoiding unpleasant surprises and anticipating treasury management. Using patient data from public patient databases and applying a machine learning approach, this article offers a model that helps to make more precise and detailed financial plans.",,2022,10.3390/app122312148,,proquest,This article proposes a machine learning model to improve financial management in healthcare institutions by predicting billing. It utilizes patient data from public databases to create more accurate financial plans.,True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:23:52.435943
4c71769a0ce9a6e9,Modeling Multi-horizon Electricity Demand Forecasts in Australia: A Term Structure Approach,"The Australian Electricity Market Operator generates one-day ahead electricity demand forecasts for the National Electricity Market in Australia and updates these forecasts over time until the time of dispatch. Despite the fact that these forecasts play a crucial role in the decision-making process of market participants, little attention has been paid to their evaluation and interpretation. Using half-hourly data from 2011 to 2015 for New South Wales and Queensland, it is shown that the official half-hourly demand forecasts do not satisfy the econometric properties required of rational forecasts. Instead there is a relationship between forecasts and forecast horizon similar to a term structure model of interest rates. To study the term structure of demand forecasts, a factor analysis that uses a small set of latent factors to explain the common variation among multiple observables is implemented. A three-factor model is identified with the factors admitting interpretation as the level, slope and curvature of the term structure of forecasts. The validity of the model is reinforced by assessing the economic value of demand forecasts. It is demonstrated that simple adjustments to long-horizon electricity demand forecasts based on the three estimated factors can enhance the informational content of the official forecasts.",,2023,10.5547/01956574.44.2.shur,,proquest,"This paper analyzes electricity demand forecasts in Australia, finding they do not meet rational forecast criteria. It proposes a term structure model using factor analysis to capture forecast horizon dynamics, identifying level, slope, and curvature factors. Adjusting long-horizon forecasts based on these factors can improve their informational content.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:23:55.747924
4af1fb5d81fb748c,Modeling the time-varying volatility of the paper-bill spread,"The spread between the rates on commercial paper and Treasury bills has received considerable attention in the literature for its role as an indicator of real economic activity. In this paper we empirically examine what happens when the volatility of the spread changes over time. We estimate a nonlinear model that enables us to discern the asymmetric impact of negative and positive shocks to the spread. We find that a positive shock has a larger impact on the volatility of the spread than does a negative shock. © 2009 Elsevier Inc. All rights reserved. © 2024 Elsevier B.V., All rights reserved.","Malik, F.; Ewing, B.T.; Kruse, J.B.; Lynch, G.J.",2009,10.1016/j.jeconbus.2009.03.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650577177&doi=10.1016%2Fj.jeconbus.2009.03.002&partnerID=40&md5=2a88f73bc55ca9f949c5fbd33d3186de,scopus,"This paper empirically investigates the time-varying volatility of the paper-bill spread, a known indicator of economic activity. It employs a nonlinear model to analyze the asymmetric impact of positive and negative shocks on this volatility, finding that positive shocks have a greater effect.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:23:58.931508
49b43eda4b92602b,Modelling of Chinese corporate bond default - A machine learning approach,"We apply machine learning techniques to construct a series of models of corporate bond defaults. By combining Chinese accounting information and corporate bond data from January 2012 to December 2019, we construct an out-of-sample forecast that significantly improves the identification rate of corporate bond defaults, with an area under the receiver operating characteristics curve greater than 90 percent. Our models are robust to different machine learning models, including stacking, boosting, and bagging ensembling models. Our models consider cross-sectional heterogeneity, such as different ownership structures, accessibility to external finance, industry heterogeneity, different sample periods, and government policy impact.","Lu, Zhou; Zhuo, Zhuyao",2021,10.1111/acfi.12846,,wos,"This study uses machine learning to model Chinese corporate bond defaults, combining accounting and bond data from 2012-2019. The models, including ensembling techniques like stacking, boosting, and bagging, significantly improve default identification with an AUC over 90%. They account for factors like ownership, finance accessibility, industry, time periods, and government policy.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:24:03.083915
2c9c05575068c7f9,Modelling portfolio defaults using hidden Markov models with covariates,"We extend the hidden Markov Model for defaults of Crowder et al. (2005, Quantitative Finance 5, 27-34) to include covariates. The covariates enhance the prediction of transition probabilities from high to low default regimes. To estimate the model, we extend the EM estimating equations to account for the time varying nature of the conditional likelihoods due to sample attrition and extension. Using empirical U.S. default data, we find that GDP growth, the term structure of interest rates and stock market returns impact the state transition probabilities. The impact, however, is not uniform across industries. We only find a weak correspondence between industry credit cycle dynamics and general business cycles. Reprinted by permission of Blackwell Publishing",,2008,10.1111/j.1368-423x.2008.00232.x,,proquest,"This paper extends the Hidden Markov Model (HMM) for defaults by incorporating covariates to improve the prediction of transitions between high and low default regimes. The authors adapt EM estimating equations to handle time-varying conditional likelihoods due to sample attrition and extension. Empirical analysis using U.S. default data reveals that GDP growth, interest rate term structure, and stock market returns influence state transition probabilities, with varying effects across industries. A weak link is observed between industry credit cycles and general business cycles.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:24:06.852019
8fd13386d94ce678,Modelling the risk premium in the black-market zloty-dollar exchange rate,"This paper tests for the presence of nonlinear dependence in the black-market Polish zloty-dollar exchange rate. Using the GARCH-M model, we illustrate use of the Marquardt (Journal of the Society of Industrial and Applied Mathematics, 2, 1963) alternative to the Berndt (Annals of Economical Social Measurement, 4, 1974) iterative nonlinear algorithm for the estimation of such models, and discrimination between estimated models on the basis of the Brock and Potter (Handbook of Statistics, 11, 1993) test for so conditional variance misspecification. We find evidence of a time-varying risk premium such that foreign speculators are compensated for increased exchange rate risk by appreciation which increases the dollar value of zloty holdings, and which is able to account for all of the apparent nonlinearity in the zloty. © 2017 Elsevier B.V., All rights reserved.","McMillan, D.G.; Speight, A.E.H.",1999,10.1080/135048599353357,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040964268&doi=10.1080%2F135048599353357&partnerID=40&md5=cee866fd77f492916ad145b02de8d1d1,scopus,"This paper investigates the black-market zloty-dollar exchange rate using a GARCH-M model to test for nonlinear dependence and a time-varying risk premium. It employs specific iterative nonlinear algorithms for model estimation and discrimination based on conditional variance misspecification tests. The findings suggest that foreign speculators are compensated for exchange rate risk through appreciation, which explains the observed nonlinearity.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:24:08.923869
79f5b96bb6a44999,Mortgage prepayment with an uncertain holding period,"A discrete-time-option pricing model is developed to value a mortgage and its embedded prepayment option when the effective life of the mortgage is a random variable with a probability distribution of known parameters. The model can be applied when the borrower's ex ante expectation of his tenure follows any probability distribution bounded to the left at zero. The Gamma distribution is used to illustrate the model.The pricing model is further applied to determine the conditions under which financially motivated prepayment is optimal. The results show that the certainty model understates the Interest Rate Differential needed to justify prepayment (IRD) for short Expected Holding Period (EHP) borrowers and overstates the IRD for long EHP borrowers. When the EHP is relatively long, the certainty model provides relatively good estimates of IRD during the beginning years of the mortgage life. Under most other conditions, the estimates of the certainty holding period model are biased.","Yang, TT; Maris, BA",1996,10.1007/bf00132266,,wos,"This paper develops a discrete-time-option pricing model for mortgages with embedded prepayment options, considering an uncertain borrower holding period. It applies the Gamma distribution to illustrate the model and analyzes conditions for financially motivated prepayment. The model reveals biases in the certainty model's Interest Rate Differential (IRD) estimates, particularly for borrowers with short or long Expected Holding Periods (EHP).",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:24:11.379870
1bbb2fffd29655e3,Multi-Country and Multi-Horizon GDP Forecasting Using Temporal Fusion Transformers,"This paper applies a new artificial intelligence architecture, the temporal fusion transformer (TFT), for the joint GDP forecasting of 25 OECD countries at different time horizons. This new attention-based architecture offers significant advantages over other deep learning methods. First, results are interpretable since the impact of each explanatory variable on each forecast can be calculated. Second, it allows for visualizing persistent temporal patterns and identifying significant events and different regimes. Third, it provides quantile regressions and permits training the model on multiple time series from different distributions. Results suggest that TFTs outperform regression models, especially in periods of turbulence such as the COVID-19 shock. Interesting economic interpretations are obtained depending on whether the country has domestic demand-led or export-led growth. In essence, TFT is revealed as a new tool that artificial intelligence provides to economists and policy makers, with enormous prospects for the future.",,2023,10.3390/math11122625,,proquest,"This paper utilizes the Temporal Fusion Transformer (TFT), an advanced AI architecture, for joint GDP forecasting across 25 OECD countries and multiple time horizons. The TFT model demonstrates interpretability, identifies temporal patterns and significant events, and handles multiple time series distributions. It outperforms traditional regression models, particularly during volatile periods like the COVID-19 pandemic, and offers economic insights into growth drivers.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:24:18.604209
0bb5683df1b01809,Multi-Factor Cox-Ingersoll-Ross Models of the Term Structure: Estimates and Tests from a Kalman Filter Model,"This paper presents a method for estimating multi-factor versions of the Cox-Ingersoll-Ross (1985b) model of the term structure of interest rates. The fixed parameters in one, two, and three factor models are estimated by applying an approximate maximum likelihood estimator in a state-space model using data for the U.S. treasury market. A nonlinear Kalman filter is used to estimate the unobservable factors. Multi-factor models are necessary to characterize the changing shape of the yield curve over time, and the statistical tests support the case for two and three factor models. A three factor model would be able to incorporate random variation in short term interest rates, long term rates, and interest rate volatility. © 2008 Elsevier B.V., All rights reserved.","Chen, R.-R.; Scott, L.",2003,10.1023/a:1024736903090,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037491892&doi=10.1023%2FA%3A1024736903090&partnerID=40&md5=4be8316ed69eeed051871feea9eb16d0,scopus,"This paper estimates multi-factor Cox-Ingersoll-Ross models for the term structure of interest rates using a Kalman filter and U.S. treasury data. It finds that two and three-factor models are statistically supported, with a three-factor model allowing for variations in short-term rates, long-term rates, and volatility.",True,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:24:21.580084
7b86fe43643e030c,Multiple Structural Breaks in Vector Error Correction Models,"The analysis of structural breaks in vector error correction models is often confined to possible level shifts and trend breaks. In contrast, only rudimentary tools are available to deal with breaks in the cointegration matrix and the adjustment towards equilibrium. Particularly, the possibility of multiple structural breaks during long sampling periods is often ignored which can lead to inconsistently estimated coefficients. In this paper, we study a two-step estimator based on a penalized regression to determine the number of structural breaks, their timing, and estimate the model's coefficients for each regime. We focus on two important cases, namely, (i) constant dynamics but changing long-run equilibria, and (ii) convergence to new long-run equilibria with new adjustment dynamics. We use simulations to investigate the finite sample performance of the two-step estimator and provide an empirical illustration using data on the term structure of interest rates. © 2025 Elsevier B.V., All rights reserved.","Franjic, D.; Möβler, M.; Schweikert, K.",2025,10.1515/snde-2025-0009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012983996&doi=10.1515%2Fsnde-2025-0009&partnerID=40&md5=9a0223f5b81c98fa7a4ed698689f1332,scopus,"This paper proposes a two-step penalized regression estimator to identify multiple structural breaks in vector error correction models, focusing on changes in cointegration matrices and adjustment dynamics. It considers scenarios with constant dynamics but shifting long-run equilibria, and new equilibria with altered adjustment dynamics. The method's performance is evaluated through simulations and an empirical application to interest rate term structure data.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:24:23.828261
bbe5ad1518e05078,Multiple yield curve modeling and forecasting using deep learning,"This manuscript introduces deep learning models that simultaneously describe the dynamics of several yield curves. We aim to learn the dependence structure among the different yield curves induced by the globalization of financial markets and exploit it to produce more accurate forecasts. By combining the self-attention mechanism and nonparametric quantile regression, our model generates both point and interval forecasts of future yields. The architecture is designed to avoid quantile crossing issues affecting multiple quantile regression models. Numerical experiments conducted on two different datasets confirm the effectiveness of our approach. Finally, we explore potential extensions and enhancements by incorporating deep ensemble methods and transfer learning mechanisms. © 2024 Elsevier B.V., All rights reserved.","Richman, R.; Scognamiglio, S.",2024,10.1017/asb.2024.26,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207006861&doi=10.1017%2Fasb.2024.26&partnerID=40&md5=af5466c8f0a797c696cf7812ccb4d0ea,scopus,"This paper presents deep learning models for simultaneous modeling and forecasting of multiple yield curves, leveraging the dependence structure between them. It combines self-attention and nonparametric quantile regression to generate point and interval forecasts, addressing quantile crossing issues. The approach is validated empirically on two datasets and discusses extensions with ensemble methods and transfer learning.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-28T10:24:27.651501
038b53d4cdf820ae,Navigating Inflation Challenges: AI-Based Portfolio Management Insights,"After 2010, the consumer price index fell to a low level in the EU. In the euro area, it remained low between 2010 and 2020. The European Central Bank has even had to take action against the emergence of deflation. The situation changed significantly in 2021. Inflation jumped to levels not seen for 40 years in the EU. Our study aims to use artificial intelligence to forecast inflation. We also use artificial intelligence to forecast stock index changes. Based on the forecasts, we propose portfolio reallocation decisions to protect against inflation. The forecasting literature does not address the importance of structural breaks in the time series, which, among other things, can affect both the pattern recognition and prediction capabilities of various machine learning models. The novelty of our study is that we used the Zivot–Andrews unit root test to determine the breakpoints and partitioned the time series into training and testing datasets along these points. We then examined which database partition gives the most accurate prediction. This information can be used to re-balance the portfolio. Two different AI-based prediction algorithms were used (GRU and LSTM), and a hybrid model (LSTM–GRU) was also included to investigate the predictability of inflation. Our results suggest that the average error of the inflation forecast is a quarter of that of the stock market index forecast. Inflation developments have a fundamental impact on equity and government bond returns. If we obtain a reliable estimate of the inflation forecast, we have time to rebalance the portfolio until the inflation shock is incorporated into government bond returns. Our results not only support investment decisions at the national economy level but are also useful in the process of rebalancing international portfolios.",,2024,10.3390/risks12030046,,proquest,"This study utilizes AI (GRU, LSTM, and LSTM-GRU models) to forecast inflation and stock index changes, incorporating structural breaks identified by the Zivot-Andrews unit root test. The goal is to inform portfolio reallocation decisions to mitigate inflation risks, suggesting that inflation forecasts are more accurate than stock market index forecasts and can aid in rebalancing both national and international portfolios.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:24:33.836298
b28e94345d9b2a0a,Neural networks in financial engineering: a study in methodology,"Neural networks have shown considerable successes in modeling financial data series. However, a major weakness of neural modeling is the lack of established procedures for performing tests for misspecified models, and tests of statistical significance for the various parameters that have been estimated. This is a serious disadvantage in applications where there is a strong culture for testing not only the predictive power of a model or the sensitivity of the dependent variable to changes in the inputs but also the statistical significance of the finding at a specified level of confidence. Rarely is this more important than in the case of financial engineering, where the data generating processes are dominantly stochastic and only partially deterministic. Partly a tutorial, partly a review, this paper describes a collection of typical applications in options pricing, cointegration, the term structure of interest rates and models of investor behavior which highlight these weaknesses and propose and evaluate a number of solutions. We describe a number of alternative ways to deal with the problem of variable selection, show how to use model misspecification tests, we deploy a novel way based on cointegration to deal with the problem of nonstationarity, and generally describe approaches to predictive neural modeling which are more in tune with the requirements for modeling financial data series.",A. . -P. N. Refenes; A. N. Burgess; Y. Bentz,1997,10.1109/72.641449,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=641449,ieeexplore,"This paper reviews and proposes solutions for weaknesses in neural network modeling for financial data, specifically addressing the lack of established procedures for model misspecification tests and statistical significance tests. It covers applications in options pricing, cointegration, interest rate term structure, and investor behavior, offering approaches for variable selection, model misspecification testing, and handling nonstationarity using cointegration, aiming for predictive neural modeling suitable for financial data.",True,True,True,gemini-2.5-flash-lite,Olav,Review,,2025-10-28T10:24:46.763918
2576c128066ee3ce,New frontiers for arch models,"In the 20 years following the publication of the ARCH model, there has been a vast quantity of research uncovering the properties of competing volatility models. Wide-ranging applications to financial data have discovered important stylized facts and illustrated both the strengths and weaknesses of the models. There are now many surveys of this literature. This paper looks forward to identify promising areas of new research. The paper lists five new frontiers. It briefly discusses three-high-frequency volatility models, large-scale multivariate ARCH models, and derivatives pricing models. Two further frontiers are examined in more detail-application of ARCH models to the broad class of non-negative processes, and use of Least Squares Monte Carlo to examine non-linear properties of any model that can be simulated. Using this methodology, the paper analyses more general types of ARCH models, stochastic volatility models, long-memory models and breaking volatility models. The volatility of volatility is defined, estimated and compared with option-implied volatilities. Copyright (C) 2002 John Wiley Sons, Ltd.","Engle, R",2002,10.1002/jae.683,,wos,"This paper reviews the ARCH model literature over the past 20 years, highlighting its applications to financial data and identifying five promising areas for future research. It discusses high-frequency volatility models, large-scale multivariate ARCH models, and derivatives pricing models. Two areas are explored in depth: applying ARCH models to non-negative processes and using Least Squares Monte Carlo for non-linear properties of simulated models. The paper also defines, estimates, and compares the volatility of volatility with option-implied volatilities.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:24:49.860364
4717510223fcce48,News Media and Attention Spillover across Energy Markets: A Powerful Predictor of Crude Oil Futures Prices,"We develop two news-based investor attention measures from the news trends function of the Bloomberg terminal and investigate their predictive power for returns on crude oil futures contracts with various maturities. Our main results after controlling for relevant macroeconomic variables show that the Oil-based Institutional Attention Index is useful in predicting oil futures returns, especially during price downturn periods, while the forecasting accuracy is further improved when the Commodity Market Institutional Attention Index is used. This forecasting accuracy decreases, however, with the maturity of oil futures contracts. Moreover, we find some evidence of Granger-causality and regime-dependent interactions between investor attention measures and oil futures returns. Finally, variable selection algorithms matter before making predictions since they create the best forecasting results in many cases considered. These findings are important for informed traders and policymakers to better understand the price dynamics of the oil markets.","Cepni, Oguzhan; Duc Khuong Nguyen; Sensoy, Ahmet",2022,10.5547/01956574.43.si1.ocep,,wos,"This study develops two news-based investor attention measures to predict crude oil futures returns. The Oil-based Institutional Attention Index and the Commodity Market Institutional Attention Index show predictive power, particularly during price downturns and for shorter-term futures contracts. The research also highlights the importance of variable selection algorithms for forecasting accuracy and suggests implications for traders and policymakers.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:24:53.532139
7c3de2cb255ed663,News implied volatility and aggregate economic activity: evidence from the Japanese government bond market,"Because options on 10-year Japanese government bond (JGB) futures are relatively new in the market, their implied volatility, JGB-VIX, is not available before 2007. For the period when JGB-VIX is available, we conduct supervised learning by using the daily newspaper articles as input and JGB-VIX as output. We then construct a new JGB market uncertainty measure, which we called JGB-NU, based on the predicted values of JGB-VIX from the estimated model and contents of the newspaper articles from 1981 to 2021. In the VAR analysis with JGB-NU, we confirm that JGB market uncertainty shocks have a negative impact on real economic activities in Japan. © 2024 Elsevier B.V., All rights reserved.","Goshima, K.; Ishijima, H.; Shintani, M.",2024,10.1080/13504851.2022.2140751,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141618967&doi=10.1080%2F13504851.2022.2140751&partnerID=40&md5=b1683f478f389f1a0e766902a8fed6b6,scopus,"This study constructs a new measure of Japanese government bond (JGB) market uncertainty, JGB-NU, by combining implied volatility (JGB-VIX) with newspaper article content. Using supervised learning with daily newspaper articles as input and JGB-VIX as output, they predict JGB-VIX and then create JGB-NU. The analysis shows that shocks to JGB market uncertainty negatively impact real economic activities in Japan.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:24:57.948248
636a11ccd9e2eab8,"Nominal US Treasuries Embed Liquidity Premiums, Too","A novel arbitrage-free model of nominal U.S. Treasuries that decomposes yields into frictionless expected rates, frictionless term premiums, and liquidity premiums produces four key results from Jan. 1987 to Aug. 2023. First, liquidity loadings are larger than for the slope and higher-order principal components. Second, the countercyclicality of required nominal Treasury returns owes to liquidity, if anything, not frictionless term premiums. Third, Federal Reserve large-scale asset purchases generally work through expected rates and frictionless term premiums, not liquidity premiums. Fourth, given similar estimates using TIPS, inflation expectations are less moored around the Federal Reserve's price objectives than other models say.","Durham, J. Benson",2025,10.1017/s0022109023001345,,wos,"This paper introduces a new arbitrage-free model for nominal U.S. Treasuries, separating yields into frictionless expected rates, frictionless term premiums, and liquidity premiums. The model reveals that liquidity premiums are significant, countercyclical returns are driven by liquidity rather than frictionless term premiums, Federal Reserve asset purchases primarily affect expected rates and frictionless term premiums, and inflation expectations are less anchored than previously thought. The analysis spans from January 1987 to August 2023.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:25:03.860153
63afa8c0f2631f86,Non-linear and asymmetric linkages between real growth in the Euro area and global financial market conditions: new evidence,"This paper deals with transition mechanisms through which financial market conditions affect real economic growth in the Euro area. The informational content of financial variables for predicting real economic growth is assessed, allowing for asymmetric responses to shocks. A nonlinear framework is developed based on a smooth transition model for which the effects of shocks can vary across business cycles when financial indicators modify both the endogenous and state variables. Global financial variables are shown to significantly affect real growth in the Euro area, particularly during periods of recession. Changes in stock market index and yield slope have asymmetric effects on real growth. In recessionary periods, the slope of the US yield curve does not have a significant impact on growth in the Euro area. All rights reserved, Elsevier",,2012,10.1016/j.econmod.2012.01.008,,proquest,"This paper investigates how global financial market conditions influence real economic growth in the Euro area, employing a nonlinear, asymmetric framework. It finds that financial variables significantly impact Euro area growth, especially during recessions, with stock market index changes and yield slope exhibiting asymmetric effects. The US yield curve's slope, however, shows no significant impact on Euro area growth during recessions.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:25:08.004106
6d1f179bacc01371,Non-linearities in the relationship of agricultural futures prices,"The movement of food prices remains a controversial issue owing to the intense rise in volatility that has been observed in recent years. Agricultural futures markets have experienced a similar pattern and simplistic linear models seem to be no longer reliable when analysing their functions. Against this background, this study contributes to the literature by adopting a non-linear smooth transition approach to examine the relationship between prices for first and second nearby futures contracts of seven agricultural commodities. Our main objective is to distinguish between contango and backwardation regimes when analysing the relationship between the futures spread and changes in the first nearby futures price. Our findings reveal that a linear framework neglects important dynamics, as futures prices adjust only under specific circumstances, and that the predictive power of the futures spread is much stronger during backwardation regimes.","Beckmann, Joscha; Czudaj, Robert",2014,10.1093/erae/jbt015,,wos,"This study uses a non-linear smooth transition approach to analyze the relationship between first and second nearby futures contracts for seven agricultural commodities. It aims to differentiate between contango and backwardation regimes and finds that linear models are insufficient, with futures prices adjusting only under specific circumstances and the futures spread being a stronger predictor during backwardation regimes.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:25:21.859873
44636fa0fc84cad0,Nonlinear adjustments in deviations from the law of one price for wholesale hog prices,"This article applies the Band-Threshold Autoregression (Band-TAR) model to investigate whether the law of one price (LOOP) holds in Taiwanese wholesale hog markets during the period from May 1987 through December 2003. We find evidence of a nonlinear mean reversion in deviations from the LOOP for relative hog prices. Our empirical study confirms the presence of thresholds and provides strong evidence in support of the view that the regional hog markets have been tightly integrated in Taiwan and that the wholesale hog market in Taiwan is an efficient market economy. Furthermore, the estimated half-lives from the nonlinear generalized impulse response analysis are as short as four months.","Chen, Pei-Fen; Lee, Chien-Chiang",2008,10.1111/j.1574-0862.2008.00320.x,,wos,"This study uses a Band-Threshold Autoregression (Band-TAR) model to examine the law of one price (LOOP) in Taiwanese wholesale hog markets. The findings indicate nonlinear mean reversion in deviations from the LOOP for relative hog prices, suggesting tight integration and efficiency in these markets. The estimated half-lives for adjustments are as short as four months.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:27:19.315757
77f546f93221829e,Nonlinear autoregressive model with stochastic volatility innovations: Semiparametric and Bayesian approach,"The first-order nonlinear autoregressive model with the stochastic volatility as the model of dependent innovations is considered and a semiparametric method is proposed to estimate the unknown function. Optimal filtering technique based on sequential Monte Carlo perspective is used for estimation of the hidden log-volatility in this model. Bayesian paradigm is applied for estimation of both the unknown parameters and hidden process using particle marginal Metropolis–Hastings scheme. Furthermore, an empirical application on simulated data and on the monthly excess returns of S&P 500 index is presented to study the performance of the schemes implemented. © 2018 Elsevier B.V., All rights reserved.","Hajrajabi, A.; Yazdanian, A.R.; Farnoosh, R.",2018,10.1016/j.cam.2018.05.036,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047786510&doi=10.1016%2Fj.cam.2018.05.036&partnerID=40&md5=02e79d768d049c5f4dbb0017d24c72aa,scopus,This paper proposes a semiparametric method to estimate a first-order nonlinear autoregressive model with stochastic volatility innovations. It utilizes optimal filtering and Bayesian inference (particle marginal Metropolis–Hastings) for parameter and hidden process estimation. The method is demonstrated on simulated data and S&P 500 index returns.,True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:27:24.220042
df4201d91d38dc14,Nonlinear dynamics in foreign exchange rates,"This paper investigates whether the behavior of real and nominal foreign exchange rates as well as interest rates are governed by nonlinear dynamics; it also explores whether observed deviations from parity conditions exhibit nonlinear dependence. Standard statistical tests for randomness, such as autocorrelation tests, have low power against a large class of deterministic, nonlinear processes. Discerning nonrandomness of innovations in exchange rates is important for a variety of reasons. For example, many models of international asset pricing assume exchange rates to follow a random walk. Furthermore, nonlinear patterns in deviations from various exchange rate parities have implications for the existence of a time-varying foreign exchange risk premium. With the use of the BDS statistic and a correlation dimension analysis, this paper's primary findings are that (1) foreign exchange markets have become increasingly complex and therefore less amenable to forecasting over time; (2) although forward exchange risk premia are statistically significant and display a deterministic structure, this structure is complex and therefore not easily discernible; and (3) innovations in real exchange rates are consistent with a Purchasing Power Parity equilibrium. © 1999 Elsevier Science Inc. © 2018 Elsevier B.V., All rights reserved.","Mahajan, A.; Wagner, A.J.",1999,10.1016/s1044-0283(99)00002-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0042597353&doi=10.1016%2FS1044-0283%2899%2900002-2&partnerID=40&md5=3947c7aa0039abb36b482bdda7e1902f,scopus,"This paper investigates nonlinear dynamics in foreign exchange rates, interest rates, and deviations from parity conditions. Using BDS statistic and correlation dimension analysis, it finds that foreign exchange markets have become more complex and less predictable over time. While forward exchange risk premia show a deterministic structure, it is complex. Innovations in real exchange rates are consistent with Purchasing Power Parity equilibrium.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:27:29.732045
f8be887256aa2835,Nonlinear error correction model and multiple-threshold cointegration,"As an extension of linear cointegration, threshold cointegration has been a vibrant research topic in finance and statistics. Existing estimation procedures of threshold cointegration are usually based on the threshold vector error correction model (TVECM); however, only one threshold cointegration is considered. In this paper, we investigate estimation of the multiple-threshold cointegration that is more widely used in application. Two proposed methods, the LSE and the Smoothed LSE are studied, via the multiple-regime TVECM. The convergence rate of the LSE is obtained and the limiting distribution of the smoothed LSE is developed. To assess the performance of these estimators, a simulation study was conducted, in which the results support the asymptotic theories. As an example, we study the term structure of interest rates by a two-threshold cointegration model. © 2023 Elsevier B.V., All rights reserved.","Wang, M.; Chan, N.H.; Yau, C.Y.",2016,10.5705/ss.2014.219t,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011856669&doi=10.5705%2Fss.2014.219t&partnerID=40&md5=777e1a0aa7630414aad0d4e9f0221332,scopus,"This paper proposes and analyzes estimation methods for multiple-threshold cointegration, extending the existing threshold vector error correction model (TVECM) which typically considers only one threshold. Two methods, Least Squares Estimation (LSE) and Smoothed LSE, are studied for the multiple-regime TVECM. The paper provides theoretical results on the convergence rate of LSE and the limiting distribution of Smoothed LSE, supported by simulation studies. An empirical application to the term structure of interest rates using a two-threshold cointegration model is presented.",True,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:27:39.979737
ba14e9af00683956,Nonlinear mean reversion in stock prices,"This paper provides new evidence on the time-series predictability of stock market returns by introducing a test of nonlinear mean reversion. The performance of extreme daily returns is evaluated in terms of their power to predict short- and long-horizon returns on various stock market indices and size portfolios. The paper shows that the speed of mean reversion is significantly higher during the large falls of the market. The parameter estimates indicate a negative and significant relation between the monthly portfolio returns and the extreme daily returns observed over the past one to eight months. Specifically, in a quarter in which the minimum daily return is -2% the expected excess return is 37 basis points higher than in a month in which the minimum return is only -1%. This result holds for the value-weighted and equal-weighted stock market indices and for each of the size decile portfolios. The findings are also robust to different sample periods, different indices, and investment horizons. All rights reserved, Elsevier",,2008,10.1016/j.jbankfin.2007.05.013,,proquest,"This paper investigates nonlinear mean reversion in stock prices, demonstrating that extreme daily returns predict future returns, with faster mean reversion occurring after large market declines. The study finds a significant negative relationship between monthly portfolio returns and extreme daily returns over several months, indicating that larger negative daily returns lead to higher expected excess returns. These findings are robust across different market indices, portfolio types, and time periods.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:27:42.275936
68770456e00079eb,"Nonparametric conditional density estimation of short-term interest rate movements: procedures, results and risk management implications","This article shows how to estimate the conditional density of daily changes in the 3-month T-bill rate, using an extension of the kernel-based estimator proposed by Rosenblatt (1969). The shape of the estimated density is allowed to vary with both the level and the lagged change in rates. Due to the nonparametric character of the estimation procedure, the model produces conditional quantile estimates that are based only on the data and are independent of the modellers' assumptions. The obtained results do not support the assumption of systematically mean-reverting behaviour underlying some theoretical models of short-term interest rate dynamics. However, they clearly indicate the presence of nonlinear first-order autocorrelation and volatility clustering effects, as well as a positive relationship between yield volatility and level. Reprinted by permission of Routledge, Taylor and Francis Ltd.",,2013,10.1080/09603107.2012.741677,,proquest,"This article proposes a nonparametric kernel-based method to estimate the conditional density of daily changes in the 3-month T-bill rate. The method allows the density shape to vary with the level and lagged change in rates, providing data-driven conditional quantile estimates. The findings suggest nonlinear autocorrelation, volatility clustering, and a positive relationship between yield volatility and level, contradicting systematic mean reversion.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:28:46.651650
bf41f43cc6cb3759,Nonparametric model validations for hidden Markov models with applications in financial econometrics,"We address the nonparametric model validation problem for hidden Markov models with partially observable variables and hidden states. We achieve this goal by constructing a nonparametric simultaneous confidence envelope for transition density function of the observable variables and checking whether the parametric density estimate is contained within such an envelope. Our specification test procedure is motivated by a functional connection between the transition density of the observable variables and the Markov transition kernel of the hidden states. Our approach is applicable for continuous-time diffusion models, stochastic volatility models, nonlinear time series models, and models with market microstructure noise. (C) 2011 Elsevier B.V. All rights reserved.","Zhao, Zhibiao",2011,10.1016/j.jeconom.2011.01.002,,wos,"This paper proposes a nonparametric model validation method for hidden Markov models with partially observable variables and hidden states. It constructs a confidence envelope for the transition density function of observable variables and checks if the parametric density estimate falls within it. This approach is applicable to various models including diffusion models, stochastic volatility models, and nonlinear time series models, with potential applications in financial econometrics.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:28:53.891905
00fd43d7d419402b,Objective Black-Litterman views through deep learning: A novel hybrid model for enhanced portfolio returns,"The Black-Litterman Model is a sophisticated approach to portfolio optimization that integrates investor views with market data. However, the model's effectiveness is highly dependent on the quality of its inputs and is often challenged by subjective or biased views. Recent advancements in deep learning have enabled the generation of more objective, data-driven views, significantly enhancing the model's accuracy and reliability. In this study, we propose a novel CGL-BL Model, which employs a hybrid deep learning approach—CEEMDAN-GLSTM-LSTM (CGL)—to generate investor views. The CGL model leverages the CEEMDAN algorithm for return-based time series decomposition, a Genetic Algorithm-optimized LSTM network to enhance prediction accuracy, and an additional LSTM network for nonlinear aggregation of prediction results. The proposed CGL-BL model was evaluated on both the SSE 50 Index in China and the DJIA in the United States. Empirical results show that the proposed CGL-BL model outperforms all benchmark portfolios in terms of excess return, Sharpe ratio, and maximum drawdown when applied with a short-term rebalancing strategy on the SSE 50 Index and a medium-to-long-term strategy on the DJIA, demonstrating strong potential for practical application in financial markets. © 2025 Elsevier B.V., All rights reserved.","Su, X.; Lu, K.; Yen, J.",2026,10.1016/j.eswa.2025.128868,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009900832&doi=10.1016%2Fj.eswa.2025.128868&partnerID=40&md5=b27f0493fa3bcfd8ab0088a189292bd7,scopus,"This study introduces a novel hybrid model, CGL-BL, that combines deep learning (CEEMDAN-GLSTM-LSTM) with the Black-Litterman model to generate objective investor views for portfolio optimization. The model was tested on the SSE 50 Index and DJIA, showing improved performance over benchmarks in excess return, Sharpe ratio, and maximum drawdown.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:28:58.275418
7bff3f5daef33b24,Oil volatility risk and stock market volatility predictability: Evidence from G7 countries,"Academic research relies extensively on stock market information to forecast oil volatility, with relatively little attention paid to the reverse evidence. Our paper fills this gap by investigating the predictive ability of oil volatility risk to forecast stock market volatility. Using oil volatility risk premium (oil VRP) as the predictor, we find that oil VRP does exhibit statistically and economically significant in-sample and out-of-sample forecasting power for G7 countries, even controlling for some popular macroeconomic variables. These findings are robust when using alternative proxies for volatilities of stock and oil. Furthermore, the strength of the predictive evidence is substantial during relatively high and low level of stock market, while is substantially higher for recessions vis-á-vis expansions. Oil VRP can also contains additional information for predicting a series of macroeconomic variables, which serves as an available explanation for its forecasting ability. © 2017 Elsevier B.V., All rights reserved.","Feng, J.; Wang, Y.; Yin, L.",2017,10.1016/j.eneco.2017.09.023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033474517&doi=10.1016%2Fj.eneco.2017.09.023&partnerID=40&md5=a075dd9efe8a8e5ec98bf304a551f6af,scopus,"This paper investigates the predictive ability of oil volatility risk for stock market volatility in G7 countries. It finds that oil volatility risk premium (oil VRP) has statistically and economically significant forecasting power for stock market volatility, even after controlling for macroeconomic variables. The predictive power is stronger during recessions and at high/low stock market levels. Oil VRP also contains information for predicting macroeconomic variables.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:29:02.531950
7bede30395f8b5fa,"On Crop Biodiversity, Risk Exposure, and Food Security in the Highlands of Ethiopia","This paper investigates the effects of crop genetic diversity on farm productivity and production risk in the highlands of Ethiopia. Using a moment‐based approach, the analysis uses a stochastic production function capturing mean, variance, and skewness effects. Welfare implications of diversity are evaluated using a certainty equivalent, measured as expected income minus a risk premium (reflecting the cost of risk). We find that the effect of diversity on skewness dominates its effect on variance, meaning that diversity reduces the cost of risk. The analysis also shows that the beneficial effects of diversity become of greater value in degraded land.",,2009,10.1111/j.1467-8276.2009.01265.x,,proquest,"This paper examines how crop genetic diversity impacts farm productivity and production risk in Ethiopia's highlands. It uses a stochastic production function to analyze mean, variance, and skewness effects, finding that diversity reduces risk costs, especially on degraded land. The study evaluates welfare implications through a certainty equivalent.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:29:42.499835
3ca42c57cf880dd3,On credit spread change of Chinese corporate bonds: credit risk or asset allocation effect?,"Purpose – Credit spread change is a key issue for investors to earn the excess return from corporate bonds. Generally, there are two kinds of different effects that support the changing of credit spread: asset allocation effect and credit risk change effect. The existing literature based on US data supports credit spread change is driven by credit risk change; however, this issue based on Chinese market data has not been investigated clearly. This paper seeks to address this issue. Design/methodology/approach – The authors adopt Markov regime switching model to investigate the changing mode of the credit spread in the Chinese bond market. They select three kinds of variables: the credit risk variables, the asset allocation variables and the liquidity condition variables. With ML estimators, the authors can find the changing mode and further they study the relationship between the regime switching and some observed variables, such as macro economy variables and turnover of stock market. Findings – The authors find it is driven by asset allocation effect in most time and by credit risk change only in shorter period. Empirical results show that the switching of dominance from one effect to another isn't closely related with macro-economy variables, but related with the turnover of stock market. Practical implications – These results indicate that in China the credit risk of corporate bonds is relatively low and the corporate bonds are still auxiliary asset for investors. Originality/value – In this paper, the authors have the following two contributions: first, they discuss the asset allocation effect in the Chinese bond market and introduce the stock market variables and bank credit variable to describe the asset allocation effect; second, based on Chinese bond market data, they find different findings from the existing literature about US and European bond markets, showing that the changing of credit spread is mostly related with asset allocation effect but not credit risk change. © 2017 Elsevier B.V., All rights reserved.","Cui, C.; Liu, H.; Zhang, Y.",2013,10.1108/cfri-02-2012-0020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015670143&doi=10.1108%2FCFRI-02-2012-0020&partnerID=40&md5=702c717b284c0dcc395735c4a2685d52,scopus,"This paper investigates the drivers of credit spread changes in the Chinese corporate bond market, distinguishing between asset allocation and credit risk effects. Using a Markov regime switching model and ML estimators, the study finds that asset allocation effects dominate most of the time, with credit risk playing a lesser role. The switching between these effects is linked to stock market turnover rather than macroeconomic variables. The findings suggest that Chinese corporate bonds are primarily an auxiliary asset with relatively low credit risk.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:29:45.571487
6c8e3fca2c525e30,On estimability of parsimonious term structure models: An experiment with the Nelson-Siegel specification,"This study addresses operational issues in estimation of parsimonious term structure models. When using price errors, objective function in term structure estimation is a nonlinear function of the model parameters. This necessarily entails using numerical optimization techniques for estimation, which brings to fore the issue of (sensitivity of final results to) the choice of initialization of the optimization routine. This study assesses the sensitivity of the final objective function value and the final parameter vector to the choice of the 'initial guess' during the estimation of the popular Nelson-Siegel model. It turns out that there exist regions in the shape of the objective function where a slight change in (seemingly reasonable) initial vector takes one far from optimum. Choice of the (range of) 'best' starting vector turns out to be an empirical matter. Grid search is recommended. One must first get to a subset of initial values that results in the objective function value near a minimum and then assess the sensitivity of the final parameter vector to those relevant (subset of) initial values. The study illustrates the process using a typical trading day's data. © 2012 Copyright Taylor and Francis Group, LLC. © 2012 Elsevier B.V., All rights reserved.","Virmani, V.",2012,10.1080/13504851.2012.657343,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863492248&doi=10.1080%2F13504851.2012.657343&partnerID=40&md5=759a05b89376f52765cdf960acd72789,scopus,"This study investigates the estimation challenges of parsimonious term structure models, specifically the Nelson-Siegel model, by examining the sensitivity of parameter estimation to initial guesses in numerical optimization. It highlights the non-linear nature of the objective function and recommends a grid search approach to identify optimal starting vectors and assess parameter sensitivity, illustrated with a trading day's data.",True,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:29:51.107515
cc726ef76b042dc2,On the nonlinear specifications of short-term interest rate behavior: Evidence from euro-currency markets,"This paper presents a coherent nonlinear interest rate model that incorporates the dynamics of the error correction specification into the traditional term structure model. The joint tests based on six Euro-Currency rates indicate that the linear specification should be rejected. The estimated equation suggests that the linear components - the change of the long-term interest rate and the error correcting term are highly significant. The nonlinear components involving the higher order of the independent variables, the cross products, the lagged error squares, and/or the ARCH effect also present significant explanatory power for predicting short-term Euro-Currency rate changes, confirming the non-linear specifications. © 1999 Kluwer Academic Publishers,. © 2018 Elsevier B.V., All rights reserved.","Chiang, T.C.; Jeanette Chiang, J.I.N.",1999,10.1023/a:1008302525246,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53149100134&doi=10.1023%2FA%3A1008302525246&partnerID=40&md5=7fbdb4800e1097cff8e0bd382f9120ca,scopus,"This paper proposes a nonlinear interest rate model for short-term Euro-currency rates, integrating error correction dynamics into traditional term structure models. Empirical tests on six Euro-Currency rates reject linear specifications, showing significant explanatory power for both linear and nonlinear components, including higher-order terms, cross-products, lagged error squares, and ARCH effects, in predicting short-term rate changes.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:29:55.107882
c9bb2437e26f8d84,Optimal Filtering of Jump Diffusions: Extracting Latent States from Asset Prices,"This paper provides an optimal filtering methodology in discretely observed continuous-time jump-diffusion models. Although the filtering problem has received little attention, it is useful for estimating latent states, forecasting volatility and returns, computing model diagnostics such as likelihood ratios, and parameter estimation. Our approach combines time-discretization schemes with Monte Carlo methods. It is quite general, applying in nonlinear and multivariate jump-diffusion models and models with nonanalytic observation equations. We provide a detailed analysis of the filter's performance, and analyze four applications: disentangling jumps from stochastic volatility, forecasting volatility, comparing models via likelihood ratios, and filtering using option prices and returns. (JEL C11, C13, C15, C51, C52, G11, G12, G17)","Johannes, Michael S.; Polson, Nicholas G.; Stroud, Jonathan R.",2009,10.1093/rfs/hhn110,,wos,"This paper presents an optimal filtering method for discretely observed continuous-time jump-diffusion models, useful for estimating latent states, forecasting, and parameter estimation. The approach combines time-discretization and Monte Carlo methods, applicable to nonlinear, multivariate, and nonanalytic models. Applications include disentangling jumps from stochastic volatility, volatility forecasting, model comparison, and filtering using option prices and returns.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:30:00.547499
429dc98f1a74b78f,Optimal Time Varying Parameters in Yield Curve Modeling and Forecasting: A Simulation Study on BRICS Countries,"The term structure of interest rates is a fundamental decision-making tool for various economic activities. Despite the huge number of contributions in the field, the development of a reliable framework for both fitting and forecasting under various market conditions (either stable or very volatile) still remains a topical issue. Motivated by this problem, this study introduces a methodology relying on optimal time-varying parameters for three and five factor models in the Nelson-Siegel class that can be employed for an effective in-sample fitting and out-of-sample forecasting of the term structure. In detail, for the in-sample fitting we discussed a two-step estimation procedure leading to optimal models parameters and evaluated the performances of this approach in terms of flexibility and fitting accuracy gains. For what it concerns the forecasting, we suggest an approach overcoming the well-known issue between the stability of factor models' parameters and the optimal dynamic decay terms. To such aim, we use either autoregressive or machine learning techniques as local data generating processes based on the optimal parameters time series derived in the in-line fitting step. The so-obtained values are then employed to get day-ahead predictions of the yield curve. We assessed the proposed framework on daily spot rates of the BRICS (Brazil, Russia, India, China and South Africa) bond market. The experimental analysis illustrated that (i) time-varying parameters ensure a significant boost in the models fitting power and a more faithful representation of the yield curves dynamics; (ii) the proposed approach provides also stable and accurate predictions.","Castello, Oleksandr; Resta, Marina",2025,10.1007/s10614-024-10619-z,,wos,"This study proposes a methodology using optimal time-varying parameters for Nelson-Siegel class models to improve yield curve fitting and forecasting. It employs a two-step estimation for in-sample fitting and utilizes autoregressive or machine learning techniques for out-of-sample forecasting, tested on BRICS countries' bond market data. The results show that time-varying parameters enhance fitting power and prediction accuracy.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-28T10:30:28.723830
61ddf22073d6a62b,Optimal asset allocation and nonlinear return predictability from the dividend-price ratio,"We study non-linear predictability of stock returns arising from the dividend-price ratio and its implications for asset allocation decisions. Using data from five countries — U.S., U.K., France, Germany and Japan — we find empirical evidence supporting non-linear and time-varying models for the equity risk premium. Building on this, we examine several model specifications that can account for non-linear return predictability, including Markov switching models, regression trees, random forests and neural networks. Although in-sample return regressions and portfolio allocation results support the use of non-linear predictability models, the out-of-sample evidence is notably weaker, highlighting the difficulty in exploiting non-linear predictability in real time. © 2025 Elsevier B.V., All rights reserved.","Ghezzi, F.; Sarkar, A.; Pedersen, T.Q.; Timmermann, A.",2025,10.1007/s10479-024-06332-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001086702&doi=10.1007%2Fs10479-024-06332-7&partnerID=40&md5=dd0ef7ddf17b2e69e5c4a37d7cf1969f,scopus,"This study investigates non-linear predictability of stock returns using the dividend-price ratio across five countries. It explores various non-linear models like Markov switching, regression trees, random forests, and neural networks. While in-sample results show promise for non-linear models in return prediction and asset allocation, out-of-sample performance is less convincing, indicating challenges in real-time application.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:30:31.491732
b2464947594c2bc5,Optimal credit allocation under regime uncertainty with sensitivity analysis,"We consider the problem of credit allocation in a regime-switching model. The global evolution of the credit market is driven by a benchmark, the drift of which is given by a two-state continuous-time hidden Markov chain. We apply filtering techniques to obtain the diffusion of the credit assets under partial observation and show that they have a specific excess return with respect to the benchmark. The investor performs a simple mean-variance allocation on credit assets. However, returns and variance matrix have to be computed by a numerical method such as Monte Carlo, because of the dynamics of the system and the non-linearity of the asset prices. We use the theory of Dirichlet forms to deal with the uncertainty on the excess returns. This approach provides an estimation of the bias and the variance of the optimal allocation, and return. We propose an application in the case of a sectorial allocation with Credit Default Swaps (CDS), fully calibrated with observable data or direct input given by the portfolio manager. Reprinted by permission of World Scientific Publishing",,2015,10.1142/s0219024915500028,,proquest,"This paper addresses credit allocation in a regime-switching model where the credit market's evolution is influenced by a two-state hidden Markov chain. It uses filtering techniques to analyze credit asset diffusion and their excess returns relative to a benchmark. The study employs mean-variance allocation and discusses the necessity of numerical methods like Monte Carlo for calculating returns and variance due to the system's dynamics. It also applies Dirichlet form theory to manage uncertainty in excess returns, providing bias and variance estimations for optimal allocation. An application involving sectorial allocation with Credit Default Swaps (CDS) is presented, calibrated with observable data or portfolio manager inputs.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:30:32.667683
b7733e59a767a900,Optimal hedging of commodity price risks in highway contracts,"Macroeconomic conditions, such as commodity prices, labor wages, and inflation rates, affect the cost of construction projects. In a volatile market environment, highway agencies often pass such risks to contractors by using fixed-price contracts. In turn, contractors respond by adding premiums in bid prices. How much of this risk highway agencies should pass to contractors is the topic of this paper. More specifically, the objective of this paper is to develop a model that can help highway agencies manage cost risks associated with commodity prices. The weighted least squares regression model is used to estimate the risk premium; the solution to a multiobjective optimization formulation considers a genetic algorithm approach to nonconvex optimization. Crude oil prices are used as an example of volatile commodities. The results of this study suggest that the optimal risk mitigation actions are conditional on owners' risk preferences. © 2023 Elsevier B.V., All rights reserved.","Zhou, X.; Damnjanović, I.D.",2011,10.3141/2228-03,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155159114&doi=10.3141%2F2228-03&partnerID=40&md5=2c006c7e567e45da4d1cba2011fb7b10,scopus,"This paper develops a model to help highway agencies manage cost risks associated with commodity prices in construction projects. It uses weighted least squares regression and a genetic algorithm approach to multiobjective optimization to estimate risk premiums and determine optimal risk mitigation strategies, considering crude oil prices as an example. The findings indicate that optimal risk mitigation depends on the owner's risk preferences.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:30:36.163580
70d6446af9a229d8,Optimal profit-making strategies in stock market with algorithmic trading,"Machine learning (ML) techniques are being increasingly applied to financial markets for analyzing trends and predicting stock prices. In this study, we compared the price prediction and profit-making performance of various ML algorithms embedded into stock trading strategies. The dataset comprised daily data from the CSI 300 Index of the China stock market spanning approximately 17 years (2006-2023). We incorporated investor sentiment indicators and relevant financial elements as features. Our trained models included support vector machines (SVMs), logistic regression, and random forest. The results show that the SVM model outperforms the others, achieving an impressive 60.52% excess return in backtesting. Furthermore, our research compared standard prediction models (such as LASSO and LSTM) with the proposed approach, providing valuable insights for users selecting ML algorithms in quantitative trading strategies. Ultimately, this work serves as a foundation for informed algorithm choice in future financial applications. © 2024 Elsevier B.V., All rights reserved.","Wang, H.; Xie, D.",2024,10.3934/qfe.2024021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205123614&doi=10.3934%2FQFE.2024021&partnerID=40&md5=02b39feb6c9d88a93900c795feb2dade,scopus,"This study compares the performance of various machine learning algorithms (SVM, logistic regression, random forest, LASSO, LSTM) for stock price prediction and profit-making in the China stock market (CSI 300 Index). The SVM model achieved the highest excess return (60.52%) in backtesting, demonstrating its effectiveness in quantitative trading strategies.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:30:39.219560
7ce39ff7ea9cbb6e,Optimal supplier testing and tolerance strategies for genetically modified (GM) wheat,"AbstractA stochastic optimization model was developed to determine optimal testing strategies, costs, and risks for dual marketing of genetically modified (GM) and non-GM wheat in an export supply chain. The optimal testing strategy is derived that minimizes disutility of additional system costs due to testing and quality loss. Cost components were estimated including those related to testing, quality loss, and a risk premium to induce shippers to undertake dual marketing as opposed to handling only non-GM crops. Uncertainties were incorporated for adventitious presence and commingling, variety declaration, and test accuracy. Sensitivities were performed for effects of variety risks and declaration, penalty differentials, buyer tolerances, risk aversion, and GM adoption. Results indicate testing and segregation can be performed at a relatively low cost and risk to buyers.",,2007,10.1111/j.1574-0862.2007.00175.x,,proquest,"This study develops a stochastic optimization model to find the best strategies for testing and managing risks in a dual-market supply chain for genetically modified (GM) and non-GM wheat. The model aims to minimize costs associated with testing, quality loss, and a risk premium for shippers. It accounts for uncertainties in GM presence, commingling, variety declaration, and test accuracy. The findings suggest that testing and segregation can be cost-effective and low-risk for buyers.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:30:40.491491
06584a65bc1a10e7,Optimizing filter rule parameters with genetic algorithm and stock selection with artificial neural networks for an improved trading: The case of Borsa Istanbul,"Filter rule along with other trading algorithms is used to identify potentially profitable trading points in stock markets. In this study, the scope of the filter rule has been expanded to include different moving average types. The filter rule parameters that will provide the highest return for each of the stocks listed in Borsa Istanbul have been optimized by using genetic algorithm. A number of 357 stocks traded in Borsa Istanbul is included in the dataset of the study between 06-07-2012 and 31-03-2020 period. To improve the poor performance in out-of-sample sets of optimal rules, the stock selection process was performed by means of artificial neural networks. The artificial neural network model predicts the performance of the stock in the test set by using the performance values in the training set. Results indicate that the returns of the selected stocks are significantly higher than the returns of the buy and hold strategy. Parameter optimization of filter rule with genetic algorithms and stock selection with the artificial neural networks can be used as a decision support system for investors, where they can make a profit above the market return. When only the genetic algorithm results are taken into account, it can be stated that Borsa Istanbul is a weak form efficient market. However, selecting the stocks with the assistance of artificial neural networks made it possible to obtain excess returns over the market. © 2022 Elsevier B.V., All rights reserved.","Özçalıcı, M.; Bumin, M.",2022,10.1016/j.eswa.2022.118120,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134630841&doi=10.1016%2Fj.eswa.2022.118120&partnerID=40&md5=4ccc46612a7b221c336ef63c952ea36c,scopus,"This study optimizes filter rule parameters using a genetic algorithm and employs artificial neural networks for stock selection on Borsa Istanbul. The approach aims to improve trading performance by identifying profitable trading points and selecting stocks with better out-of-sample performance. Results show that the optimized strategy yields significantly higher returns than a buy-and-hold strategy, suggesting its utility as a decision support system for investors.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:30:44.355347
4c6402a19fd533ec,Option Pricing Under a Stochastic Interest Rate and Volatility Model with Hidden Markovian Regime-Switching,"In this paper we discuss an option pricing problem in a hidden Markovian regime-switching model with a stochastic interest rate and volatility. Regime switches are attributed to structural changes in an hidden economic environment and are described by a continuous-time, finite-state, unobservable Markov chain. The model is then applied to the valuation of a standard European option. By means of the standard separation principle, filtering and option valuation problems are separated. Robust filters for the hidden states of the economy and their robust filtered estimates of unknown parameters from the expectation maximization algorithm are presented based on standard techniques in filtering theory. Then an explicit expression of a conditional characteristic function relevant to option pricing is presented and the valuation of the option is discussed using the inverse Fourier transformation approach. Using the limiting behavior of the conditional characteristic function, an efficient implementation of the transform inversion integral is considered. Numerical experiments are given to illustrate the flexibility of filtering algorithms and the significance of regime-switching in option pricing.","Zhu, Dong-Mei; Lu, Jiejun; Ching, Wai-Ki; Siu, Tak-Kuen",2019,10.1007/s10614-017-9754-9,,wos,"This paper presents an option pricing model incorporating hidden Markovian regime-switching, stochastic interest rates, and volatility. It separates filtering and option valuation, develops robust filters using the expectation maximization algorithm, and derives an explicit expression for the conditional characteristic function. The valuation is performed using inverse Fourier transformation, with numerical experiments demonstrating the model's flexibility and the impact of regime-switching.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:30:49.923563
624b406f55b3ee95,Option-implied correlation between iTraxx Europe Financials and Non-Financials Indexes: A measure of spillover effect in European debt crisis,"This paper proposes an analytic method to estimate the option-implied correlation embedded in options on the iTraxx Europe CDS indexes. The option-implied correlation is suggested as a measure of the spill-over effect of default risk between the financial and corporate sectors in Europe. In particular, the correlation between the iTraxx Financials and Non-Financials sub-indexes is estimated from options on the iTraxx Main Index, which is considered as a basket option with the two sub-indexes being its underlyings. The abrupt changes of the realized correlation anticipated information of the corresponding option prices. The sovereign default risk, funding liquidity risk, level of risk aversion, and equity market performance are identified to be significant determinants of the option-implied correlation, implying interdependence amongst various markets during the European debt crisis. (C) 2013 Elsevier B.V. All rights reserved.","Hui, Cho-Hoi; Lo, Chi-Fai; Lau, Chun-Sing",2013,10.1016/j.jbankfin.2013.05.030,,wos,"This paper introduces a method to calculate option-implied correlation between iTraxx Europe Financials and Non-Financials indexes, using options on the iTraxx Main Index. This implied correlation serves as a metric for default risk spillover between European financial and corporate sectors. The study identifies sovereign default risk, funding liquidity risk, risk aversion, and equity market performance as key drivers of this correlation, highlighting market interdependencies during the European debt crisis.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:30:52.475568
b3a10d9772e08e18,"Oracle Properties, Bias Correction, and Bootstrap Inference for Adaptive Lasso for Time Series M-Estimators","We derive new theoretical results on the properties of the adaptive least absolute shrinkage and selection operator (adaptive lasso) for possibly nonlinear time series models. In particular, we investigate the question of how to conduct inference on the parameters given an adaptive lasso model. Central to this study is the test of the hypothesis that a given adaptive lasso parameter equals zero, which therefore tests for a false positive. To this end, we introduce a recentered bootstrap procedure and show, theoretically and empirically through extensive Monte Carlo simulations, that the adaptive lasso can combine efficient parameter estimation, variable selection, and inference in one step. Moreover, we analytically derive a bias correction factor that is able to significantly improve the empirical coverage of the test on the active variables. Finally, we apply the adaptive lasso and the recentered bootstrap procedure to investigate the relation between the short rate dynamics and the economy, thereby providing a statistical foundation (from a model choice perspective) for the classic Taylor rule monetary policy model.","Audrino, Francesco; Camponovo, Lorenzo",2018,10.1111/jtsa.12270,,wos,"This paper presents theoretical results for the adaptive lasso in time series models, focusing on inference for parameters and hypothesis testing. It introduces a recentered bootstrap procedure and a bias correction factor to improve parameter estimation, variable selection, and inference. The method is applied to analyze the relationship between short-term interest rates and the economy, providing a statistical basis for the Taylor rule.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:30:57.443344
d501874745d997c5,PMCMC for Term Structure of Interest Rates under Markov Regime Switching and Jumps,"A parameter estimation method, called PMCMC in this paper, is proposed to estimate a continuous-time model of the term structure of interests under Markov regime switching and jumps. There is a closed form solution to term structure of interest rates under Markov regime. However, the model is extended to be a CKLS model with non-closed form solutions which is a typical nonlinear and non-Gaussian state-space model(SSM) in the case of adding jumps. Although the difficulty of parameter estimation greatly prevents from researching models, we prove that the nonlinear and non-Gaussian state-space model has better performances in studying volatility. The method proposed in this paper will be implemented in simulation and empirical study for SHIBOR. Empirical results illustrate that the PMCMC algorithm has powerful advantages in tackling the models. © 2023 Elsevier B.V., All rights reserved.","Liu, X.; Li, X.; Zheng, S.; Qian, H.",2020,10.21078/jssi-2020-159-11,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151287143&doi=10.21078%2FJSSI-2020-159-11&partnerID=40&md5=c49a814702a6f8c4d212641b9b08fbb4,scopus,"This paper proposes a Particle Markov Chain Monte Carlo (PMCMC) method for estimating a continuous-time model of the term structure of interest rates, incorporating Markov regime switching and jumps. While a closed-form solution exists for Markov regime models, the extension to a CKLS model with jumps results in a nonlinear and non-Gaussian state-space model without a closed-form solution. The authors demonstrate that this complex model offers better performance in volatility studies and implement the PMCMC method in simulations and an empirical study using SHIBOR data, showing its effectiveness.",True,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:31:20.987513
85ea7436fd04653c,Package CovRegpy: Regularized covariance regression and forecasting in Python,"This paper will outline the functionality available in the CovRegpy package which was written for actuarial practitioners, wealth managers, fund managers, and portfolio analysts in the language of Python 3.11. The objective is to develop a new class of covariance regression factor models for covariance forecasting, along with a library of portfolio allocation tools that integrate with this new covariance forecasting framework. The novelty is in two stages: the type of covariance regression model and factor extractions used to construct the covariates used in the covariance regression, along with a powerful portfolio allocation framework for dynamic multi-period asset investment management. The major contributions of package CovRegpy can be found on the GitHub repository for this library in the scripts: CovRegpy.py, CovRegpy_DCC.py, CovRegpy_RPP.py, CovRegpy_SSA.py, CovRegpy_SSD.py, and CovRegpy_X11.py. These six scripts contain implementations of software features including multivariate covariance time series models based on the regularized covariance regression (RCR) framework, dynamic conditional correlation (DCC) framework, risk premia parity (RPP) weighting functions, singular spectrum analysis (SSA), singular spectrum decomposition (SSD), and X11 decomposition framework, respectively. These techniques can be used sequentially or independently with other techniques to extract implicit factors to use them as covariates in the RCR framework to forecast covariance and correlation structures and finally apply portfolio weighting strategies based on the portfolio risk measures based on forecasted covariance assumptions. Explicit financial factors can be used in the covariance regression framework, implicit factors can be used in the traditional explicit market factor setting, and RPP techniques with long/short equity weighting strategies can be used in traditional covariance assumption frameworks. We examine, herein, two real-world case studies for actuarial practitioners. The first of these is a modification (demonstrating the regularization of covariance regression) of the original example from Hoff & Niu ((2012). Statistica Sinica, 22(2), 729-753) which modeled the covariance and correlative relationship that exists between forced expiratory volume (FEV) and age and FEV and height. We examine this within the context of making probabilistic predictions about mortality rates in patients with chronic obstructive pulmonary disease. The second case study is a more complete example using this package wherein we present a funded and unfunded UK pension example. The decomposition algorithm isolates high-, mid-, and low-frequency structures from FTSE 100 constituents over 20 years. These are used to forecast the forthcoming quarter’s covariance structure to weight the portfolio based on the RPP strategy. These fully funded pensions are compared against the performance of a fully unfunded pension using the FTSE 100 index performance as a proxy. © 2024 Elsevier B.V., All rights reserved.","van Jaarsveldt, C.; Peters, G.; Ames, M.; Chantler, M.",2024,10.1017/s1748499524000101,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193064038&doi=10.1017%2FS1748499524000101&partnerID=40&md5=44609d79e4c0defef3678b5fb8df5c39,scopus,"This paper introduces the Python package CovRegpy, designed for actuarial practitioners and financial professionals. It offers a new class of covariance regression factor models for covariance forecasting and integrates them with a portfolio allocation framework. The package implements techniques such as regularized covariance regression (RCR), dynamic conditional correlation (DCC), singular spectrum analysis (SSA), and others for forecasting covariance and correlation structures. Two case studies are presented: one modifying a statistical model for mortality rate prediction and another demonstrating a pension fund portfolio allocation strategy using FTSE 100 data.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:31:23.899501
008bfdd1c354cda8,Permutation betting markets: Singleton betting with extra information,"We study permutation betting markets, introduced by Chen et al. (Proceedings of the ACM Conference on Electronic Commerce, 2007). For these markets, we consider subset bettings in which each trader can bet on a subset of candidates ending up in a subset of positions. We consider the revenue maximization problem for the auctioneer in two main frameworks: the risk-free revenue maximization (studied in Chen et al., Proceedings of the ACM Conference on Electronic Commerce, 2007), and the probabilistic revenue maximization. We also explore the use of some certain knowledge or extra information about the possible outcomes of the market. We first show that finding the optimal revenue in the risk-free model for the subset betting problem is inapproximable. This resolves an open question posed by Chen et al. (Proceedings of the ACM Conference on Electronic Commerce, 2007). In order to identify solvable variants of the problem, we propose the singleton betting language which allows traders to bet an arbitrary value on one candidate for one position. For singleton bettings, we first provide a linear-time implementable necessary and sufficient condition for existence of a solution with positive revenue for any possible outcome. Furthermore, we develop an LP-based polynomial-time algorithm to find the optimum solution of this problem. In addition, we show how to extend this LP-based method to handle some extra information about the possible outcomes. Finally, we consider the revenue maximization problem in a probabilistic setting. For this variant, we observe that the problem of maximizing the expected revenue is polynomial-time solvable, but we show that maximizing the probability of achieving a pre-specified revenue is #P-Complete. © 2009 Springer Science+Business Media, LLC. © 2012 Elsevier B.V., All rights reserved.","Ghodsi, M.; Mahini, H.; Mirrokni, V.S.; Zadimoghaddam, M.",2011,10.1007/s00453-009-9378-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959223300&doi=10.1007%2Fs00453-009-9378-0&partnerID=40&md5=50e50d408bf76c116353904d5f36e299,scopus,"This paper studies permutation betting markets, focusing on subset and singleton bettings. It addresses risk-free and probabilistic revenue maximization for the auctioneer, considering the impact of extra information. The study finds the optimal revenue problem for subset betting to be inapproximable, resolves an open question, and proposes a linear-time condition for positive revenue in singleton betting. An LP-based algorithm is developed for singleton betting optimization, extendable to handle extra information. In the probabilistic setting, maximizing expected revenue is polynomial-time solvable, but maximizing the probability of a pre-specified revenue is #P-Complete.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:31:25.803396
dde88ce8c7f12863,Physical vs. Transition climate risks: Asymmetric effects on stock return predictability,"This paper examines the predictive role of two dominant climate risk categories-physical and transition risks-in forecasting U.S. equity market risk premiums. The results reveal a pronounced asymmetry: physical climate risk significantly and negatively predicts stock returns both in-sample and out-of-sample, whereas transition climate risk demonstrates insignificant forecasting ability. This superior performance of physical risk delivers greater economic gains to investors and remains robust even after controlling for widely used economic predictors. However, its predictability is state-dependent, weakening during economic disruptions and strengthening following the COP21 Agreement. Further analysis shows that the cash flow and sentiment channels potentially drive the strong predictability of physical risk. Overall, our findings underscore the importance of incorporating physical climate risk into equity return forecasting models, offering actionable insights for financial decision-making processes.","Zhou, Mingtao; Ma, Yong",2025,10.1016/j.irfa.2025.104266,,wos,"This study investigates the predictive power of physical and transition climate risks on U.S. equity market risk premiums. It finds that physical climate risk significantly predicts stock returns, while transition climate risk does not. The predictability of physical risk is state-dependent and potentially driven by cash flow and sentiment channels, offering insights for financial decision-making.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:31:27.851553
ea1a6443c4789db2,Political uncertainty and financial market reactions: A new test,"Recent literature highlights the crucial role of understanding the mechanism between political uncertainty and financial market reactions. Along the lines of this topic, our study stresses a clear causal framework. Exploiting one unique natural experiment of the Taiwan Strait Crisis (1995-96), we provide a simple testing strategy which could precisely quantify the effects of political shocks on stock markets. This approach combines the features of one innovative panel estimator and new statistical learning methods for causal inference. Our results indicate, separating true signal from noise via the optimal benchmark, the political crisis had a substantial and significant negative impact on Taiwan's stock prices. This finding is consistent with the empirical evidence of risk premium in recent studies. Moreover, the optimal counterfactual could be an alternative option for the ceteris paribus assumption in non-lab controlled settings. Finally, this study shows predictor selection is needed for a convincing causal estimate in counterfactual studies. © 2019 Elsevier B.V., All rights reserved.","Wang, H.; Boatwright, A.L.",2019,10.1016/j.inteco.2019.07.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075956787&doi=10.1016%2Fj.inteco.2019.07.004&partnerID=40&md5=7644f7e70f84f5bfd12bd3d90c207b0f,scopus,"This study uses a natural experiment (Taiwan Strait Crisis 1995-96) and a combination of an innovative panel estimator and statistical learning methods to quantify the impact of political shocks on stock markets. The findings suggest a significant negative impact on Taiwan's stock prices, consistent with risk premium theories. The study also highlights the importance of predictor selection in causal inference for counterfactual studies.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:31:30.515176
b6af4763daadded6,Portfolio creation using artificial neural networks and classification probabilities: a Canadian study,"This study aims to verify whether using artificial neural networks (ANNs) to establish classification probabilities generates portfolios with higher excess returns than using ANNs in their traditional role of predicting portfolio returns. Our sample includes all companies listed on the Toronto Stock Exchange from 1994 to 2014 with a monthly average of 16,324 company-month observations. Results indicate that portfolios based on the classification probabilities yield mean returns ranging from 7.81 to 14.40% annually over a 16-year period and that portfolios based on both predicted returns and classification probabilities generate returns that are superior to the market index. In addition, there is evidence that ranking securities based on their probability of beating the market has some benefit. © 2020 Elsevier B.V., All rights reserved.","Morris, T.; Comeau, J.",2020,10.1007/s11408-020-00350-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083775124&doi=10.1007%2Fs11408-020-00350-8&partnerID=40&md5=6e1777fb9bc75bc769cdc7de72b223df,scopus,"This Canadian study investigates whether using artificial neural networks (ANNs) to generate classification probabilities leads to portfolios with higher excess returns compared to traditional ANN methods for predicting returns. The study analyzed companies listed on the Toronto Stock Exchange from 1994 to 2014. Results showed that portfolios based on classification probabilities achieved annual mean returns between 7.81% and 14.40%, outperforming the market index. The findings suggest that ranking securities by their probability of market outperformance is beneficial.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:31:33.682680
df698990aeb42d1d,Portfolio optimization in the presence of tail correlation,"We investigate the relative performance of optimal versus naive portfolio strategies. The accepted status on this question is that naive diversification outperforms optimal strategies. We revisit this question using U.S. data for equity, Treasury bonds, Gold and Crude Oil between 2002 and 2022 by analyzing the portfolio of investors displaying constant relative risk aversion who also consider tail behavior in the dynamics of assets. We use moment generating functions applied to non-Gaussian processes to obtain accurate model estimation as well as an efficient control variate for the utility maximization problem. Our results show that risk-averse investors that are aware of tail dynamics consistently outperform the most standard portfolio strategies. In particular, highly risk-averse investors substantially outperform the so-called naive 1/N portfolio in both pre-COVID-19 and post-COVID-19 periods. Thus, true portfolio diversification requires considering both the complexity of asset dynamics and realistic risk aversion structures. © 2023 Elsevier B.V., All rights reserved.","Ben Abdelaziz, F.; Chibane, M.",2023,10.1016/j.econmod.2023.106235,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149471432&doi=10.1016%2Fj.econmod.2023.106235&partnerID=40&md5=35edc3923027f51972407c1948146451,scopus,"This study compares optimal and naive portfolio strategies, challenging the notion that naive diversification is superior. Using US equity, Treasury bonds, Gold, and Crude Oil data from 2002-2022, the research incorporates tail correlation and constant relative risk aversion. The findings indicate that risk-averse investors considering tail dynamics achieve better performance than standard strategies, particularly outperforming the 1/N portfolio in both pre- and post-COVID-19 periods. The study emphasizes the importance of accounting for asset dynamics complexity and realistic risk aversion for effective diversification.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:31:35.747386
c1bf075f530f6e7a,Predictability and habit persistence,"This paper highlights the role of persistence in explaining predictability of excess returns. To this end, we develop a CCAPM model with habit formation when the growth rate of endowments follows a first order Gaussian autoregression. We provide a closed form solution of the price-dividend ratio and determine conditions that guarantee the existence of a bounded equilibrium. The habit stock model is found to possess internal propagation mechanisms that increase persistence. It outperforms the time separable and a 'Catching up with the Joneses' version of the model in terms of predictability therefore highlighting the role of persistence in explaining the puzzle. (c) 2005 Elsevier B.V. All rights reserved.","Collard, Fabrice; Feve, Patrick; Ghattassi, Imen",2006,10.1016/j.jedc.2005.06.016,,wos,"This paper develops a CCAPM model with habit formation to explain the predictability of excess returns, emphasizing the role of persistence. The model, which incorporates a first-order Gaussian autoregression for endowment growth, yields a closed-form solution for the price-dividend ratio and establishes conditions for a bounded equilibrium. The habit stock model demonstrates internal propagation mechanisms that enhance persistence and outperforms simpler models in predictability, thus underscoring the significance of persistence in addressing this financial puzzle.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:31:37.619584
daa22f7f68680d55,Predictability of HK-REITs returns using artificial neural network,"Purpose: The purpose of this paper is to determine if artificial neural network (ANN) works better than linear regression in predicting Hong Kong real estate investment trusts’ (REITs) excess return. Design/methodology/approach: Both ANN and the regression were applied in this study to forecast the Hong Kong REITs’ (HK-REITs) return using the capital asset pricing model and Fama and French’s three-factor models. Each result was further split into annual time series as a measure to investigate the consistency of the performance across time. Findings: ANN had produced a better forecasting results than the regression based on their trading performance. However, the forecasting performance varied across individual REITs and time periods. Practical implications: ANN should be considered for use when one were to attempt forecasting the HK-REITs excess returns. However, the trading performance should be always compared with buy and hold strategy prior to make any investment decisions. Originality/value: This paper tested the predicting power of ANN on the HK-REITs and the consistency of its predicting power. © 2021 Elsevier B.V., All rights reserved.","Loo, W.K.",2020,10.1108/jpif-07-2019-0090,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075160296&doi=10.1108%2FJPIF-07-2019-0090&partnerID=40&md5=dd4d84e73e04fa2b017a3e756beadbe1,scopus,"This paper investigates the predictability of Hong Kong REITs' excess returns using Artificial Neural Networks (ANNs) and compares their performance against linear regression. Both methods were applied using the capital asset pricing model and Fama-French three-factor models. The study found that ANNs generally produced better forecasting results than regression, although performance varied across individual REITs and time periods. The authors suggest considering ANNs for forecasting HK-REITs returns but advise comparing trading performance with a buy-and-hold strategy before making investment decisions.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:31:46.155272
52ddf32cfa8a6bc0,Predictability of sugar futures: evidence from the Indian commodity market,"Purpose – The forecasting power of commodity futures is a matter of intensive research as evidenced by a number of related publications. The purpose of this paper is to illustrate how advanced forecasting techniques improve the predictability of sugar futures in the Indian commodity market. Design/methodology/approach – The forward premium is estimated using ordinary least square regression technique. Different linear and nonlinear models are used to forecast the sugar future spot prices from the futures prices. The forecasting accuracy of each pair of models is then compared by estimating the corresponding Diebold-Mariano test statistics. Findings – From the estimated forward premiums, it is found that there is more volatility toward the date of maturity for a three-month horizon compared to six-month, and 12-month horizons. It is established that the futures prices of sugar, when used in a model, are able to generate better forecasts for the future spot prices. Moreover, the forecasting accuracy is found to be better for a shorter futures horizon. Research limitations/implications – The present study is restricted only to sugar. If sufficient data are available, the same study could be extended to other commodities as well. The findings imply that technical traders would benefit by using advanced forecasting techniques along with futures prices of sugar to determine the expected future spot prices. Practical implications – The findings in this paper suggest that though simple statistical models may be adopted to relate future spot prices to futures prices, more accurate prediction of the price behavior is possible with advanced forecasting methods like the artificial neural network. Social implications – The findings will help market participants such as traders to be better informed about the future spot prices and hence get a better deal. Originality/value – This is one of the first investigations to assess the predictability of commodity futures by employing advanced forecasting techniques.",,2015,10.1108/afr-02-2014-0002,,proquest,"This paper investigates the predictability of sugar futures in the Indian commodity market using advanced forecasting techniques. It compares linear and nonlinear models to forecast sugar future spot prices from futures prices, finding that futures prices improve forecasts, especially for shorter horizons. The study suggests technical traders can benefit from these advanced methods.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:32:00.875571
27f7eabc787858e0,Predictable variation and profitable trading of US equities: A trading simulation using neural networks,"A switching rule conditioned on out-of-sample one-step-ahead predictions of returns is used to establish investment positions in either stocks or Treasury bills. The economic significance of any discernible patterns of predictability is assessed by incorporating transaction costs in the simulated trading strategies. We find that ANN models produce switching signals that could have been exploited by investors in an out-of-sample context to achieve superior cumulative and risk-adjusted returns when compared to either regression or a simple buy-and-hold strategy in the market indices. The robustness of these results across a large number of stock market indices is encouraging. Scope and purpose A large body of evidence has accumulated suggesting that stock returns are predictable by means of publicly available information on a number of financial and macroeconomic variables with an important business cycle component. Previous research has, for the most part, relied on standard statistical techniques (e.g., regression analysis) with unduly restrictive assumptions presumed to hold in the underlying data-generating process. This paper reexamines the evidence regarding predictable variation in US stock returns using both artificial neural network (ANN) and regression, and compares simulated trading results obtained from ANN models with those obtained from regression. (C) 2000 Elsevier Science Ltd. All rights reserved.; A switching rule conditioned on out-of-sample one-step-ahead predictions of returns is used to establish investment positions in either stocks or Treasury bills. The economic significance of any discernible patterns of predictability is assessed by incorporating transaction costs in the simulated trading strategies. We find that ANN models produce switching signals that could have been exploited by investors in an out-of-sample context to achieve superior cumulative and risk-adjusted returns when compared to either regression or a simple buy-and-hold strategy in the market indices. The robustness of these results across a large number of stock market indices is encouraging. © 2004 Elsevier Science B.V., Amsterdam. All rights reserved.","Motiwalla, L.; Wahab, M.",2000,10.1016/s0305-0548(99)00148-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034046387&doi=10.1016%2FS0305-0548%2899%2900148-3&partnerID=40&md5=fc139aa19f3bbf2ee1db63ecc6efa633,scopus,"This study investigates predictable variations in US stock returns using Artificial Neural Networks (ANNs) and regression analysis. A trading simulation incorporating transaction costs demonstrates that ANN models can generate profitable trading signals, outperforming regression and buy-and-hold strategies in terms of cumulative and risk-adjusted returns. The findings suggest that ANNs can exploit predictable patterns in stock returns more effectively than traditional methods.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:32:04.002932
314959b9aa6feaf1,Predicting Chinese bond risk premium with machine learning,This paper investigates whether bond yield curve and macroeconomic factors have nonlinear relationships with bond risk premia in the Chinese bond market. We apply machine learning approaches to forecast Chinese treasury bond one-year holding period excess returns. Our results show that the bond yield curve has significant nonlinear predictive relationships with bond risk premia. We find evidence that ‘monetary policy’ and ‘tax’ macroeconomic groups have stronger nonlinear relationships with risk premia while ‘invest’ macroeconomic factors matter more for bonds with longer maturities. This paper provides statistical evidence for a significant relationship between expected bond risk premia and several economic drivers including range of forecast of GDP and bond volatility variables. We further document the economic values of our forecasting results by showing they can generate statistically higher certain equivalent values than those from the benchmark forecast.,,2025,10.1080/1351847x.2024.2446719,,proquest,"This study uses machine learning to predict Chinese treasury bond risk premia, finding significant nonlinear relationships with the yield curve and macroeconomic factors like monetary policy and tax. The model's economic value is demonstrated through higher certain equivalent values compared to benchmarks.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-28T10:32:24.411662
2c07afa57f6fdf97,Predicting Future Earnings Changes Using Machine Learning and Detailed Financial Data,"We use machine learning methods and high‐dimensional detailed financial data to predict the direction of one‐year‐ahead earnings changes. Our models show significant out‐of‐sample predictive power: the area under the receiver operating characteristics curve ranges from 67.52% to 68.66%, significantly higher than the 50% of a random guess. The annual size‐adjusted returns to hedge portfolios formed based on the prediction of our models range from 5.02% to 9.74%. Our models outperform two conventional models that use logistic regressions and small sets of accounting variables, and professional analysts’ forecasts. Analyses suggest that the outperformance relative to the conventional models stems from both nonlinear predictor interactions missed by regressions and the use of more detailed financial data by machine learning.",,2022,10.1111/1475-679x.12429,,proquest,"This study employs machine learning and detailed financial data to predict one-year-ahead earnings changes, achieving significant out-of-sample predictive power (AUC 67.52%-68.66%) and generating positive hedge portfolio returns (5.02%-9.74%). The models outperform conventional methods and analyst forecasts, attributed to capturing nonlinear interactions and utilizing richer financial data.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:32:29.883218
dc45f56e400e0bc4,Predicting Recessions with Leading Indicators: Model Averaging and Selection over the Business Cycle,"Four methods of model selectionequally weighted forecasts, Bayesian model-averaged forecasts, and two models produced by the machine-learning algorithm boostingare applied to the problem of predicting business cycle turning points with a set of common macroeconomic variables. The methods address a fundamental problem faced by forecasters: the most useful model is simple but makes use of all relevant indicators. The results indicate that successful models of recession condition on different economic indicators at different forecast horizons. Predictors that describe real economic activity provide the clearest signal of recession at very short horizons. In contrast, signals from housing and financial markets produce the best forecasts at longer forecast horizons. A real-time forecast experiment explores the predictability of the 2001 and 2007 recessions. Copyright (c) 2015 John Wiley & Sons, Ltd.","Berge, Travis J.",2015,10.1002/for.2345,,wos,"This study compares four methods (equal weighting, Bayesian model averaging, and two boosting models) for predicting business cycle turning points using macroeconomic variables. It finds that different indicators are useful at different forecast horizons, with real activity indicators best for short horizons and housing/financial market indicators best for longer horizons. A real-time experiment tests the models' ability to predict the 2001 and 2007 recessions.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:32:32.005192
d40c4143dfb51f9e,Predicting Stock Jumps and Crashes Using Options,"This paper investigates the informativeness of option-implied volatility and Greeks in forecasting extreme stock returns. Using a large data set of U.S. stocks and options from 1996 to 2022 and employing Light Gradient-Boosting Machine as a machine learning algorithm, we show that option characteristics, particularly implied volatility and delta, are strong predictors of extreme returns. The long-short portfolio utilizing option variables significantly outperforms a benchmark using only stock characteristics, suggesting that options provide information beyond what can be inferred from stock characteristics. Put options are revealed to be more informative than call options, and crashes are easier to predict than jumps.","Andreou, Panayiotis C.; Han, Chulwoo; Li, Nan",2025,10.1002/fut.22609,,wos,"This study explores the use of option-implied volatility and Greeks to predict extreme stock returns (jumps and crashes). Utilizing a Light Gradient-Boosting Machine on a large dataset from 1996-2022, the research demonstrates that option characteristics, especially implied volatility and delta, are effective predictors. The findings indicate that options offer valuable information beyond stock characteristics, with put options being more predictive than call options, and crashes being more predictable than jumps.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:32:34.410842
a924be2f5f701f16,Predicting Wheat Futures Prices in India,"Futures markets perform their economic roles of price discovery and hedging only when they are efficient. One of the important features of efficient market is that one cannot make abnormal profits from the futures markets by trading in it. This paper addresses the question of whether Indian wheat futures prices can be forecast. This would add to our knowledge whether wheat futures market is efficient, and would enable brokers, traders and speculators to develop profitable trading strategy. We employ the economic variable model to predict the wheat futures prices, and employ out of sample point forecasts. We also evaluate the robustness of our results by employing several alternative specifications, viz. ARMA process and artificial neural network technique. We then test the statistical significance of point forecast using the Diebold and Mariano test. We consider random walk orecast as the bench mark. In order to predict the evolution of wheat futures prices, we use traders' expectations about the futures prices, a number of economic variables and futures prices (lagged) of wheat. The study finds that the futures price of wheat cannot be forecast, and the wheat futures market is efficient.","Kumar, Raushan",2021,10.1007/s10690-020-09320-6,,wos,"This paper investigates the efficiency of the Indian wheat futures market by attempting to forecast wheat futures prices using an economic variable model, ARMA process, and artificial neural networks. The study concludes that wheat futures prices cannot be forecast, indicating an efficient market.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:32:35.971441
77577a29281bb651,"Predicting interest rates using shrinkage methods, real‐time diffusion indexes, and model combinations","In the context of predicting the term structure of interest rates, we explore the marginal predictive content of real‐time macroeconomic diffusion indexes extracted from a “data rich” real‐time data set, when used in dynamic Nelson–Siegel (NS) models of the variety discussed in Svensson (NBER technical report, 1994; NSS) and Diebold and Li (Journal of Econometrics, 2006, 130, 337–364; DNS). Our diffusion indexes are constructed using principal component analysis with both targeted and untargeted predictors, with targeting done using the lasso and elastic net. Our findings can be summarized as follows. First, the marginal predictive content of real‐time diffusion indexes is significant for the preponderance of the individual models that we examine. The exception to this finding is the post “Great Recession” period. Second, forecast combinations that include only yield variables result in our most accurate predictions, for most sample periods and maturities. In this case, diffusion indexes do not have marginal predictive content for yields and do not seem to reflect unspanned risks. This points to the continuing usefulness of DNS and NSS models that are purely yield driven. Finally, we find that the use of fully revised macroeconomic data may have an important confounding effect upon results obtained when forecasting yields, as prior research has indicated that diffusion indexes are often useful for predicting yields when constructed using fully revised data, regardless of whether forecast combination is used, or not. Nevertheless, our findings also underscore the potential importance of using machine learning, data reduction, and shrinkage methods in contexts such as term structure modeling.",,2020,10.1002/jae.2768,,proquest,"This study investigates the predictive power of real-time macroeconomic diffusion indexes for interest rates, using dynamic Nelson-Siegel models. While diffusion indexes show some predictive content, forecast combinations of yield variables alone yield the most accurate predictions. The use of fully revised data can confound results, but the study highlights the potential of machine learning and shrinkage methods in term structure modeling.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-28T10:32:52.411394
5f36e752a1a66b22,"Prediction and Allocation of Stocks, Bonds, and REITs in the US Market","This study employs dynamic model averaging and selection of Vector Autoregressive and Time-Varying Parameters Vector Autoregressive models to forecast out-of-sample monthly returns of US stocks, bonds, and Real Estate Investment Trusts (REITs) indexes from October 2006 to December 2021. The models were recursively estimated using 17 additional predictors chosen by a genetic algorithm applied to an initial list of 155 predictors. These forecasts were then used to dynamically choose portfolios formed by these assets and the riskless asset proxied by the 3-month US treasury bills. Although we did not find any predictability in the stock market, positive results were obtained for REITs and especially for bonds. The Bayesian-based approaches applied to just the returns of the three risky assets resulted in portfolios that remarkably outperform the portfolios based on the historical means and covariances and the equally weighted portfolio in terms of certainty equivalent return, Sharpe ratio, Sortino ratio and even Conditional Value-at-Risk at 5%. This study points out that Constant Relative Risk Averse investors should use Bayesian-based approaches to forecast and choose the investment portfolios, focusing their attention on different types of assets.",,2025,10.1007/s10614-024-10589-2,,proquest,"This study uses dynamic model averaging and selection of VAR and TVP-VAR models to forecast monthly returns of US stocks, bonds, and REITs from October 2006 to December 2021. It employs 17 predictors selected by a genetic algorithm from an initial list of 155. The forecasts are used to dynamically select portfolios including these assets and riskless assets. While stock market predictability was not found, positive results were observed for REITs and bonds. Bayesian-based approaches for forecasting and portfolio selection outperformed traditional methods in terms of various performance metrics, suggesting their utility for Constant Relative Risk Averse investors.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:33:31.435232
6c48d26410f11006,Prediction of the implied volatility surface–An empirical analysis of the SSE 50ETF option based on CNNs,"With advancements in artificial intelligence, deep learning techniques have been widely used in predicting financial market volatility. This study forecasts the implied volatility of stock options of the top 50 companies listed on the Shanghai Stock Exchange(SSE) using a convolutional neural network (CNN) with a scaled exponential linear unit activation function and no pooling layer. The CNN model is compared to a back-propagation (BP) neural network to evaluate predictive performance. Results show that the CNN model shows superior performance in predicting implied volatility compared to the BP neural network, accurately fitting data patterns as well as smile and term structures. © 2025 Elsevier B.V., All rights reserved.","Shao, H.; Zhou, B.; Gong, S.",2025,10.1016/j.frl.2025.107119,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219544528&doi=10.1016%2Fj.frl.2025.107119&partnerID=40&md5=55f23bdf3a42f4f7eba6a6d771cac644,scopus,"This study uses a convolutional neural network (CNN) to predict the implied volatility surface of SSE 50ETF options, comparing its performance against a back-propagation (BP) neural network. The CNN model demonstrated superior accuracy in fitting data patterns and capturing smile and term structures.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:33:36.619360
72f324761d5c5db3,Pricing real options based on linear loss functions and conditional value at risk,"The main purpose of this paper is to expand real option analysis out of the realm of pure financial option pricing techniques. To overcome many of the well-known concerns by adopting the financial option pricing techniques for modeling real options problems such as replicating portfolio concept, geometric Brownian motion as underlying stochastic process, and estimating project volatility, we propose an alternative real option valuation based on the loss function approach. The option value determined by the loss function approach is equivalent to the expected value of perfect information (EVPI) in decision analysis. It basically sets the upper bound of risk premium to pay in retaining the options. In practice, many firms utilize the concept of Value at Risk to manage their portfolio risk. If a firm sets a target VAR, then we may be able to link this VAR in refining the actual risk premium to pay in hedging the risk embedded in the investment. With this practice in mind, we present a logic to figure out an appropriate amount of real option premium to pay for a given level of risk tolerance. A comprehensive example is presented to demonstrate the computational procedures as well as economic interpretations on the outcomes.","Kim, Kyongsun; Park, Chan S.",2020,10.1080/0013791x.2020.1867273,,wos,"This paper proposes an alternative method for valuing real options by using a loss function approach, which is equivalent to the expected value of perfect information. It aims to overcome limitations of traditional financial option pricing techniques for real options. The approach incorporates Value at Risk (VaR) to determine an appropriate risk premium for hedging investment risks, with a comprehensive example provided.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:33:38.914930
eb633e8b54bef901,Pricing the volatility risk premium with a discrete stochastic volatility model,"Investors’ decisions on capital markets depend on their anticipation and preferences about risk, and volatility is one of the most common measures of risk. This paper proposes a method of estimating the market price of volatility risk by incorporating both conditional heteroscedasticity and nonlinear effects in market returns, while accounting for asymmetric shocks. We develop a model that allows dynamic risk premiums for the underlying asset and for the volatility of the asset under the physical measure. Specifically, a nonlinear in mean time series model combining the asymmetric autoregressive conditional heteroscedastic model with leverage (NGARCH) is adapted for modeling return dynamics. The local risk-neutral valuation relationship is used to model investors’ preferences of volatility risk. The transition probabilities governing the evolution of the price of the underlying asset are adjusted for investors’ attitude towards risk, presenting the asset returns as a function of the risk premium. Numerical studies on asset return data show the significance of market shocks and levels of asymmetry in pricing the volatility risk. Estimated premiums could be used in option pricing models, turning options markets into volatility trading markets, and in measuring reactions to market shocks. © 2021 Elsevier B.V., All rights reserved.","Šimović, P.; Tafro, A.",2021,10.3390/math9172038,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114048678&doi=10.3390%2Fmath9172038&partnerID=40&md5=b61140065f8e83e82fbeffb2d275f40c,scopus,"This paper proposes a method to estimate the market price of volatility risk using a discrete stochastic volatility model that incorporates conditional heteroscedasticity, nonlinear effects, and asymmetric shocks. It adapts a nonlinear time series model (NGARCH) for return dynamics and uses local risk-neutral valuation to model investor preferences for volatility risk. The model adjusts transition probabilities for risk premiums, showing that market shocks and asymmetry are significant in pricing volatility risk. The estimated premiums can be used in option pricing and for measuring reactions to market shocks.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:33:42.442485
c2f42ac2b768296a,Quantitative assessment of common practice procedures in the fair evaluation of embedded options in insurance contracts,"This work analyses the common industry practice used to evaluate financial options written on with profit policies issued by European insurance companies. In the last years regulators introduced, with the Solvency II directive, a market consistent valuation framework for determining the fair value of asset and liabilities of insurance funds. A relevant aspect is how to deal with the estimation of sovereign credit and liquidity risk, that are important components in the valuation of the majority of insurance funds, which are usually heavily invested in treasury bonds. The common practice is the adoption of the certainty equivalent approach (CEQ) for the risk neutral evaluation of insurance liabilities, which results in a deterministic risk adjustment of the securities cash flows. In this paper, we propose an arbitrage free stochastic model for interest rate, credit and liquidity risks, that takes into account the dependences between different government bond issuers. We test the impact of the common practice against our proposed model, via Monte Carlo simulations. We conclude that in the estimation of options whose pay-off is determined by statutory accounting rules, which is often the case for European traditional with-profit insurance products, the deterministic adjustment for risk of the securities cash flows is not appropriate, and that a more complete model such as the one described in this article is a viable and sensible alternative in the context of market consistent evaluations. (C) 2017 Elsevier B.V. All rights reserved.","Gambaro, Anna Maria; Casalini, Riccardo; Fusai, Gianluca; Ghilarducci, Alessandro",2018,10.1016/j.insmatheco.2017.10.005,,wos,"This paper analyzes the common industry practice for evaluating financial options in European insurance contracts, specifically the certainty equivalent approach (CEQ) for risk-neutral valuation. It proposes an arbitrage-free stochastic model for interest rate, credit, and liquidity risks, considering inter-issuer dependencies. The study concludes that the CEQ approach is inappropriate for options based on statutory accounting rules, common in European with-profit insurance products, and advocates for more comprehensive models like the one presented for market-consistent evaluations.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:33:47.579113
457adc903852506e,Quantitative law describing market dynamics before and after interest-rate change,"We study the behavior of U.S. markets both before and after U.S. Federal Open Market Commission meetings and show that the announcement of a U.S. Federal Reserve rate change causes a financial shock, where the dynamics after the announcement is described by an analog of the Omori earthquake law. We quantify the rate n(t) of aftershocks following an interest-rate change at time T and find power-law decay which scales as n(t-T) similar to (t-T)(-Omega), with Omega positive. Surprisingly, we find that the same law describes the rate n'(vertical bar t-T vertical bar) of preshocks before the interest-rate change at time T. This study quantitatively relates the size of the market response to the news which caused the shock and uncovers the presence of quantifiable preshocks. We demonstrate that the news associated with interest-rate change is responsible for causing both the anticipation before the announcement and the surprise after the announcement. We estimate the magnitude of financial news using the relative difference between the U.S. Treasury Bill and the Federal Funds effective rate. Our results are consistent with the sign effect, in which bad news has a larger impact than good news. Furthermore, we observe significant volatility aftershocks, confirming a market under-reaction that lasts at least one trading day.","Petersen, Alexander M.; Wang, Fengzhong; Havlin, Shlomo; Stanley, H. Eugene",2010,10.1103/physreve.81.066121,,wos,"This study investigates U.S. market dynamics before and after Federal Reserve interest-rate changes, revealing that rate changes act as financial shocks. The authors found that both aftershocks and preshocks follow a power-law decay, similar to the Omori earthquake law. They quantified the magnitude of financial news and observed that bad news has a larger impact than good news, with significant volatility aftershocks indicating market under-reaction.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:33:49.323282
b042472b0d9b86fb,Re-examination of the predictability of economic activity using the yield spread: A nonlinear approach,"This paper examines the feasibility of using the term structure of nominal interest rates in empirical predictive relationships with future real activity growth serving as the dependent variable. In particular, we will focus on the strength and stability of the spread-output relationship. We employ smooth transition nonlinear models that can accommodate (a) regime switching type nonlinear behaviour and (b) time-varying parameters. We verify that the link exhibits strong threshold effects with respect to near past spread values implying that the relation is sufficiently strong in economic terms if past spread values did not exceed a positive threshold value. Furthermore, we are able to explicitly model time-variation in the preceding effects reaching the conclusion that the importance of the spread as an output predictor has been significantly diminished if not eradicated during recent years. The timing of the change in the information content of the spread appears to be related to a turn in certain monetary policy practices, in particular, the turn towards stronger inflation targeting practices. © 2002 Elsevier Science Inc. All rights reserved. © 2004 Elsevier Science B.V., Amsterdam. All rights reserved.","Venetis, I.A.; Paya, I.; Peel, D.A.",2003,10.1016/s1059-0560(02)00147-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037278128&doi=10.1016%2FS1059-0560%2802%2900147-8&partnerID=40&md5=4669263ed695d853f5f4317c4773535b,scopus,"This paper investigates the predictability of economic activity using the yield spread, employing nonlinear models to account for regime switching and time-varying parameters. It finds that the spread's predictive power is strong only when past spread values do not exceed a positive threshold. The study also notes a significant decline in the spread's importance as an output predictor in recent years, potentially linked to changes in monetary policy and inflation targeting.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:34:01.763104
eae365d7b19c2a45,Real economic activity leading indicators: should we have paid more attention?,"The ability to predict business cycle activity is an invaluable skill for governments and policy makers alike, especially before an economy enters a downturn. We analyse causality relationships between key leading economic indicators and economic growth for three countries from 1970 to 2010. We find that while many indicators do not help explain current movements in GDP growth, lags of these indicators do. In addition, the direction of the change and the size of the change in the lagged economic indicators are very important in many cases. This is particularly true for housing indicators.","Ryan, Geraldine; Shinnick, Edward",2011,10.1080/17487870.2011.577645,,wos,"This study investigates the predictive power of leading economic indicators for GDP growth in three countries between 1970 and 2010. It finds that while current indicator values are not always predictive, lagged values, particularly changes in housing indicators, are significant predictors of economic growth.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:34:03.555207
7098321857875dd1,Real estate climate index and aggregate stock returns: Evidence from China,"We show that China's real estate climate index (RECI) can be used to forecast the aggregate stock market return. It outperforms popular return predictors both in- and out-of-sample, especially at the monthly horizon. Additionally, RECI's predictive ability is stronger among stocks of small market capitalization and low momentum. For a typical mean-variance investor, RECI's predictive power may provide an additional utility gain of 3.41%. We discuss three potential sources of RECI's predictive ability and present the corresponding evidence, including the cash flow channel, the firm fundamental channel, and the investment substitution channel.","Jiang, Yuexiang; Fu, Tao; Long, Huaigang; Zaremba, Adam; Zhou, Wenyu",2022,10.1016/j.pacfin.2022.101841,,wos,"This study demonstrates that China's Real Estate Climate Index (RECI) can predict aggregate stock market returns, outperforming existing predictors, particularly at the monthly horizon. The predictive power is more pronounced for small-cap and low-momentum stocks. The research explores three potential mechanisms behind this predictability: the cash flow channel, the firm fundamental channel, and the investment substitution channel.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:34:05.210751
4ab5048307f07e1c,Recovering Yield Curves from Dynamic Term Structure Models with Time-Varying Factors,"A dynamic version of the Nelson-Siegel-Svensson term structure model with time-varying factors is considered for predicting out-of-sample maturity yields. Simple linear interpolation cannot be applied to recover yields at the very short- and long- end of the term structure where data are often missing. This motivates the use of dynamic parametric term structure models that exploit both time series and cross-sectional variation in yield data to predict missing data at the extreme ends of the term structure. Although the dynamic Nelson–Siegel–Svensson model is weakly identified when the two decay factors become close to each other, their predictions may be more accurate than those from more restricted models depending on data and maturity. © 2024 Elsevier B.V., All rights reserved.","Kawakatsu, H.",2020,10.3390/stats3030020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138041887&doi=10.3390%2Fstats3030020&partnerID=40&md5=745ede452bb52fd755252d659d8e40c8,scopus,"This paper explores a dynamic Nelson-Siegel-Svensson term structure model with time-varying factors to predict bond yields, particularly addressing the challenge of missing data at the short and long ends of the yield curve. It suggests that while the model has identification issues, its predictions can be more accurate than simpler models in certain scenarios.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-28T10:34:55.587188
0a7b16e8ef5ab3d7,Recovering default risk from CDS spreads with a nonlinear filter,"We propose a nonlinear filter to estimate the time-varying default risk from the term structure of credit default swap (CDS) spreads. Based on the numerical solution of the Fokker-Planck equation (FPE) using a meshfree interpolation method, the filter performs a joint estimation of the risk-neutral default intensity and CIR model parameters. As the FPE can account for nonlinear functions and non-Gaussian errors, the proposed framework provides outstanding flexibility and accuracy. We test the nonlinear filter on simulated spreads and apply it to daily CDS data of the Dow Jones Industrial Average component companies from 2005 to 2010 with supportive results. © 2013 Elsevier B.V. © 2013 Elsevier B.V., All rights reserved.","Guarín, A.; Liu, X.; Ng, W.L.",2014,10.1016/j.jedc.2013.09.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890117707&doi=10.1016%2Fj.jedc.2013.09.006&partnerID=40&md5=abd3e9e9a830f35514ed25356804906c,scopus,"This paper introduces a nonlinear filter to estimate time-varying default risk from credit default swap (CDS) spreads. It uses a meshfree interpolation method to solve the Fokker-Planck equation, jointly estimating default intensity and CIR model parameters. The method is tested on simulated data and applied to CDS data of Dow Jones Industrial Average companies from 2005-2010.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:34:58.307148
187d1db1c854c026,Recovering the probability density function of asset prices using garch as diffusion approximations,"This paper uses Garch models to estimate the objective and risk-neutral density functions of financial asset prices and by comparing their shapes, recover detailed information on economic agents' attitudes toward risk. It differs from recent papers investigating analogous issues because it uses Nelson's result that Garch schemes are approximations of the kind of differential equations typically employed in finance to describe the evolution of asset prices. This feature of Garch schemes usually has been overshadowed by their well-known role as simple econometric tools providing reliable estimates of unobserved conditional variances. We show instead that the diffusion approximation property of Garch gives good results and can be extended to situations with (i) non-standard distributions for the innovations of a conditional mean equation of asset price changes and (ii) volatility concepts different from the variance. The objective PDF of the asset price is recovered from the estimation of a nonlinear Garch fitted to the historical path of the asset price. The risk-neutral PDF is extracted from cross-sections of bond option prices, after introducing a volatility risk premium function. The direct comparison of the shapes of the two PDF<inf>s</inf> reveals the price attached by economic agents to the different states of nature. Applications are carried out with regard to the futures written on the Italian 10-year bond. © 2001 Elsevier Science B.V. © 2005 Elsevier Science B.V., Amsterdam. All rights reserved.","Fornari, F.; Mele, A.",2001,10.1016/s0927-5398(01)00021-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0041830388&doi=10.1016%2FS0927-5398%2801%2900021-4&partnerID=40&md5=19f6c9e38044f3279ef58d4ab148d647,scopus,"This paper estimates objective and risk-neutral probability density functions (PDFs) of financial asset prices using Garch models as diffusion approximations. By comparing the shapes of these PDFs, the study extracts information about economic agents' risk attitudes. The methodology leverages Nelson's result that Garch schemes approximate differential equations used in finance for asset price evolution. The research extends this to non-standard innovation distributions and alternative volatility concepts. The objective PDF is recovered from historical asset price data using a nonlinear Garch model, while the risk-neutral PDF is derived from bond option prices with a volatility risk premium function. The comparison of these PDFs reveals the market's valuation of different states of nature. An application is demonstrated using futures on the Italian 10-year bond.",True,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:35:01.163178
0eff708fe81a1f18,Recursive bayesian estimation in forward price models implied by fair pricing,"In this paper we describe a recursive Bayesian algorithm for the estimation of forward price models. The forward price is modeled within the benchmark framework for a forward price volatility function which includes a stochastic variable; a forward price with a liquidly traded maturity. A relationship between the bond price, the spot price and certain forward prices is stated. We set up the stochastic real world dynamics for these discretely compounded market observed forward prices. We propose a dynamic Bayesian estimation algorithm for a Monte Carlo time-discretized version of the resulting forward prices dynamics. The parameter to be estimated is a vector consisting of the forward price volatility parameters and the benchmarked bond price volatility parameters. © 2010 World Scientific Publishing Company. © 2010 Elsevier B.V., All rights reserved.","El Qalli, Y.",2010,10.1142/s0219024910005784,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952516296&doi=10.1142%2FS0219024910005784&partnerID=40&md5=5245caaa7c31ebe7cd25a2cde564d272,scopus,"This paper presents a recursive Bayesian algorithm for estimating forward price models, incorporating a stochastic volatility function and a liquidly traded maturity. It establishes a relationship between bond prices, spot prices, and forward prices, and proposes a dynamic Bayesian estimation algorithm for a Monte Carlo time-discretized version of the forward price dynamics. The parameters estimated include forward price volatility and benchmarked bond price volatility.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T10:35:04.122841
32ae7d077ced29f9,Reducing Waiting Times to Improve Patient Satisfaction: A Hybrid Strategy for Decision Support Management,"Patient satisfaction and operational efficiency are critical in healthcare. Long waiting times negatively affect patient experience and hospital performance. Addressing these issues requires accurate system time predictions and actionable strategies. This paper presents a hybrid framework combining predictive modeling and optimization to reduce system times and enhance satisfaction, focusing on registration, vitals, and doctor consultation. We evaluated three predictive models: multiple linear regression (MLR), log-transformed regression (LTMLR), and artificial neural networks (ANN). The MLR model had the best performance, with an (Formula presented.) of 0.93, an MAE of 7.29 min, and an RMSE of 9.57 min. MLR was chosen for optimization due to its accuracy and efficiency, making it ideal for implementation. The hybrid framework combines the MLR model with a simulation-based optimization system to reduce waiting and processing times, considering resource constraints like staff and patient load. Simulating various scenarios, the framework identifies key bottlenecks and allocates resources effectively. Reducing registration and doctor consultation wait times were identified as primary areas for improvement. Efficiency factors were applied to optimize waiting and processing times. These factors include increasing staff during peak hours, improving workflows, and automating tasks. As a result, registration wait time decreased by 15%, vitals by 20%, and doctor consultation by 25%. Processing times improved by 10–15%, leading to an average reduction of 22.5 min in total system time. This paper introduces a hybrid decision support system that integrates predictive analytics with operational improvements. By combining the MLR model with simulation, healthcare managers can predict patient times and test strategies in a risk-free, simulated environment. This approach allows real-time decision-making and scenario exploration without disrupting operations. This methodology highlights how reducing waiting times has a direct impact on patient satisfaction and hospital operational efficiency, offering an applicable solution that does not require significant structural changes. The results are practical and implementable in resource-constrained healthcare environments, allowing for optimized staff management and patient flow. © 2024 Elsevier B.V., All rights reserved.","Morales, J.; Silva-Aravena, F.; Sáez, P.",2024,10.3390/math12233743,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211951995&doi=10.3390%2Fmath12233743&partnerID=40&md5=e8a747b3971e274a71c35b49e1f544cf,scopus,"This paper presents a hybrid decision support system that combines predictive modeling (specifically, multiple linear regression, log-transformed regression, and artificial neural networks) with simulation-based optimization to reduce patient waiting times in healthcare settings. The study found that MLR performed best for prediction and was integrated into the optimization framework. The system identified registration and doctor consultation wait times as key areas for improvement. By implementing strategies such as increasing staff during peak hours and improving workflows, the system achieved significant reductions in waiting and processing times, leading to an average decrease of 22.5 minutes in total system time and a direct impact on patient satisfaction and operational efficiency. The approach is practical, implementable in resource-constrained environments, and allows for risk-free scenario testing.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:03:18.524090
abbff00861afe4d6,Reducing the cost of capital to finance the energy transition in developing countries,"Climate stabilization requires the mobilization of substantial investments in low- and zero-carbon technologies, especially in emerging and developing economies. However, access to stable and affordable finance varies dramatically across countries. Models used to evaluate the energy transition do not differentiate regional financing costs and therefore cannot study risk-sharing mechanisms for renewable electricity generation. In this study, we incorporated the empirically estimated cost of capital differentiated by country and technology into an ensemble of five climate–energy–economy models. We quantified the additional financing cost of decarbonization borne by developing regions and explored policies of risk premium convergence across countries. We found that alleviating financial constraints benefits both climate and equity as a result of more renewable and affordable energy in the developing world. This highlights the importance of fair finance for energy availability, affordability and sustainability, as well as the need to include financial considerations in model-based assessments.Fair finance in the energy sector is modelled in five climate–energy–economy models. The results show that convergence costs of capital could improve energy availability, affordability and sustainability in developing countries, thereby increasing the international equity of the energy transition.",,2024,10.1038/s41560-024-01606-7,,proquest,"This study incorporates country- and technology-specific costs of capital into climate-energy-economy models to analyze the financing of the energy transition in developing countries. It quantifies the additional financing costs faced by developing regions and explores policies for risk premium convergence. The findings suggest that alleviating financial constraints benefits both climate and equity by promoting more affordable and accessible renewable energy in the developing world, emphasizing the need to include financial considerations in model-based assessments.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:03:21.075072
116127da35f3f613,Removing Bias in Estimating Financial Contagion: An Empirical Analysis Based on European Economies,"The degree of contagion is frequently measured by the size and significance of linear correlation coefficients. In this paper, we show that such linear measures are inappropriate for three reasons: contagion is likely to be nonlinear, the structural contagion model is unknown, and the contagion itself will be time-varying. Instead, we use a time-varying coefficient method to give a time-varying, unbiased measure of bilateral contagion between two countries, which shows that simple correlation measures over-estimate the average contagion from the source country and how the degree of contagion varies over the sample period. To illustrate, we use Greece as an exemplar source country and Belgium, France, Italy, Ireland, Netherlands, Portugal, and Spain as recipient countries over the period 2009 to 2022.",,2025,10.1007/s11079-024-09788-z,,proquest,"This paper proposes a time-varying coefficient method to measure bilateral financial contagion between countries, arguing that linear correlation coefficients are inappropriate due to nonlinearity, unknown structural models, and time-varying contagion. The method is illustrated using Greece as a source country and several European countries as recipients from 2009-2022, showing that simple correlation measures overestimate contagion.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:03:27.420084
fcef337c3b0fa757,Renewable integration and energy storage management and conversion in grid systems: A comprehensive review,"The dynamic behaviours of battery energy storage systems (BESSs) make their cutting-edge technology for power grid applications. A BESS must have a Battery Management System (BMS) for dependable, efficient, and risk-free operation. With an emphasis on BESSs and the control strategies for their state-of-charge (SoC) balancing, this article thoroughly reviews energy storage systems (ESSs) on a grid scale. It delves into the future of grid-scale BESSs and the function of ESS, focusing on Li-ion battery systems and drawing attention to the essential features and integration hurdles of Li-ion cells. This review examines the many sides, specifically the cost-benefit analysis, operational efficiencies, and financial incentives that push people to use ESSs. To further improve energy storage and utilization, the article delves into managing hybrid storage systems, which combine photovoltaics (PV), batteries, and supercapacitors. Innovative solutions and technological advancements are the main focus of this examination of current trends in power conversion systems (PCS) associated with BESSs. Finally, future developments in energy storage technology are discussed and how they could solve current problems while making the grid more stable and reliable.","Ahmad, Ashraf Bani; Ooi, Chia Ai; Ali, Omer; Charin, Chanuri; Maharum, Siti Marwangi Mohamad; Swadi, Mahmood; Salem, Mohamed",2025,10.1016/j.egyr.2025.02.008,,wos,"This review comprehensively examines grid-scale battery energy storage systems (BESSs), focusing on Battery Management Systems (BMS), State-of-Charge (SoC) balancing, and Li-ion battery integration. It discusses cost-benefit analyses, operational efficiencies, financial incentives, and the management of hybrid storage systems (PV, batteries, supercapacitors). The review also covers power conversion systems (PCS) and future developments for grid stability and reliability.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:03:29.283337
da93425cec293619,Estimating global bank network connectedness,"We use LASSO methods to shrink, select, and estimate the high‐dimensional network linking the publicly traded subset of the world's top 150 banks, 2003–2014. We characterize static network connectedness using full‐sample estimation and dynamic network connectedness using rolling‐window estimation. Statically, we find that global bank equity connectedness has a strong geographic component, whereas country sovereign bond connectedness does not. Dynamically, we find that equity connectedness increases during crises, with clear peaks during the Great Financial Crisis and each wave of the subsequent European Debt Crisis, and with movements coming mostly from changes in cross‐country as opposed to within‐country bank linkages.",,2018,10.1002/jae.2585,,proquest,"This study estimates global bank network connectedness from 2003-2014 using LASSO methods on publicly traded banks. It analyzes both static and dynamic network structures, finding that equity connectedness is geographically concentrated and increases during crises, particularly the Great Financial Crisis and European Debt Crisis, driven by cross-country linkages.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:17:02.201164
753a88b9bf9f6fc4,Estimating latent asset-pricing factors,"We develop an estimator for latent factors in a large-dimensional panel of financial data that can explain expected excess returns. Statistical factor analysis based on Principal Component Analysis (PCA) has problems identifying factors with a small variance that are important for asset pricing. We generalize PCA with a penalty term accounting for the pricing error in expected returns. Our estimator searches for factors that can explain both the expected return and covariance structure. We derive the statistical properties of the new estimator and show that our estimator can find asset-pricing factors, which cannot be detected with PCA, even if a large amount of data is available. Applying the approach to portfolio data we find factors with Sharpe-ratios more than twice as large as those based on conventional PCA and with smaller pricing errors.",,2020,10.1016/j.jeconom.2019.08.012,,proquest,"This paper proposes a new estimator for latent factors in financial data that aims to improve upon Principal Component Analysis (PCA) for asset pricing. The estimator incorporates a penalty term to account for pricing errors, allowing it to identify factors with small variance that are important for explaining expected excess returns and the covariance structure. The authors demonstrate that their method can uncover factors missed by PCA and show superior performance in an application to portfolio data, yielding factors with higher Sharpe ratios and lower pricing errors.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:17:03.391390
b46293a022ea475b,Research on characterization and prediction of bond risk factors based on machine learning: evidence from the China,"Introduction: The scale of default on credit bonds in China has been expanding. Credit bond defaults not only increase the financing costs of enterprises but also affect the efficiency of debt issuance and even lead to the spread of risks in the financial market. Accurately identifying bond default risks, clarifying the characteristics of bond defaults, and understanding the default risk mechanism are of crucial importance. Methods: This paper takes corporate credit bonds as the research object and analyzes bond defaults from both macro and micro perspectives. From a macro perspective, it confirms the logical transmission between macro factors and bond defaults through causal relationships and grasps the overall characteristics of bond defaults by combining association rule mining and descriptive statistical research methods. Bonds are divided into a risk-free bond group and a risky bond group, and association rules are mined in four dimensions: the bond issuance region of the enterprise, whether the issuer is listed, the attributes of the issuing enterprise, and whether the enterprise bond is guaranteed. Based on these rules, a cross-analysis of bond risk factors is conducted. From a micro perspective, taking each bond as the research object, a bond default identification system is established, and default predictions are made based on the ensemble learning algorithm. The important characteristics of default bonds are analyzed from the perspective of whether the issuer is a state-owned enterprise, and further cause difference analysis is conducted. Results: The results show that M1 and M2 have an impact on bond defaults, and the ensemble machine learning algorithm can accurately predict bond default risks and obtain key factors for bond risk identification. It is reasonable to choose macro indicators to predict bond defaults. Discussion: Based on the experimental conclusions, this paper discusses and analyzes the bond risk evolution process and the reasons for risk concentration in certain industries, which is helpful for a comprehensive understanding of bond default risks. Our research can provide tool references and guidance for risk management in the actual bond market. © 2025 Elsevier B.V., All rights reserved.","Zhang, Y.; Cui, W.",2025,10.3389/fphy.2025.1559283,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002254323&doi=10.3389%2Ffphy.2025.1559283&partnerID=40&md5=0a0beaee8fb234ad9a3519e69b26697d,scopus,"This study investigates bond default risk in China using machine learning. It analyzes defaults from macro and micro perspectives, employing association rule mining and ensemble learning for prediction. The research identifies key factors influencing defaults and discusses risk evolution, offering insights for bond market risk management.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:18:58.853487
b8d5dbfcb1824da4,Research on quantitative investment strategies based on deep learning,"This paper takes 50 ETF options in the options market with high transaction complexity as the research goal. The Random Forest (RF) model, the Long Short-Term Memory network (LSTM) model, and the Support Vector Regression (SVR) model are used to predict 50 ETF price. Firstly, the original quantitative investment strategy is taken as the research object, and the 15 min trading frequency, which is more in line with the actual trading situation, is used, and then the Delta hedging concept of the options is introduced to control the risk of the quantitative investment strategy, to achieve the 15 min hedging strategy. Secondly, the final transaction price, buy price, highest price, lowest price, volume, historical volatility, and the implied volatility of the time segment marked with 50 ETF are the seven key factors affecting the price of 50 ETF. Then, two different types of LSTM-SVR models, LSTM-SVR I and LSTM-SVR II, are used to predict the final transaction price of the 50 ETF in the next time segment. In LSTM-SVR I model, the output of LSTM and seven key factors are combined as the input of SVR model. In LSTM-SVR II model, the hidden state vectors of LSTM and seven key factors are combined as the inputs of the SVR model. The results of the two LSTM-SVR models are compared with each other, and the better one is applied to the trading strategy. Finally, the benefit of the deep learning-based quantitative investment strategy, the resilience, and the maximum drawdown are used as indicators to judge the pros and cons of the research results. The accuracy and deviations of the LSTM-SVR prediction models are compared with those of the LSTM model and those of the RF model. The experimental results show that the quantitative investment strategy based on deep learning has higher returns than the traditional quantitative investment strategy, the yield curve is more stable, and the anti-fall performance is better. © 2019 Elsevier B.V., All rights reserved.","Fang, Y.; Chen, J.; Xue, Z.",2019,10.3390/a12020035,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061984447&doi=10.3390%2Fa12020035&partnerID=40&md5=72e2e75bf78b20e8f6b2ebc3a1549289,scopus,"This paper investigates quantitative investment strategies for 50 ETF options using deep learning models, specifically Random Forest (RF), Long Short-Term Memory (LSTM), and Support Vector Regression (SVR). It introduces a 15-minute trading frequency and Delta hedging for risk control. Two LSTM-SVR models are developed to predict ETF prices, with the better performing model applied to the trading strategy. The strategy's performance is evaluated based on returns, stability, and maximum drawdown, showing superior results compared to traditional strategies.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:19:02.331789
f3768e92a7ac96fb,"Research on the Risks of Financial Informatization Construction in Colleges and Universities, Its Prevention and Control and Path Optimization","With the rapid growth of the demand for college and university funding, the financial management of colleges and universities from the traditional meaning of the risk-free state to the risky mode of change, for the college and university financial risk of accurate and reasonable prevention and control has become an important issue that needs to be resolved at this stage. This paper selects a university finance as the research object, designs 12 financial indicators as sample data, processes the financial risk indicator system through principal component analysis, and obtains 8 principal factor components as the input data of the risk prediction model. Then the particle swarm algorithm is combined with BP neural network to overcome the defects of BP neural network as a way to strengthen its prediction accuracy of financial risk. Through simulation experiments, comparative analysis of prediction rates of different models, it is found that the PSO-BP prediction model achieves an identification accuracy of 91.7% for 60 test samples, which improves the identification correctness by 23.7% and 8.4% compared with the traditional BP model and GA-BP model, respectively. It confirms that the PSO-BP neural network model has a higher prediction rate and is effective in introducing university finance for risk prediction. Finally, the article proposes an optimization strategy for the path of university finance information technology construction, in order to improve the effectiveness of university finance information technology construction.",,2025,10.2478/amns-2025-1039,,proquest,"This study investigates financial risks in university funding, proposing a prediction model using Principal Component Analysis and a Particle Swarm Optimization-optimized BP neural network (PSO-BP). The PSO-BP model achieved 91.7% accuracy in identifying financial risks, outperforming traditional BP and GA-BP models. The research also suggests an optimization strategy for university financial IT construction.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:19:05.581462
bdf7a90ab5ce3943,Restrictions on Risk Prices in Dynamic Term Structure Models,"Restrictions on the risk-pricing in dynamic term structure models (DTSMs) tighten the link between cross-sectional and time-series variation of interest rates, and make absence of arbitrage useful for inference about expectations. This article presents a new econometric framework for estimation of affine Gaussian DTSMs under restrictions on risk prices, which addresses the issues of a large model space and of model uncertainty using a Bayesian approach. A simulation study demonstrates the good performance of the proposed method. Data for U.S.Treasury yields calls for tight restrictions on risk pricing: only level risk is priced, and only changes in the slope affect term premia. Incorporating the restrictions changes the model-implied short-rate expectations and term premia. Interest rate persistence is higher than in a maximally flexible model, hence expectations of future short rates are more variablerestrictions on risk prices help resolve the puzzle of implausibly stable short-rate expectations in this literature. Consistent with survey evidence and conventional macro wisdom, restricted models attribute a large share of the secular decline in long-term interest rates to expectations of future nominal short rates. Supplementary materials for this article are available online.","Bauer, Michael D.",2018,10.1080/07350015.2016.1164707,,wos,"This article proposes a Bayesian econometric framework for estimating affine Gaussian dynamic term structure models (DTSMs) with restrictions on risk prices. The method addresses challenges related to a large model space and model uncertainty. Empirical analysis of U.S. Treasury yields suggests that only level risk is priced, and term premia are influenced by changes in the slope. These restrictions lead to higher interest rate persistence and more variable short-rate expectations compared to more flexible models, helping to resolve issues of implausibly stable short-rate expectations in existing literature. The restricted models attribute a significant portion of the long-term decline in interest rates to expectations of future nominal short rates, aligning with survey data and conventional macroeconomic understanding.",False,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:19:10.605558
da3315d9fa0074d1,Revisiting asset co-movement: Does network topology really matter?,"Asset co-movement has garnered increasing attention from researchers in both traditional finance and emerging interdisciplinary disciplines. However, there is limited knowledge regarding the consistency of market-wide co-movement proxies constructed based on these two perspectives. Employing Fama–French industry portfolios as samples, we construct market-wide co-movement proxies in terms of R2-based and network-based approaches. We further examine if and how market-wide co-movement is priced using supervised principal analysis (SPCA) proposed by Giglio et al. (2021). Our findings include: First, most topological properties are highly correlated with the R2-based proxies. Second, the risk-premium estimates by the SPCA range from 90 bps to 130 bps per month, depending on the size of the latent factors considered. Furthermore, most co-movement proxies are linked to the characteristics regarding asset fundamentals. However, the associated R2 values remain around only 15%, highlighting the challenges in hedging the risks associated with market-wide co-movement in equity markets. © 2023 Elsevier B.V., All rights reserved.","Shi, H.-L.; Chen, H.",2023,10.1016/j.ribaf.2023.102064,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169609130&doi=10.1016%2Fj.ribaf.2023.102064&partnerID=40&md5=552d46f0a087e1821531a343302ab073,scopus,"This study investigates asset co-movement using both R2-based and network-based approaches with Fama-French industry portfolios. It examines whether market-wide co-movement is priced using supervised principal analysis (SPCA). The findings indicate a strong correlation between topological properties and R2-based proxies, with risk-premium estimates ranging from 90 to 130 bps per month. While co-movement proxies are linked to asset fundamentals, R2 values are low, suggesting challenges in hedging related risks.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:19:13.613403
950e074ff9401528,Risk aversion and the yield of corporate debt,"This paper develops a model to estimate the implied default probability of corporate bonds. The model explicitly considers the risk averse behavior of investors to provide a more precise framework for estimating the implied default probability. A Kalman filter method is used to estimate time-varying risk premium associated with the investor's risk aversion. The results of nonlinear regressions indicate that previous risk-neutrality models consistently overestimate the implied default rates of corporate bonds. The results also suggest that investors may have been adequately compensated for investment in risky bonds. © 2017 Elsevier B.V., All rights reserved.","Wu, C.; Yu, C.-H.",1996,10.1016/0378-4266(94)00099-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030100137&doi=10.1016%2F0378-4266%2894%2900099-9&partnerID=40&md5=d3ffd14c6e4c6ebb72f65d65b6f2180e,scopus,"This paper presents a model to estimate the implied default probability of corporate bonds, incorporating investor risk aversion. It uses a Kalman filter for time-varying risk premiums and finds that risk-neutrality models overestimate default rates. The study suggests investors are compensated for holding risky bonds.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:19:15.331981
9f6e559ccffae23b,Risk factors selection with data mining methods for insurance premium ratemaking·,"Osiguravajuća društva koja su prva usvojila primjenu metoda rudarenja podataka u svom poslovanju postali su konkurentniji na tržištu osiguranja. Metode rudarenja podataka osiguravajućoj industriji pružaju brojne prednosti: kraće vrijeme obrade podataka, sofisticiranije metode za precizniju analizu podataka, bolje donošenje odluka itd. Osiguravajuća društva koriste metode rudarenja podataka u razne svrhe, od marketinških kampanja do sprečavanja prijevara, a med strok signu prvima je ta metoda bila u postupku odred strok signivanja premija osiguranja. Primjena metode rudarenja podataka u ovom radu ima za cilj poboljšati rezultate u procesu izračuna stope premije neživotnih osiguranja. Poboljšanje se ogleda u odabiru varijabli predvid strok signanja ili faktora rizika koji utječu na stope premija osiguranja. Istražene su sljedeće metode rudarenja podataka za odabir varijabli predvid strok signanja: Postepena regresija, Stabla odlučivanja i Neuronske mreže. Za izračun premijskih stopa korišteni su Generalizirani linearni modeli (GLM), koji su danas glavni statistički model odred strok signivanja premija neživotnih osiguranja u većini razvijenih tržišta osiguranja u svijetu.",,2020,10.18045/zbefri.2020.2.667,,proquest,"This paper explores the use of data mining techniques, specifically stepwise regression, decision trees, and neural networks, to select risk factors for non-life insurance premium ratemaking. The goal is to improve the accuracy of premium calculations by identifying key predictive variables. Generalized Linear Models (GLM) are used for the final premium rate calculation.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:19:17.707585
81e929d489f783c5,Risk premia and seasonality in commodity futures,We develop and estimate a multifactor affine model of commodity futures that allows for stochastic seasonality. We document the existence of stochastic seasonal fluctuations in commodity futures and that properly accounting for the cost‐of‐carry curve requires at least three factors. We estimate the model using data on heating oil futures and analyze the contribution of the factors to risk premia. Correctly specifying seasonality as stochastic is important to avoid erroneously assigning those fluctuations to other risk factors. We also estimate a nonlinear version of the model that imposes the zero lower bound on interest rates and find similar results.,,2018,10.1002/jae.2631,,proquest,"This paper proposes a multifactor affine model for commodity futures, incorporating stochastic seasonality. The model requires at least three factors to accurately represent the cost-of-carry curve. The study finds that stochastic seasonality is crucial for avoiding misattribution of fluctuations to other risk factors. A nonlinear version of the model, accounting for the zero lower bound on interest rates, yields similar results.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:19:20.099306
11277b48778899ce,Robust out-of-sample inference,"This paper presents analytical, empirical and simulation results concerning inference about the moments of nondifferentiable functions of out-of-sample forecasts and forecast errors. Special attention is given to the measurement of a model's predictive ability using the test of equal mean absolute error. Tests for equal mean absolute error and mean square error are used to evaluate predictions of excess returns to the S & P 500 composite. Simulations indicate that appropriately constructed tests for equal mean absolute error can provide more accurately sized and more powerful tests than inappropriately constructed tests for equal mean absolute error and mean square error. © 2000 Elsevier Science S.A. All rights reserved. © 2017 Elsevier B.V., All rights reserved.","McCracken, M.W.",2000,10.1016/s0304-4076(00)00022-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001909961&doi=10.1016%2FS0304-4076%2800%2900022-1&partnerID=40&md5=92f2aa5d4aad4c137b65fa13fd612370,scopus,"This paper discusses methods for making inferences about the moments of out-of-sample forecasts and forecast errors, particularly for nondifferentiable functions. It focuses on evaluating predictive ability using tests for equal mean absolute error and mean square error, with an application to predicting excess returns for the S&P 500. Simulations suggest that well-constructed tests for equal mean absolute error are superior to other methods.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:19:22.178310
32f440cd8991d935,SIMULTANEOUS SPECIFICATION TESTING OF MEAN AND VARIANCE STRUCTURES IN NONLINEAR TIME SERIES REGRESSION,"This paper proposes a nonparametric simultaneous test for parametric specification of the conditional mean and variance functions in a time series regression model. The test is based on an empirical likelihood (EL) statistic that measures the goodness of fit between the parametric estimates and the nonparametric kernel estimates of the mean and variance functions. A unique feature of the test is its ability to distribute natural weights automatically between the mean and the variance components of the goodness-of-fit measure. To reduce the dependence of the test on a single pair of smoothing bandwidths, we construct an adaptive test by maximizing a standardized version of the empirical likelihood test statistic over a set of smoothing bandwidths. The test procedure is based on a bootstrap calibration to the distribution of the empirical likelihood test statistic. We demonstrate that the empirical likelihood test is able to distinguish local alternatives that are different from the null hypothesis at an optimal rate.","Chen, Song Xi; Gao, Jiti",2011,10.1017/s0266466610000502,,wos,"This paper introduces a nonparametric simultaneous test for parametric specification of conditional mean and variance functions in nonlinear time series regression. It utilizes an empirical likelihood (EL) statistic comparing parametric and nonparametric kernel estimates, with automatic weighting between mean and variance components. An adaptive test is developed by maximizing a standardized EL statistic over smoothing bandwidths, and bootstrap calibration is used for its distribution. The test can distinguish local alternatives from the null hypothesis at an optimal rate.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:19:24.922565
031c94725788365c,Satellite-detected gain in built-up area as a leading economic indicator,"Leading indicators of future economic activity include measures such as new housing starts, managers purchasing index, money supply, and bond yields. Such macroeconomic and financial indicators hold predictive power in signaling recessionary periods. However, many indicators are constrained by the fact that data are often published with some delay and are subject to constant revision (Bandholz and Funke 2003, Huanget al 2018, Orphanides 2003). In this research, we propose a leading indicator derived from satellite imagery, the expansion of anthropogenic bare ground. Satellite-detected gain in built-up area, a major land cover and land use (LCLU) outcome of anthropogenic bare ground gain (ABGG), provides an inexpensive, consistent, and near-real-time indicator of global and regional macroeconomic change. Our panel data analysis across four major regions of the world from 2001 to 2012 shows that the logarithm of total ABGG, mostly owing to its major LCLU outcome, the expansion of built-up land in either year t, t - 1 or t - 2, significantly correlated with the year t logarithm of gross domestic product (GDP, de-trended by Hodrick-Prescott filter). Global ABGG between 2001 and 2012 averaged 7875 km(2) yr(-1), with a peak gain of 11 875 (+/- 2014 km(2) at the 95% confidence interval) in 2006, prior to the 2007-2008 global financial crisis. The curve of global ABGG or its major LCLU outcome of built-up area in year t - 1 accords well with that of the de-trended logarithm of the global GDP in year t. Given the 40 year archive of free satellite data, a growing satellite constellation, advances in machine learning, and scalable methods, this study suggests that analyses of ABGG as a whole or its LCLU outcomes can provide valuable information in near-real time for socioeconomic research, development planning, and economic forecasting.","Ying, Qing; Hansen, Matthew C.; Sun, Laixiang; Wang, Lei; Steininger, Marc",2019,10.1088/1748-9326/ab443e,,wos,"This research proposes using satellite-detected built-up area expansion as a near-real-time leading economic indicator. The study analyzes panel data from 2001-2012 across four major regions, finding a significant correlation between the gain in built-up area and the de-trended logarithm of GDP. The authors suggest that satellite data analysis can provide valuable information for economic forecasting.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:19:27.608655
8952329c631edb64,Sentiment spillover effects for US and European companies,"The fast-growing literature on news analytics provides evidence that financial markets are partially driven by sentiments. In contrast with previous studies that have almost exclusively focused on the direct effects of the news related to single companies or sectors, we investigate the time-varying dynamics of news’ cross-industry influences for a set of US and European stocks over a period of 10 years. The graphical Granger causality of the news sentiments-excess return networks is estimated by applying the adaptive lasso. We find significant spillover effects and show the importance of sentiments related to certain sectors for the whole cross-section of stocks. © 2019 Elsevier B.V., All rights reserved.","Audrino, F.; Tetereva, A.",2019,10.1016/j.jbankfin.2019.07.022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070492911&doi=10.1016%2Fj.jbankfin.2019.07.022&partnerID=40&md5=a82745431816b9d30c291b182aa0c89b,scopus,"This study investigates the time-varying dynamics of news sentiment's cross-industry influences on US and European stocks over 10 years, using graphical Granger causality and adaptive lasso to identify significant spillover effects and the importance of sector-specific sentiments.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:19:30.555590
809a91c999a43a8c,Short term forecasting with support vector machines and application to stock price prediction,"Forecasting a stock price movement is one of the most difficult problems in finance. The reason is that financial time series are complex, non stationary. Furthermore, it is also very difficult to predict this movement with parametric models. Instead of parametric models, we propose two techniques, which are data driven and non parametric. Based on the idea that excess returns would be possible with publicly available information, we developed two models in order to forecast the short term price movements by using technical indicators. Our assumption is that the future value of a stock price depends on the financial indicators although there is no parametric model to explain this relationship. This relationship comes from the technical analysis. Comparison shows that support vector regression (SVR) out performs the multi layer perceptron (MLP) networks for a short term prediction in terms of the mean square error. If the risk premium is used as a comparison criterion, then the SVR technique is as good as the MLP method or better.",,2008,10.1080/03081070601068595,,proquest,"This study proposes two non-parametric, data-driven models, including Support Vector Regression (SVR), to forecast short-term stock price movements using technical indicators. SVR demonstrated superior performance compared to Multi-Layer Perceptron (MLP) networks in terms of mean square error for short-term prediction.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:19:35.783977
c82a5d7d6b70548d,Simplicity and Risk,"I introduce and test for preference for simplicity in choice under risk. I characterize the theory axiomatically, and derive its properties and unique predictions relative to canonical models. By designing and running theoretically motivated experiments, I document that people value simplicity in ways not fully captured by existing models that study risk premia in financial markets. Participants' risk premia increase as complexity increases, holding moments fixed; their dominance violations increase in complexity; their behavior is predicted by simplicity's characterizing axiom; and their complexity aversion is heterogeneous in cognitive ability. None of expected utility theory, cumulative prospect theory, prospect theory, rational inattention, sparsity, salience, or probability weighting that differs by number of outcomes fully capture the experimental findings. I generalize the underlying theory to additionally capture broader measures of complexity, including obfuscation, computation, and language effects.",,2025,10.1111/jofi.13417,,proquest,"This paper introduces and empirically tests a theory of preference for simplicity in choice under risk. Experiments show that participants value simplicity, with risk premia increasing and dominance violations increasing with complexity. Existing models like expected utility theory and cumulative prospect theory do not fully capture these findings. The theory is generalized to include broader measures of complexity.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:19:37.087854
f8146118cc97279a,Simulation-based estimation of contingent-claims prices,"A new methodology is proposed to estimate theoretical prices of financial contingent claims whose values are dependent on some other underlying financial assets. In the literature, the preferred choice of estimator is usually maximum likelihood (ML). ML has strong asymptotic justification but is not necessarily the best method in finite samples. This paper proposes a simulation-based method. When it is used in connection with ML, it can improve the finite-sample performance of the ML estimator while maintaining its good asymptotic properties. The method is implemented and evaluated here in the Black-Scholes option pricing model and in the Vasicek bond and bond option pricing model. It is especially favored when the bias in ML is large due to strong persistence in the data or strong nonlinearity in pricing functions. Monte Carlo studies show that the proposed procedures achieve bias reductions over ML estimation in pricing contingent claims when ML is biased. The bias reductions are sometimes accompanied by reductions in variance. Empirical applications to U.S. Treasury bills highlight the differences between the bond prices implied by the simulation-based approach and those delivered by ML. Some consequences for the statistical testing of contingent-claim pricing models are discussed. Reprinted by permission of Oxford University Press",,2009,10.1093/rfs/hhp009,,proquest,"This paper proposes a simulation-based methodology to estimate theoretical prices of financial contingent claims, aiming to improve upon the finite-sample performance of Maximum Likelihood (ML) estimators. The method is applied to option pricing models (Black-Scholes) and bond pricing models (Vasicek), showing bias reduction compared to ML, especially in cases of strong persistence or nonlinearity. Empirical applications to U.S. Treasury bills highlight differences in implied bond prices.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:19:39.352639
0e58fa6aadafd7b6,Sovereign CDS Spread and Term Structure Forecasting Based on Neural Network,"The article aims to forecast credit risk for BRICS countries using daily credit default swaps (CDS) spreads obtained from Datastream data base from 2018 to 2023. Our approach consists, first, of forecasting the CDS spread in order to estimate the forecasted CDS term structure. The general regression neural network (GRNN) is used to predict the CDS spread. By checking the accuracy of the prediction, the results show that the GRNN model can be recommended as an effective forecasting tool for CDS spread. Second, the predicted spreads are used to estimate the forecasted CDS term structure using the Nelson–Siegel model. The results show that for Russia, overall, the CDS spreads in the long term are less than those in the short term, which implies that the future outlook is more optimistic, given the events that occurred during the study period, but it still retains the highest level of credit risk compared to other countries. Unlike Brazil, India and South Africa, the future outlook is more pessimistic. For China, the term structure is unstable; in the short term, there is a tendency to reduce the risk, but for longer horizons, the risk will increase. Thus, BRICS countries have different risk profiles depending on investment horizons. The study’s findings help policymakers in developing tailored risk management strategies for BRICS countries and guide investors in making informed credit investment decisions. The use of advanced forecasting tools like GRNN and Nelson–Siegel models emphasizes the importance of sophisticated techniques in enhancing financial market resilience. © 2024 Elsevier B.V., All rights reserved.","Abid, A.; Souissi, N.",2024,10.1177/09721509241276952,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206434819&doi=10.1177%2F09721509241276952&partnerID=40&md5=e2af5a9387b092f3129034c1c1b3e1d7,scopus,"This study forecasts credit risk for BRICS countries using daily Credit Default Swap (CDS) spreads from 2018-2023. A General Regression Neural Network (GRNN) is employed to predict CDS spreads, demonstrating its effectiveness as a forecasting tool. The predicted spreads are then used with the Nelson–Siegel model to estimate the forecasted CDS term structure. The analysis reveals distinct risk profiles and future outlooks for BRICS countries based on investment horizons, offering insights for policymakers and investors.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:27:09.078114
5b09e441b05c7dfd,Spatio-Temporal Momentum: Jointly Learning Time-Series and Cross-Sectional Strategies,"The authors introduce spatio-temporal momentum strategies, a class of models that unify both time-series and cross-sectional momentum strategies by trading assets based on their cross-sectional momentum features over time. Although both time-series and cross-sectional momentum strategies are designed to systematically capture momentum risk premiums, these strategies are regarded as distinct implementations and do not consider the concurrent relationship and predictability between temporal and cross-sectional momentum features of different assets. They model spatio-temporal momentum with neural networks of varying complexities and demonstrate that a simple neural network with only a single fully connected layer learns to simultaneously generate trading signals for all assets in a portfolio by incorporating both their time-series and cross-sectional momentum features. Back testing on portfolios of 46 actively traded US equities and 12 equity index futures contracts, they demonstrate that the model is able to retain its performance over benchmarks in the presence of high transaction costs of up to 5–10 basis points. In particular, they find that the model when coupled with least absolute shrinkage and turnover regularization results in the best performance over various transaction cost scenarios. © 2023 Elsevier B.V., All rights reserved.","Tan, W.L.; Roberts, S.; Zohren, S.",2023,10.3905/jfds.2023.1.130,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179918020&doi=10.3905%2Fjfds.2023.1.130&partnerID=40&md5=f0dcc0bfe7da5374392a0b389a7636f9,scopus,"This paper introduces spatio-temporal momentum strategies, which combine time-series and cross-sectional momentum by analyzing cross-sectional momentum features over time. The authors use neural networks to model these strategies, finding that even a simple neural network can generate trading signals for all assets by considering both their time-series and cross-sectional momentum. Backtesting on US equities and equity index futures shows the model's effectiveness, especially when combined with regularization, even with high transaction costs.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:27:11.161248
bfec2270e5af77cc,"Statistical actuarial estimation of the Capitation Payment Unit from copula functions and deep learning: historical comparability analysis for the Colombian health system, 2015–2021","The Capitation Payment Unit (CPU) financing mechanism constitutes more than 70% of health spending in Colombia, with a budget allocation of close to 60 trillion Colombian pesos for the year 2022 (approximately 15.7 billion US dollars). This article estimates actuarially, using modern techniques, the CPU for the contributory regime of the General System of Social Security in Health in Colombia, and compares it with what is estimated by the Ministry of Health and Social Protection. Using freely available information systems, by means of statistical copulas functions and artificial neural networks, pure risk premiums are calculated between 2015 and 2021. The study concludes that the weights by risk category are systematically different, showing historical pure premiums surpluses in the group of 0–1 years and deficits (for the regions normal and cities) in the groups over 54 years of age.",,2023,10.1186/s13561-022-00416-5,,proquest,"This study estimates the Capitation Payment Unit (CPU) for Colombia's health system using copula functions and deep learning, comparing its findings to official estimates. The analysis reveals systematic differences in risk category weights, leading to surpluses in younger age groups and deficits in older age groups.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:27:14.934087
08cd3cdeb999b4d8,Stochastic interest rates in the analysis of energy investments: Implications on economic performance and sustainability,"A systematic impact assessment of stochastic interest and inflation rates on the analysis of energy investments is presented. A real-options algorithm has been created for this task. Constant interest rates incorporating high risk premium have been extensively used for economic calculations, within the framework of traditional direct cash flow methods, thus favouring immediate, irreversible investments in the expense of, sometimes, insubstantially low anticipated yields. In this article, not only incomes and expenses but also interest and inflation rates are considered stochastically evolving according to specific probabilistic models. The numerical experiments indicated that the stochastic interest rate forecasts fluctuate in such low levels that may signal delayed investment entry in favour of higher expected yields. The implementation of stochastically evolving interest rates in energy investment analysis may have a controversial effect on sustainability. Displacements of inefficient plants may be significantly delayed, thus prolonging high CO sub(2) emission rates. Under the current CO sub(2) allowance prices or their medium-term forecasts, this situation may not be improved and flexible policy interventions may be necessitated.",,2010,10.1016/j.apenergy.2009.11.033,,proquest,"This paper presents a systematic impact assessment of stochastic interest and inflation rates on energy investments using a real-options algorithm. It argues that traditional methods using constant interest rates may favor immediate investments, while stochastic modeling suggests delayed investment entry due to fluctuating low interest rates. This can have controversial effects on sustainability, potentially delaying the displacement of inefficient plants and prolonging high CO2 emissions, necessitating policy interventions.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:27:16.584482
f2cb1b203a86e3d4,Stochastic period and cohort effect state-space mortality models incorporating demographic factors via probabilistic robust principal components,"In this study we develop a multi-factor extension of the family of Lee-Carter stochastic mortality models. We build upon the time, period and cohort stochastic model structure to extend it to include exogenous observable demographic features that can be used as additional factors to improve model fit and forecasting accuracy. We develop a dimension reduction feature extraction framework which (a) employs projection based techniques of dimensionality reduction; in doing this we also develop (b) a robust feature extraction framework that is amenable to different structures of demographic data; (c) we analyse demographic data sets from the patterns of missingness and the impact of such missingness on the feature extraction, and (d) introduce a class of multi-factor stochastic mortality models incorporating time, period, cohort and demographic features, which we develop within a Bayesian state-space estimation framework; finally (e) we develop an efficient combined Markov chain and filtering framework for sampling the posterior and forecasting. We undertake a detailed case study on the Human Mortality Database demographic data from European countries and we use the extracted features to better explain the term structure of mortality in the UK over time for male and female populations when compared to a pure Lee-Carter stochastic mortality model, demonstrating our feature extraction framework and consequent multi-factor mortality model improves both in sample fit and importantly out-off sample mortality forecasts by a non-trivial gain in performance. © 2021 Elsevier B.V., All rights reserved.","Toczydłowska, D.; Peters, G.; Fung, M.C.; Shevchenko, P.V.",2017,10.3390/risks5030042,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055749896&doi=10.3390%2Frisks5030042&partnerID=40&md5=8e1387341d859cffdee037ca8e59109b,scopus,"This study extends the Lee-Carter stochastic mortality model by incorporating exogenous demographic factors using a probabilistic robust principal components framework for dimension reduction. The developed multi-factor model, estimated within a Bayesian state-space framework, improves both in-sample fit and out-of-sample mortality forecasts compared to the standard Lee-Carter model, as demonstrated by a case study on European demographic data.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:27:19.929411
b637484b643b9a37,Stock investment strategy combining earnings power index and machine learning,"We propose an intermediate-term stock investment strategy based on fundamental analysis and machine learning. The approach uses predictors from the Earnings Power Index (EPI) as input variables derived from cross-sectional and time-series data from a company's financial statements. The analytical methods of machine learning allow us to validate the link between financial factors and excess returns directly. We then select stocks for which returns are likely to increase at the time of the next disclosed financial statement. To verify the proposed approach's usefulness, we use company data listed publicly on the Korean stock market from 2013 to 2019. We examine the profitability of trading strategy based on ten machine-learning techniques by forming long, short, and hedge portfolios with three different measures. As a result, most portfolios, including EPI-related variables, present positive returns regardless of the period. Especially, the neural network of the two layers with sigmoid function presents the best performance for the period of 3 months and 6 months, respectively. Our results show that incorporating machine learning is useful for mid-term stock investment. Further research into the possible convergence of financial statement analysis and machine-learning techniques is warranted. © 2022 Elsevier B.V., All rights reserved.","Jun, S.Y.; Kim, D.S.; Jung, S.Y.; Jun, S.G.; Kim, J.W.",2022,10.1016/j.accinf.2022.100576,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138051303&doi=10.1016%2Fj.accinf.2022.100576&partnerID=40&md5=c12843ef5be8986f6a9785fb0232e51f,scopus,"This study proposes a stock investment strategy using the Earnings Power Index (EPI) and machine learning. It analyzes financial statement data to predict stock returns and tests the strategy on Korean stock market data from 2013-2019 using ten machine learning techniques. The results indicate that incorporating machine learning with EPI variables is beneficial for intermediate-term stock investment, with a two-layer neural network showing the best performance.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:27:38.397929
357b115e59209cb9,Stock selection with random forest: An exploitation of excess return in the Chinese stock market,"In recent years, a variety of research fields, including finance, have begun to place great emphasis on machine learning techniques because they exhibit broad abilities to simulate more complicated problems. In contrast to the traditional linear regression scheme that is usually used to describe the relationship between the stock forward return and company characteristics, the field of finance has experienced the rapid development of tree-based algorithms and neural network paradigms when illustrating complex stock dynamics. These nonlinear methods have proved to be effective in predicting stock prices and selecting stocks that can outperform the general market. This article implements and evaluates the robustness of the random forest (RF) model in the context of the stock selection strategy. The model is trained for stocks in the Chinese stock market, and two types of feature spaces, fundamental/technical feature space and pure momentum feature space, are adopted to forecast the price trend in the long run and the short run, respectively. It is evidenced that both feature paradigms have led to remarkable excess returns during the past five out-of-sample period years, with the Sharpe ratios calculated to be 2.75 and 5 for the portfolio net value of the multi-factor space strategy and momentum space strategy, respectively. Although the excess return has weakened in recent years with respect to the multi-factor strategy, our findings point to a less efficient market that is far from equilibrium.In recent years, a variety of research fields, including finance, have begun to place great emphasis on machine learning techniques because they exhibit broad abilities to simulate more complicated problems. In contrast to the traditional linear regression scheme that is usually used to describe the relationship between the stock forward return and company characteristics, the field of finance has experienced the rapid development of tree-based algorithms and neural network paradigms when illustrating complex stock dynamics. These nonlinear methods have proved to be effective in predicting stock prices and selecting stocks that can outperform the general market. This article implements and evaluates the robustness of the random forest (RF) model in the context of the stock selection strategy. The model is trained for stocks in the Chinese stock market, and two types of feature spaces, fundamental/technical feature space and pure momentum feature space, are adopted to forecast the price trend in the long run and the short run, respectively. It is evidenced that both feature paradigms have led to remarkable excess returns during the past five out-of-sample period years, with the Sharpe ratios calculated to be 2.75 and 5 for the portfolio net value of the multi-factor space strategy and momentum space strategy, respectively. Although the excess return has weakened in recent years with respect to the multi-factor strategy, our findings point to a less efficient market that is far from equilibrium.",,2019,10.1016/j.heliyon.2019.e02310,,proquest,"This study evaluates the Random Forest (RF) model for stock selection in the Chinese market, utilizing both fundamental/technical and momentum feature spaces. The RF model demonstrated significant excess returns over a five-year out-of-sample period, with Sharpe ratios of 2.75 and 5 for the respective strategies, suggesting market inefficiency.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:27:41.574545
c38e480a7ebbb97e,Strongly-typed genetic programming and fuzzy inference system: An embedded approach to model and generate trading rules,"Generating trading signals is an interesting topic and a hard problem to solve. This work uses fuzzy inference system (FIS) and strongly typed genetic programming (STGP) to generate trading rules for the US stock market, a framework that we call FISTGP. The two embedded models have not been widely evaluated in financial applications, and according to the literature, their combination could improve forecasting performance. The fitness function used to train the STGP model is based on accuracy, optimizing the buy and sell signals, taking a different approach to the classic optimization of return–risk ratio. The rules are generated in a FIS framework, and the final signal depends on the amount of information that the investor relies on. The model is suited to each investor as a recommendation of when to change portfolio composition according to his or her particular criteria. Ternary rules are generated based on an economic interpretation, considering the risk-free rate as a part of more demanding rules. The model is applied to 90 of the most traded and active stocks in the US stock market. This approach generates important recommendations and delivers useful information to investors. The results show that the proposed model outperforms the Buy and Hold (B&H) strategy by 28.62% in the test period, considering excesses of return, with almost the same risk (1.28% higher). The other base models underperform in comparison to the B&H, with the proposed model also outperforming them. © 2020 Elsevier B.V., All rights reserved.","Michell, K.; Kristjanpoller R., W.",2020,10.1016/j.asoc.2020.106169,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079842746&doi=10.1016%2Fj.asoc.2020.106169&partnerID=40&md5=7d044e5fcb41907b98487380158cfe03,scopus,"This study introduces FISTGP, a novel framework combining Fuzzy Inference System (FIS) and Strongly Typed Genetic Programming (STGP) to generate trading rules for the US stock market. Unlike traditional methods optimizing return-risk ratio, FISTGP uses an accuracy-based fitness function. The generated rules are interpretable and adaptable to individual investor criteria, incorporating the risk-free rate. Applied to 90 active US stocks, FISTGP demonstrated a 28.62% excess return compared to the Buy and Hold strategy with comparable risk, outperforming other base models.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:27:45.079315
24e3321504f74b62,Structural Laplace Transform and Compound Autoregressive Models,"This paper presents a new general class of compound autoregressive (Car) models for non-Gaussian time series. The distinctive feature of the class is that Car models are specified by means of the conditional Laplace transforms. This approach allows for simple derivation of the ergodicity conditions and ensures the existence of forecasting distributions in closed form, at any horizon. The last property is of particular interest for applications to finance and economics that investigate the term structure of variables and/or of their nonlinear transforms. The Car class includes a number of time-series models that already exist in the literature, as well as new models introduced in this paper. Their applications are illustrated by examples of portfolio management, term structure and extreme risk analysis.",,2006,10.1111/j.1467-9892.2006.00479.x,,proquest,"This paper introduces a new class of compound autoregressive (Car) models for non-Gaussian time series, defined using conditional Laplace transforms. This method simplifies the derivation of ergodicity conditions and provides closed-form forecasting distributions, which are valuable for financial and economic applications involving term structures and nonlinear transformations. The Car class encompasses existing and novel time-series models, with applications demonstrated in portfolio management, term structure analysis, and extreme risk assessment.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:27:52.913467
a252363c576b1816,Systematic Pricing and Trading of Municipal Bonds,"In this article, the authors propose a systematic approach for pricing and trading municipal bonds that leverages the feature-rich information available at the individual bond level. Based on the proposed pricing framework, they estimate several models using ridge regression and Kalman filtering. In their empirical work, they show that the models compare favorably in pricing accuracy to those available in the literature. In addition, the models can quickly adapt to changing market conditions. Incorporating the pricing models into relative value trading strategies, the authors demonstrate that the resulting portfolios generate significant excess returns and positive alpha relative to the Vanguard Long-Term Tax-Exempt Fund, one of the largest mutual funds in the municipal space. © 2022 Elsevier B.V., All rights reserved.","Kolm, P.N.; Purushothaman, S.",2022,10.3905/jfds.2021.1.079,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127392362&doi=10.3905%2Fjfds.2021.1.079&partnerID=40&md5=0d3f3669aec2fa0af60630b8e7f93f28,scopus,"This article presents a systematic method for pricing and trading municipal bonds using individual bond data and advanced modeling techniques like ridge regression and Kalman filtering. The proposed models demonstrate superior pricing accuracy and adaptability to market changes compared to existing literature. When integrated into trading strategies, these models yield significant excess returns and alpha.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:27:54.847623
6c0ac1888b8f742d,TEND: A Target-Dependent Representation Learning Framework for News Document,"Real-time news documents published on the Internet have global financial and political impacts. Pioneering statistical approaches investigate manually defined features to capture lexical, sentiment, and event information, which suffer from feature sparsity. As a remedy, recent work has considered learning dense vector representations for documents. Such representations are general, which can not model target-dependent scenarios, such as stance detection towards a specific claim. There has been work on target-specific word and sentence representations, but little was done on target-dependent document representation. Moreover, documents contain more potentially helpful information, but also noise compared to events and sentences. To address the above issues, we focus on models that are: 1. task-driven, which optimize the neural network representations for the end task; 2. target-specific, learning news representations by considering the influence of specific targets. In particular, we propose a novel document-level target-dependent learning framework TEND. The framework employs the information of the target and the news abstract as clues, obtaining relatively informative sentences from the entire document for our objectives. The framework assembles a document representation by integrating the news abstract representation and a weighted sum of sentence representations in the document. To the best of our knowledge, we are among the first to investigate target-dependent document representation. Existing text representation models can be easily integrated into our TEND framework, and it is general enough to be applied to different target-dependent document representation tasks. We empirically evaluate our framework on two target-dependent document-level tasks, including a cumulative abnormal return prediction task and a news stance detection task. Results show that our models give the best performances compared to state-of-the-art document embedding methods, yielding robust and consistent performances across datasets.",J. Duan; X. Ding; Y. Zhang; T. Liu,2019,10.1109/taslp.2019.2947364,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8868172,ieeexplore,"This paper introduces TEND, a novel framework for learning target-dependent document representations, particularly for news documents. TEND addresses limitations of general document representations by incorporating target-specific information and optimizing for end tasks. It integrates news abstract and weighted sentence representations to create a document representation. The framework is evaluated on cumulative abnormal return prediction and news stance detection, outperforming existing methods.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:28:03.893032
fe46b71e2ee515f3,THE SEARCH FOR TIME-SERIES PREDICTABILITY-BASED ANOMALIES,". This paper introduces a new algorithm for exploiting time-series predictability-based patterns to obtain an abnormal return, or alpha, with respect to a given benchmark asset pricing model. The algorithm proposes a deterministic daily market timing strategy that decides between being fully invested in a risky asset or in a risk-free asset, with the trading rule represented by a parametric perceptron. The optimal parameters are sought in-sample via differential evolution to directly maximize the alpha. Successively using two modern asset pricing models and two different portfolio weighting schemes, the algorithm was able to discover an undocumented anomaly in the United States stock market cross-section, both out-of-sample and using small transaction costs. The new algorithm represents a simple and flexible alternative to technical analysis and forecast-based trading rules, neither of which necessarily maximizes the alpha. This new algorithm was inspired by recent insights into representing reinforcement learning as evolutionary computation. © 2022 Elsevier B.V., All rights reserved.","Ospina-Holguin, J.H.; Padilla-Ospina, A.M.",2021,10.3846/jbem.2021.15650,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123798331&doi=10.3846%2Fjbem.2021.15650&partnerID=40&md5=24f80df9c903c2537c0327df647bf65c,scopus,"This paper presents a novel algorithm that uses time-series predictability to generate abnormal returns (alpha) relative to a benchmark asset pricing model. The algorithm employs a deterministic daily market timing strategy, deciding between investing in a risky or risk-free asset using a parametric perceptron. Optimal parameters are found using differential evolution to maximize alpha. Applied to US stock markets with modern asset pricing models and different portfolio weighting schemes, the algorithm identified an undocumented anomaly out-of-sample, even with small transaction costs. It's presented as a simpler, more flexible alternative to technical analysis and forecast-based trading rules, drawing inspiration from reinforcement learning and evolutionary computation.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:28:07.590415
61208aa7bc89fbf7,THE SPECULATIVE DEMAND FOR MONEY - AN ALTERNATIVE APPROACH,"THE SPECULATIVE DEMAND FOR MONEY HAS OCCUPIED AN IMPORTANT ROLE IN KEYNES' GENERAL THEORY.  SINCE THEN IT HAS RECEIVED ONLY SPORADIC ATTENTION, DUE LARGELY TO THE FACT THAT THE THEORY IN ITS ORIGINAL FORM HAS USUALLY FAILED TO SURVIVE THE EMPIRICAL TESTS.  THE SPECULATIVE DEMAND FOR MONEY IS ESTIMATED USING THE GENERAL FRAMEWORK OF THE 'EFFICIENT MARKETS THEORY'.  THE PERIOD COVERED IS OF A RELATIVELY STABLE INSTITUTIONAL SETTING FOLLOWING THE 1951 U.S.  TREASURY-FEDERAL RESERVE ACCORD.  THE RESULTS FOR THIS PERIOD STRONGLY INDICATE THE EXISTENCE OF THE SPECULATIVE DEMAND FOR MONEY.  TABLE.  NOTES.",,1976,10.1111/j.1536-7150.1976.tb01210.x,,proquest,"This paper estimates the speculative demand for money using the efficient markets theory framework, focusing on the period after the 1951 U.S. Treasury-Federal Reserve Accord. The results suggest the existence of speculative demand for money during this stable institutional period.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:28:09.495920
80e3352b0bafafdc,Technical trading rules and the size of the risk premium in security returns,"Among analysts, technical trading rules are widely used for forecasting security returns. Recent literature provides evidence that these rules may provide positive profits after accounting for transaction costs. This would be contrary to the theory of the efficient market hypothesis which states that security prices cannot be forecasted from their past values or other past variables. This paper uses the daily Dow Jones Industrial Average Index from 1963 to 1988 to examine the linear and nonlinear predictability of stock market returns with simple technical trading rules, by using the nearest neighbors and the feedforward network regressions. Evidence of nonlinear predictability is found in the stock market returns by using the past returns and the buy and sell signals of the moving average rules.","Gencay, R; Stengos, T",1997,10.2202/1558-3708.1026,,wos,"This paper investigates the predictability of stock market returns using technical trading rules and machine learning techniques like nearest neighbors and feedforward network regressions. The study finds evidence of nonlinear predictability in the Dow Jones Industrial Average Index returns from 1963-1988, contradicting the efficient market hypothesis.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:28:14.407403
1adc7c7a8c852e5d,Technological bias at the exchange rate market,"Prediction of exchange rates has been a topic for debate in economic literature since the late 1980s. The recent development of machine learning techniques has spurred a plethora of studies that further improves the prediction models for currency markets. This high‐tech progress may create challenges for market efficiency along with information asymmetry and irrationality of decision‐making. This technological bias emerges from the fact that recent innovative approaches have been used to solve trading tasks and to find the best trading strategies. This paper demonstrates that traders can leverage technological bias for financial market forecasting. Those traders who adapt faster to the changes in market innovations will get excess returns. To support this hypothesis we compare the performance of deep learning methods, shallow neural networks with baseline prediction methods and a random walk model using daily closing rate between three currency pairs: Euro and US Dollar (EUR/USD), British Pound and US Dollar (GBP/USD), and US Dollar and Japanese Yen (USD/JPY). The results demonstrate that deep learning achieves higher accuracy than alternate methods. The shallow neural network outperforms the random walk model, but cannot surpass ARIMA accuracy significantly. The paper discusses possible outcomes of the technological shift for financial market development and accounting conforming also to adaptive market hypothesis.",,2017,10.1002/isaf.1408,,proquest,"This paper investigates the impact of technological advancements, specifically machine learning, on exchange rate market efficiency and forecasting. It demonstrates that traders can exploit this 'technological bias' for excess returns by adapting faster to innovations. The study compares deep learning, shallow neural networks, and baseline models (ARIMA, random walk) for predicting EUR/USD, GBP/USD, and USD/JPY exchange rates, finding deep learning to be the most accurate.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:28:16.613137
4c66380b2d740598,Term Structure Modeling and Forecasting of Government Bond Yields,"Accurate modelling and precise estimation of the term structure of interest rate are of crucial importance in many areas of finance and macroeconomics as it is the most important factor in the capital market and probably the economy. This study compares the in-sample fit and out-of-sample forecast accuracy of the Cox-Ingersoll-Ross (CIR) and Nelson-Siegel models. For the in-sample fit, there is a significant lack of information on the short-term CIR model. The CIR model should also be considered too poor to describe the term structure in a simulation-based context. It generates a downward slope average yield curve. Contrary to CIR model, Nelson-Siegel model is not only compatible to fit attractively the yield curve but also accurately forecast the future yield for various maturities. Furthermore, the non-linear version of the Nelson-Siegel model outperforms the linearised one. In a simulation-based context, the Nelson-Siegel model is capable to replicate most of the stylised facts of the Japanese market yield curve. Therefore, it turns out that the Nelson-Siegel model (non-linear version) could be a good candidate among various alternatives to study the evolution of the yield curve in Japanese market. Adapted from source document.",,2013,10.1111/1759-3441.12046,,proquest,"This study compares the Cox-Ingersoll-Ross (CIR) and Nelson-Siegel models for term structure modeling and forecasting of government bond yields. The Nelson-Siegel model, particularly its non-linear version, demonstrates superior in-sample fit and out-of-sample forecasting accuracy compared to the CIR model. The Nelson-Siegel model is also capable of replicating stylized facts of the Japanese market yield curve, making it a suitable candidate for studying yield curve evolution in that market.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:28:26.737594
5b36dfbcd718497b,"Testing for UIP-Type Relationships: Nonlinearities, Monetary Announcements and Interest Rate Expectations","This paper tests for UIP-type relationships by estimating first a benchmark linear Cointegrated VAR including the nominal exchange rate and the interest rate differential as well as central bank announcements, and then a Smooth Transition Cointegrated VAR (STCVAR) model incorporating nonlinearities and also taking into account the role of interest rate expectations. The analysis is conducted for five inflation targeting countries (the UK, Canada, Australia, New Zealand and Sweden) and three non-targeters (the US, the Euro-Area and Switzerland) using daily data from January 2000 to December 2020. While we cannot confirm the validity of UIP in its strictest theoretical sense, we find evidence for the existence of an equilibrium relationship between the exchange rate and the interest rate differential. Specifically, the nonlinear framework appears to be more appropriate to capture the adjustment towards the long-run equilibrium, since the estimated speed of adjustment is substantially faster and the short-run dynamic linkages more significant. Further, interest rate expectations play an important role: a fast adjustment only occurs when the market expects the interest rate to increase in the near future, namely central banks are perceived as more credible when sticking to their goal of keeping inflation at a low and stable rate. Also, central bank announcements have a more sizeable short-run effect in the nonlinear model. Finally, the equilibrium relationship between the exchange rate and the interest rate differential holds better in inflation targeting countries, where monetary authorities appear to achieve a higher degree of credibility.","Anderl, Christina; Caporale, Guglielmo Maria",2022,10.1007/s11079-021-09640-8,,wos,"This paper investigates the UIP-type relationship between exchange rates and interest rate differentials using linear and nonlinear (STCVAR) models for several inflation targeting and non-targeting countries. It finds that nonlinearities and interest rate expectations are crucial for capturing the equilibrium relationship, with faster adjustments and more significant short-run dynamics in the nonlinear framework. Central bank credibility, particularly in inflation targeting countries, enhances this relationship and the impact of announcements.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:28:31.902818
3d256d8a11b4dd5d,Testing for two-regime threshold cointegration in vector error-correction models,"This paper examines a two-regime vector error-correction model with a single cointegrating vector and a threshold effect in the error-correction term. We propose a relatively simple algorithm to obtain maximum likelihood estimation of the complete threshold cointegration model for the bivariate case. We propose a SupLM test for the presence of a threshold. We derive the null asymptotic distribution, show how to simulate asymptotic critical values, and present a bootstrap approximation. We investigate the performance of the test using Monte Carlo simulation, and find that the test works quite well. Applying our methods to the term structure model of interest rates, we find strong evidence for a threshold effect. (C) 2002 Published by Elsevier Science B.V.","Hansen, BE; Seo, B",2002,10.1016/s0304-4076(02)00097-0,,wos,"This paper proposes a method for testing two-regime threshold cointegration in vector error-correction models, including an algorithm for maximum likelihood estimation and a SupLM test for the threshold effect. Monte Carlo simulations show the test performs well, and an application to the term structure of interest rates provides evidence for a threshold effect.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:28:34.110070
3eebe90718f24024,Testing the expectations theory of the term structure of interest rates in threshold models,"We test the expectations theory of the term structure of U.S. interest rates in nonlinear systems. These models allow the response of the change in short rates to past values of the spread to depend upon the level of the spread. The nonlinear system is tested against a linear system, and the results of testing the expectations theory in both models are contrasted. We find that the results of tests of the implications of the expectations theory depend on the size and sign of the spread. The long maturity spread predicts future changes of the short rate only when it is high. © 2008 Elsevier B.V., All rights reserved.","Clements, M.P.; Galvão, A.B.",2003,10.1017/s1365100502020163,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0141636266&doi=10.1017%2FS1365100502020163&partnerID=40&md5=1acc28231c51f29898d54323e04bb347,scopus,"This study empirically tests the expectations theory of the term structure of U.S. interest rates using nonlinear threshold models. These models allow the relationship between short rates and the spread to vary based on the spread's level. The findings indicate that the predictive power of the long maturity spread for future short rate changes is contingent on the spread being high, suggesting that the expectations theory's implications are sensitive to the spread's magnitude and sign.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:28:42.816812
5c693775f7a5ce82,The COVID-19 pandemic and Bitcoin: Perspective from investor attention,"The response of the Bitcoin market to the novel coronavirus (COVID-19) pandemic is an example of how a global public health crisis can cause drastic market adjustments or even a market crash. Investor attention on the COVID-19 pandemic is likely to play an important role in this response. Focusing on the Bitcoin futures market, this paper aims to investigate whether pandemic attention can explain and forecast the returns and volatility of Bitcoin futures. Using the daily Google search volume index for the ""coronavirus"" keyword from January 2020 to February 2022 to represent pandemic attention, this paper implements the Granger causality test, Vector Autoregression (VAR) analysis, and several linear effects analyses. The findings suggest that pandemic attention is a granger cause of Bitcoin returns and volatility. It appears that an increase in pandemic attention results in lower returns and excessive volatility in the Bitcoin futures market, even after taking into account the interactive effects and the influence of controlling other financial markets. In addition, this paper carries out the out-of-sample forecasts and finds that the predictive models with pandemic attention do improve the out-of-sample forecast performance, which is enhanced in the prediction of Bitcoin returns while diminished in the prediction of Bitcoin volatility as the forecast horizon is extended. Finally, the predictive models including pandemic attention can generate significant economic benefits by constructing portfolios among Bitcoin futures and risk-free assets. All the results demonstrate that pandemic attention plays an important and non-negligible role in the Bitcoin futures market. This paper can provide enlightens for subsequent research on Bitcoin based on investor attention sparked by public emergencies.The response of the Bitcoin market to the novel coronavirus (COVID-19) pandemic is an example of how a global public health crisis can cause drastic market adjustments or even a market crash. Investor attention on the COVID-19 pandemic is likely to play an important role in this response. Focusing on the Bitcoin futures market, this paper aims to investigate whether pandemic attention can explain and forecast the returns and volatility of Bitcoin futures. Using the daily Google search volume index for the ""coronavirus"" keyword from January 2020 to February 2022 to represent pandemic attention, this paper implements the Granger causality test, Vector Autoregression (VAR) analysis, and several linear effects analyses. The findings suggest that pandemic attention is a granger cause of Bitcoin returns and volatility. It appears that an increase in pandemic attention results in lower returns and excessive volatility in the Bitcoin futures market, even after taking into account the interactive effects and the influence of controlling other financial markets. In addition, this paper carries out the out-of-sample forecasts and finds that the predictive models with pandemic attention do improve the out-of-sample forecast performance, which is enhanced in the prediction of Bitcoin returns while diminished in the prediction of Bitcoin volatility as the forecast horizon is extended. Finally, the predictive models including pandemic attention can generate significant economic benefits by constructing portfolios among Bitcoin futures and risk-free assets. All the results demonstrate that pandemic attention plays an important and non-negligible role in the Bitcoin futures market. This paper can provide enlightens for subsequent research on Bitcoin based on investor attention sparked by public emergencies.",,2023,10.3389/fpubh.2023.1147838,,proquest,"This paper investigates the impact of investor attention on the COVID-19 pandemic on the Bitcoin futures market. Using Google search volume for ""coronavirus"" as a proxy for pandemic attention, the study employs Granger causality tests, VAR analysis, and linear effects analyses. Findings indicate that pandemic attention Granger-causes Bitcoin returns and volatility, leading to lower returns and increased volatility. The study also demonstrates that incorporating pandemic attention improves out-of-sample forecasts for Bitcoin returns and can generate economic benefits through portfolio construction. The research highlights the significant role of pandemic attention in the Bitcoin futures market and suggests avenues for future research on investor attention during public emergencies.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:28:44.798191
32219b4eb50b7726,The Co-Integrated Vector Autoregression with Errors–in–Variables,"The co-integrated vector autoregression is extended to allow variables to be observed with classical measurement errors (ME). For estimation, the model is parametrized as a time invariant state-space form, and an accelerated expectation-maximization algorithm is derived. A simulation study shows that (i) the finite-sample properties of the maximum likelihood (ML) estimates and reduced rank test statistics are excellent (ii) neglected measurement errors will generally distort unit root inference due to a moving average component in the residuals, and (iii) the moving average component may–in principle–be approximated by a long autoregression, but a pure autoregression cannot identify the autoregressive structure of the latent process, and the adjustment coefficients are estimated with a substantial asymptotic bias. An application to the zero-coupon yield-curve is given. © 2021 Elsevier B.V., All rights reserved.","Nielsen, H.B.",2016,10.1080/07474938.2013.806853,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948710369&doi=10.1080%2F07474938.2013.806853&partnerID=40&md5=c5d9ff605a01be7d486ad7381bf29666,scopus,"This paper extends the co-integrated vector autoregression model to account for measurement errors in variables. It proposes a state-space form and an expectation-maximization algorithm for estimation. A simulation study demonstrates the model's effectiveness, highlighting the distortions caused by unaddressed measurement errors in unit root inference and the limitations of pure autoregressions in capturing latent process structures. An application to the zero-coupon yield curve is presented.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:28:49.575238
8df5c5a1183a28ea,The London Business School With Gower Publishing: FIGHTING YESTERDAY'S BATTLES,"With still no firm evidence at home of a recovery in non‐oil GDP, the government's main worries centre on the path of output ahead of the General Election. In a forecast, which relies heavily on exports to stimulate demand in 1992, the Treasury cannot regard the rising probability of renewed recession in the US or the very sharp slowdown currently taking place in Europe as the post‐unification German boom runs out of steam with equanimity. The fear mist remain in Conservative politicians' minds that there will be no meaningful recovery within an electorally significant timescale. We sketch out this background, but our focus here is not on the prospects for recovery; rather we ask whether the recession has achieved its objectives. The recession was, it should be remembered, the direct product of government policy ‐ interest rates were raised to 15per cent ahead of ERM membership ‐ aimed at reversing the excesses of the late 198Os'boom and in particular at bringing inflation quickly down to acceptable European levels and reducing the deficit on the current account, which at its peak in 1989 amounted to 4 per cent of GDP. Our answer is that, over the last year of recession, considerable progress has been made: the rate of inflation is now in line with that in Germany and the current account deficit has fallen to under 1 per cent of GDP. But, on the government's own forecasts contained in the Autumn Statement, there will be some slippage on both counts in 1992. It is this worrying feature that we consider here. Our overall conclusion is that the recession has not completely delivered its objectives and that, even as the politicians turn their attention to recovery, we still have to fight yesterday's battles. Copyright © 1991, Wiley Blackwell. All rights reserved © 2016 Elsevier B.V., All rights reserved.","Dicks, G.",1991,10.1111/j.1468-0319.1991.tb00158.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978558611&doi=10.1111%2Fj.1468-0319.1991.tb00158.x&partnerID=40&md5=b960fc38fda667cdd4f0e4365e15cf2e,scopus,"This article analyzes the effectiveness of the UK government's recessionary policies, implemented to curb inflation and reduce the current account deficit. While progress has been made, the authors argue that the recession has not fully achieved its objectives, and further efforts are needed to address lingering economic issues.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:28:52.597614
cafdecb4b42b0a46,The London Business School with Gower Publishing,"Collapsing oil prices and a falling dollar set the background to a Budget in which the Chancellor, hamstrung by lower oil revenues, was seen as having little room for manoeuvre. In fact the sharp fall in the sterling price of oil has provided him with the perfect excuse for not making significant cuts in personal income tax that were largely irrelevant to the needs of the economy. Instead of a boost to household demand we have had, thanks to OPEC, a transfer to companies in the form of a reduction in costs. This should enable them to expand output against a background of falling inflation. Our post‐Budget assessment of macroeconomic prospects (Section I), made on the Treasury's assumption of a $15 oil price, shows output growing by 2 1/2 per cent this year and inflation falling below 3 per cent in 1987. We are thus less optimistic than the Treasury about output but more optimistic about inflation. How was the Chancellor able, within the confines of the Medium‐Term Financial Strategy, to give anything away having lost so much oil revenue? A detailed analysis of the PSBR forecast (Section II) reveals good reasons why non‐oil tax revenues should be some £3 1/2n higher than forecast this time last year. But, because we still expect public spending to be above the official figures, our PSBR forecast is £1bn higher than the Treasury's. Although the macroeconomic impact of the Budget was small (especially in relation to that of the fall in oil prices which preceded it), it continued the process of tax reform. We focus, in Section III, on the new proposals to deal with the problem of the pension fund surpluses to which we drew attention in the November issue of Financial Outlook. We conclude that the proposed measures could have a larger effect on tax revenues in the longer term than is indicated by the Treasury's Budget estimates. Copyright © 1986, Wiley Blackwell. All rights reserved © 2016 Elsevier B.V., All rights reserved.","Dicks, G.; Keating, G.; Robinson, B.",1986,10.1111/j.1468-0319.1986.tb00132.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978583037&doi=10.1111%2Fj.1468-0319.1986.tb00132.x&partnerID=40&md5=b54956c2c887c359c6614df6985def4e,scopus,"This article analyzes the UK's budget in the context of falling oil prices and a weakening dollar. It discusses the Chancellor's limited room for maneuver, the shift in benefits from households to companies, and macroeconomic forecasts for output and inflation. The analysis also delves into the Public Sector Borrowing Requirement (PSBR) forecast and tax reforms, particularly concerning pension fund surpluses, suggesting potential long-term impacts on tax revenues.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:28:54.148894
6e5ee856b4eecc9e,The Markov-switching jump diffusion LIBOR market model,"In this paper, we introduce an extension to the LIBOR Market Model (LMM) that is suitable to incorporate both sudden market shocks as well as changes in the overall economic climate into the interest rate dynamics. This is achieved by substituting the simple diffusion process of the original LMM by a regime-switching jump diffusion. We demonstrate that the new Markov-switching jump diffusion (MSJD) LMM can be embedded into a generalized regime-switching Heath-Jarrow-Morton model and prove that the considered market is arbitrage-free. We derive pricing formulas for caps, floors and swaptions using Fourier pricing techniques and show how the model can be calibrated to real market data.","Steinruecke, L.; Zagst, R.; Swishchuk, A.",2015,10.1080/14697688.2014.962594,,wos,"This paper extends the LIBOR Market Model (LMM) by incorporating a regime-switching jump diffusion process to capture market shocks and economic climate changes. The proposed Markov-switching jump diffusion (MSJD) LMM is shown to be embeddable in a generalized Heath-Jarrow-Morton model, ensuring an arbitrage-free market. The authors derive pricing formulas for caps, floors, and swaptions using Fourier techniques and demonstrate model calibration to market data.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:28:59.759649
3c22a3ab919ed331,The Pruned State-Space System for Non-Linear DSGE Models: Theory and Empirical Applications,"This article studies the pruned state-space system for higher-order perturbation approximations to dynamic stochastic general equilibrium (DSGE) models. We show the stability of the pruned approximation up to third order and provide closed-form expressions for first and second unconditional moments and impulse response functions. Our results introduce generalized method of moments (GMM) estimation and impulse-response matching for DSGE models approximated up to third order and provide a foundation for indirect inference and simulated method of moments (SMM). As an application,we consider a New Keynesian model with Epstein–Zin preferences and two novel feedback effects from long-term bonds to the real economy, allowing us to match the level and variability of the $10$-year term premium in the U.S. with a low relative risk aversion of $5$.",,2018,10.1093/restud/rdx037,,proquest,"This article develops a pruned state-space system for higher-order perturbation approximations of non-linear DSGE models, demonstrating stability up to third order and providing analytical solutions for moments and impulse responses. It introduces GMM and impulse-response matching for DSGE models approximated to third order, laying groundwork for indirect inference and SMM. An application to a New Keynesian model with Epstein-Zin preferences and novel feedback effects matches U.S. term premium data with low relative risk aversion.",True,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:29:10.880153
403a1d83ec233556,The Relationship between Default Risk and Asset Pricing: Empirical Evidence from Pakistan,"This paper examines the efficacy of the default risk factor in an emerging market context using the Fama-French five-factor model. Our aim is to test whether the Fama-French five-factor model augmented with a default risk factor improves the predictability of returns of portfolios sorted on the firm’s characteristics as well as on industry. The default risk factor is constructed by estimating the probability of default using a hybrid version of dynamic panel probit and artificial neural network (ANN) to proxy default risk. This study also provides evidence on the temporal stability of risk premiums obtained using the Fama-MacBeth approach. Using a sample of 3,806 firm-year observations on non-financial listed companies of Pakistan over 2006–2015 we found that the augmented model performed better when tested across size-investment-default sorted portfolios. The investment factor contains some default-related information, but default risk is independently priced and bears a significantly positive risk premium. The risk premiums are also found temporally stable over the full sample and more recent sample period 2010–2015 as evidence by the Fama-MacBeth regressions. The finding suggests that the default risk factor is not a useless factor and due to mispricing, default risk anomaly prevails in the Pakistani equity market. © 2021 Elsevier B.V., All rights reserved.","Khan, U.E.; Iqbal, J.",2021,10.13106/jafeb.2021.vol8.no3.0717,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102257576&doi=10.13106%2Fjafeb.2021.vol8.no3.0717&partnerID=40&md5=0a2f393054f1fd8ea20a186359d1a9cc,scopus,"This study investigates the effectiveness of a default risk factor within the Fama-French five-factor model in Pakistan's emerging market. It constructs a default risk factor using a hybrid approach of dynamic panel probit and artificial neural networks (ANN). The findings indicate that the augmented model, incorporating default risk, enhances return predictability for portfolios sorted by firm characteristics and industry. Default risk is found to be independently priced with a positive risk premium, and this premium is temporally stable. The study concludes that default risk is a significant factor in the Pakistani equity market, suggesting a default risk anomaly due to mispricing.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:29:13.650108
26ee0779c0e53b1a,The Stern Review on the Economic Effects of Climate Change,"In a study of the economics of climate change commissioned by the British government, released on 30 October, the former World Bank chief economist Sir Nicholas Stern presents a vigorously argued case for early curtailment of greenhouse gas emissions and proposes mitigation strategies that appear to offer highly favorable benefit-cost ratios. An excerpt from the Executive Summary of the Stern Review, concerned with the nature and magnitude of the deleterious economic consequences of anticipated climate change, is printed below. The principal scientific reviews of knowledge of climate change, its consequences, and mitigation strategies are the (roughly) quinquennial reports of the Intergovernmental Panel on Climate Change (IPCC)-the work of hundreds of lead authors, subjected in turn to elaborate peer review and line-by-line scrutiny by interested governments. They represent a broad, though not total, expert consensus. The third IPCC assessment was issued in 2001; the fourth, already in draft, will be released next year. The Stern Review draws heavily on this scientific underpinning, but goes further than the IPCC exercise in computing economic values for the projected changes and costing out remedial policy responses. More forthright in style and emphatic in its conclusions, it reads as a resounding call to international action. The Review explores the implications of atmospheric concentrations of carbon dioxide and other greenhouse gases being capped at 550ppm (parts per million), double the preindustrial level, an objective it argues is feasible. That concentration would be reached by 2050 at current emission rates, or by 2035 if emissions rise as expected. The resulting warming, it believes, would be 2-5 degree C, roughly in accord with the IPCC's third-assessment estimates (see the Documents section of PDR 27, no. 1 for the IPCC projections). The positive feedbacks identified in some recent studies, generated by processes such as release of methane from permafrost, could lead to still higher temperatures. The forecast effects described are by now familiar, though no less grim for being so: species extinctions, expanding disease zones, reductions in surface water availability, coastal flooding, ocean acidification, and so on. The Review translates these effects into economic losses, adjusting for risk, using Monte Carlo simulation applied to an integrated assessment model (the so-called PAGE 2002 model). The exercise, requiring many heroic-and often contestable-assumptions, produces the most quoted figures in the report: that climate change 'will reduce welfare by an amount equivalent to a reduction in consumption per head of between 5 and 20%'-now and into the future. The absolute magnitude of those projected economic losses is made arbitrarily large by their permanence. Typical benefit-cost calculations applied to appraisal of development projects convert such long-term trajectories into a present value using a discount rate comparable to a market interest rate or some (lower) assumed rate of time preference. The Stern Review, however, argues that any discounting is ethically inappropriate for this global issue: 'if a future generation will be present, we suppose that it has the same claim on our ethical attention as the current one' (p. 31). The only exception is an allowance for the possibility that future generations are not present-through human extinction-which is held to justify a minuscule discount rate of 0.1 percent per annum (p. 161). The percentage economic losses from climate change appear less daunting if set against the recent pace of expansion in the world economy. Real per capita income growth since 1990 has averaged about 1.5 percent per year worldwide, and about 3 percent in developing countries. In such a regime, a 5 percent one-time drop to a lower expansion path is no more than a two- or three-year delay in attaining a given income level. For China and India, whose economies are doubling in size each decade, even a 20 percent reduction in income would be a mere hiccough on the path to affluence-hardly enough to motivate major shifts in lifestyle ambitions. The dire repercussions on global environments of a greenhouse warming at the upper end of the forecast range are poorly captured by those percentages. Demography has a marginal place in the Review. The underlying IPCC emission scenarios incorporate expected population growth, using the UN medium projections. Many of the climate-change effects incur costs that are similarly magnified by population growth. One-sixth of the world's population is 'threatened' by water scarcities; 1 in 20 people may be displaced by a rising sea level; mortality may increase from vector-borne diseases and from malnutrition linked to income losses. The later part of the Review is concerned with mitigation and adaptation strategies. It lays out an ambitious set of policies for transition to a low-carbon economy that could stabilize greenhouse gas concentrations over the next several decades. By 2050, emissions would have to be 25 percent below today's and emissions per unit of GDP 75 percent below. In perhaps the most problematic part of the exercise the Review asserts that such cuts could be achieved at a cost of only around 1 percent of annual global GDP-implying that investment in mitigation should be strongly favored on straightforward economic grounds. (This figure, like others in the Review, is acknowledged to lie within a substantial envelope of uncertainty-here a range of -1.0 percent to +3.5 percent of global GDP (p. 212), or, drawing on a wider range of models, -4 percent to +15 percent (p. 241).) In the decades before the investment pays off, adverse consequences of the warming trends already underway must be dealt with by adaptation, such as through better disaster preparedness, lessening the vulnerability of infrastructure, and risk-pooling measures. The excerpt is from pp. iii-iv and vi-xi. The full Stern Review (579 pages), the executive summary, and the commissioned background papers are available online at 'http://www.hm-treasury.gov.uk/independent_reviews/stern_review_ec o nomics_climate_change/sternreview_index.cfm'. A hard copy of the Review will be issued by Cambridge University Press.",,2006,10.1111/j.1728-4457.2006.00153.x,,proquest,"This article summarizes the Stern Review on the Economic Effects of Climate Change, highlighting its argument for early greenhouse gas emission curtailment and proposing mitigation strategies with favorable benefit-cost ratios. It discusses the review's reliance on IPCC scientific assessments, its economic valuation of climate change impacts (estimated as a 5-20% reduction in per capita consumption), and its controversial stance against discounting future welfare. The review also outlines ambitious mitigation policies to transition to a low-carbon economy, suggesting these could be achieved at a low cost (around 1% of global GDP), and emphasizes the need for adaptation strategies.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:29:16.987147
97060010610a3aec,The Term Structure of Machine Learning Alpha,"Machine learning (ML) models for predicting stock returns are typically trained on one-month forward returns. Although these models show impressive full-sample gross alphas, their performance net of transaction costs post-2004 is close to zero. By training on longer prediction horizons and using efficient portfolio construction rules, the authors demonstrate that ML-based investment strategies can still yield significant positive net returns. Longer-horizon strategies select slower signals and load more on traditional asset pricing factors but still unlock unique alpha. The authors conclude that design choices are critical for the success of ML models in real-life applications. © 2023 Elsevier B.V., All rights reserved.","Blitz, D.; Hanauer, M.X.; Hoogteijling, T.; Howard, C.",2023,10.3905/jfds.2023.1.135,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176099492&doi=10.3905%2Fjfds.2023.1.135&partnerID=40&md5=d9c1c7c51b12768f2f8c95b1d8c8e9bc,scopus,"This study investigates the effectiveness of machine learning (ML) models in predicting stock returns, finding that while short-term models show high gross alphas, their net returns are negligible after transaction costs. By extending prediction horizons and optimizing portfolio construction, the authors show that ML strategies can achieve significant positive net returns, suggesting that design choices are crucial for successful real-world ML applications.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:29:32.092504
7964653838d9f322,The Use of Simulation in Vascular Surgery Education: Current State and Future Directions,"Simulation-based training (SBT) has become essential in vascular surgery education, providing a risk-free environment for skill development. This scoping review evaluates the current state of vascular surgery simulation, highlighting validated models, educational impact, and areas for improvement. A systematic literature search was conducted in PubMed, Embase, and Scopus, following PRISMA-ScR guidelines. Studies assessing validated simulation models for open and endovascular procedures, vascular anastomosis, carotid interventions, peripheral vascular interventions, and nontechnical skills training were included. Data extraction focused on fidelity, skill acquisition, procedural efficiency, and accessibility. Validated high-fidelity models, including 3D-printed, virtual reality (VR), and pulsatile cadaveric systems, significantly enhance technical proficiency and confidence. Bench and porcine models improve vascular anastomosis training, while VR-based simulators enhance catheter manipulation and decision-making. However, simulation remains limited by high costs, accessibility challenges, and lack of standardized nontechnical skills training. Simulation improves competency in vascular surgery but requires further integration into training curricula. AI-driven assessments, hybrid simulation models, and expanded cost-effective solutions are needed to bridge existing gaps. Standardization and broader adoption of simulation will enhance competency-based training and improve patient outcomes.Simulation-based training (SBT) has become essential in vascular surgery education, providing a risk-free environment for skill development. This scoping review evaluates the current state of vascular surgery simulation, highlighting validated models, educational impact, and areas for improvement. A systematic literature search was conducted in PubMed, Embase, and Scopus, following PRISMA-ScR guidelines. Studies assessing validated simulation models for open and endovascular procedures, vascular anastomosis, carotid interventions, peripheral vascular interventions, and nontechnical skills training were included. Data extraction focused on fidelity, skill acquisition, procedural efficiency, and accessibility. Validated high-fidelity models, including 3D-printed, virtual reality (VR), and pulsatile cadaveric systems, significantly enhance technical proficiency and confidence. Bench and porcine models improve vascular anastomosis training, while VR-based simulators enhance catheter manipulation and decision-making. However, simulation remains limited by high costs, accessibility challenges, and lack of standardized nontechnical skills training. Simulation improves competency in vascular surgery but requires further integration into training curricula. AI-driven assessments, hybrid simulation models, and expanded cost-effective solutions are needed to bridge existing gaps. Standardization and broader adoption of simulation will enhance competency-based training and improve patient outcomes.",,2025,10.1053/j.semvascsurg.2025.03.001,,proquest,"This scoping review examines the current use and future potential of simulation-based training (SBT) in vascular surgery education. It identifies validated simulation models (3D-printed, VR, cadaveric) that improve technical skills and confidence. Challenges include cost, accessibility, and standardized nontechnical skills training. The review suggests AI-driven assessments and hybrid models for future integration to enhance competency-based training and patient outcomes.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:29:34.408827
f5d2f8581abc1e8f,The conundrum of stock versus bond prices,"In a general way, stock and bond prices do not display any significant correlation. Yet, if we concentrate our attention on the specific episodes marked by a crash followed by a rebound, then we observe that stock prices have a strong connection with interest rates on one hand, and with bond yield spreads on the other hand. That second relationship is particularly stable in the course of time having been observed for over 140 years. Throughout the paper we use a quasi-experimental approach. By observing how markets respond to well-defined exogenous shocks (such as the shock of 11 September 2001) we are able to determine how investors organize their ""flight to safety"": which safe haven they select, how long their collective panic lasts, and so on. As rebounds come to an end the correlation of stock and bond prices fades away, a clear sign that the collective behavior of investors loses some of its coherence; this observation can be used as an objective criterion for assessing the end of a market rebound. Based on the behavior of investors, we introduce a distinction between ""genuine stock market rallies"", as opposed to spurious rallies such as those brought about by the buyback programs implemented by large companies. The paper ends with a discussion of testable predictions. © 2003 Elsevier B.V. All rights reserved. © 2008 Elsevier B.V., All rights reserved.","Maslov, S.; Roehner, B.M.",2004,10.1016/j.physa.2003.11.031,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0742272492&doi=10.1016%2Fj.physa.2003.11.031&partnerID=40&md5=37ee85a0dd59a70b6cb37fc7c52bcf8c,scopus,"This paper investigates the relationship between stock and bond prices, particularly during market crashes and rebounds. It uses a quasi-experimental approach, analyzing investor behavior in response to exogenous shocks like 9/11 to understand their 'flight to safety'. The study finds a stable, long-term correlation between stock prices and bond yields during rebounds, which fades as the rebound ends. This observation is proposed as a criterion to distinguish genuine market rallies from spurious ones (e.g., those driven by stock buybacks).",False,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:29:38.724453
8daf101573bc8967,The cost of carbon capture and storage for natural gas combined cycle power plants,"This paper examines the cost of CO(2) capture and storage (CCS) for natural gas combined cycle (NGCC) power plants. Existing studies employ a broad range of assumptions and lack a consistent costing method. This study takes a more systematic approach to analyze plants with an amine-based postcombustion CCS system with 90% CO(2) capture. We employ sensitivity analyses together with a probabilistic analysis to quantify costs for plants with and without CCS under uncertainty or variability in key parameters. Results for new baseload plants indicate a likely increase in levelized cost of electricity (LCOE) of $20-32/MWh (constant 2007$) or $22-40/MWh in current dollars. A risk premium for plants with CCS increases these ranges to $23-39/MWh and $25-46/MWh, respectively. Based on current cost estimates, our analysis further shows that a policy to encourage CCS at new NGCC plants via an emission tax or carbon price requires (at 95% confidence) a price of at least $125/t CO(2) to ensure NGCC-CCS is cheaper than a plant without CCS. Higher costs are found for nonbaseload plants and CCS retrofits.This paper examines the cost of CO(2) capture and storage (CCS) for natural gas combined cycle (NGCC) power plants. Existing studies employ a broad range of assumptions and lack a consistent costing method. This study takes a more systematic approach to analyze plants with an amine-based postcombustion CCS system with 90% CO(2) capture. We employ sensitivity analyses together with a probabilistic analysis to quantify costs for plants with and without CCS under uncertainty or variability in key parameters. Results for new baseload plants indicate a likely increase in levelized cost of electricity (LCOE) of $20-32/MWh (constant 2007$) or $22-40/MWh in current dollars. A risk premium for plants with CCS increases these ranges to $23-39/MWh and $25-46/MWh, respectively. Based on current cost estimates, our analysis further shows that a policy to encourage CCS at new NGCC plants via an emission tax or carbon price requires (at 95% confidence) a price of at least $125/t CO(2) to ensure NGCC-CCS is cheaper than a plant without CCS. Higher costs are found for nonbaseload plants and CCS retrofits.",,2012,10.1021/es204514f,,proquest,"This study systematically analyzes the cost of CO(2) capture and storage (CCS) for natural gas combined cycle (NGCC) power plants using an amine-based postcombustion CCS system with 90% CO(2) capture. It employs sensitivity and probabilistic analyses to quantify costs under uncertainty and variability. Results indicate a likely increase in the levelized cost of electricity (LCOE) for new baseload plants, with a risk premium further increasing these costs. The analysis also suggests a minimum carbon price of $125/t CO(2) is required to incentivize CCS at new NGCC plants.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:29:40.497849
d85d0baae566aaa4,The cross-sectional stock return predictions via quantum neural network and tensor network,"In this paper, we investigate the application of quantum and quantum-inspired machine learning algorithms to stock return predictions. Specifically, we evaluate the performance of quantum neural network, an algorithm suited for noisy intermediate-scale quantum computers, and tensor network, a quantum-inspired machine learning algorithm, against classical models such as linear regression and neural networks. To evaluate their abilities, we construct portfolios based on their predictions and measure investment performances. The empirical study on the Japanese stock market shows the tensor network model achieves superior performance compared to classical benchmark models, including linear and neural network models. Though the quantum neural network model attains the lowered risk-adjusted excess return than the classical neural network models over the whole period, both the quantum neural network and tensor network models have superior performances in the latest market environment, which suggests capability of model’s capturing non-linearity between input features. © 2023 Elsevier B.V., All rights reserved.","Kobayashi, N.; Suimon, Y.; Miyamoto, K.; Mitarai, K.",2023,10.1007/s42484-023-00136-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178906367&doi=10.1007%2Fs42484-023-00136-x&partnerID=40&md5=5778bbe192db850d188778151b988196,scopus,"This paper explores the use of quantum and quantum-inspired machine learning algorithms, specifically quantum neural networks and tensor networks, for stock return predictions. The study compares these models against classical methods like linear regression and neural networks using the Japanese stock market. Results indicate that the tensor network model outperforms classical benchmarks, and both quantum and tensor network models show superior performance in recent market conditions, suggesting an ability to capture non-linear relationships.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:29:43.127360
d8878047801020d8,The economic value of advanced time series methods for modelling and trading 10-year government bonds,"The motivation for this paper is to determine the potential economic value of advanced modelling methods for devising trading decision tools for 10-year Government bonds. Two advanced methods are used: time-varying parameter models with the implementation of state space modelling using a Kalman filter and nonparametric nonlinear models with Neural Network Regression (NNR). These are benchmarked against more traditional forecasting techniques to ascertain their potential as a forecasting tool and their economic value as a base for a trading decision tool. The models were developed using data from the UK Gilt market, US T-Bond market and German Bund market. Using in-sample data from April 2001 to January 2003 to develop the models, their results were assessed using the out-of-sample period of January 2003 to June 2003. Performance evaluation was based upon forecasting accuracy measures and financial criteria using a simulated trading strategy incorporating realistic trading costs. It is concluded that for the time series studied and for the period under investigation, the performance of the advanced models is mixed. While the NNR models have the ability to forecast the 10-year Government bond yield and add economic value as a trading decision tool, the Kalman filter models' performance is not as conclusive. The Kalman filter models outperformed the traditional techniques using forecasting accuracy measures, however they did not perform as well in the simulated trading strategy. Reprinted by permission of Routledge, Taylor and Francis Ltd.",,2007,10.1080/13518470600880010,,proquest,"This paper evaluates the economic value of advanced time series methods, specifically time-varying parameter models with Kalman filtering and Neural Network Regression (NNR), for modeling and trading 10-year government bonds. These methods are compared against traditional forecasting techniques using data from UK, US, and German bond markets. The study found that NNR models showed potential for forecasting bond yields and adding economic value to trading strategies, while Kalman filter models had mixed results, outperforming traditional methods in accuracy but not in simulated trading.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-28T11:29:50.910262
fcd1ed39b63802de,The effect of managerial ownership on the cost of debt: Evidence from Japan,"This article examines the effect of managerial ownership (MO) on the cost of debt as measured by the interest rate spread on corporate bonds for Japanese firms. First, the authors find that the MO is positively associated with interest rate spread after controlling for the other Japanese ownership structure, cross-shareholdings, and the stable shareholdings by financial institutions. Second, by employing factor analysis to measure the agency cost of debt (ACD) based on financial variables, the authors also find that MO has higher correlation with interest rate spread when the ACD at the time of bond issue is already larger. The results are robust to additional analyses, including the possibility of nonlinear relationship, bond rating, endogeneity problem, and Fama and MacBeth approach. The results suggest that prospective bondholders use MO information to anticipate a firm's future ACD and estimate it higher when the current ACD at issuing bond is already larger. The results also suggest that accounting information is useful to estimate the ACD and increase the efficiency of bond contracting. Finally, although previous studies are often prone to emphasizing the findings on the Japanese unique ownership structure, the results of this article reveal that traditional agency theory on MO apply to Japanese bond market, which is consistent with the findings of U.S. firms. © The Author(s) 2011. © 2011 Elsevier B.V., All rights reserved.","Shuto, A.; Kitagawa, N.",2011,10.1177/0148558x11401553,https://www.scopus.com/inward/record.uri?eid=2-s2.0-81255200514&doi=10.1177%2F0148558X11401553&partnerID=40&md5=27550a2d37ccf3c421fd165a5c3d8a92,scopus,"This study investigates the relationship between managerial ownership (MO) and the cost of debt (interest rate spread on corporate bonds) in Japanese firms. It finds a positive association between MO and the interest rate spread, suggesting that higher MO leads to a higher cost of debt. The study also explores the role of agency costs of debt and concludes that traditional agency theory regarding MO is applicable to the Japanese bond market, similar to findings in the U.S.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:29:54.276134
64cac22fea2065b6,The expectations hypothesis of the term structure when interest rates are close to zero,"In an economy where cash can be stored costlessly in nominal terms, the nominal interest rate is bounded below by zero. This paper derives the implications of this non-negativity constraint for the term structure and shows that it induces a nonlinear and convex relation between short- and long-term interest rates. The long-term rate responds asymmetrically to changes in the short-term rate, and by less than that is predicted by the benchmark linear model. In particular, a decrease in the short-term rate produces a smaller response in the long-term rate than an increase of the same magnitude. The empirical predictions of the model are examined using data from Japan. All rights reserved, Elsevier",,2006,10.1016/j.jmoneco.2005.07.014,,proquest,"This paper explores the implications of the zero lower bound on nominal interest rates for the term structure of interest rates. It demonstrates that this constraint leads to a nonlinear and convex relationship between short- and long-term rates, causing asymmetric responses of long-term rates to changes in short-term rates. The model's predictions are tested using Japanese data.",True,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:30:01.333252
09eb13bc70199528,The fundamentals of commodity futures returns,"Commodity futures risk premiums vary across commodities and over time depending on the level of physical inventories. The convenience yield is a decreasing, nonlinear function of inventories. Price measures, such as the futures basis, prior futures returns, prior spot returns, and spot price volatilities reflect the state of inventories and are informative about commodity futures risk premiums. We verify these theoretical predictions using a comprehensive data set on 31 commodity futures and physical inventories between 1971 and 2010. We find no evidence that the positions of participants in futures markets predict risk premiums on commodity futures. © 2012 The Authors. © 2012 Elsevier B.V., All rights reserved.","Gorton, G.B.; Hayashi, F.; Rouwenhorst, K.G.",2013,10.1093/rof/rfs019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871192702&doi=10.1093%2Frof%2Frfs019&partnerID=40&md5=16b0c06ebb4692992c6f341dd79c6265,scopus,"This paper investigates the relationship between commodity futures risk premiums and physical inventory levels. It proposes that the convenience yield is a nonlinear function of inventories and that various price measures can predict risk premiums. The study uses data from 31 commodity futures between 1971 and 2010 to test these predictions, finding no evidence that futures market participant positions predict risk premiums.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:30:03.029736
d9eba663f81460f3,The impact of oil price shocks on Turkish sovereign yield curve,"Purpose>This paper aims to investigate the impact of oil price shocks on the Turkish sovereign yield curve factors.Design/methodology/approach>To extract the latent factors (level, slope and curvature) of the Turkish sovereign yield curve, we estimate conventional Nelson and Siegel (1987) model with nonlinear least squares. Then, we decompose oil price shocks into supply, demand and risk shocks using structural VAR (structural VAR) models. After this separation, we apply Engle (2002) dynamic conditional correlation GARCH (DCC-GARCH (1,1)) method to investigate time-varying co-movements between yield curve factors and oil price shocks. Finally, using the LP (local projections) proposed by Jorda (2005), we estimate the impulse-response functions to examine the impact of different oil price shocks on yield curve factors.Findings>Our results demonstrate that the various oil price shocks influence the yield curve factors quite differently. A supply shock leads to a statistically significant increase in the level factor. This result shows that elevated oil prices due to supply disruptions are interpreted as a signal of a surge in inflation expectations since the cost channel prevails. Besides, unanticipated demand shocks have a positive impact on the slope factor as a result of the central bank policy response for offsetting the elevated inflation expectations. Finally, a risk shock is associated with a decrease in the curvature factor indicating that risk shocks influence the medium-term bonds due to the deflationary pressure resulting from depressed economic conditions.Practical implications>Our results provide new insights to understand the driving forces of yield curve movements induced by various oil shocks to formulate appropriate policy responses.Originality/value>The study contributes to the literature by two main dimensions. First, the recent oil shock identification scheme of Ready (2018) is modified using the “geopolitical oil price risk index” to capture the changes in the risk perceptions of oil markets driven by geopolitical tensions such as terrorism and conflicts and sanctions. The modified identification scheme attributes more power to demand shocks in explaining the variation of the oil price compared to that of the baseline scheme. Second, it provides recent evidence that distinguishes the impact of oil demand and supply shocks on Turkey's yield curve.",,2022,10.1108/ijoem-06-2020-0681,,proquest,"This paper investigates how different types of oil price shocks (supply, demand, and risk) affect the Turkish sovereign yield curve factors (level, slope, and curvature). Using advanced econometric models like structural VAR, DCC-GARCH, and local projections, the study finds that supply shocks increase the level factor, demand shocks impact the slope factor, and risk shocks affect the curvature factor. The research contributes by modifying oil shock identification and providing recent evidence specific to Turkey.",True,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:30:05.953181
ae0436eaafdd494a,The impact of serial correlation on testing for structural change in binary choice model: Monte Carlo evidence,"This paper examines the finite sample properties of structural change tests with an unknown breakpoint for the probit model in the presence of serial correlation. The combination of structural change and serial correlation renders model estimation challenging, affecting the consistency of coefficient estimates. Although there is vast literature concerning structural change tests for linear time series models, the literature for such tests in the context of binary choice models is somewhat sparse. More importantly, the empirical literature has applied the standard tests of structural change on the discrete choice model, despite the fact that most of these tests were developed specifically for the linear regression model. Subsequently, the theoretical properties of these tests in the context of non-linear models are unknown. This includes the class of discrete choice models, such as probit and logit. The issue becomes even more complicated in the presence of serial correlation, since typical tests for structural change often require the assumption of independence in the error terms. Even when the tests allow for a weakly dependent structure in the data, their finite sample performance remains unknown. This paper conducts simulation analysis on the size of 'supremum' Wald, LR and LM tests for structural change in the context of the probit model with varying levels of serial correlation. It is found that the shortcomings of the tests in linear models are magnified in probit models. In particular, the tests exhibit greater size distortion for the probit model than the linear model with the same level of serial correlation. Bootstrapping is also considered as an alternative approach to obtaining critical values, and though it reduces the size distortion in finite samples, it is unable to accommodate the distortion associated with a high level of serial correlation. (C) 2012 IMACS. Published by Elsevier B.V. All rights reserved.","Chan, Felix; Pauwels, Laurent L.; Wongsosaputro, Johnathan",2013,10.1016/j.matcom.2012.11.001,,wos,"This paper investigates the performance of structural change tests in probit models with serial correlation, finding that standard tests designed for linear models perform poorly and exhibit significant size distortion in non-linear contexts. Bootstrapping offers some improvement but cannot fully address the issues caused by high serial correlation.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:30:09.785601
3708ca7e2a4ab89a,The impact of sponsorship announcements on shareholder wealth in Australia,"Purpose – The purpose of this paper is to examine the impact of 51 sponsorship announcements upon the stock prices of firms sponsoring in Australia. The research examines the broader question of whether sponsorship has the potential to transcend cultural boundaries and contribute to financial performance in regional markets. Design/methodology/approach – The methodology is based on the event study technique which is applied to the estimation of excess returns that arise in response to announcements of corporate sponsorship made by leading industrial stocks trading on the Australian Stock Exchange. Regressions examine whether the cost and duration of sponsorship signal information of importance to investors regarding the financial prospects of sponsoring firms. Findings – A small, fleeting positive increase in wealth effects is observed indicating that economically, sponsorship expenditure in Australia is more or less value neutral. While investors appear indifferent to sponsorship cost, they value shortterm sponsorships of less than two years in particular. Research limitations/implications – Future research needs to examine the role of associated variables such as contract size and length, and the type and level of sponsorship investment. Originality/value – For firms, the study indicates that sponsorship in smaller regional markets should be valued by investors especially when firms keep the duration of the sponsorship as short. As stock prices tend to rise briefly following sponsorship announcements, marketers should leverage sponsorships immediately to gain the attention of investors. For a regional market, short and sharp sponsorships appear to be the optimal approach. © 2010, Emerald Group Publishing Limited © 2016 Elsevier B.V., All rights reserved.","Johnston, M.A.",2010,10.1108/13555851011026926,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958474981&doi=10.1108%2F13555851011026926&partnerID=40&md5=d22fd09480a4e4c7d08d82235d6f52f7,scopus,"This study investigates the impact of 51 sponsorship announcements on the stock prices of Australian sponsoring firms using an event study technique. It found a small, temporary positive increase in wealth effects, suggesting sponsorship expenditure is largely value-neutral. Investors seem indifferent to sponsorship cost but value shorter-term sponsorships (under two years). The research implies that for regional markets, short and sharp sponsorships are optimal, and marketers should leverage them immediately.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:30:13.462573
e0d0deaef6ebe9c8,The impact of variability and correlation of selected geological parameters on the economic assessment of bituminous coal deposits with use of non parametric bootstrap and copula-based Monte Carlo simulation,"This paper presents an assessment of the impact of variability and interdependencies of selected deposit parameters on the net present value (NPV) and internal rate of return (IRR). The subjects of the analyses were three economically viable seams at one of the bituminous coal deposits in Poland. The source of information was the geological model and operational data of the mine X. The simulation was developed based on non-parametric bootstrapping, where the influence of coal quality parameters, seam thickness, spatial density of coal, and waste rock derived from coal partings, floor cutting and dinting, and roof falls, was tested.The interdependencies of geological and mining parameters were replicated in a simulation model using Gaussian and empirical copulas. In the model, the relationship between the amount of total waste rock and operating costs was associated with the use of elaborate mathematical formulas. Economic appraisal was based on an income approach, using the free cash flow for the firm (FCFF) analysis and discounting process.Based on the Gaussian copula, in the X-1 and X-2 seams, the average NPV differences achieved were a maximum of 39%. In the case of IRR, the mean difference did not exceed 3.6% points (pp). The quantified spread between the correlated and uncorrelated average values of NPV was at most 45% and 4.8 pp for IRR. Empirical copula limits the range of variation of input and output parameters, resulting in different values for the average NPV, at a maximum of 11.8%, and IRR, 2.4 pp.If the IRR reflects the level of expected return of investment, it can be stated that the additional risk premium resulting from the volatility and correlation of analysed deposits parameters of bituminous coal should be relatively low and less than 2.4 pp in similar cases. The analyses also revealed that the amount of available geological information is of secondary importance in the valuation process, as it does not negatively affect the regularity and symmetry of predicted outcomes.","Kopacz, Michal; Sobczyk, Eugeniusz J.; Galica, Dominik",2018,10.1016/j.resourpol.2017.11.015,,wos,"This study assesses how variability and interdependencies of geological parameters (coal quality, seam thickness, density, waste rock) affect the economic viability (NPV, IRR) of bituminous coal deposits. Using non-parametric bootstrapping and copula-based Monte Carlo simulations on data from a Polish mine, the research quantifies the impact of these factors on financial metrics. Results show that correlations can lead to significant differences in NPV and IRR, with empirical copulas showing less variation than Gaussian copulas. The study suggests the additional risk from parameter volatility and correlation is relatively low and that the amount of geological information is of secondary importance for valuation.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:30:18.164739
e257b7e9d88f79c0,The impact of word sense disambiguation on stock price prediction,"State-of-the-art decision support systems for stock price prediction incorporate pattern-based event detection in text into their predictions. These systems typically fail to account for word meaning, even though word sense disambiguation is crucial for text understanding. Therefore, we propose an advanced natural language processing pipeline for event-based stock price prediction, that allows for word sense disambiguation to be incorporated in the event detection process. We identify events in natural language news messages and subsequently weight these events for their historical impact on stock prices. We assess the merit of word sense disambiguation in event-based stock price prediction in two evaluation scenarios for NASDAQ-100 companies, based on historical stock prices and news articles retrieved from Dow Jones Newswires over a 2-year period. We evaluate the precision of generated buy and sell signals based on our predicted stock price movements, as well as the excess returns generated by a trading strategy that acts upon these signals. Event-based stock price predictions seem most reliable about 2 days into the future. The number of detected events tends to reduce with over 30% when graph-based word sense disambiguation using a degree centrality measure is applied in the event detection process, thus reducing the noise introduced into the stock price movement predictions by high-impact ambiguous events. As a result, modest improvements in the precision of buy and sell signals generated based on these predictions tend to lead to vast improvements of on average about 70% in the associated excess returns. © 2021 Elsevier B.V., All rights reserved.","Hogenboom, A.; Brojba-Micu, A.; Frasincar, F.",2021,10.1016/j.eswa.2021.115568,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109887638&doi=10.1016%2Fj.eswa.2021.115568&partnerID=40&md5=dfaeda381e54eef7aab7c102e3b6ad12,scopus,"This study proposes an advanced NLP pipeline for event-based stock price prediction that incorporates word sense disambiguation (WSD) to improve event detection and weighting. By applying WSD to news messages, the system aims to reduce noise from ambiguous events, leading to more reliable predictions and potentially higher excess returns from trading strategies. Evaluations on NASDAQ-100 companies over a 2-year period showed that WSD can improve the precision of buy/sell signals and significantly increase excess returns.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:30:21.378187
f87c91979270a5eb,The market for commonwealth government securities,"This study of the market for Commonwealth government securities in Australia pays particular attention to the influence of interest rate expectations on demand. Attempts to estimate expectations with current and past information, including the use of a Box-Jenkins time series model are unsuccessful. An assumption that interest rate expectations are based on perfect foresight proves valuable in explaining the demand for government securities. The picture of the market which emerges is one of a private sector which acts on new information more quickly than it is incorporated in the official yield curve. © 1980, SAGE Publications. All rights reserved. © 2016 Elsevier B.V., All rights reserved.","Evans, W.H.; Rozenstein, H.A.",1980,10.1177/031289628000500206,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965949166&doi=10.1177%2F031289628000500206&partnerID=40&md5=f511b472fcc843f2453db0ea9c6b3733,scopus,"This study examines the Australian Commonwealth government securities market, focusing on how interest rate expectations affect demand. It found that attempts to estimate expectations using current and past data, including time series models, were unsuccessful. However, assuming perfect foresight regarding interest rates helped explain demand. The market appears to be characterized by a private sector that reacts to new information faster than it is reflected in the official yield curve.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:30:24.020961
02ab8350d2328fd4,The maturity premium,"We show that firms with longer debt maturities earn risk premia not explained by unconditional factors. Embedding dynamic capital structure choices in an asset-pricing framework where the market price of risk evolves with the business cycle, we find that firms with long-term debt exhibit more countercyclical leverage. The induced covariance between betas and the market price of risk generates a maturity premium similar in size to our empirical estimate of 0.21% per month. We also provide direct evidence for the model mechanism and confirm that the maturity premium is consistent with observed leverage dynamics of long- and short-maturity firms.",,2022,10.1016/j.jfineco.2021.07.008,,proquest,"This paper proposes a theoretical asset-pricing model where firms with longer debt maturities earn a risk premium. The model incorporates dynamic capital structure choices and a business-cycle-dependent market price of risk, leading to countercyclical leverage for long-term debt firms. This mechanism generates a maturity premium consistent with empirical estimates.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:30:31.745490
25fb4c8caf97c256,"The meaning of structural breaks for risk management: new evidence, mechanisms, and innovative views for the post-COVID-19 era","This paper quantitatively reveals the meaning of structural breaks for risk management by analyzing US and major European banking sector stocks. Applying newly extended Glosten-Jagannathan-Runkle generalized autoregressive conditional heteroscedasticity models, we supply the following new evidence. First, we find that incorporating structural breaks is always effective in estimating banking stock volatilities. Second, we clarify that structural breaks partially explain the tail fatness of banking stock returns. Third, we find that when incorporating structural breaks, the estimated volatilities more accurately capture their downside risk, proving that structural breaks matter for risk management. Fourth, our news impact curve and model parameter analyses also uncover that when incorporating structural breaks, the asymmetry in volatility responses to return shocks is more accurately captured. This proves why the estimated volatilities by incorporating structural breaks better explain downside risk. In addition, we further reveal that the estimated volatilities obtained through incorporating structural breaks increase sharply during momentous events such as the Lehman crisis, the European debt crisis, Brexit, and the recent COVID-19 crisis. Moreover, we also clarify that the volatility spreads between models with and without structural breaks rise during the Lehman and COVID-19 crises. Finally, based on our findings, we derive many significant and beneficial interpretations, implications, and innovative views for risk management using artificial intelligence in the post-COVID-19 era.","Tsuji, Chikashi",2022,10.3934/qfe.2022012,,wos,"This paper examines the impact of structural breaks on risk management in the US and European banking sectors. Using an extended GJR-GARCH model, it demonstrates that incorporating structural breaks improves volatility estimation, explains tail fatness in returns, and better captures downside risk and asymmetric volatility responses. The study also highlights increased volatility during major crises (Lehman, European debt, Brexit, COVID-19) and suggests implications for AI-driven risk management post-COVID-19.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:30:35.892194
7234cd73a718c563,The multifactor nature of the volatility of futures markets,"This paper estimates a model of interest rate dynamics containing multi-factor Wiener and single-factor Poisson jump volatility components. Data from the highly liquid but short term futures markets are used. The difficult numerical problem of estimating such multi-factor models is resolved by using a genetic algorithm to carry out the optimization procedure. It is established that the multi-factor Wiener volatility components are adequate to model the interest rate dynamics without the need to incorporate Poisson jump components, the existence of which would create difficulties in the practical use of interest rate models. Reprinted by permission of Springer",,2006,10.1007/s10614-006-9023-9,,proquest,"This paper estimates a model of interest rate dynamics with multi-factor Wiener and single-factor Poisson jump volatility components using futures market data. A genetic algorithm is employed to overcome estimation challenges. The study concludes that multi-factor Wiener volatility components are sufficient for modeling interest rate dynamics, negating the need for Poisson jump components.",True,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:30:39.156709
eac1579c9a56f66f,The risk premia embedded in index options,"We study the dynamic relation between market risks and risk premia using time series of index option surfaces. We find that priced left tail risk cannot be spanned by market volatility (and its components) and introduce a new tail factor. This tail factor has no incremental predictive power for future volatility and jump risks, beyond current and past volatility, but is critical in predicting future market equity and variance risk premia. Our findings suggest a wide wedge between the dynamics of market risks and their compensation, which typically displays a far more persistent reaction following market crises. (C) 2015 Elsevier B.V. All rights reserved.","Andersen, Torben G.; Fusari, Nicola; Todorov, Viktor",2015,10.1016/j.jfineco.2015.06.005,,wos,"This study investigates the relationship between market risks and risk premia using index option data. It identifies a priced left tail risk that is not explained by market volatility alone, introducing a new tail factor. This factor is crucial for predicting future market equity and variance risk premia, indicating a persistent discrepancy between market risk dynamics and compensation, especially after market crises.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:30:41.399436
e3949ad7076546c5,The role of an aligned investor sentiment index in predicting bond risk premia of the U.S,"In this paper, we develop a new investor sentiment index that is aligned to predict the excess returns on U.S. government bonds that have 2–5 years maturities. The new index is constructed by eliminating a common noise component in underlying sentiment proxies using the partial least squares (PLS) approach. The findings show that the new aligned sentiment index has much greater predictive power than the original principal component analysis (PCA)-based sentiment index both in- and out-of-sample. In addition, predictability is statistically significant, especially for bond premia with shorter maturities, even after controlling for a large number of financial and macro factors, as well as investor attention and manager sentiment indexes. Given the role of U.S. Treasury securities in forecasting of output and inflation, as well as in portfolio allocation decisions, our findings have significant implications for investors, policymakers, and researchers interested in accurately the forecasting return dynamics for these assets. © 2020 Elsevier B.V., All rights reserved.","Cepni, O.; Güney, I.E.; Gupta, R.; Wohar, M.E.",2020,10.1016/j.finmar.2020.100541,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079393274&doi=10.1016%2Fj.finmar.2020.100541&partnerID=40&md5=8f38616f6da46e99c620db8243d7aeb2,scopus,"This paper introduces a novel investor sentiment index, constructed using partial least squares (PLS) to remove noise from sentiment proxies, which demonstrates superior predictive power for U.S. government bond risk premia (2-5 year maturities) compared to a PCA-based index. The index's predictability is significant, particularly for shorter maturities, even when accounting for various financial, macro, investor attention, and manager sentiment factors. The findings have implications for forecasting return dynamics of U.S. Treasury securities.",True,True,True,gemini-2.5-flash-lite,Olav,Y,,2025-10-28T11:30:58.064073
4b19ed1da48dc4e8,The role of uncertainty and sentiment for intraday volatility connectedness between oil and financial markets,"We quantify intraday volatility connectedness between oil and key financial assets and assess how it is related to uncertainty and sentiment measures. For that purpose, we integrate the well-known spillover methodology with a TVP VAR model estimated on a unique, vast dataset of roughly 300 thousand 5 min quotations for most heavily traded financial assets: crude oil, the US dollar, S&P 500 index, gold and US treasury bonds. This distinguishes our investigation from previous studies, which usually employ relatively short samples of daily or weekly data and focus on connectedness between two asset classes. We contribute to the literature across three margins. First, we document that market connectedness at intraday frequency presents a different picture on markets co-movement compared to the estimates obtained using daily data. Second, we show that at 5 min frequency volatility is mostly transmitted from the stock market and absorbed by the bond and dollar markets, with oil and gold markets being occasionally important for volatility transmission. Third, we present evidence that daily averages of intraday connectedness measures respond to changes in sentiment and market- specific uncertainty. Interestingly, our results contrast with earlier findings, as they show that connectedness among markets decreases in periods of high volatility owing to market-specific factors. Our study points to the importance of using high-frequency data in order to better understand financial and commodity markets dynamics.","Szafranek, Karol; Rubaszek, Michal; Uddin, Gazi Salah",2024,10.1016/j.eneco.2024.107760,,wos,"This study quantifies intraday volatility connectedness between oil and financial assets (US dollar, S&P 500, gold, US treasury bonds) using a TVP VAR model on 5-minute data. It finds that market connectedness differs from daily estimates, with volatility primarily transmitted from the stock market to bond and dollar markets. Daily connectedness measures are influenced by sentiment and market-specific uncertainty, decreasing during periods of high volatility driven by specific factors. The study emphasizes the value of high-frequency data for understanding market dynamics.",True,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:31:04.752216
f275f02d334fb4b6,The shape of the risk premium: Evidence from a semiparametric generalized autoregressive conditional heteroscedasticity model,"We examine the relationship between the risk premium on the Center for Research on Security Prices (CRSP) value-weighted index total return and its conditional variance. We propose a new serniparametric model in which the conditional variance process is parametric and the conditional mean is an arbitrary function of the conditional variance. For monthly CRSP value-weighted excess returns, the relationship between the two moments that we uncover is nonlinear and nonmonotonic.","Linton, O; Perron, B",2003,10.1198/073500103288619052,,wos,This paper investigates the link between the risk premium of the CRSP value-weighted index total return and its conditional variance using a novel semiparametric GARCH model. The findings indicate a nonlinear and nonmonotonic relationship between these two moments for monthly excess returns.,True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:31:22.459510
1c272d0569feb4e8,The term structure of Japanese interest rates: The equilibrium spread with asymmetric dynamics,"This paper examines the dynamic adjustment to long-run relationship between Japanese interest rates of different maturities. We employ a new estimation methodology that permits threshold and the momentum-threshold adjustment towards equilibrium. The results support the expectations hypothesis of the term structure of interest rate using Japanese interest rates. As in the case of the United States, it shown that the error-correction process is best estimated as asymmetric. © 2003 Elsevier Inc. All rights reserved. © 2017 Elsevier B.V., All rights reserved.","Kuo, S.-H.; Enders, W.",2004,10.1016/s0889-1583(03)00046-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1442355381&doi=10.1016%2FS0889-1583%2803%2900046-7&partnerID=40&md5=c7948c0dbdc2c0c80ba7960123a64018,scopus,"This paper investigates the dynamic relationship between Japanese interest rates of different maturities, utilizing a novel estimation method that incorporates threshold and momentum-threshold adjustments to equilibrium. The findings support the expectations hypothesis of the term structure of interest rates, indicating that the error-correction process is best modeled as asymmetric, similar to observations in the United States.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:31:25.604135
693d8e2872e52e11,The value of options for time charterparty extension: an artificial neural networks (ANN) approach,"The most frequently associated options in the physical shipping market are options to extend the charter period on time charters and additional shipment options on contracts of affreightment. The value of freight options, in practice, is estimated mostly by referring to forward curves. An option on freight has different properties from its financial counterparts, and the straightforward adoption of theoretical models does not produce promising results. In this paper, extension options, which have the property of options on futures, were transformed into regular European options before the application of the Black-Scholes model (BSM). The efficient market hypothesis, which justifies the parity of the performance of a long-term charter to that of repetitive short-term charters, worked as the basis for the transformation. The option values determined by the BSM were compared with actual realized values. Additionally, the artificial neural networks (ANN) was employed to derive the option values. This study is meaningful as the first-time application of both the closed-form solution and the ANN to the valuation of physical freight options. The research results can contribute to the quality of chartering decisions. The results could also be used in quantifying credit risk, as extension options tend to be granted to charterers with more creditability.","Yun, Heesung; Lim, Sangseop; Lee, Kihwan",2018,10.1080/03088839.2017.1392630,,wos,This paper explores the valuation of time charterparty extension options in the shipping market using both the Black-Scholes model and Artificial Neural Networks (ANN). It addresses the limitations of traditional financial models for freight options and proposes a method to transform extension options into European options for BSM application. The study highlights the novelty of using both closed-form solutions and ANN for valuing physical freight options and suggests implications for chartering decisions and credit risk assessment.,True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:31:31.880869
62055b07417c68fc,The value premium and uncertainty: An approach by support vector regression algorithm,"Risk premium plays an important role in stock investing. Experiments have shown that value stocks typically have a higher average return than growth stocks; however, this effect persists indefinitely, even disappearing in some stages. Some studies suggested high volatility in the series of returns, broken structures, market volatility, or the impact of financial crises. This study aimed to build the uncertainty index and control it in the regression analysis model to solve the limitations above. The empirical analysis in Ho Chi Minh Stock Exchange (HOSE) showed that a value premium exists, and value stocks have a higher average return than growth stocks due to the higher overall risk. Furthermore, this study combined the Support Vector Regression (SVR) algorithm with the risk premium theoretical framework for the forecasting model; consequently, it is the most efficient model.",,2023,10.1080/23322039.2023.2191459,,proquest,This study investigates the value premium in the Ho Chi Minh Stock Exchange using Support Vector Regression (SVR). It proposes an uncertainty index to explain the persistence of the value premium and finds that value stocks yield higher returns due to higher risk. The SVR model combined with the risk premium framework is found to be the most efficient for forecasting.,True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:31:39.278135
d715fc9fe9bf9d9c,The volatility of the instantaneous spot interest rate implied by arbitrage pricing-A dynamic Bayesian approach,"This paper considers the estimation of the volatility of the instantaneous short interest rate from a new perspective. Rather than using discretely compounded market rates as a proxy for the instantaneous short rate of interest, we derive a relationship between observed LIBOR rates and certain unobserved instantaneous forward rates. We determine the stochastic dynamics for these rates under the risk-neutral measure and propose a filtering estimation algorithm for a time-discretised version of the resulting interest rate dynamics based on dynamic Bayesian updating in order to estimate the volatility function. Our time discretisation can be justified by the fact that data are observed discretely in time. The method is applied to US Treasury rates of various maturities to compute a (posterior) distribution for the parameters of the volatility specification. © 2006 Elsevier Ltd. All rights reserved. © 2011 Elsevier B.V., All rights reserved.","Bhar, R.; Chiarella, C.; Hung, H.; Runggaldier, W.J.",2006,10.1016/j.automatica.2005.12.027,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745492982&doi=10.1016%2Fj.automatica.2005.12.027&partnerID=40&md5=ec0449cd3c7a17d3e872a04c83968d09,scopus,This paper proposes a dynamic Bayesian approach to estimate the volatility of the instantaneous short interest rate by deriving a relationship between observed LIBOR rates and unobserved instantaneous forward rates. It uses a filtering estimation algorithm for a time-discretised version of the interest rate dynamics and applies the method to US Treasury rates.,True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:31:47.329641
38b1d9a67bf5a236,Third-party Logistics in Bio-medical Waste System: a Path Towards a Risk-free Sector,"After the sudden advent of COVID-19, the amount of medical waste has escalated to a great extent. The incremented medical waste amidst the pandemic exposes the improper waste management system of various developing countries. India, being one of the prominent developing countries, produces the largest waste in the world. Nonetheless, the Indian waste management system is not able to manage the massive amount of waste generated. Henceforth, this research study approaches to reveal the prominent factors which are causing failure in the system of medical waste management in India. This manuscript mainly focuses on two aspects. Firstly, this paper illuminates the factors which are hindering medical waste management by third-party logistics (3PL). Secondly, this study discusses a unique interval-value intuitionistic fuzzy set (IVIFS) based on Decision Making Trial and Evaluation Laboratory (DEMATEL) to depict graphical causal interrelationships among the factors. In addition, the analytic network process (ANP) is utilized to estimate the influence ranking of each factor. The results of this research anticipate that the transportation and disposal-related constraining factors require more attention from 3PL managers. The current study is unique as it enriches the various hindering factors on 3PL BMW management by discussing the ranking and relationship among factors. © 2022 Elsevier B.V., All rights reserved.","Dwivedi, N.; Sharma, H.; Shanker, S.; Barve, A.",2022,10.1007/s41660-022-00259-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131091442&doi=10.1007%2Fs41660-022-00259-x&partnerID=40&md5=a5ee5ac604d47fbacde57222e113ccfe,scopus,"This study investigates the challenges faced by third-party logistics (3PL) in managing bio-medical waste (BMW) in India, particularly in the context of the COVID-19 pandemic. It identifies hindering factors and uses a combination of Decision Making Trial and Evaluation Laboratory (DEMATEL) with interval-value intuitionistic fuzzy sets (IVIFS) and Analytic Network Process (ANP) to analyze causal relationships and rank the influence of these factors. The research highlights the need for greater attention to transportation and disposal-related constraints within the 3PL BMW management system.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:31:49.905141
ded66d2a8d3a91c4,Threshold Dynamics of Short-term Interest Rates: Empirical Evidence and Implications for the Term Structure,"This paper studies a nonlinear one-factor term structure model in discrete time. The short-term interest rate follows a self-exciting threshold autoregressive (SETAR) process that allows for shifts in the intercept and the variance. In comparison with a linear model, we find empirical evidence in favour of the threshold model for Germany and the US. Based on the estimated short-rate dynamics we derive the implied arbitrage-free term structure of interest rates. Since analytical solutions are not feasible, bond prices are computed by means of Monte Carlo integration. The resulting term structure captures stylized facts of the data. In particular, it implies a nonlinear relation between long rates and the short rate.",,2008,10.1111/j.1468-0300.2008.00189.x,,proquest,"This paper investigates a nonlinear term structure model for short-term interest rates using a SETAR process, finding empirical support for this nonlinear approach over linear models in Germany and the US. It derives the implied arbitrage-free term structure and uses Monte Carlo integration to compute bond prices, capturing stylized facts and implying a nonlinear relationship between long and short rates.",True,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:31:59.596216
dba4d319451eec06,Tools for non-linear time series forecasting in economics - An empirical comparison of regime switching vector autoregressive models and recurrent neural networks,"The purpose of this study is to contrast the forecasting performance of two non-linear models, a regime-switching vector autoregressive model (RS-VAR) and a recurrent neural network (RNN), to that of a linear benchmark VAR model. Our specific forecasting experiment is U.K. inflation and we utilize monthly data from 1969 to 2003. The RS-VAR and the RNN perform approximately on par over both monthly and annual forecast horizons. Both non-linear models perform significantly better than the VAR model.","Binner, JM; Elger, T; Nilsson, B; Tepper, JA",2004,10.1016/s0731-9053(04)19003-8,,wos,This study compares the forecasting performance of a regime-switching vector autoregressive model (RS-VAR) and a recurrent neural network (RNN) against a linear benchmark VAR model using UK inflation data from 1969 to 2003. Both non-linear models performed similarly and significantly better than the linear model for both monthly and annual forecast horizons.,True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:32:19.398532
f19ff24e943e287a,Tracking market and non-traditional sources of risks in procyclical and countercyclical hedge fund strategies under extreme scenarios: a nonlinear VAR approach,"The subprime crisis was quite damaging for hedge funds. Using the local projection method (Jordà 2004, 2005, 2009), we forecast the dynamic responses of the betas of hedge fund strategies to macroeconomic and financial shocks-especially volatility and illiquidity shocks-over the subprime crisis in order to investigate their market timing activities. In a robustness check, using TVAR (Balke 2000), we simulate the reaction of hedge fund strategies' betas in extreme scenarios allowing moderate and strong adverse shocks. Our results show that the behavior of hedge fund strategies regarding the monitoring of systematic risk is highly nonlinear in extreme scenarios-especially during the subprime crisis. We find that countercyclical strategies have an investment technology which differs from procyclical ones. During crises, the former seek to capture non-traditional risk premia by deliberately increasing their systematic risk while the later focus more on minimizing risk. Our results suggest that the hedge fund strategies' betas respond more to illiquidity uncertainty than to illiquidity risk during crises. We find that illiquidity and VIX shocks are the major drivers of systemic risk in the hedge fund industry.The subprime crisis was quite damaging for hedge funds. Using the local projection method (Jordà 2004, 2005, 2009), we forecast the dynamic responses of the betas of hedge fund strategies to macroeconomic and financial shocks-especially volatility and illiquidity shocks-over the subprime crisis in order to investigate their market timing activities. In a robustness check, using TVAR (Balke 2000), we simulate the reaction of hedge fund strategies' betas in extreme scenarios allowing moderate and strong adverse shocks. Our results show that the behavior of hedge fund strategies regarding the monitoring of systematic risk is highly nonlinear in extreme scenarios-especially during the subprime crisis. We find that countercyclical strategies have an investment technology which differs from procyclical ones. During crises, the former seek to capture non-traditional risk premia by deliberately increasing their systematic risk while the later focus more on minimizing risk. Our results suggest that the hedge fund strategies' betas respond more to illiquidity uncertainty than to illiquidity risk during crises. We find that illiquidity and VIX shocks are the major drivers of systemic risk in the hedge fund industry.",,2022,10.1186/s40854-021-00316-3,,proquest,"This study uses a nonlinear VAR approach to analyze hedge fund strategies' responses to market and non-traditional risks during extreme scenarios, particularly the subprime crisis. It finds that countercyclical strategies increase systematic risk during crises to capture non-traditional risk premia, while procyclical strategies focus on risk minimization. Illiquidity and VIX shocks are identified as key drivers of systemic risk in the hedge fund industry.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:33:23.156082
82d9f4b0a102aaea,Tractable nonlinear production planning models for semiconductor wafer fabrication facilities,"We describe a simulation study of a production planning model for multistage production inventory systems that reflects the nonlinear relationship between resource utilization and lead time. The model is based on the use of clearing functions that capture the nonlinear relationship between workload and throughput. We show how these clearing functions can be estimated from empirical data using a simulation model as a surrogate for observation of the production system under study. We then examine the sensitivity of the estimated clearing function to different dispatching algorithms, different demand patterns, and production planning techniques. Computational experiments based on a scaled-down model of a semiconductor wafer fabrication facility illustrate the potential benefits of the clearing function model relative to conventional linear programming models.",J. Asmundsson; R. L. Rardin; R. Uzsoy,2006,10.1109/tsm.2005.863214,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1588867,ieeexplore,"This paper presents a simulation study of a production planning model for multistage production inventory systems, focusing on the nonlinear relationship between resource utilization and lead time. It uses clearing functions to model workload and throughput, estimating them from empirical data via simulation. The study examines the sensitivity of these functions to dispatching algorithms, demand patterns, and planning techniques, using a semiconductor fabrication facility model to demonstrate benefits over linear programming models.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:33:27.040360
e33e0490e032325b,Trading the FX volatility risk premium with machine learning and alternative data,"In this study, we show how both machine learning and alternative data can be successfully leveraged to improve and develop trading strategies. Starting from a trading strategy that harvests the EUR/USD volatility risk premium by selling one-week straddles every weekday, we present a machine learning approach to more skillfully time new trades and thus prevent unfavorable ones. To this end, we build probability-calibrated Random Forests on various predictors, extracted from both traditional market data and financial news, to predict the closing Sharpe ratio of short one-week delta-hedged straddles. We then demonstrate how the output of these calibrated machine learning models can be used to engineer intuitive new trading strategies. Ultimately, we show that our proposed strategies outperform the original strategy on risk-based performance measures. Moreover, the features that we derived from financial news articles significantly improve the performance of the approach. © 2022 Elsevier B.V., All rights reserved.","Dierckx, T.; Davis, J.; Schoutens, W.",2022,10.1016/j.jfds.2022.07.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135525638&doi=10.1016%2Fj.jfds.2022.07.001&partnerID=40&md5=7121ebeb12981bacae46150f14854560,scopus,"This study demonstrates the use of machine learning and alternative data to enhance trading strategies for the EUR/USD volatility risk premium. A Random Forest model, using traditional market data and financial news, predicts the Sharpe ratio of short straddles to time trades more effectively. The proposed strategies, particularly those incorporating financial news features, show improved risk-based performance compared to the original strategy.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:33:29.097149
361f9221ee0d7559,Transition densities for interest rate and other nonlinear diffusions,"This paper applies to interest rate models the theoretical method developed in Ait-Sahalia (1998) to generate accurate closed-form approximations to the transition function of an arbitrary diffusion. While the main focus of this paper is on the maximum-likelihood estimation of interest rate models with otherwise unknown transition functions, applications to the valuation of derivative securities are also briefly discussed.","Aït-Sahalia, Y",1999,10.1111/0022-1082.00149,,wos,"This paper adapts a theoretical method to approximate transition functions of diffusions, focusing on interest rate models for maximum-likelihood estimation and derivative valuation.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:33:35.104921
ed3aad09e82d6a77,Twin Picks: Disentangling the Determinants of Risk- Taking in Household Portfolios,"This paper investigates risk-taking in the liquid portfolios held by a large panel of Swedish twins. We document that the portfolio share invested in risky assets is an increasing and concave function of financial wealth, leading to different risk sensitivities across investors. Human capital, which we estimate directly from individual labor income, also affects risk-taking positively, while internal habit and expenditure commitments tend to reduce it. Our microfindings lend strong support to decreasing relative risk aversion and habit formation preferences. Furthermore, heterogeneous risk sensitivities across investors help reconcile individual preferences with representative-agent models.","Calvet, Laurent E.; Sodini, Paolo",2014,10.1111/jofi.12125,,wos,"This study examines risk-taking behavior in household investment portfolios using a large dataset of Swedish twins. It finds that financial wealth, human capital, and habit formation preferences influence risk-taking. The results support theories of decreasing relative risk aversion and habit formation, and suggest that heterogeneous risk sensitivities can bridge individual preferences with representative-agent models.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:33:36.596404
49f77c5799aeb94b,Two-Stage Classification Method for Individual Workout Status Prediction with Machine Learning Approach,"The default risk, one of the main risk factors for bonds, should be measured and reflected in the bond yield. Particularly, in the case of financial companies that treat bonds as a major product, failure to properly identify and filter customers' workout status adversely affects returns. This study proposes a two-stage classification algorithm for workout prediction based on the history data of individual customers such as transaction details of financial companies secured after loans, which is collected over 10 years. The first stage is to rank variables that are closely related to the workout application based on feature selection. In the second step, the first to nth cumulative variables input to each machine learning method generate n candidate classifiers, respectively. Among the total candidates, the model with the highest classification accuracy was selected as the optimal one, which is the Gradient Boost combined with F-score-based feature selection.",,2024,10.1080/15366367.2023.2246109,,proquest,"This study proposes a two-stage classification algorithm using machine learning to predict the workout status of individual customers for financial companies. The method involves feature selection to identify relevant variables and then uses these variables to train multiple classifiers, selecting the best performing one (Gradient Boost with F-score feature selection) for optimal accuracy.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:33:44.818373
deab06de18be4232,U.S. leveraged loan and debt markets: Implications for optimal portfolio and hedging,"This paper offers fresh empirical evidence on the relationship between leverage loans and US debt markets by investigating the distributional predictability and directional predictability between leveraged loans and treasury bonds, fixed income securities and corporate bonds in the U.S economy. We use daily price data from January 2013 to April 2021. First, we analyze the causal relationship between variables by applying non-parametric causality-in-quantiles test and find that quantile causality in variance shows the stronger impact of leverage loan market returns on US debt market returns over the entire quantile range. Second, quantile dependence and directional predictability between leverage loan market and US debt markets are analyzed by applying cross-quantilogram approach and estimated results show the heterogeneous quantile relations from leverage loan market to US debt market. Moreover, the cross-quantile correlation results demonstrate the evidence of negative predictability from leverage loan market to US debt market in low, medium and high quantile range. These evidences are important for US investors and portfolio managers. © 2023 Elsevier B.V., All rights reserved.","Abakah, E.J.A.; Nasreen, S.; Tiwari, A.K.; Lee, C.-C.",2023,10.1016/j.irfa.2023.102514,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148346724&doi=10.1016%2Fj.irfa.2023.102514&partnerID=40&md5=c81900d99e18ad5d1fd4f21f01d0a1fd,scopus,"This paper empirically investigates the relationship between leveraged loans and U.S. debt markets (treasury bonds, fixed income securities, and corporate bonds) from January 2013 to April 2021. Using non-parametric causality-in-quantiles tests and a cross-quantilogram approach, the study finds that leveraged loan market returns have a stronger impact on U.S. debt market returns across quantiles. It also reveals heterogeneous quantile relations and negative predictability from the leveraged loan market to the U.S. debt market, which is significant for investors and portfolio managers.",False,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:34:03.929896
60fc52bc2a9b4fc3,US Funds’ returns-based ESG extraction and implementation: a multifaceted quantile regression approach,"This study introduces a novel ESG intrinsic-based return factor and its application in asset pricing. This factor is extracted using a parallelized rolling window estimation and extreme value-weighted quantile portfolios. It carries a positive risk premium, indicating that investors are willing to assess its risk exposure. We further show that higher returns can be obtained in the top 30% quantiles using a long-only trading strategy. We apply a Monotone Composite Quantile Regression Neural Network (MCQRNN) model to explain US fund returns and address the needs of investors seeking to optimize their investment strategies. This model surpasses traditional benchmark models by performing deep quantile estimation and considering the nonlinear relationships between fund returns and six firm-based characteristics. This approach empowers investors by explaining the core principles of impact investing and highlighting how our constructed ESG risk factor can generate competitive returns even in volatile markets when its risk is well assessed. © 2025 Elsevier B.V., All rights reserved.","Nasri, F.; Ben Sassi, S.B.",2025,10.1080/20430795.2024.2420916,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209061239&doi=10.1080%2F20430795.2024.2420916&partnerID=40&md5=e6baeb1542709665fb74b6cb180af04d,scopus,"This study develops a new ESG intrinsic-based return factor and demonstrates its use in asset pricing. The factor is derived using parallelized rolling window estimation and extreme value-weighted quantile portfolios, showing a positive risk premium. The research also presents a long-only trading strategy that yields higher returns in the top 30% quantiles. A Monotone Composite Quantile Regression Neural Network (MCQRNN) model is employed to explain US fund returns, outperforming traditional benchmarks by considering nonlinear relationships and performing deep quantile estimation. The findings suggest that the constructed ESG risk factor can deliver competitive returns, even in volatile markets, when its risk is properly assessed.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:34:06.898669
ae8fca5c4958de8d,USING NON-PARAMETRIC SEARCH ALGORITHMS TO FORECAST DAILY EXCESS STOCK RETURNS,"Are the learning procedures of genetic algorithms (GAs) able to generate optimal architectures for artificial neural networks (ANNs) in high frequency data? In this experimental study, GAs are used to identify the best architecture for ANNs. Additional learning is undertaken by the ANNs to forecast daily excess stock returns. No ANN architectures were able to outperform a random walk, despite the finding of non-linearity in the excess returns. This failure is attributed to the absence of suitable ANN structures and further implies that researchers need to be cautious when making inferences from ANN results that use high frequency data. © 2004 Elsevier Ltd. All rights reserved. © 2008 Elsevier B.V., All rights reserved.","Joseph, N.L.; Brée, D.S.; Kalyvas, E.",2004,10.1016/s0731-9053(04)19004-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748517846&doi=10.1016%2FS0731-9053%2804%2919004-X&partnerID=40&md5=5097515a501d6e10702723cf743f9dfd,scopus,"This study investigates whether genetic algorithms (GAs) can optimize artificial neural network (ANN) architectures for forecasting daily excess stock returns using high-frequency data. Despite exploring non-linearity in the data, no ANN architecture outperformed a random walk, suggesting limitations in current ANN structures for this task and cautioning against inferences from high-frequency data.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:34:10.086030
bac2a47404767287,USING NONLINEAR METHODS TO SEARCH FOR RISK PREMIA IN CURRENCY FUTURES,"This paper uses currency futures prices to test the joint null hypotheses of rational expectations and absence of a time-varying risk premium in the foreign exchange market. We find no linear predictability in the logarithm of futures price changes, either using its own past or past interest differentials. Also we establish that there is no non-linear predictability in log price changes, conditioning on its own past, or past interest rate differentials. Thus, if a time-varying risk premium exists in currency futures market, it is not related to its own past or past interest rate differentials.","HSIEH, DA",1993,10.1016/0022-1996(93)90007-k,,wos,"This paper investigates the presence of risk premia in currency futures markets using both linear and nonlinear methods. The study finds no evidence of predictability in currency futures price changes, whether linear or nonlinear, based on past price movements or past interest rate differentials. Therefore, if a time-varying risk premium exists, it is not explained by these factors.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:34:13.159993
0a0c2a619cb3d88f,Uncertainty and Forecasts of US Recessions,"We estimate Boosted Regression Trees (BRT) on a sample of monthly data that extends back to 1889 to recover the predictive value of disaggregated news-based uncertainty indexes for U.S recessions. We control for widely-studied standard predictors and use out-of-sample metrics to assess forecast performance. We find that war-related uncertainty is among the top five predictors of recessions at three different forecast horizons (3, 6, and 12 months). The predictive value of war-related uncertainty has fallen in the second half of the 20th century. Uncertainty regarding the state of securities markets has gained in relative importance. The probability of a recession is a nonlinear function of war-related and securities-markets uncertainty. Receiver-operating-characteristic curves show that uncertainty improves out-of-sample forecast performance at the longer forecast horizons. A dynamic version of the BRT approach sheds light on the importance of various lags of government-related uncertainty for recession forecasting at the long forecast horizon.","Pierdzioch, Christian; Gupta, Rangan",2020,10.1515/snde-2018-0083,,wos,"This study uses Boosted Regression Trees (BRT) on monthly US data from 1889 to assess the predictive power of news-based uncertainty indexes for recessions. It finds that war-related uncertainty is a significant predictor, though its importance has waned. Uncertainty in securities markets has become more relevant. The study also notes the nonlinear relationship between these uncertainties and recession probability, and that uncertainty improves out-of-sample recession forecasts at longer horizons.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:34:15.494687
39f83bbe481f5679,Understanding Two Remarkable Findings about Stock Yields and Growth,"Two regularities regarding stock prices and expected inflation have received less attention than they deserve. First, earnings and dividend yields move with long-term expected inflation and risk-free rates. Second, analysts' forecasts of nominal growth, not real growth, vary little with expected inflation. These patterns are remarkable because financial economists predict exactly the opposite. One explanation for these contrary findings is that stock prices are too high (low) when inflation is low (high), because investors confuse nominal and real growth rates. The authors assert that investors are unlikely to be so systematically naive about expected inflation and argue that the contrary evidence is, in fact, consistent with a rational market. The key insight offered by the authors is that reported earnings include inflationary holding gains, which causes higher earnings yields when inflation is high (the first regularity) and, in turn, explains why forecasts of nominal growth need not vary with inflation (the second regularity).","Thomas, Jacob; Zhang, Frank",2009,10.3905/jpm.2009.35.4.158,,wos,"This paper discusses two underappreciated regularities concerning stock prices and expected inflation: earnings and dividend yields correlate with long-term expected inflation and risk-free rates, and analysts' forecasts of nominal growth, not real growth, show little variation with expected inflation. The authors propose that these patterns, contrary to financial economists' predictions, are consistent with a rational market. They explain that reported earnings include inflationary holding gains, which lead to higher earnings yields during high inflation periods, thus explaining the second regularity.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:34:45.434098
553cda2381c2542a,United States banking stability: An explanation through machine learning,"In this paper, an analysis of the prediction of bank stability in the United States from 1990 to 2017 is carried out, using bank solvency, delinquency and an ad hoc bank stability indicator as variables to measure said stability. Different machine learning assembly models have been used in the study, a random forest is developed because it is the most accurate of all those tested. Another novel element of the work is the use of partial dependency graphs (PDP) and individual conditional expectation curves (ICES) to interpret the results that allow observing for specific values how the banking variables vary, when the macro-financial variables vary.It is concluded that the most determining variables to predict bank solvency in the United States are interest rates, specifically the mortgage rate and the 5 and 10-year interest rates of treasury bonds, reducing solvency as these rates increase. For delinquency, the most important variable is the unemployment rate in the forecast. The financial stability index is made up of the normalized difference between the two factors obtained, one for solvency and the other for delinquency. The index prediction concludes that stability worsens as BBB corporate yield increases.",,2020,10.21511/bbs.15(4).2020.12,,proquest,"This study uses machine learning, specifically a random forest model, to predict bank stability in the United States from 1990 to 2017. It analyzes bank solvency, delinquency, and a composite stability indicator, utilizing variables like interest rates, unemployment, and corporate yield. The research also employs partial dependency graphs and individual conditional expectation curves for result interpretation, concluding that higher interest rates negatively impact solvency, unemployment affects delinquency, and increased BBB corporate yield worsens financial stability.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:34:52.691751
09d07317676ce634,Using Markov-switching models with Markov chain Monte Carlo inference methods in agricultural commodities trading,"In this work, the use of Markov-switching GARCH (MS-GARCH) models is tested in an active trading algorithm for corn and soybean future markets. By assuming that a given investor lives in a two-regime world (with low- and high-volatility time periods), a trading algorithm was simulated (from January 2000 to March 2019), which helped the investor to forecast the probability of being in the high-volatility regime att + 1. Once this probability was known, the investor could decide to invest either in commodities, during low-volatility periods or in the 3-month US Treasury bills, during high-volatility periods. Our results suggest that the Gaussian MS-GARCH model is the most appropriate to generate alpha or extra returns (from a passive investment strategy) in the corn market and thet-Student MS-GARCH is the best one for soybean trading.","De la Torre-Torres, Oscar, V; Aguilasocho-Montoya, Dora; Alvarez-Garcia, Jose; Simonetti, Biagio",2020,10.1007/s00500-019-04629-5,,wos,"This study applies Markov-switching GARCH (MS-GARCH) models with Markov chain Monte Carlo inference to an active trading algorithm for corn and soybean futures. The algorithm simulates a two-regime world (low and high volatility) to forecast the probability of being in a high-volatility regime. Based on this forecast, an investor decides between commodities (low volatility) or US Treasury bills (high volatility). The Gaussian MS-GARCH model proved best for corn, while the t-Student MS-GARCH was optimal for soybean trading, suggesting potential for generating alpha.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:35:00.200378
c77bf0ed0e86b54c,Using proxies for the short rate: When are three months like an instant?,"The dynamics of the unobservable short rate are frequently estimated directly using a proxy. We examine the biases resulting From this practice (the proxy problem). Analytic results show that the proxy problem is not economically significant for single-factor affine models. In the two-factor affine model of Longstaff and Schwartz (1992), the proxy problem is only economically significant for pricing discount bonds with maturities of more than five years. We also describe two different numerical procedures for assessing the magnitude of the proxy problem in a general interest rate model. When applied to a nonlinear single-factor model, they suggest that the proxy problem can be economically significant.","Chapman, DA; Long, JB; Pearson, ND",1999,10.1093/rfs/12.4.763,,wos,"This paper investigates the biases that arise when using proxies for the unobservable short rate in estimating interest rate dynamics. It finds that the 'proxy problem' is not economically significant for single-factor affine models but can be significant for two-factor affine models when pricing long-maturity discount bonds. The study also proposes numerical methods to assess the proxy problem's magnitude in general interest rate models, suggesting it can be significant in nonlinear single-factor models.",False,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:35:07.462028
2212fd81e8d3b6aa,Volatility measures and Value-at-Risk,"We evaluate and compare the abilities of the implied volatility and historical volatility models to provide accurate Value-at-Risk forecasts. Our empirical tests on the S&P 500, Dow Jones Industrial Average and Nasdaq 100 indices over long time series of more than 20 years of daily data indicate that an implied volatility based Value-at-Risk cannot beat, and tends to be outperformed by, a simple GJR-GARCH based Value-at-Risk. This finding is robust to the use of the likelihood ratio, the dynamic quantile test or a statistical loss function for evaluating the Value-at-Risk performance.The poor performance of the option based Value-at-Risk is due to the volatility risk premium embedded in implied volatilities. We apply both non-parametric and parametric adjustments to correct for the negative price of the volatility risk. However, although this adjustment is effective in reducing the bias, it still does not allow the implied volatility to outperform the historical volatility models.These results are in contrast to the volatility forecasting literature, which favors implied volatilities over the historical volatility model. We show that forecasting the volatility and forecasting a quantile of the return distribution are two different objectives. While the implied volatility is useful for the earlier objective function, it is not for the latter, due to the non-linear and regime changing dynamics of the volatility risk premium. (C) 2017 International Institute of Forecasters. Published by Elsevier B.V. All rights reserved.","Bams, Dennis; Blanchard, Gildas; Lehnert, Thorsten",2017,10.1016/j.ijforecast.2017.04.004,,wos,"This study compares implied volatility and historical volatility models for Value-at-Risk (VaR) forecasting using S&P 500, Dow Jones Industrial Average, and Nasdaq 100 indices. Results show that GJR-GARCH based VaR outperforms implied volatility based VaR, even after adjustments for volatility risk premium. The authors suggest that while implied volatility is good for volatility forecasting, it's less effective for VaR prediction due to the non-linear dynamics of the volatility risk premium.",False,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:35:09.540098
5e5d1d9ac9087b48,What Does the Individual Option Volatility Smirk Tell Us About Future Equity Returns?,"The shape of the volatility smirk has significant cross-sectional predictive power for future equity returns. Stocks exhibiting the steepest smirks in their traded options underperform stocks with the least pronounced volatility smirks in their options by 10.9% per year on a risk-adjusted basis. This predictability persists for at least 6 months, and firms with the steepest volatility smirks are those experiencing the worst earnings shocks in the following quarter. The results are consistent with the notion that informed traders with negative news prefer to trade out-of-the-money put options, and that the equity market is slow in incorporating the information embedded in volatility smirks.","Xing, Yuhang; Zhang, Xiaoyan; Zhao, Rui",2010,10.1017/s0022109010000220,,wos,"The study finds that the shape of the volatility smirk in stock options predicts future equity returns. Stocks with steeper smirks underperform those with flatter smirks by 10.9% annually on a risk-adjusted basis. This predictability lasts for at least 6 months and is linked to firms experiencing negative earnings shocks, suggesting informed traders use out-of-the-money put options and that markets are slow to incorporate this information.",True,False,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:35:13.297318
960e01a37822bb97,What is the Effect of Restrictions Imposed by Principal Components Analysis on the Empirical Performance of Dynamic Term Structure Models?,"This paper investigates the effect of restrictions imposed by principal components analysis (PCA) on the empirical performance of dynamic term structure models (DTSM). The application of PCA maximizes the explained variance of a linear combination of bond yields by selecting weights that are as uncorrelated as possible. In the context of factor construction, this choice of weights imposes orthogonality on the state variable bringing about restrictions on model estimation. We quantify the effect of these restrictions, measured through internal consistency equations, on the empirical performance of DTSM contained within linear-affine and linear-quadratic state space formulations (SSF) characterized by Gaussian and non-Gaussian transition dynamics for the state variable. When looking across DTSM, we find the smallest effect of restrictions imposed by PCA on empirical performance when it is used in the context of a linear-affine SSF, and the state transition dynamics are postulated based upon the conditional mean and variance. We also document that the magnitude of this effect is more pronounced when the probability distribution for the data is described by the first four moments relative to just the first two and the parameter dependencies defining the relationship between bond yields and factors are more analytically or computationally complicated (e.g., involves a special function like the Bessel function or cumbersome algebraic manipulations). Suggestions for future research are provided. © 2025 Elsevier B.V., All rights reserved.","Juneja, J.",2025,10.1007/s10614-024-10644-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003372195&doi=10.1007%2Fs10614-024-10644-y&partnerID=40&md5=62f9386a35827a9677596c284837a0a1,scopus,"This paper examines how Principal Components Analysis (PCA) restrictions impact the empirical performance of Dynamic Term Structure Models (DTSM). PCA aims to maximize explained variance by selecting uncorrelated weights for bond yields, which imposes orthogonality on state variables and restricts model estimation. The study quantifies these restrictions' effects on DTSMs within linear-affine and linear-quadratic state space formulations, considering Gaussian and non-Gaussian dynamics. The findings suggest that PCA restrictions have the least impact on empirical performance within a linear-affine state space formulation with conditional mean and variance-based transition dynamics. The effect is more significant when the data's probability distribution is described by more moments or when the relationship between bond yields and factors is complex.",True,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:35:21.181615
be1da456434c76af,When there is no place to hide: Correlation risk and the cross-section of hedge fund returns,"Using a novel data set on correlation swaps, we study the relation between correlation risk, hedge fund characteristics, and their risk-return profile. We find that the ability of hedge funds to create market-neutral returns is often associated with a significant exposure to correlation risk, which helps to explain the large abnormal returns found in previous models. We also estimate a significant negative market price of correlation risk, which accounts for the cross-section of hedge fund excess returns. Finally, we detect a pronounced nonlinear relation between correlation risk exposure and the tail risk of hedge fund returns. © 2013 The Author 2013. Published by Oxford University Press on behalf of The Society for Financial Studies. All rights reserved. © 2019 Elsevier B.V., All rights reserved.","Buraschi, A.; Kosowski, R.; Trojani, F.",2014,10.1093/rfs/hht070,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892709151&doi=10.1093%2Frfs%2Fhht070&partnerID=40&md5=3a788d460fa87cfbf5f6abca17b1e105,scopus,"This study investigates the link between correlation risk, hedge fund attributes, and their risk-return profiles using correlation swap data. It reveals that hedge funds' market-neutral returns often stem from substantial correlation risk exposure, explaining prior findings of large abnormal returns. A significant negative market price of correlation risk is identified, contributing to the cross-section of hedge fund excess returns. Additionally, a strong nonlinear relationship between correlation risk exposure and tail risk in hedge fund returns is observed.",False,False,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:35:25.978534
e8e54ffb300f3e5d,"Which exogenous driver is informative in forecasting European carbon volatility: Bond, commodity, stock or uncertainty?","This study relies on 45 exogenous drivers to improve the accuracy in forecasting EUA volatility. Several popular linear and nonlinear predictive regressions, including individual factor analysis, the combination forecast method, the diffusion index model and the supervised learning method, are used to generate volatility forecasts at the monthly frequency. Our empirical results reveal that the diffusion index model and combination forecast method can hardly drive the EUA volatility in a data-rich world owing to worse forecasting performance of individual factors; however, the supervised learning method can successfully predict the EUA volatility. Additionally, the WilderHill new energy global innovation index, Euro corporate bond return spread, GSCI gold index and Euro Area government bond yield spread can extremely drive EUA volatility in terms of individual factor analysis, frequency of variable selection and factor importance. Our findings provide crucial implications to market participants and emission companies, who should pay more attention to the price movement of European bond market, gold and clean energy. © 2023 Elsevier B.V., All rights reserved.","Wang, J.; Guo, X.; Tan, X.; Chevallier, J.; Ma, F.",2023,10.1016/j.eneco.2022.106419,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143371148&doi=10.1016%2Fj.eneco.2022.106419&partnerID=40&md5=a98b91635a2961bde62e843cfeb069b3,scopus,"This study investigates the predictive power of 45 exogenous drivers for European carbon (EUA) volatility using various forecasting models, including linear, nonlinear, diffusion index, and supervised learning methods. The supervised learning approach demonstrates success in predicting EUA volatility. Key drivers identified include the WilderHill new energy global innovation index, Euro corporate bond return spread, GSCI gold index, and Euro Area government bond yield spread. The findings suggest market participants should monitor European bond markets, gold, and clean energy prices.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:35:36.970010
1e13e1a829d5e1b8,Who Values Economist Forecasts? Evidence From Trading in Treasury Markets,"While economic forecasting is ubiquitous within the industry, its role in the trading process has received little attention in the literature. We examine how economist forecasts are related to trading activity in the OTC treasury bond market at the participant level. Consistent with models of heterogeneous opinions, we show that the forecasting economists employing institution places a disproportionately large reliance on the forecast. There is pervasive evidence that this reliance is asymmetric. Only forecasts which imply a fall in future treasury bond prices are associated with an abnormal trading reaction consistent with the forecast. Reference dependence and loss aversion offer one possible explanation for this asymmetric trading response. © 2021 Elsevier B.V., All rights reserved.","James, R.; Jarnecic, E.; Leung, H.",2022,10.1016/j.jfi.2021.100934,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117369955&doi=10.1016%2Fj.jfi.2021.100934&partnerID=40&md5=71ce8126b166106e851aa13340ffe07d,scopus,"This study investigates the relationship between economist forecasts and trading activity in the Over-The-Counter (OTC) treasury bond market at the participant level. It finds that institutions place a disproportionately large reliance on their own forecasts, and this reliance is asymmetric: only forecasts predicting a fall in future treasury bond prices are associated with abnormal trading reactions. This asymmetric response may be explained by reference dependence and loss aversion.",False,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:36:30.118269
b79a2c7acc268fae,Wiener chaos expansion and numerical solutions of the Heath–Jarrow–Morton interest rate model,"In this paper, we propose and analyze a simple and fast numerical method for the solution of the stochastic Heath–Jarrow–Morton (HJM) interest rate model under the Musiela parameterization, based on theWiener chaos expansion (WCE). Through the proposed method, the infinite-dimensional HJM equation is approximated by a finite system of partial differential equations (PDEs), which can be addressed by standard techniques. To illustrate the general construction, we approximate the value of the US treasury bond in an HJM framework, and the results are compared with those derived by the Monte Carlo method and the ensemble Kalman filter. The proposed method is computationally efficient compared with the standard techniques, and it provides a convenient way to compute the statistical moments of the solution numerically. Numerical results and useful formulas for estimating the stochastic duration and immunization are presented. © 2017 Elsevier B.V., All rights reserved.","Kalpinelli, E.A.; Frangos, N.E.; Yannacopoulos, A.",2016,10.21314/jcf.2016.211,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973596637&doi=10.21314%2FJCF.2016.211&partnerID=40&md5=e9ecc7c14a8535386c2c97a43a8082bc,scopus,"This paper introduces a numerical method using Wiener chaos expansion (WCE) to solve the stochastic Heath–Jarrow–Morton (HJM) interest rate model. The method approximates the HJM equation with a finite system of PDEs, allowing for efficient computation of bond values and statistical moments. The approach is demonstrated by valuing US treasury bonds and comparing results with Monte Carlo and ensemble Kalman filter methods. It also provides formulas for stochastic duration and immunization.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:37:05.487407
15912911fb10cffd,Yield curve and recession forecasting in a machine learning framework,"In this paper, we investigate the forecasting ability of the yield curve in terms of the U.S. real GDP cycle. More specifically, within a Machine Learning framework, we use data from a variety of short (treasury bills) and long term interest rates (bonds) for the period from 1976:Q3 to 2011:Q4 in conjunction with the real GDP for the same period, to create a model that can successfully forecast output fluctuations (inflation and output gaps) around its long-run trend. We focus our attention in correctly forecasting the instances of output gaps referred for the purposes of our analysis here as recessions. In this effort, we applied a Support Vector Machines technique for classification. The results show that we can achieve an overall forecasting accuracy of 66.7 and 100_% accuracy in forecasting recessions. These results are compared to the alternative standard logit and probit model, to provide further evidence about the significance of our original model. Reprinted by permission of Springer",,2015,10.1007/s10614-014-9432-0,,proquest,"This paper explores the use of a Machine Learning framework, specifically Support Vector Machines, to forecast U.S. real GDP recessions using yield curve data (short and long-term interest rates) from 1976:Q3 to 2011:Q4. The model achieved a 100% accuracy in forecasting recessions and an overall accuracy of 66.7%, outperforming standard logit and probit models.",True,True,True,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:37:27.965872
b72cfbf62eb061e3,Yield curve in an estimated nonlinear macro model,"This paper estimates a sticky price macro model with US macro and term structure data using Bayesian methods. The model is solved by a nonlinear method. The posterior distribution of the parameters in the model is found to be bi-modal. The degree of nominal rigidity is high at one mode (''sticky price mode'') but is low at the other mode (''flexible price mode''). I find that the degree of nominal rigidity is important for identifying macro shocks that affect the yield curve. When prices are more flexible, a slowly varying inflation target of the central bank is the main driver of the overall level of the yield curve by changing long-run inflation expectations. In contrast, when prices are more sticky, a highly persistent markup shock is the main driver. The posterior probability of each mode is sensitive to the use of observed proxies for inflation expectations. Ignoring additional information from survey data on inflation expectations significantly reduces the posterior probability of the flexible price mode. Incorporating this additional information suggests that yield curve fluctuations can be better understood by focusing on the flexible price mode. Considering nonlinearities of the model solution also increases the posterior probability of the flexible price mode, although to a lesser degree than using survey data information. All rights reserved, Elsevier",,2011,10.1016/j.jedc.2011.03.003,,proquest,"This paper estimates a sticky price macro model using US macro and term structure data, employing Bayesian methods and a nonlinear solution approach. The model's posterior distribution is bi-modal, indicating either high (sticky price) or low (flexible price) nominal rigidity. The degree of nominal rigidity significantly impacts the identification of macro shocks affecting the yield curve. In the flexible price scenario, central bank inflation targets drive the yield curve's level via long-run inflation expectations. In the sticky price scenario, persistent markup shocks are the primary driver. Survey data on inflation expectations and nonlinear model solutions increase the posterior probability of the flexible price mode, suggesting its greater relevance for understanding yield curve fluctuations.",True,True,False,gemini-2.5-flash-lite,Olav,N,,2025-10-28T11:38:01.838855
