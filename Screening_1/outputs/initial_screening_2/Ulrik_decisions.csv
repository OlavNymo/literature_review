paper_id,title,abstract,authors,year,doi,link,database,llm_summary,i1_hint,i2_hint,i3_hint,model,reviewer,decision,notes,timestamp
6a7a6955695693a6,130 years of fiscal vulnerabilities and currency crashes in advanced economies,"This paper investigates the empirical link between fiscal vulnerabilities and currency crashes in advanced economies over the last 130 years, building on a new data set of real effective exchange rates andfiscal balances for 21 countries since 1880. The paper finds evidence that crashes depend more on prospective fiscal deficits than on actual ones, and more on the composition of public debt (that is, rollover/sudden stop risk) than on its level. The paper also uncovers significant nonlinear effects at high levels of public debt as well as significantly negative risk premiums for major reserve currencies, which enjoy a lower probability of currency crash than other currencies ceteris paribus. Yet, the estimates indicate that such premiums remain small in size relative to the conditional probability of a currency crash if prospective fiscal deficits or rollover/sudden stop risk are high. © 2011 International Monetary Fund. © 2016 Elsevier B.V., All rights reserved.","Fratzscher, M.; Mehl, A.; Vansteenkiste, I.",2011,10.1057/imfer.2011.23,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984735363&doi=10.1057%2Fimfer.2011.23&partnerID=40&md5=10412f6505b457691d3ab1474055c9fc,scopus,"This paper examines the relationship between fiscal vulnerabilities and currency crashes in advanced economies over 130 years using a new dataset. It finds that prospective fiscal deficits and the composition of public debt (rollover/sudden stop risk) are more influential than actual deficits or debt levels. The study also notes nonlinear effects at high debt levels and smaller risk premiums for major reserve currencies, though these premiums are insufficient to mitigate crash probability when fiscal risks are high.",False,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:28:43.973676
adc084ce37ecd5d8,A Bayesian Approach for Dynamic Variation of Specific Sectors in Stock Exchange: A Case Study of Stock Exchange Thailand (SET) Indexes,"This paper aims to investigate the identification of sectors of stock exchange that were positively or negatively driven by fundamental monetary tools.A Bayesian approach for dynamic model averaging (DMA) algorithm is analyzed and implemented to deliver the results for identifying what actually drives specific sector of stock exchange, where the posterior inclusion probability is crucial quantity for this investigation. Stock Exchange Thailand (SET) indexes was used as a case study for this work. The predictors used in this paper are: borrowing rate: BR, policy rate: PR, treasury bill rate: TBR, government bond yield: GBYLT, minimum overdraft rate: MOR, minimum loan rate: MLR, minimum retail rate: MRR, discount rate: DISR, savings rate: SAVER, deposit rate: DEPR and lending rate: LENDR. These factors are considered as the most important monetary policy tools from Bank of Thailand. The empirical results demonstrated that each SET index sector responses to those economic variables differently. Some of them are not actually related for specific point of time; some other times, however, they affect SET indexes. Filtered time-varying parameters allow us to indicate the relationship between stock price return and interest rates. According to statistical evidence, we found that the relationship is as of the time-varying characteristic and is unable to absolute identify whether it is positively or negatively related to stock price return for each sector. The impact of a sector on SET indexes depends on a period of time and can also be considered by using seasoning parameters. © 2020 Elsevier B.V., All rights reserved.","Taveeapiradeecharoen, P.; Aunsri, N.",2020,10.1007/s11277-020-07217-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082804266&doi=10.1007%2Fs11277-020-07217-1&partnerID=40&md5=81cc5fcbff5e37f99c07219bc7a0a7d3,scopus,"This paper uses a Bayesian dynamic model averaging (DMA) algorithm to identify how monetary policy tools affect different sectors of the Stock Exchange of Thailand (SET). The study analyzes the relationship between various interest rates and SET index sectors, finding that these relationships are time-varying and sector-specific. The DMA approach, particularly posterior inclusion probability, is used to determine the influence of predictors.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:28:46.876107
7887726f5a97f696,A Bayesian approach to term structure modeling using heavy-tailed distributions,"In this paper, we introduce a robust extension of the three-factor model of Diebold and Li (J. Econometrics, 130: 337-364, 2006) using the class of symmetric scale mixtures of normal distributions. Specific distributions examined include the multivariate normal, Student-t, slash, and variance gamma distributions. In the presence of non-normality in the data, these distributions provide an appealing robust alternative to the routine use of the normal distribution. Using a Bayesian paradigm, we developed an efficient MCMC algorithm for parameter estimation. Moreover, the mixing parameters obtained as a by-product of the scale mixture representation can be used to identify outliers. Our results reveal that the Diebold-Li models based on the Student-t and slash distributions provide significant improvement in in-sample fit and out-of-sample forecast to the US yield data than the usual normal-based model. Copyright © 2011 John Wiley & Sons, Ltd. © 2012 Elsevier B.V., All rights reserved.","Abanto-Valle, C.A.; Lachos, V.H.; Ghosh, P.",2012,10.1002/asmb.920,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867581627&doi=10.1002%2Fasmb.920&partnerID=40&md5=d0718ed07907681fd60bc5e4fb51dca0,scopus,"This paper proposes a robust extension of the Diebold and Li three-factor model for term structure modeling by incorporating heavy-tailed distributions (multivariate normal, Student-t, slash, and variance gamma). A Bayesian approach with an MCMC algorithm is used for parameter estimation, and mixing parameters help identify outliers. The study demonstrates that models using Student-t and slash distributions offer improved in-sample fit and out-of-sample forecasts for US yield data compared to the standard normal-based model.",True,True,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T09:28:48.692496
512099f188d82d66,"A Network and Machine Learning Approach to Factor, Asset, and Blended Allocation","The main idea of this article is to approach and compare factor and asset allocation portÂfolios using both traditional and alternative allocation techniques: Inverse variance optimization, minimum-variance optimization, and centrality-based techniques from network science. Analysis of the interconnectedÂness between assets and factors shows that their relaÂtionship is strong. The authors compare the allocation techniques, considering centrality and hierarchal-based networks. They demonstrate the advantages of graph theory to explain the advantages to portfolio manageÂment and the dynamic nature of assets and factors with their ""importance score."" They find that asset allocaÂtion can be efficiently derived using directed networks, dynamically driven by both US Treasuries and curÂrency returns with significant centrality scores. AlterÂnatively, the inverse variance weight estimation and correlation-based networks generate factor allocation with favorable risk-return parameters. Furthermore, factor allocation is driven mostly by the importance scores of the Fama-French-Carhart factors: SMB, HML, CMA, RMW, and MOM. The authors confirm previous results and argue that both factors and assets are interconnected with different value and momentum factors. Therefore, a blended strategy comprising factors and assets can be defensible for investors. As argued in previous research, factors are much more overcrowded than assets. Therefore, the centrality scores help to identify the crowded expoÂsure and build diversified allocation. The authors run LASSO regressions and show how the network-based allocation can be implemented using machine learning. © 2020 Elsevier B.V., All rights reserved.","Konstantinov, G.; Chorus, A.; Rebmann, J.",2020,10.3905/jpm.2020.1.147,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088367857&doi=10.3905%2Fjpm.2020.1.147&partnerID=40&md5=6e1dee8ae9c04309ccd1b8d63a1dbd71,scopus,"This article proposes a network and machine learning approach to compare factor and asset allocation portfolios using traditional and alternative techniques. It analyzes the interconnectedness between assets and factors, demonstrating the advantages of graph theory for portfolio management and identifying key drivers for asset and factor allocation. The study suggests a blended strategy of factors and assets can be beneficial, using machine learning (LASSO regressions) to implement network-based allocation.",True,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:29:09.031555
3c7ed3cc4ae5de19,A Neural Network Architecture for Maximizing Alpha in a Market Timing Investment Strategy,"In finance, assuming more risk often corresponds to the expectation of higher, compensating returns. In this setting, alpha stands out as one of the most prevalent and refined measures of risk-adjusted return ever postulated, allowing for the estimation of the excess return that cannot be explained by the risk factors impacting an asset. This article introduces a neural network architecture designed to formulate an investment strategy with the explicit goal of maximizing alpha. The strategy, centered around market timing, determines on a daily basis, based on past returns of the risky asset, whether to fully invest in the risky asset or opt for the risk-free alternative. The neural network architecture comprises two components: a policy network for strategy implementation and an evaluation network for long-term alpha computation during parameter optimization. Employing value-weighted US size decile portfolios as risky assets, the study achieves significant out-of-sample alphas ranging from 3.6% to 8.2% per year under the  $q^{5}$  asset pricing model (with a transaction cost assumption of one basis point). By construction, these alphas are not generated by risky asset growth. Robustness tests yield similar results with equal-weighted decile portfolios or under the Fama and French six-factor asset pricing model. Variations in transaction cost, number of past returns used as inputs, policy network design, or training sample size produce similar outcomes. This study underscores the effectiveness of reinforcement learning-inspired techniques in uncovering alpha in financial markets.",J. H. Ospina-Holguín; A. M. Padilla-Ospina,2024,10.1109/access.2024.3446708,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10640102,ieeexplore,"This article presents a neural network architecture for a market timing investment strategy aimed at maximizing alpha. The strategy uses a policy network and an evaluation network to decide daily between investing in a risky asset or a risk-free alternative, based on past returns. The study achieved significant out-of-sample alphas (3.6%-8.2% annually) using US size decile portfolios and the $q^{5}$ asset pricing model, with robustness checks confirming these results across different portfolio types, factor models, and parameter variations. The findings highlight the potential of reinforcement learning-inspired methods for alpha generation.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:29:14.283542
88f2b0ee6b8ed45b,A New Credit Spread to Predict Economic Activities in China,"In recent years, the relationship between bond spreads and macro economy has been studied extensively by economists in western countries. However, few attentions were paid on this topic in China. This essay regards Chinese bond market as a complex system and constructs bond indices for China with the bottom-up approach. The authors use the data of 3,205 non-financial corporate bonds from February 2010 to October 2017 and construct a new spread noted as the PE_SOE spread. The authors find that the PE_SOE spread has a negative impact on economic activities and has the best predictive ability at short-run forecasting horizons, owing to the institutional superiority of the state-owned enterprises in China. The Treasury bond yields are found to have the best predictive ability at long-run horizons. Both spread shock and Treasury yield shock could lead to deflation and declines in economic activities, and the Treasury yield shock has a more severe and persistent impact on the economy due to the financial accelerator mechanism. PE_SOE spread is proved to be a better indicator for Chinese corporate bond market and can be widely used not only in future Chinese economic studies, but also for Chinese government’s macroeconomic monitoring and warning.",,2019,10.1007/s11424-019-8033-3,,proquest,"This study constructs a new credit spread, the PE_SOE spread, using data from 3,205 non-financial corporate bonds in China from February 2010 to October 2017. The PE_SOE spread negatively impacts economic activities and is a strong short-run predictor, while Treasury bond yields are better long-run predictors. Both spread and Treasury yield shocks can lead to deflation and economic decline, with Treasury yield shocks having a more severe and persistent impact. The PE_SOE spread is identified as a superior indicator for the Chinese corporate bond market and useful for macroeconomic monitoring.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:29:16.997242
d67f32ecdbb90051,A Structural Credit Risk Model with Jumps Based on Uncertainty Theory,"This study, within the framework of uncertainty theory, employs an uncertain differential equation with jumps to model the asset value process of a company, establishing a structured model of uncertain credit risk that incorporates jumps. This model is applied to the pricing of two types of credit derivatives, yielding pricing formulas for corporate zero-coupon bonds and Credit Default Swap (CDS). Through numerical analysis, we examine the impact of asset value volatility and jump magnitude on corporate default uncertainty, as well as the influence of jump magnitude on the pricing of zero-coupon bonds and CDS. The results indicate that an increase in volatility levels significantly enhances default uncertainty, and an expansion in the magnitude of negative jumps not only directly elevates default risk but also leads to a significant increase in the value of zero-coupon bonds and the price of CDS through a risk premium adjustment mechanism. Therefore, when assessing corporate default risk and pricing credit derivatives, the disturbance of asset value jumps must be considered a crucial factor.",,2025,10.3390/math13060897,,proquest,This study develops a structural credit risk model using uncertainty theory and uncertain differential equations with jumps to price corporate zero-coupon bonds and Credit Default Swaps (CDS). Numerical analysis shows that asset value volatility and negative jumps increase default uncertainty and impact derivative pricing. The study emphasizes considering asset value jumps in default risk assessment and derivative pricing.,False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:29:20.453604
84a48054af586950,A Study of Stock Market Predictability Based on Financial Time Series Models,"In today’s era of economic globalization and financial integration, the stock market is constantly complex, showing many deviations that cannot be explained by classical financial analysis, but at the same time, some classic financial statistical features have striking similarities. This suggests that although the stock market is intricate, there are universal laws that can be found through data mining to find its underlying operating rules. In this paper, we construct financial time series models such as ARIMA, ARCH, and GARCH to predict the stock market price fluctuations and trends. The ARIMA model is used to fit the linear financial time series, and the GARCH model is used to fit the nonlinear time series residuals. The results show that the integrated tree model based on the idea of weight voting has high accuracy in predicting stock market bulls and bears, with XGBoost prediction accuracy up to 96%, and the neural network model is also very effective, with an accuracy rate of over 90%.",,2022,10.1155/2022/8077277,,proquest,"This paper explores stock market predictability using financial time series models like ARIMA, ARCH, and GARCH. It applies these models, including XGBoost and neural networks, to predict price fluctuations and trends, achieving high accuracy rates.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:29:23.493749
4229103779a54ec5,A TVM-Copula-MIDAS-GARCH model with applications to VaR-based portfolio selection,"This paper develops a novel time-varying multivariate Copula-MIDAS-GARCH (TVM-Copula-MIDAS-GARCH) model with exogenous explanatory variables to model the joint distribution of returns. The model accounts for mixed frequency factors that affect the time-varying dependence structure of financial assets. Furthermore, we examine the effectiveness of the proposed model in VaR-based portfolio selection. We conduct an empirical analysis on estimating the 90%, 95%, 99% VaRs of the portfolio constituted of the Shanghai Composite Index, Shanghai SE Fund Index, and Shanghai SE Treasury Bond Index. The empirical results show that the proposed TVM-Copula-MIDAS-GARCH model is effective to investigate the nonlinear time-varying dependence among those three indices and performs better in portfolio selection. © 2020 Elsevier B.V., All rights reserved.","Jiang, C.; Ding, X.; Xu, Q.; Tong, Y.",2020,10.1016/j.najef.2019.101074,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072602228&doi=10.1016%2Fj.najef.2019.101074&partnerID=40&md5=73863b38a88325f20a4b8f7eddc0f364,scopus,"This paper introduces a new TVM-Copula-MIDAS-GARCH model to capture the time-varying dependence structure of financial assets using mixed-frequency factors. The model's effectiveness is demonstrated through an empirical analysis of VaR-based portfolio selection using the Shanghai Composite Index, Shanghai SE Fund Index, and Shanghai SE Treasury Bond Index, showing improved performance in nonlinear dependence investigation and portfolio selection.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:29:39.685857
23dde43cefbb3d8b,A comparison of linear regression and neural network methods for predicting excess returns on large stocks,"Recent studies have shown that there is predictable variation in returns of financial assets over time. We investigate whether the predictive power of the economic and financial variables employed in the above studies can be enhanced if the statistical method of linear regression is replaced by feedforward neural networks with backpropagation of error. A shortcoming of backpropagation networks is that too many free parameters allow the neural network to fit the training data arbitrarily closely resulting in an ""overfitted"" network. Overfitted networks have poor generalization capabilities. We explore two methods that attempt to overcome this shortcoming by reducing the complexity of the network. The results of our experiments confirm that an ""overfitted"" network, while making better predictions for within-sample data, makes poor predictions for out-of-sample data. The methods for reducing the complexity of the network, explored in this paper, clearly help improve out-of-sample forecasts. We show that one cannot say that the linear regression forecasts are conditionally efficient with respect to the neural networks forecasts with any degree of confidence. However, one can say that the neural networks forecasts are conditionally efficient with respect to the linear regression forecasts with some confidence. © 2020 Elsevier B.V., All rights reserved.","Desai, V.S.; Bharati, R.",1998,10.1023/a:1018993831870,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032376229&doi=10.1023%2Fa%3A1018993831870&partnerID=40&md5=9522b8d59588be335e6542c062a56e6f,scopus,"This study compares linear regression and neural networks for predicting excess stock returns, finding that while neural networks can overfit, methods to reduce complexity improve out-of-sample forecasts. Neural network forecasts are found to be conditionally efficient with respect to linear regression forecasts.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:29:43.832763
bb18143d023a488b,A dynamic asset allocation approach under technical analysis and machine learning classification,"This research evaluates the effectiveness of machine learning models by comparing the performance of seven models in predicting the return movement of four asset types over different return periods. The assets include S&P 500 Futures, Gold Futures, US 10Y Treasury Bond Futures, and a risk-free investment. The study considers 1-week, 4-week, and 12-week forward return periods. The analysis assesses model accuracy and portfolio performance using data spanning two decades. ML-based portfolios modestly outperform an equal-weighted benchmark in terms of risk-adjusted metrics over longer horizons, even though the differences are generally insignificant. However, the study emphasizes the importance of considering diverse economic cycles to understand asset behaviour comprehensively. Nevertheless, the study did not find significant evidence that combining machine learning prediction with dynamic asset allocation can outperform passive portfolio investment. The study underscores the potential of non-linear models in short-term trading but also notes limitations in adaptiveness, threshold design, and model generalization under atypical conditions. © 2025 Elsevier B.V., All rights reserved.","Luc, G.B.; Wu, C.-C.",2025,10.1080/00036846.2025.2526177,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010230429&doi=10.1080%2F00036846.2025.2526177&partnerID=40&md5=1bfee009edd82f459adff74af01afe83,scopus,"This research evaluates seven machine learning models for predicting asset returns (S&P 500 Futures, Gold Futures, US 10Y Treasury Bond Futures, risk-free investment) over 1, 4, and 12-week periods. While ML-based portfolios showed modest outperformance over an equal-weighted benchmark in risk-adjusted metrics for longer horizons, the differences were generally insignificant. The study found no significant evidence that combining ML prediction with dynamic asset allocation outperforms passive investment, highlighting the potential of non-linear models for short-term trading but noting limitations in adaptiveness and generalization.",True,True,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T09:29:59.140887
58faaf67a8d7f201,A framework to measure integrated risk,A framework underlying various models that measure the credit risk of a portfolio is extended in this paper to allow the integration of credit risk with a range of market risks using Monte Carlo simulation. A structural model is proposed that allows interest rates to be stochastic and provides closed-form expressions for the market value of a firm's equity and its probability of default. This model is embedded within the integrated framework and the general approach illustrated by measuring the risk of a foreign exchange forward when there is a significant probability of default by the counterparty. For this example moving from a market risk calculation to an integrated risk calculation reduces the expected future value of the instrument by an amount that Could not be calculated using the common pre-settlement exposure technique for estimating the credit risk of a derivative.,"Medova, EA; Smith, RG",2005,10.1080/14697680500117583,,wos,"This paper extends a framework for measuring credit risk to integrate it with market risks using Monte Carlo simulation. It proposes a structural model with stochastic interest rates, providing closed-form expressions for equity value and default probability. The approach is illustrated by measuring the risk of a foreign exchange forward with counterparty default risk, showing that integrated risk calculation significantly reduces the expected future value compared to standard pre-settlement exposure techniques.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:30:02.448745
8cbc033b8fc4ec78,A genetic algorithm estimation of the term structure of interest rates,"The term structure of interest rates is a key instrument for financial research. It provides relevant information for pricing deterministic financial cash flows, it measures economic market expectations and it is extremely useful when assessing the effectiveness of monetary policy decisions. However, it is not directly observable and needs to be estimated by smoothing asset pricing data through statistical techniques. The most popular techniques adjust parsimonious functional forms based on bond yields to maturity. Unfortunately, these functions, which need to be optimised, are highly non-linear which make them very sensitive to the initial conditions. In this context, this paper proposes the use of genetic algorithms to find the values for the initial conditions and to reduce the risk of false convergence, showing that stable parameters are obtained without imposing arbitrary restrictions.",,2009,10.1016/j.csda.2008.10.030,,proquest,"This paper proposes using genetic algorithms to estimate the term structure of interest rates, addressing the challenges of non-linear functional forms and sensitivity to initial conditions in traditional methods. The approach aims to find optimal initial conditions and reduce the risk of false convergence, leading to stable parameter estimates without arbitrary restrictions.",True,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:30:23.135185
b5b17374b22243de,A hybrid approach for portfolio construction: Combing two‐stage ensemble forecasting model with portfolio optimization,"Combining the stock prediction with portfolio optimization can improve the performance of the portfolio construction. In this article, we propose a novel portfolio construction approach by utilizing a two‐stage ensemble model to forecast stock prices and combining the forecasting results with the portfolio optimization. To be specific, there are two phases in the approach: stock prediction and portfolio optimization. The stock prediction has two stages. In the first stage, three neural networks, that is, multilayer perceptron (MLP), gated recurrent unit (GRU), and long short‐term memory (LSTM) are used to integrate the forecasting results of four individual models, that is, LSTM, GRU, deep multilayer perceptron (DMLP), and random forest (RF). In the second stage, the time‐varying weight ordinary least square model (OLS) is utilized to combine the first‐stage forecasting results to obtain the ultimate forecasting results, and then the stocks having a better potential return on investment are chosen. In the portfolio optimization, a diversified mean‐variance with forecasting model named DMVF is proposed, in which an average predictive error term is considered to obtain excess returns, and a 2‐norm cost function is introduced to diversify the portfolio. Using the historical data from the Shanghai stock exchange as the study sample, the results of the experiments indicate the DMVF model with two‐stage ensemble prediction outperforms benchmarks in terms of return and return‐risk characteristics.",,2024,10.1111/coin.12617,,proquest,"This paper proposes a hybrid approach for portfolio construction that combines a two-stage ensemble forecasting model with portfolio optimization. The forecasting model uses neural networks (MLP, GRU, LSTM) and other models (DMLP, RF) in the first stage, and time-varying weighted OLS in the second stage. The portfolio optimization uses a diversified mean-variance model (DMVF) that considers forecasting errors and a 2-norm cost function for diversification. Experiments on Shanghai stock exchange data show the proposed model outperforms benchmarks.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:30:25.647171
28c83a8bcd39edfd,A hybrid model integrating artificial neural network with multiple GARCH-type models and EWMA for performing the optimal volatility forecasting of market risk factors,"The 2008 financial crisis has highlighted the lack of precision in the market risk metrics that financial institutions must report to the regulator. The use of Machine Learning techniques in stock markets and the treasury (Front–Back Office) of financial institutions is a key tool for optimizing its own resources, internal processes and risk measures. We propose a hybrid methodology to better capture the volatility of market risk factors with Value-at-risk models in periods of stress, but also in periods of stability compared to traditional metrics. This hybrid model uses different types of artificial neural networks and traditional metrics to perform the optimal forecast of volatility applied to the main market risk nodes of the Spanish stock market. We use data from the following main market risk factors: returns on Santander Bank shares, the Spanish Stock Market Index, Euro/Dollar exchange rates and the index that measures the total return performance of a funded long credit position in the on-the-run iTraxx Crossover 5-Year-Index. Our contribution is a hybrid model that combines correct sequential pattern learnings with an improved prediction performance in the volatility of market risk factors. Our findings show that the Support Vector Machine and the Long-Short-Term Memory Model present better prediction results in all factors in the stability periods. Therefore, the proposed method is promising for application in risk management systems. © 2023 Elsevier B.V., All rights reserved.","Pérez-Hernández, F.; Arévalo-De-Pablos, A.; Camacho-Miñano, M.-D.-M.",2024,10.1016/j.eswa.2023.122896,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180444826&doi=10.1016%2Fj.eswa.2023.122896&partnerID=40&md5=35a9237d6bbd45c16c7e293c52c5f62e,scopus,"This paper proposes a hybrid model combining artificial neural networks (specifically Support Vector Machine and Long-Short-Term Memory) with traditional GARCH-type models and EWMA for improved volatility forecasting of market risk factors. The model aims to enhance Value-at-Risk calculations, particularly during market stress. Empirical results on Spanish stock market data (Santander Bank shares, IBEX 35 index, EUR/USD exchange rate, and iTraxx Crossover index) indicate that the hybrid model offers better prediction performance, especially in stable market periods.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:30:27.356775
7e93c19fba02abbf,A measure of Turkey's sovereign and banking sector credit risk: Asset swap spreads,"The existence of the credit derivatives written on the eurobonds such as credit default swaps or asset swaps allows policymakers and investors to monitor the evolvement of credit risk. However, these instruments are mostly available in advanced economies, whereas the market for credit derivatives in emerging market countries, including Turkey, is limited in terms of liquidity and maturity. In this regard, this study aims to construct a proxy for the credit risk of the Turkish Treasury and banking sector in international markets by calculating asset swap spread for US dollar-denominated fixed coupon eurobonds, which requires a robust estimation of the relevant yield curves. The study firstly presents the estimation of the sovereign and banking sector yield curves and then constructs a synthetic asset swap structure to obtain embedded credit risk premia in the eurobond curves. Our findings show that the proposed credit risk indicator is vastly correlated with credit default swap premium. In addition to this, estimated eurobond curves are also useful for monitoring borrowing cost dynamics of the Turkish Treasury and banking sector in international markets. (C) 2021 Production and hosting by Elsevier B.V. on behalf of Central Bank of The Republic of Turkey.","Kucuksarac, Doruk; Kazdal, Abdullah; Korkmaz, Halil Ibrahim; Onay, Yigit",2021,10.1016/j.cbrev.2021.05.001,,wos,This study constructs a proxy for the credit risk of the Turkish Treasury and banking sector by calculating asset swap spreads for US dollar-denominated eurobonds. It estimates sovereign and banking sector yield curves and uses a synthetic asset swap structure to derive credit risk premia. The findings indicate a strong correlation with credit default swap premiums and suggest the yield curves can monitor borrowing costs.,False,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:30:29.148813
ead79b7f6088813e,A nonparametric data mining approach for risk prediction in car insurance: A case study from the Montenegrin market,"For prediction of risk in car insurance we used the nonparametric data mining techniques such as clustering, support vector regression (SVR) and kernel logistic regression (KLR). The goal of these techniques is to classify risk and predict claim size based on data, thus helping the insurer to assess the risk and calculate actual premiums. We proved that used data mining techniques can predict claim sizes and their occurrence, based on the case study data, with better accuracy than the standard methods. This represents the basis for calculation of net risk premium. Also, the article discusses advantages of data mining methods compared to standard methods for risk assessment in car insurance, as well as the specificities of the obtained results due to small insurance market, such as Montenegrin. © 2017 Elsevier B.V., All rights reserved.","Kašćelan, V.; Kašćelan, L.; Novovic Buric, M.N.",2016,10.1080/1331677x.2016.1175729,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009997041&doi=10.1080%2F1331677X.2016.1175729&partnerID=40&md5=7f3a8611352e0c53d21f363d9b290835,scopus,"This study applies nonparametric data mining techniques, including clustering, Support Vector Regression (SVR), and Kernel Logistic Regression (KLR), to predict risk and claim size in car insurance. The methods demonstrated higher accuracy than standard approaches, providing a basis for net risk premium calculation. The article also discusses the advantages of data mining over traditional methods and the impact of a small market like Montenegro on the results.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:32:40.425718
c3bd093b2f2483fb,"A note on the impact of the United States federal budget deficit on the intermediate-term interest rate, 1960-94","There is an extensive literature investigating the impact of federal budget deficits in the US on interest rate yields. This literature focuses almost entirely on short-term rates (under one year to maturity) and long-term rates (ten years or more to maturity). However, almost no attention has been directed at intermediate-term interest rate yields, that is yields on bonds maturing more than one year and less than ten years in the future. Also, this literature essentially ignores the second half of the 1980s and the 1990s. Accordingly, this note empirically investigates the impact of budget deficits in the US on the interest rate yield on three-year US Treasury notes for the period 1960 through the end of 1994. Based on OLS and IV estimates, two of which include net international capital inflows, it is found that budget deficits do raise intermediate-term rates, which may strengthen arguments that budget deficits lead to crowding out. © 2017 Elsevier B.V., All rights reserved.","Cebula, R.J.",1997,10.1080/758530655,https://www.scopus.com/inward/record.uri?eid=2-s2.0-5844313200&doi=10.1080%2F758530655&partnerID=40&md5=c8a1fd58caaa68d76aef4209a45c7f92,scopus,"This note empirically investigates the impact of US federal budget deficits on intermediate-term interest rates (specifically, three-year US Treasury notes) from 1960 to 1994. Using OLS and IV estimates, the study finds that budget deficits tend to increase intermediate-term rates, potentially supporting the crowding-out hypothesis. The research highlights a gap in existing literature that primarily focused on short-term and long-term rates, and largely ignored the latter half of the 1980s and the 1990s.",False,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:40:16.798633
08245f9f6de44032,"A novel, rule-based technical pattern identification mechanism: Identifying and evaluating saucers and resistant levels in the US stock market","This paper has two main purposes. The first one is the development of a rigorous rule-based mechanism for identifying the rounding bottoms (also known as saucers) pattern and resistant levels. The design of this model is based solely on principles of technical analysis, and thus making it a proper system for evaluating the efficacy of the aforementioned technical trading patterns. The second aim of this paper is measuring the predictive power of buy-signals generated by these technical patterns. Empirical results obtained from seven US tech stocks indicate that simple resistant levels outperform saucers patterns. Furthermore, positive statistical significant excess returns are being generated only in first sub-periods of examination. These returns decline or even vanish as the experiment proceeds to recent years. Our findings are aligned with the results reported by various former studies. The proposed identification mechanism can be used as a component of an expert system to assist academic community in evaluating trading strategies where technical patterns are embedded. (C) 2011 Elsevier Ltd. All rights reserved.","Zapranis, Achilleas; Tsinaslanidis, Prodromos E.",2012,10.1016/j.eswa.2011.11.079,,wos,"This paper develops a rule-based mechanism to identify and evaluate 'saucers' and 'resistant levels' in the US stock market, based on technical analysis principles. It then measures the predictive power of buy-signals from these patterns. Empirical results on seven US tech stocks show that resistant levels outperform saucers, and significant excess returns are observed only in earlier periods, diminishing over time. The authors suggest their mechanism can aid in evaluating trading strategies involving technical patterns.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:40:23.278722
5c800f91626cad4f,A production function analysis of commercial dairy farms in the Highlands of Eritrea using ridge regression,"This study presents a production function analysis of fresh milk production in the Highlands of Eritrea, where most dairy farmers in Eritrea are located. To ensure representative production functions, this region was divided into three relatively homogenous study areas, namely Central Zone, Mendefera and Dekemhare. Most data for the study were collected in a survey of 120 respondents using a structured questionnaire. To obviate the problem of multicollinearity among explanatory variables, ridge regression was used to estimate milk production functions for each study area. Production elasticities of variable inputs, marginal products (MPx), values of marginal products (VMPx), marginal rates of input substitution (MRS) and leastcost combinations of purchased concentrates and forage were estimated for the three regions. The VMPs of all inputs for Central Zone dairy farmer respondents were estimated to be greater than their input prices, implying that the resources were under-utilized from a profit-maximising perspective (i.e. where VMPx = Px). However, respondents in Mendefera and Dekemhare used concentrates in excess of optimum levels (i.e. VMPx < Px). Analysis of the least-cost combination of purchased concentrates and forage suggests that dairy farmer respondents were also not allocating these resources on a minimum-cost basis. However, the profit maximizing and least-cost criteria assume perfect knowledge, a risk-free environment and competitive markets. Improved information, farmer training and better infrastructure (roads and telecommunications) to promote competitive markets could help to enhance resource allocation decisions by dairy producers. © 2013 Elsevier B.V., All rights reserved.","Ghebremariam, W.K.; Ortmann, G.F.; Nsahlai, I.V.",2006,10.1080/03031853.2006.9523745,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887173439&doi=10.1080%2F03031853.2006.9523745&partnerID=40&md5=47b75f133719f2c7f00f4ae602b92772,scopus,"This study analyzes milk production on commercial dairy farms in Eritrea's Highlands using ridge regression to address multicollinearity. It estimates production elasticities, marginal products, and least-cost input combinations for three regions. Findings indicate resource under-utilization in the Central Zone and over-utilization of concentrates in Mendefera and Dekemhare, suggesting suboptimal resource allocation. The study highlights the need for improved information, training, and infrastructure to enhance decision-making.",True,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:40:24.944157
6eebbf4ce06ab357,A static replication approach for callable interest rate derivatives: mathematical foundations and efficient estimation of SIMM–MVA,"The computation of credit risk measures such as exposure and Credit Value Adjustments (CVA) requires the simulation of future portfolio prices. Recent metrics, such as dynamic Initial Margin (IM) and Margin Value Adjustments (MVA) additionally require the simulation of future conditional sensitivities. For portfolios with non-linear instruments that do not admit closed-form valuation formulas, this poses a significant computational challenge. This problem is addressed by proposing a static replication algorithm for interest rate options with early-exercise features under an affine term-structure model. Under the appropriate conditions, we can find an equivalent portfolio of vanilla options that replicate these products. Specifically, we decompose the product into a portfolio of European swaptions. The weights and strikes of the portfolio are obtained by regressing the target option value with interpretable, feed-forward neural networks. Once an equivalent portfolio of European swaptions is determined, we can leverage on closed-form expressions to obtain the conditional prices and sensitivities, which serve as an input to exposure and SIMM-driven MVA quantification. For a consistent forward sensitivity estimation, this involves the differentiation of the portfolio-weights. The accuracy and convergence of the method is demonstrated through several representative numerical examples, benchmarked against the established least-square Monte Carlo method. © 2024 Elsevier B.V., All rights reserved.","Hoencamp, J.H.; Jain, S.; Kandhai, B.D.",2024,10.1080/14697688.2024.2312523,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185970359&doi=10.1080%2F14697688.2024.2312523&partnerID=40&md5=e658f86122a239c289b291ec3fe810ae,scopus,"This paper proposes a static replication method for valuing callable interest rate derivatives and estimating SIMM-MVA. It uses neural networks to replicate complex options with a portfolio of vanilla options, enabling efficient calculation of future prices and sensitivities, and is benchmarked against the least-square Monte Carlo method.",True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:40:49.110387
8fb4c155f69de669,A transformer-based model for default prediction in mid-cap corporate markets,"In this paper, we study mid-cap companies, i.e. publicly traded companies with less than US$10 billion in market capitalisation. Using a large dataset of US mid-cap companies observed over 30 years, we look to predict the default probability term structure over the short to medium term and understand which data sources (i.e. fundamental, market or pricing data) contribute most to the default risk. Whereas existing methods typically require that data from different time periods are first aggregated and turned into cross-sectional features, we frame the problem as a multi-label panel data classification problem. To tackle it, we then employ transformer models, a state-of-the-art deep learning model emanating from the natural language processing domain. To make this approach suitable to the given credit risk setting, we use a loss function for multi-label classification, to deal with the term structure, and propose a multi-channel architecture with differential training that allows the model to use all input data efficiently. Our results show that the proposed deep learning architecture produces superior performance, resulting in a sizeable improvement in AUC (Area Under the receiver operating characteristic Curve) over traditional models. In order to interpret the model, we also demonstrate how to produce an importance ranking for the different data sources and their temporal relationships, using a Shapley approach for feature groups. © 2023 Elsevier B.V., All rights reserved.","Korangi, K.; Mues, C.; Bravo, C.",2023,10.1016/j.ejor.2022.10.032,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143550150&doi=10.1016%2Fj.ejor.2022.10.032&partnerID=40&md5=629f9a4355496d502e429e10a7d84773,scopus,"This paper introduces a transformer-based deep learning model for predicting default probabilities in mid-cap corporate markets. It utilizes a large dataset of US mid-cap companies over 30 years, framing the problem as a multi-label panel data classification task. The model demonstrates superior performance compared to traditional methods and includes an analysis of feature importance using a Shapley approach.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:40:51.223733
1d0805954066098a,ABS inflows to the United States and the global financial crisis,"Relative to the 'global savings glut' (GSG) hypothesis, we present a more complete picture of how capital flows contributed to the financial crisis, drawing attention to the sizable inflows from European investors into U.S. private-label asset-backed securities (ABS), including mortgage-backed securities and other structured investment products. The GSG hypothesis argues that the surge in capital inflows from emerging market economies to the United States led to significant declines in long-term interest rates in the United States and other industrial economies. In turn, these lower interest rates, when combined with both innovations and deficiencies of the U.S. credit market, are believed to have contributed to the U.S. housing bubble and to the buildup in financial vulnerabilities that led to the financial crisis. Because the GSG countries for the most part restricted their U.S. purchases to Treasuries and Agency debt, their provision of savings to ultimately risky subprime mortgage borrowers was necessarily indirect, pushing down yields on safe assets and increasing the appetite for alternative investments on the part of other investors. Foreign acquisitions of private-label ABS, primarily by Europeans, provided credit more directly and, by adding to domestic demand for these securities, contributed to the decline in their spreads over Treasury yields. Through a combination of empirical estimation and model simulation, we verify that both GSG inflows into Treasuries and Agencies, as well as European acquisitions of ABS, played a role in contributing to downward pressures on U.S. interest rates. All rights reserved, Elsevier",,2012,10.1016/j.jinteco.2012.04.001,,proquest,"This paper examines the role of capital inflows, specifically from European investors into U.S. asset-backed securities (ABS), in contributing to the global financial crisis. It contrasts this with the 'global savings glut' (GSG) hypothesis, which focuses on inflows from emerging market economies into U.S. Treasuries and Agency debt. The authors argue that European ABS inflows provided credit more directly and influenced interest rates and spreads, playing a significant role alongside GSG inflows in pressuring U.S. interest rates downwards.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:40:52.881482
af7ab673b17e0c3d,AI-Driven Dental Caries Management Strategies: From Clinical Practice to Professional Education and Public Self Care,"Dental caries is one of the most prevalent chronic diseases among both children and adults, despite being largely preventable. This condition has significant negative impacts on human health and imposes a substantial economic burden. In recent years, scientists and dentists have increasingly started to utilize artificial intelligence (AI), particularly machine learning, to improve the efficiency of dental caries management. This study aims to provide an overview of the current knowledge about the AI-enabled approaches for dental caries management within the framework of personalized patient care. Generally, AI works as a promising tool that can be used by both dental professionals and patients. For dental professionals, it predicts the risk of dental caries by analyzing dental caries risk and protective factors, enabling to formulate personalized preventive measures. AI, especially those based on machine learning and deep learning, can also analyze images to detect signs of dental caries, assist in developing treatment plans, and help to make a risk assessment for pulp exposure during treatment. AI-powered tools can also be used to train dental students through simulations and virtual case studies, allowing them to practice and refine their clinical skills in a risk-free environment. Additionally, AI tracks brushing patterns and provides feedback to improve oral hygiene practices of the patients and the general population, thereby improving their understanding and compliance. This capability of AI can inform future research and the development of new strategies for dental caries management and control.Dental caries is one of the most prevalent chronic diseases among both children and adults, despite being largely preventable. This condition has significant negative impacts on human health and imposes a substantial economic burden. In recent years, scientists and dentists have increasingly started to utilize artificial intelligence (AI), particularly machine learning, to improve the efficiency of dental caries management. This study aims to provide an overview of the current knowledge about the AI-enabled approaches for dental caries management within the framework of personalized patient care. Generally, AI works as a promising tool that can be used by both dental professionals and patients. For dental professionals, it predicts the risk of dental caries by analyzing dental caries risk and protective factors, enabling to formulate personalized preventive measures. AI, especially those based on machine learning and deep learning, can also analyze images to detect signs of dental caries, assist in developing treatment plans, and help to make a risk assessment for pulp exposure during treatment. AI-powered tools can also be used to train dental students through simulations and virtual case studies, allowing them to practice and refine their clinical skills in a risk-free environment. Additionally, AI tracks brushing patterns and provides feedback to improve oral hygiene practices of the patients and the general population, thereby improving their understanding and compliance. This capability of AI can inform future research and the development of new strategies for dental caries management and control.",,2025,10.1016/j.identj.2025.04.007,,proquest,"This study reviews the application of AI, particularly machine learning and deep learning, in dental caries management. AI tools are discussed for risk prediction, image analysis for detection and treatment planning, and training dental students. Furthermore, AI can monitor and provide feedback on patient oral hygiene practices, contributing to improved self-care and informing future research.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:40:54.930578
d0e42311667ba639,Adaptive investment strategies for periodic environments,"In this paper, an adaptive investment strategy for environments with periodic returns on investment is presented. In this approach, an investment model is considered where the agent decides at every time step the proportion of wealth to invest in a risky asset, keeping the rest of the budget in a risk-free asset. Every investment is evaluated in the market via stylized return on investment function (RoI), which is modeled by a stochastic process with unknown periodicities and levels of noise. For comparison, two reference strategies are presented which represent the case of agents with zero knowledge and complete knowledge of the dynamics of the returns. An investment strategy based on technical analysis to forecast the next return is also considered. To account for the performance of the different strategies, some computer experiments are performed to calculate the average budget that can be obtained with them over a certain number of time steps. To assure fair comparisons, the parameters of each strategy are first tuned for budget maximization. Afterward, the performance of these strategies is compared for RoI's with different periodicities and levels of noise. © 2008 World Scientific Publishing Company. © 2017 Elsevier B.V., All rights reserved.","Navarro-Barrientos, J.-E.",2008,10.1142/s0219525908001933,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57249084067&doi=10.1142%2FS0219525908001933&partnerID=40&md5=b1124d852118bf010bb60d870df16faa,scopus,"This paper presents an adaptive investment strategy for periodic return environments, where an agent allocates wealth between a risky and a risk-free asset. The return on investment is modeled as a stochastic process with unknown periodicities and noise. The proposed strategy is compared against reference strategies with zero and complete knowledge, as well as a technical analysis-based strategy. Performance is evaluated through computer experiments, with strategy parameters tuned for budget maximization before comparing performance across different periodicities and noise levels.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:40:56.563962
8527a7f34521c485,Adaptive online portfolio selection with transaction costs,"As an application of machine learning techniques in financial fields, online portfolio selection has been attracting great attention from practitioners and researchers, which makes timely sequential decision making available when market information is constantly updated. For online portfolio selection, transaction costs incurred by changes of investment proportions on risky assets have a significant impact on the investment strategy and the return in long-term investment horizon. However, in many online portfolio selection studies, transaction costs are usually neglected in the decision making process. In this paper, we consider an adaptive online portfolio selection problem with transaction costs. We first propose an adaptive online moving average method (AOLMA) to predict the future returns of risky assets by incorporating an adaptive decaying factor into the moving average method, which improves the accuracy of return prediction. The net profit maximization model (NPM) is then constructed where transaction costs are considered in each decision making process. The adaptive online net profit maximization algorithm (AOLNPM) is designed to maximize the cumulative return by integrating AOLMA and NPM together. Numerical experiments show that AOLNPM dominates several state-of-the-art online portfolio selection algorithms in terms of various performance metrics, i.e., cumulative return, mean excess return, Sharpe ratio, Information ratio and Calmar ratio. © 2021 Elsevier B.V., All rights reserved.","Guo, S.; Gu, J.-W.; Ching, W.-K.",2021,10.1016/j.ejor.2021.03.023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103966971&doi=10.1016%2Fj.ejor.2021.03.023&partnerID=40&md5=09d67e9398dc3fb20f5fd921477927f0,scopus,This paper proposes an adaptive online portfolio selection method (AOLNPM) that incorporates transaction costs and uses an adaptive moving average method (AOLMA) for return prediction. The method aims to maximize net profit and has shown superior performance compared to existing algorithms in numerical experiments.,True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:40:58.191284
3dfd04e4fb4d9063,Alternative Maximum Likelihood Estimation of Structural Vector Autoregressive Models Partially Identified with Short-Run Restrictions,"This paper presents an alternative maximum likelihood estimation method for partially identified vector autoregressive models. This method might be especially useful to handle very large systems of variables by reducing the dimension of the likelihood space. As an application, we consider an open economy model to investigate the effects of monetary policy on exchange rates and term structures. We find that exchange rates tend to overshoot and term structures have hump-shaped responses to monetary policy shocks. © 2013 The Ohio State University. © 2013 Elsevier B.V., All rights reserved.","Jang, K.",2013,10.1111/jmcb.12010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875290853&doi=10.1111%2Fjmcb.12010&partnerID=40&md5=21a61ad4420d752980f482e339f0aa47,scopus,"This paper proposes an alternative maximum likelihood estimation method for partially identified structural vector autoregressive models, particularly beneficial for large systems. An application to an open economy model demonstrates its use in analyzing monetary policy effects on exchange rates and term structures, revealing overshooting exchange rates and hump-shaped term structure responses.",False,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:41:07.117032
5be924db9dfca63e,An Evaluation of Machine Learning Models for Forecasting Short-Term U.S. Treasury Yields,"This study explores the historical evolution and short-term predictive modeling of the U.S. 10-year Treasury bond yield, a critical indicator in global financial markets. Recognizing its sensitivity to macroeconomic conditions, the research integrates economic variables, including the federal funds rate, core Consumer Price Index (CPI), real Gross Domestic Product (GDP) growth rate, and the U.S. federal debt growth rate, to assess their influence on yield movements. Four forecasting models are employed for comparative analysis: linear regression (LR), decision tree (DT), random forest (RF), and multilayer perceptron (MLP) neural networks. Using historical data from the Federal Reserve Economic Data (FRED), this study finds that the RF model offers the most accurate short-term predictions, achieving the lowest mean squared error (MSE) and mean absolute error (MAE), with an R2 value of 0.5760. The results highlight the superiority of ensemble-based nonlinear models in capturing complex interactions between economic indicators and yield dynamics. This research not only provides empirical support for using machine learning in economic forecasting but also offers practical implications for bond traders, system developers, and financial institutions aiming to enhance predictive accuracy and risk management.",,2025,10.3390/app15126903,,proquest,"This study evaluates four machine learning models (linear regression, decision tree, random forest, and multilayer perceptron) for forecasting the U.S. 10-year Treasury bond yield, incorporating economic variables. The random forest model demonstrated the highest accuracy, suggesting the effectiveness of ensemble-based nonlinear models for predicting yield movements and offering practical insights for financial markets.",True,True,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T09:41:11.834380
289cad8e7325e40a,Artificial Intelligence in Manufacturing Industry Worker Safety: A New Paradigm for Hazard Prevention and Mitigation,"The phenomenal rise of artificial intelligence (AI) in the last decade, and its evolution as a versatile addition to various fields, necessitates its usage for novel purposes in multidimensional fields like the manufacturing industry. Even though AI has been rigorously studied for process optimization, wastage reduction, and other quintessential aspects of the manufacturing industry, there has been limited focus on worker safety as a theme in the current literature. Safety standards contribute to worker safety, but there is no one-size-fits-all approach in these standards or policies, which warrants evaluation and integration of new ideas and technologies to reach the closest to ideal standards. This includes but is not limited to health, regulation of operations, predictive maintenance, and automation and control. The rise of Industry 4.0 and the migration towards Industry 5.0 facilitate easy integration of advanced technologies like AI into the manufacturing industry with real-time predictive capabilities, and this can help reduce human errors and mitigate hazards in processes where sensitivity is crucial or hazards are frequent. Keeping the future outlook in focus, AI can contribute to training workers in risk-free environments, promote engineering education for easy adaptation to new technology, and reduce resistance to changes in the industry. Furthermore, there is an urgent need for standards and regulations to govern and integrate AI technologies judiciously into the manufacturing industry, which holds AI models and their creators accountable for their decisions. This could further extend to preventing the adversarial use of new technology. This study exhaustively discusses the potential and ongoing contributions of this technology to the safety of workers in the manufacturing industry. © 2025 Elsevier B.V., All rights reserved.","Khurram, M.; Zhang, C.; Muhammad, S.; Kishnani, H.; An, K.; Abeywardena, K.; Chadha, U.; Behdinan, K.",2025,10.3390/pr13051312,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006749937&doi=10.3390%2Fpr13051312&partnerID=40&md5=62881fa545a36f4d695feaaad90cb4af,scopus,"This article explores the application of Artificial Intelligence (AI) in enhancing worker safety within the manufacturing industry. It highlights AI's potential in hazard prevention, mitigation, and creating safer work environments, especially with the advent of Industry 4.0 and 5.0. The study also emphasizes the need for standards and regulations to govern AI integration and ensure accountability.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:41:14.160306
7dddef039290f6a0,Ask BERT: How Regulatory Disclosure of Transition and Physical Climate Risks Affects the CDS Term Structure,"We use BERT, an AI-based algorithm for language understanding, to quantify regulatory climate risk disclosures and analyze their impact on the term structure in the credit default swap (CDS) market. Risk disclosures can either increase or decrease CDS spreads, depending on whether the disclosure reveals new risks or reduces uncertainty. Training BERT to differentiate between transition and physical climate risks, we find that disclosing transition risks increases CDS spreads after the Paris Climate Agreement of 2015, while disclosing physical risks decreases the spreads. In addition, we also find that the election of Trump had a negative impact on CDS spreads for firms exposed to transition risk. These impacts are consistent with theoretical predictions and economically and statistically significant. © 2024 Elsevier B.V., All rights reserved.","Kölbel, J.F.; Leippold, M.; Rillaerts, J.; Wang, Q.",2024,10.1093/jjfinec/nbac027,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182901564&doi=10.1093%2Fjjfinec%2Fnbac027&partnerID=40&md5=23e2756786ad479db12b6ac26122ffba,scopus,"This study employs BERT, an AI language model, to quantify regulatory climate risk disclosures and their effect on the credit default swap (CDS) market's term structure. The findings indicate that disclosing transition risks widens CDS spreads post-Paris Agreement, while disclosing physical risks narrows them. Trump's election also negatively impacted CDS spreads for firms exposed to transition risk. These effects align with theoretical expectations and are statistically significant.",True,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:41:16.297561
668802cdc78345c9,Asset Returns: Reimagining Generative ESG Indexes and Market Interconnectedness,"Financial economists have long studied factors related to risk premiums, pricing biases, and diversification impediments. This study examines the relationship between a firm’s commitment to environmental, social, and governance principles (ESGs) and asset market returns. We incorporate an algorithmic protocol to identify three nonobservable but pervasive E, S, and G time-series factors to meet the study’s objectives. The novel factors were tested for information content by constructing a six-factor Fama and French model following the imposition of the isolation and disentanglement algorithm. Realizing that nonlinear relationships characterize models incorporating both observable and nonobservable factors, the Fama and French model statement was estimated using an enhanced shallow-learning neural network. Finally, as a post hoc measure, we integrated explainable AI (XAI) to simplify the machine learning outputs. Our study extends the literature on the disentanglement of investment factors across two dimensions. We first identify new time-series-based E, S, and G factors. Second, we demonstrate how machine learning can be used to model asset returns, considering the complex interconnectedness of sustainability factors. Our approach is further supported by comparing neural-network-estimated E, S, and G weights with London Stock Exchange ESG ratings.",,2024,10.3390/jrfm17100463,,proquest,"This study investigates the link between ESG commitments and asset returns by developing three novel ESG time-series factors using an algorithmic protocol. It constructs a six-factor Fama and French model, incorporating these factors and using an enhanced shallow-learning neural network to account for nonlinear relationships. Explainable AI (XAI) is used for post hoc analysis. The research extends factor disentanglement by identifying new ESG factors and demonstrating machine learning's utility in modeling asset returns with complex sustainability factor interconnections, validated against London Stock Exchange ESG ratings.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:41:18.653778
3c7234e3f11f883f,Asset pricing with neural networks: Significance tests,"This study proposes a novel hypothesis test for evaluating the statistical significance of input variables in multi-layer perceptron (MLP) regression models. Theoretical foundations are established through consistency results and estimation rate analysis using the sieves method. To validate the test's performance in complex and realistic settings, an extensive Monte Carlo simulation is conducted. Results of the simulation reveal that the test has a high power and low rate of false positives, making it a powerful tool for detecting true effects in data. The test is further applied to identify the most influential predictors of equity risk premiums, with results indicating that only a small number of characteristics have statistical significance and all macroeconomic predictors are insignificant at the 1% level. These findings are consistent across a variety of neural network architectures. © 2023 Elsevier B.V., All rights reserved.","Fallahgoul, H.; Franstianto, V.; Lin, X.",2024,10.1016/j.jeconom.2023.105574,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175615182&doi=10.1016%2Fj.jeconom.2023.105574&partnerID=40&md5=266d86eec03bf13321f13fad852108df,scopus,"This study introduces a new statistical significance test for input variables in MLP regression models, grounded in sieve method theory. Simulations show the test is powerful and accurate. Applied to equity risk premiums, it found few significant predictors and no significant macroeconomic predictors, consistent across various neural network architectures.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:41:21.210477
cc2d45a53ccb3e61,Asymmetry in the link between the yield spread and industrial production: Threshold effects and forecasting,"We analyse the nonlinear behaviour of the information content in the spread for future real economic activity. The spread linearly predicts one-year-ahead real growth in nine industrial production sectors of the USA and four of the UK over the last 40 years. However, recent investigations on the spread-real activity relation have questioned both its linear nature and its time-invariant framework. Our in-sample empirical evidence suggests that the spread-real activity relationship exhibits asymmetries that allow for different predictive power of the spread when past spread values were above or below some threshold value. We then measure the out-of-sample forecast performance of the nonlinear model using predictive accuracy tests. The results show that significant improvement in forecasting accuracy, at least for one-step-ahead forecasts, can be obtained over the linear model. Copyright (C) 2004 John Wiley Sons, Ltd.","Paya, I; Venetis, IA; Peel, DA",2004,10.1002/for.921,,wos,"This study investigates the nonlinear relationship between the yield spread and industrial production, finding that the spread's predictive power for future economic activity is asymmetric and depends on threshold values. A nonlinear model demonstrates improved forecasting accuracy over a linear model for one-year-ahead real growth in US and UK industrial sectors.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:41:23.369671
dd98079f619d0fee,Bayesian inference for the hazard term structure with functional predictors using Bayesian predictive information criteria,"A Bayesian method for estimation of a hazard term structure is presented in a functional data analysis framework. The hazard terms structure is designed to include the effects of changes in economic conditions, as well as trends in stock prices and accounting variables from financial statements. The hazard function contains time-varying parameters that are modelled using splines. To estimate the model parameters, a Markov-chain Monte Carlo sampling algorithm is developed. The Bayesian predictive information criterion is employed to assess the default predictive power of the estimated model. The method is then applied to a Japanese firm's default data listed on the Japanese Stock Exchange. The results demonstrate that the proposed method performs well. © 2007 Elsevier B.V. All rights reserved. © 2009 Elsevier B.V., All rights reserved.","Ando, T.",2009,10.1016/j.csda.2007.12.014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-61549125754&doi=10.1016%2Fj.csda.2007.12.014&partnerID=40&md5=c8e5911b96b6a03f4d653c0cb4b296cb,scopus,"This paper proposes a Bayesian method for estimating hazard term structures using functional data analysis, incorporating economic conditions, stock prices, and accounting variables. The model uses time-varying parameters estimated via splines and a Markov-chain Monte Carlo algorithm. The Bayesian predictive information criterion is used for model assessment, and the method is applied to Japanese firm default data.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:41:25.358377
4b816497adef29ec,Bayesian inference in a Stochastic Volatility Nelson-Siegel model,"Bayesian inference is developed and applied for an extended Nelson-Siegel term structure model capturing interest rate risk. The so-called Stochastic Volatility Nelson-Siegel (SVNS) model allows for stochastic volatility in the underlying yield factors. A Markov chain Monte Carlo (MCMC) algorithm is proposed to efficiently estimate the SVNS model using simulation-based inference. The SVNS model is applied to monthly US zero-coupon yields. Significant evidence for time-varying volatility in the yield factors is found. The inclusion of stochastic volatility improves the model's goodness-of-fit and clearly reduces the forecasting uncertainty, particularly in low-volatility periods. The proposed approach is shown to work efficiently and is easily adapted to alternative specifications of dynamic factor models revealing (multivariate) stochastic volatility. © 2010 Elsevier B.V. All rights reserved. © 2012 Elsevier B.V., All rights reserved.","Hautsch, N.; Yang, F.",2012,10.1016/j.csda.2010.07.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862010805&doi=10.1016%2Fj.csda.2010.07.003&partnerID=40&md5=2473ed923e34cceeeb4420f58f6ec03d,scopus,"This paper develops and applies Bayesian inference for a Stochastic Volatility Nelson-Siegel (SVNS) model, which extends the Nelson-Siegel term structure model to include stochastic volatility in interest rate risk factors. A Markov chain Monte Carlo (MCMC) algorithm is used for estimation. Applied to US zero-coupon yields, the SVNS model demonstrates time-varying volatility, improves goodness-of-fit, and reduces forecasting uncertainty, especially in low-volatility periods. The method is adaptable to other dynamic factor models with stochastic volatility.",True,True,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T09:41:40.479106
fd600085dd1ac45f,Bayesian inference via filtering equations for ultrahigh frequency data (I): Model and estimation,"We propose a general partially observed framework of Markov processes with marked point process observations for ultrahigh frequency (UHF) data. The model fits well the stylized facts of UHF data in both macro- and micro-movements, subsumes important existing models, and incorporates the influence of other observable economic or market factors. We develop the corresponding Bayes estimation via a filtering equation to quantify parameter uncertainty. Namely, we derive the normalized filtering equation to characterize the evolution of the posterior distribution, present a weak convergence theorem, and construct consistent, easily parallelizable, recursive algorithms to calculate the joint posteriors and the Bayes estimates for streaming UHF data. Moreover, a sufficient condition for the consistency of the Bayes estimators is provided. The general estimation theory is illustrated by four specific models built for U.S. Treasury notes transactions data from GovPX. We show that in this market, both information-based and inventory management-based motives are significant factors in the trade-to-trade price volatility. © 2021 Elsevier B.V., All rights reserved.","Hu, G.X.; Kuipers, D.R.; Zeng, Y.",2018,10.1137/16m1094762,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046007287&doi=10.1137%2F16M1094762&partnerID=40&md5=09a7b04e5b748673463cf6778c6055f0,scopus,"This paper introduces a Bayesian inference framework for ultrahigh frequency (UHF) data using partially observed Markov processes and marked point process observations. It develops a filtering equation for parameter estimation and provides algorithms for recursive calculation of posterior distributions and Bayes estimates for streaming UHF data. The model is illustrated with U.S. Treasury notes transactions data, revealing significant factors influencing price volatility.",True,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:41:52.748997
6ce0745ed5df403e,Benchmark bonds interactions under regime shifts,"In the present paper we examine the interactions among five benchmark ten year government bonds, namely those of the USA, Germany, France, Italy and the Netherlands. Our aim is to illustrate empirically a net of interactions existing among the major bond markets of Europe and the US market taking into account shifts in the underlying stochastic processes. For this purpose, differing from the rest of the relevant empirical literature, after specifying the long run equilibrium relations, we estimate the linkages between the bond markets as subject to hidden Markov chains, by applying the Markov Switching Vector Error Correction framework (MS-VECM). This formulation is found to efficiently reflect the shifts brought about by significant economic events, such as the European monetary unification. As a result we illustrate different short-run relations referring to the periods before and after the monetary union. Overall, our empirical results indicate that stronger interactions among the markets of the system exist in the period after the EMU. Also, by means of a variance decomposition analysis we assess leader-follower relations which indicate that the benchmark status of bonds has changed since the introduction of the common monetary policy framework in Europe. Reprinted by permission of Blackwell Publishers",,2012,10.1111/j.1468-036x.2009.00535.x,,proquest,"This paper empirically examines the interactions among five benchmark ten-year government bonds (USA, Germany, France, Italy, and Netherlands) using a Markov Switching Vector Error Correction (MS-VECM) framework. The study accounts for regime shifts, particularly those related to the European monetary unification, and illustrates different short-run relations before and after the EMU. Results show stronger interactions post-EMU and changing leader-follower dynamics among the benchmark bonds.",True,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:42:08.686130
4353d80b6dc1ec37,Bias in estimating multivariate and univariate diffusions,"Multivariate continuous time models are now widely used in economics and finance. Empirical applications typically rely on some process of discretization so that the system may be estimated with discrete data. This paper introduces a framework for discretizing linear multivariate continuous time systems that includes the commonly used Euler and trapezoidal approximations as special cases and leads to a general class of estimators for the mean reversion matrix. Asymptotic distributions and bias formulae are obtained for estimates of the mean reversion parameter. Explicit expressions are given for the discretization bias and its relationship to estimation bias in both multivariate and in univariate settings. in the univariate context, we compare the performance of the two approximation methods relative to exact maximum likelihood (ML) in terms of bias and variance for the Vasicek process. The bias and the variance of the Euler method are found to be smaller than the trapezoidal method, which are in turn smaller than those of exact ML. Simulations suggest that when the mean reversion is slow, the approximation methods work better than ML, the bias formulae are accurate, and for scalar models the estimates obtained from the two approximate methods have smaller bias and variance than exact ML For the square root process, the Euler method outperforms the Nowman method in terms of both bias and variance. Simulation evidence indicates that the Euler method has smaller bias and variance than exact ML, Nowman's method and the Milstein method. (C) 2010 Elsevier B.V. All rights reserved.","Wang, Xiaohu; Phillips, Peter C. B.; Yu, Jun",2011,10.1016/j.jeconom.2010.12.006,,wos,"This paper presents a framework for discretizing linear multivariate continuous time systems, including Euler and trapezoidal approximations, to estimate the mean reversion matrix. It derives asymptotic distributions and bias formulas for the mean reversion parameter, detailing discretization bias and its relation to estimation bias in both multivariate and univariate settings. The study compares the Euler and trapezoidal methods against exact maximum likelihood (ML) for the Vasicek process, finding that approximation methods generally have smaller bias and variance than ML, especially for scalar models with slow mean reversion. For the square root process, the Euler method is shown to outperform other methods in terms of bias and variance.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:42:10.389633
3cf97d4770aaca9e,Binomial Markov-Switching Multifractal model with Skewed t innovations and applications to Chinese SSEC Index,"This paper presents the Binomial Markov-switching Multifractal (BMSM) model of asset returns with Skewed t innovations (BMSM-Skewed t for short), which considers the fat tails, skewness and multifractality in asset returns simultaneously. The parameters of BMSM-Skewed t model can be estimated by Maximum Likelihood (ML) methods, and volatility forecasting can be accomplished via Bayesian updating. In order to evaluate the performance of BMSM-Skewed t model, BMSM model with Normal innovations (BMSM-N), BMSM model with Student-t innovations (BMSM-t) and GARCH(1,1) models (GARCH-N, GARCH-t and GARCH-Skewed t) are chosen for comparison. Through empirical studies on Shanghai Stock Exchange Composite Index (SSEC), we find that for sample estimation, BMSM models outperform the GARCH(1,1) models through BIC and AIC rules, and BMSM-Skewed t performs the best among all the models due to its fat tails, skewness and multifractality. In addition, BMSM-Skewed t model dominates other models at most forecasting horizons for out-of-sample volatility forecasts in terms of MSE, MAE and SPA test. (C) 2016 Elsevier B.V. All rights reserved.","Liu, Yufang; Zhang, Weiguo; Fu, Junhui",2016,10.1016/j.physa.2016.06.014,,wos,"This paper introduces the Binomial Markov-switching Multifractal (BMSM) model with Skewed t innovations to simultaneously capture fat tails, skewness, and multifractality in asset returns. The model is estimated using Maximum Likelihood methods and volatility forecasting is done via Bayesian updating. Empirical studies on the Shanghai Stock Exchange Composite Index (SSEC) show that the BMSM models outperform GARCH(1,1) models, with the BMSM-Skewed t model performing best. The BMSM-Skewed t model also demonstrates superior out-of-sample volatility forecasting performance.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:42:13.448559
72beeece03449e4d,Bitcoin transaction strategy construction based on deep reinforcement learning,"The emerging cryptocurrency market has lately received great attention for asset allocation due to its decentralization uniqueness. However, its volatility and brand new trading mode has made it challenging to devising an acceptable automatically-generating strategy. This study proposes a framework for automatic high-frequency bitcoin transactions based on a deep reinforcement learning algorithm — proximal policy optimization (PPO). The framework creatively regards the transaction process as actions, returns as awards and prices as states to align with the idea of reinforcement learning. It compares advanced machine learning-based models for static price predictions including support vector machine (SVM), multi-layer perceptron (MLP), long short-term memory (LSTM), temporal convolutional network (TCN), and Transformer by applying them to the real-time bitcoin price and the experimental results demonstrate that LSTM outperforms. Then an automatically-generating transaction strategy is constructed building on PPO with LSTM as the basis to construct the policy. Extensive empirical studies validate that the proposed method perform superiorly to various common trading strategy benchmarks for a single financial product. The approach is able to trade bitcoins in a simulated environment with synchronous data and obtains a 31.67% more return than that of the best benchmark, improving the benchmark by 12.75%. The proposed framework can earn excess returns through both the period of volatility and surge, which opens the door to research on building a single cryptocurrency trading strategy based on deep learning. Visualizations of trading the process show how the model handles high-frequency transactions to provide inspiration and demonstrate that it can be expanded to other financial products. © 2021 Elsevier B.V., All rights reserved.","Liu, F.; Li, Y.; Li, B.; Li, J.; Xie, H.",2021,10.1016/j.asoc.2021.107952,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117191473&doi=10.1016%2Fj.asoc.2021.107952&partnerID=40&md5=3f33a1f33bc09779c5d4f95e042f6975,scopus,"This study proposes a deep reinforcement learning framework using Proximal Policy Optimization (PPO) with Long Short-Term Memory (LSTM) for automated high-frequency Bitcoin trading. The framework treats transactions as actions, returns as rewards, and prices as states. It compares various machine learning models for price prediction, finding LSTM to be superior. The PPO-LSTM strategy outperforms common trading benchmarks, achieving higher returns and demonstrating effectiveness during volatile periods. The approach is validated in a simulated environment and can potentially be expanded to other financial products.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:42:16.367292
b347734b53091f80,Black-Scholes Fuzzy Numbers as Indexes of Performance,"We use the set of propositions of some previous papers to define a fuzzy version of the Black-Scholes value where the risk free instantaneous interest intensity, the volatility and the initial stock price are fuzzy numbers whose parameters are built with statistical financial data. With our Black-Scholes fuzzy numbers we define indexes of performance varing in time. As an example, with data of the Italian Stock Exchange on MIB30, we see that in 2004 and 2006 our indexes are negative, that is, they are indexes of the refuse to invest and this refuse increased. So, on November 11, 2006 we could forecast that the market will become with more risk: the risk of loss will increase. Now, on January 25, 2010, we know that this forecast has happened. Obviously, the parameters of our Black-Scholes fuzzy numbers can be valued also with incomplete, possibilistic data. With respect to the probabilistic one, our fuzzy method is more simple and immediate to have a forecast on the financial market.",,2010,10.1155/2010/607214,,proquest,"This paper introduces a fuzzy version of the Black-Scholes model, treating key parameters like interest rates and stock prices as fuzzy numbers derived from financial data. These fuzzy numbers are used to create time-varying performance indexes. An example using Italian Stock Exchange data shows these indexes can indicate a 'refusal to invest' and forecast increased market risk, a prediction that reportedly came true. The authors suggest their fuzzy approach is simpler and more immediate than probabilistic methods for market forecasting, and can handle incomplete or possibilistic data.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:42:17.896863
244baf9a591723da,Bond Market Prediction using an Ensemble of Neural Networks,"The characteristics of a successful financial forecasting system are the exploitation of inefficiencies of a given market and the precise application to that market. Overwhelming evidence indicates that opportunities exist for consistent positive returns over a given period of time. This project aims to provide means for the yield curve projection of government bonds. An ensemble of networks such as back propagation, radial basis function, linear regression, is used to predict the yield. The yield is forecasted using technical analysis using historical data and the output is tested for accuracy and accordingly assigned weights. Using the ensemble of neural networks, accuracy has been tried to be maximized and offer near to actual prediction. Using the yield curve, the investor can assess not only the yield of that bond, but can also the interest rates, and hence, has a very useful tool in his hand for investment purpose, thus making decisions about whether to invest or not , and if invest then when to invest. The yield curve prediction not only provides the investor a tool to make investment decisions in bond market, but it also serves as a tool to gauge the macroeconomic conditions of the country and hence predict the movement in various other markets as well, and hence make investment decisions accordingly.",,2013,10.5120/14105-2144,,proquest,"This project proposes an ensemble of neural networks (back propagation, radial basis function, linear regression) to predict government bond yields and project the yield curve. The model uses historical data and technical analysis, with weights assigned based on accuracy to maximize prediction performance. The yield curve serves as a tool for investors to make decisions and gauge macroeconomic conditions.",True,True,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T09:42:23.417995
72088005c119975b,Bond risk premiums at the zero lower bound,"We document that the spread between long- and short-term government bond yields is a stronger predictor of excess bond returns when the U.S. economy is at the zero lower bound (ZLB) than away from this bound. The Gaussian shadow rate model with a linear or quadratic shadow rate is unable to explain this change in return predictability. The same holds for the quadratic term structure model and the autoregressive gamma-zero model that also enforce the ZLB. In contrast, the linear-rational square-root model explains our new empirical finding because the model allows for unspanned stochastic volatility as seen in bond yields.","Andreasen, Martin M.; Jorgensen, Kasper; Meldrum, Andrew",2025,10.1016/j.jeconom.2024.105939,,wos,"This paper investigates bond risk premiums at the zero lower bound (ZLB). It finds that the spread between long- and short-term government bond yields is a better predictor of excess bond returns at the ZLB. Existing models like the Gaussian shadow rate model and the quadratic term structure model fail to explain this, but a linear-rational square-root model succeeds by incorporating unspanned stochastic volatility.",False,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:42:31.923307
0fdd961fa4794d25,Building Risk into the Mitigation/Adaptation Decisions simulated by Integrated Assessment Models,"This paper proposes an operationally simple and easily generalizable methodology to incorporate climate change damage uncertainty into Integrated Assessment Models (IAMs). First uncertainty is transformed into a risk measure by extracting damage distribution means and variances from an ensemble of socio economic and climate change scenarios. Then a risk premium is computed under different degrees of risk aversion, quantifying what society would be willing to pay to insure against the uncertainty of the damages. Our estimates show that the premium for the risk is a potentially significant addition to the “standard average damage”, but highly sensitive to the attitudes toward risk. In the last research phase, the risk premium is incorporated into the climate change damage function of a widely used IAM which shows, consequently, a substantial increase in both mitigation and adaptation efforts, reflecting a more precautionary attitude by the social planner. Interestingly, adaptation is stimulated more than mitigation in the first half of this century, while the situation reverses afterwards.",,2019,10.1007/s10640-019-00384-1,,proquest,"This paper introduces a method to integrate climate change damage uncertainty into Integrated Assessment Models (IAMs) by transforming uncertainty into a risk measure. It calculates a risk premium based on societal risk aversion, which significantly increases mitigation and adaptation efforts within the IAM, leading to a more precautionary approach. The study also observes that adaptation is prioritized over mitigation in the early 21st century, with the trend reversing later.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:42:34.966564
2aa1f42bb57aced5,Building portfolios based on machine learning predictions,"This paper demonstrates that portfolio optimization techniques represented by Markowitz mean-variance and Hierarchical Risk Parity (HRP) optimizers increase the risk-adjusted return of portfolios built with stocks preselected with a machine learning tool. We apply the random forest method to predict the cross-section of expected excess returns and choose n stocks with the highest monthly predictions. We compare three different techniques—mean-variance, HRP, and 1/N— for portfolio weight creation using returns of stocks from the S&P500 and STOXX600 for robustness. The out-of-sample results show that both mean-variance and HRP optimizers outperform the 1/N rule. This conclusion is in the opposition to a common criticism of optimizers’ efficiency and presents a new light on their potential practical usage. © 2022 Elsevier B.V., All rights reserved.","Kaczmarek, T.; Perez, K.",2022,10.1080/1331677x.2021.1875865,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100727648&doi=10.1080%2F1331677X.2021.1875865&partnerID=40&md5=e01a1a40410b7343778ed004d9e97d84,scopus,"This paper explores the use of machine learning (ML) to enhance portfolio optimization. It applies the random forest method to predict stock returns and then uses Markowitz mean-variance and Hierarchical Risk Parity (HRP) optimizers to construct portfolios. The study compares these methods against a simple 1/N rule using S&P500 and STOXX600 stock data, finding that mean-variance and HRP outperform the 1/N rule, challenging common criticisms of optimizer efficiency.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:42:37.741985
acc0253aba3ad46f,CAPM model for the valuation of shares of companies in the construction market during the period 2015 - 2020; Modelo CAPM para la valoración de acciones de las empresas en el mercado de la construcción durante el periodo 2015 - 2020,"This article consisted of the application of the CAPM model on companies listed on the stock exchange of and related to the construction sector in Colombia during the period from January 1, 2015 to December 31, 2020. This research has a quantitative approach with a type of descriptive and longitudinal research. Its methodology consisted in the application of ordinary least squares to the daily volatilities of the asset based on the estimation of the betas, which mostly complied with the probabilistic values and the intercepts were not different from 0, which is consistent with the model hypothesis. As for the variables that accompany the model, the risk-free rate TFIT16240724 was chosen and the ICOLCAP index was chosen as the variable that measures the market risk. Finally obtained the results, these were evaluated, concluding that the beta coefficient is an acceptable indicator in the risk-return assessment of the asset during the period in question, however, as an estimator it is not effective, which reflects the ineffectiveness of the model. © 2023 Elsevier B.V., All rights reserved.","González, G.A.O.; Domínguez, M.R.",2023,10.46661/revmetodoscuanteconempresa.7350,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162148528&doi=10.46661%2Frevmetodoscuanteconempresa.7350&partnerID=40&md5=f21ca467e8fee4b68a7af008bd550b76,scopus,"This study applies the Capital Asset Pricing Model (CAPM) to construction companies listed on the Colombian stock exchange between 2015 and 2020. Using ordinary least squares to estimate betas, the research found that while the beta coefficient is an acceptable indicator for risk-return assessment, it is not an effective estimator, suggesting the CAPM model's ineffectiveness for this market during the period. The study used the TFIT16240724 as the risk-free rate and the ICOLCAP index for market risk.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:42:39.706583
21724fc90a15e265,CONDITIONAL HETEROSKEDASTICITY IN ASSET RETURNS - A NEW APPROACH,"GARCH models have been applied in modelling the relation between conditional variance and asset risk premia. These models, however, have at least three major drawbacks in asset pricing applications: (i) Researchers beginning with Black (1976) have found a negative correlation between current returns and future returns volatility. GARCH models rule this out by assumption. (ii) GARCH models impose parameter restrictions that are often violated by estimated coefficients and that may unduly restrict the dynamics of the conditional variance process. (iii) Interpreting whether shocks to conditional variance persist or not is difficult in GARCH models, because the usual norms measuring persistence often do not agree. A new form of ARCH is proposed that meets these objections. The method is used to estimate a model of the risk premium on the CRSP Value-Weighted Market Index from 1962 to 1987.","NELSON, DB",1991,10.2307/2938260,,wos,"This paper proposes a new approach to modeling conditional heteroskedasticity in asset returns, addressing limitations of existing GARCH models such as the inability to capture the negative correlation between current returns and future volatility, parameter restriction issues, and difficulties in interpreting shock persistence. The new method is applied to estimate the risk premium on the CRSP Value-Weighted Market Index.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:43:03.281408
64bed7c0c4bb24cd,Can google search volume index predict the returns and trading volumes of stocks in a retail investor dominant market,"This research examines whether Google search volume index (GSVI), a proxy of investor attention, can predict the excess returns and abnormal trading volumes of TPEx 50 index constituents. It also explores the motive underlying GSVI based on positive or negative shocks to stock prices. The empirical data include 48 companies from TPEx 50 index constituents and cover a period from 1 September 2016 to 31 August 2019. The empirical results present that (1) lagged GSVI negatively affects current excess returns, perhaps due to the characteristics of TPEx, in which there are a higher proportion of retail investors, smaller listed companies, and a higher information asymmetry problem. (2) Lagged GSVI can positively affect abnormal current trading volumes. (3) If GSVI is driven by positive shocks, then it can predict excess returns and abnormal trading volumes positively.",,2022,10.1080/23322039.2021.2014640,,proquest,"This study investigates if Google Search Volume Index (GSVI), as a measure of investor attention, can predict stock returns and trading volumes for companies in the TPEx 50 index. The findings indicate that GSVI negatively impacts current excess returns but positively affects abnormal trading volumes. Furthermore, positive shocks to GSVI are associated with positive predictions for both excess returns and abnormal trading volumes. The study suggests that the characteristics of the TPEx market, such as a high proportion of retail investors and information asymmetry, may influence these relationships.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:43:06.409429
56598c57394f26e9,Can we measure inflation expectations using Twitter?,"Drawing on Italian tweets, we employ textual data and machine learning techniques to build new real-time measures of consumers' inflation expectations. First, we select keywords to identify tweets related to prices and expectations thereof. Second, we build a set of daily measures of inflation expectations around the selected tweets, combining the Latent Dirichlet Allocation (LDA) with a dictionary-based approach, using manually labeled bi-grams and tri-grams. Finally, we show that Twitter-based indicators are highly correlated with both monthly survey-based and daily market-based inflation expectations. Our new indicators anticipate consumers' expectations, proving to be a good real-time proxy, and provide additional information beyond market based expectations, professional forecasts, and realized inflation. The results suggest that Twitter can be a new timely source for eliciting beliefs. (C) 2022 Elsevier B.V. All rights reserved.","Angelico, Cristina; Marcucci, Juri; Miccoli, Marcello; Quarta, Filippo",2022,10.1016/j.jeconom.2021.12.008,,wos,"This study explores the use of Twitter data and machine learning (LDA and dictionary-based approach) to create real-time measures of consumer inflation expectations in Italy. The developed indicators show a strong correlation with survey-based and market-based expectations, suggesting Twitter as a timely source for eliciting beliefs.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:43:08.171000
7a8f143dc92d2032,"Capital accumulation, external indebtedness, and macroeconomic performance of emerging countries","This paper aims at presenting a nonlinear post Keynesian growth model to evaluate at the theoretical and empirical levels the relationship between external indebtedness and economic growth in emerging countries. To this end, a post Keynesian endogenous growth model is presented, in which: (1) the desired rate of capital accumulation is assumed to be a nonlinear function of external indebtedness as a share of capital stock; (2) an endogenous country risk premium is assumed to be an increasing (linear) function of external indebtedness (as a share of capital stock); (3) there is a fixed exchange rate regime and perfect capital mobility in the sense of Mundell and Fleming. The main theoretical result of the model is the existence of two long-run equilibrium positions, one of which has a high level of external indebtedness (as a ratio of capital stock) and a low profit rate and the other has a low level of external indebtedness and a high profit rate. This means that ""excessive"" external indebtedness can result in stagnant growth due to its negative effect on the rate of profit. To test the effects of external indebtedness on the rate of economic growth in emerging economies, a dynamic panel is estimated to evaluate whether external debt has an effective negative influence on economic growth in emerging countries. An empirical test of demand-led growth equations with a dynamic panel for fifty-five emerging countries confirms the potential negative effects of external debt on long-term growth rates in the sample countries. © 2013 M.E. Sharpe, Inc. All rights reserved. © 2013 Elsevier B.V., All rights reserved.","Rocha, M.; Oreiro, J.L.",2013,10.2753/pke0160-3477350405,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880320477&doi=10.2753%2FPKE0160-3477350405&partnerID=40&md5=fb91bb6c33303ec63cde6a40b6bd6773,scopus,This paper develops and empirically tests a nonlinear Post Keynesian growth model to examine the relationship between external indebtedness and economic growth in emerging countries. The model suggests that excessive external debt can lead to stagnant growth by negatively impacting the profit rate. An empirical analysis using a dynamic panel of 55 emerging countries confirms this potential negative effect of external debt on long-term growth.,False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:43:10.129403
5eb44d63138a04b7,Capitalisation rates for commercial real estate investments: evidence from Australia,"Purpose: The paper is motivated by the paucity of empirical research on the determinants of capitalisation rates/yield in the commercial property market. Compared to property price determinants, the capitalisation rate has received significantly less attention. This is somewhat surprising given that the capitalisation rate is a more insightful indicator for investors on commercial property market performance than merely price changes or trends. The capitalisation rate, measured as the ratio of net operating income to the property’s capital value, captures the asset’s overall ability to generate income which is crucial for investors who typically invest in property for their income-generating capacity. The purpose of this paper is to address these issues. Design/methodology/approach: To evaluate the determinants of capitalisation rates, time series analysis was used. The data capture performance in the Australian commercial property market between 2005 and 2018. All macroeconomic and financial data are freely available from official sources such as the Australian Bureau of Statistics and the nation’s central bank. Methodology wise, given the problematic nature of the data such as a mixed order of integration and the possibility of cointegration amongst some of the I (1) variables, the autoregressive distributed lag model was selected given its flexibility and relative lack of assumptions. Findings: Bond rates, market risk premiums, stock market excess returns and other macroeconomic variables were found to drive capitalisation rates of Australian commercial properties. A 1% increase in the bond rate results in approximately 0.3–2.4% increase in capitalisation rates depending on the sub-market. Further, a 1% increase in excess market returns results in a 0.01–0.02% increase in capitalisation rates. Regarding risk premiums, a 100 basis point increase in the BBB spread results in approximately 0.92–1.27% reduction in cap rates in certain markets. Practical implications: Asset managers will find these results useful in asset allocation strategies. Commercial properties offer attractive investment qualities such as yield stability in periods of economic uncertainty while allowing for the possibility of capital growth through appreciation of the underlying asset. By understanding the factors that affect the capitalisation rate, practitioners may predict emerging trends and identify threats to portfolio return and stability. This allows better integration of commercial property in the construction of portfolios that remain robust in a variety of market conditions. Originality/value: The contribution to literature is significant given the lack of similar studies in the Australian market. The performance of real estate assets using cap rates as a comparative measure to equities and bonds influences decisions in asset allocation strategies. It provides crucial information for investors to estimate the performance of commercial property. This research supports the notion that both space and capital market indicators jointly affect capitalisation rates. The findings expand the knowledge base relating to commercial properties and validate the assessments of investors, developers and valuers who utilise yield as a performance benchmark for asset allocation strategies. © 2023 Elsevier B.V., All rights reserved.","Wong, W.W.; Mintah, K.; Baako, K.; Wong, P.Y.",2023,10.1108/jpif-09-2022-0063,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146597624&doi=10.1108%2FJPIF-09-2022-0063&partnerID=40&md5=a7597a55a32cf9357caad38cd9cdc398,scopus,"This study investigates the determinants of capitalisation rates in the Australian commercial property market from 2005 to 2018 using time series analysis and an autoregressive distributed lag model. Key findings indicate that bond rates, market risk premiums, and stock market excess returns significantly influence capitalisation rates. The research provides practical implications for asset managers in their allocation strategies and contributes to the literature by offering insights into commercial property performance relative to other asset classes.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:43:11.696560
925a15bd08c4a54b,Changes in predictive ability with mixed frequency data,"When assessing the predictive power of financial variables for economic activity, researchers usually aggregate higher-frequency data before estimating a forecasting model that assumes the relationship between the financial variable and the dependent variable to be linear. This paper proposes a model called smooth transition mixed data sampling (STMIDAS) regression, which relaxes both of these assumptions. Simulation exercises indicate that the improvements in forecasting accuracy from the use of mixed data sampling are larger in nonlinear than in linear specifications. When forecasting output growth with financial variables in real time, statistically significant improvements over a linear regression are more likely to arise from forecasting with STMIDAS than with MIDAS regressions. (C) 2012 International Institute of Forecasters. Published by Elsevier B.V. All rights reserved.","Galvao, Ana Beatriz",2013,10.1016/j.ijforecast.2012.10.006,,wos,"This paper introduces the smooth transition mixed data sampling (STMIDAS) regression model, which improves forecasting accuracy by relaxing assumptions of linear relationships and data aggregation. Simulation exercises and real-time forecasting of output growth with financial variables show that STMIDAS outperforms traditional MIDAS regressions, especially in nonlinear specifications.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:43:20.666730
b127fcd8bbc8c65b,Changing beliefs and the term structure of interest rates: Cross-equation restrictions with drifting parameters,"This paper shows how to estimate a Bayesian VAR with drifting parameters and nonlinear cross-equation restrictions. The restrictions promote parsimony by reducing the dimension of the drifting component in conditional mean parameters. As an application, the paper investigates an anticipated-utility version of the expectations model of the term structure. The estimates suggest that changing beliefs matter for understanding the yield curve and point to an intriguing clue about risk premia. Local approximations to the mean yield spread are highly correlated with the variance of the trend short rate, suggesting a connection between uncertainty about the long-run target of monetary policy and risk premia on long-term bonds. © 2005 Elsevier Inc. All rights reserved. © 2018 Elsevier B.V., All rights reserved.","Cogley, T.",2005,10.1016/j.red.2005.01.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-16244390866&doi=10.1016%2Fj.red.2005.01.004&partnerID=40&md5=0033d48127db50efe621f016c846fa59,scopus,"This paper presents a method for estimating a Bayesian VAR with drifting parameters and nonlinear cross-equation restrictions, applied to the expectations model of the term structure. The findings suggest that evolving beliefs influence the yield curve and are linked to risk premia, with uncertainty about monetary policy's long-run target correlating with risk premia on long-term bonds.",True,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:43:22.953195
d649020ccc11a84c,Classification Ratemaking Using Decision Tree in the Insurance Market of Bosnia and Herzegovina,"This paper investigates the impact of risk classification on life insurance ratemaking with particular reference to Bosnia and Herzegovina (BiH). The research is based on a sample of over eighteen thousand insurance policies for passenger vehicles collected over the period 2015-2020. In our empirical investigation we develop a standard risk model based on the application of Poisson Generalized linear models (GLM) for claims frequency estimate and Gamma GLM for claim severity estimate. The analysis reveals that GLM does not provide a reliable parameter estimates for Multi-level factor (MLF) categorical predictors. Although GLM is widely used method to deter insurance premiums, improvements of GLM by using the data mining methods identified in this paper may solve practical challenges for the risk models. The popularity of applying data mining methods in the actuarial community has been growing in recent years due to its efficiency and precision. These models are recommended to be considered in BiH and South East European region in general. © 2021 Elsevier B.V., All rights reserved.","Omerašević, A.; Selimović, J.",2020,10.2478/jeb-2020-0020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099800105&doi=10.2478%2Fjeb-2020-0020&partnerID=40&md5=c11ce006692f8cbd8140a430aeca56e2,scopus,"This paper explores the use of data mining methods, specifically decision trees, to improve risk classification and ratemaking in the insurance market of Bosnia and Herzegovina. It compares these methods to traditional Generalized Linear Models (GLMs) using a dataset of over 18,000 passenger vehicle insurance policies from 2015-2020. The study suggests that data mining techniques offer more reliable parameter estimates for categorical predictors than GLMs and recommends their adoption in the region.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:43:26.196044
45a2cb3e694beb29,Cointegration and detectable linear and nonlinear causality: Analysis using the London Metal Exchange lead contract,"This study applies linear and nonlinear Granger causality tests to examine the dynamic relation between London Metal Exchange (LME) cash prices and three possible predictors. The analysis uses matched quarterly inventory, UK Treasury bill interest rates, futures prices and cash prices for the commodity lead traded on the LME. The effects of cointegration on both linear and nonlinear Granger causality tests is also examined. When cointegration is not modelled, evidence is found of both linear and nonlinear causality between cash prices and analysed predictor variables. However, after controlling for cointegration, evidence of significant nonlinear causality is no longer found. These results contribute to the empirical literature on commodity price forecasting by highlighting the relationship between cointegration and detectable linear and nonlinear causality. The importance of interest rate and inventory as well as futures price in forecasting cash prices is also illustrated. Failure to detect significant nonlinearity after controlling for cointegration may also go some way to explaining the reason for the disappointing forecasting performances of many nonlinear models in the general finance literature. It may be that the variables are correct, but the functional form is overly complex and a standard VAR or VECM may often apply. © 2004 Taylor and Francis Ltd. © 2008 Elsevier B.V., All rights reserved.","Chen, A.-S.; Lin, J.W.",2004,10.1080/0003684042000247352,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3543067056&doi=10.1080%2F0003684042000247352&partnerID=40&md5=4a669c06e8048b8dafca13e8c66096a8,scopus,"This study investigates the relationship between London Metal Exchange (LME) cash prices and predictor variables (inventory, interest rates, futures prices) using linear and nonlinear Granger causality tests. It examines the impact of cointegration on these tests. While initial analysis shows both linear and nonlinear causality, controlling for cointegration eliminates significant nonlinear causality. The findings suggest that cointegration is crucial for understanding causality and that simpler models like VAR or VECM might be more appropriate than complex nonlinear models for forecasting commodity prices.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:43:29.365588
81fa20cb81e0b280,Combining the wisdom of crowds and technical analysis for financial market prediction using deep random subspace ensembles,"Many researchers and practitioners have attempted to predict financial market trends for excess returns using multiple information sources including social media. Recent studies have investigated the relation between public sentiment and stock price movements and demonstrated that investment decisions are affected by public opinion. In this paper, we design a novel framework that combines the wisdom of crowds and technical analysis for financial market prediction using a new fusion strategy. A machine learning technique called deep random subspace ensembles (DRSE), which integrates deep learning algorithms and ensemble learning methods, is proposed according to the characteristics of the prediction task. Based on collected real-world datasets, the experimental results show that our proposed method outperforms the baseline models in predicting stock market by at least 14.2% in terms of AUC value, indicating the efficacy of DRSE as a viable mechanism for financial market prediction. © 2019 Elsevier B.V., All rights reserved.","Wang, Q.; Xu, W.; Zheng, H.",2018,10.1016/j.neucom.2018.02.095,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045111057&doi=10.1016%2Fj.neucom.2018.02.095&partnerID=40&md5=f9d301ef6362de9274f7f73bb4fbd365,scopus,"This paper proposes a novel framework combining crowd wisdom and technical analysis for financial market prediction using a deep random subspace ensemble (DRSE) method. Experiments on real-world datasets show DRSE outperforms baseline models in stock market prediction, achieving at least a 14.2% improvement in AUC.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:43:37.655233
c75986c18e735da3,Communicating the promise for ocular gene therapies: Challenges and recommendations,"Purpose To identify challenges and pose solutions for communications about ocular gene therapy between patients and clinicians as clinical research progresses. Design Literature review with recommendations. Methods Literature review of science communication best practices to inform recommendations for patient-clinician discussions about ocular gene therapy. Results Clinicians need to employ communications about ocular gene therapy that are both attentive to patient priorities and concerns and responsive to other sources of information, including overly positive news media and the Internet. Coverage often conflates research with therapy - clinical trials are experimental and are not risk free. If proven safe and efficacious, gene therapy may present a treatment but not a cure for patients who have already experienced vision loss. Clinicians can assist patients by providing realistic estimates for lengthy clinical development timelines and positioning current research within models of clinical translation. This enables patients to weigh future therapeutic options when making current disease management decisions. Conclusions Ocular gene therapy clinical trials are raising hopes for treating a myriad of hereditary retinopathies, but most such therapies are many years in the future. Clinicians should be prepared to counter overly positive messaging, found in news media and on the Internet, with optimism tempered by evidence to support the ethical translation of gene therapy and other novel biotherapeutics. © 2018 Elsevier B.V., All rights reserved.","Benjaminy, S.; Kowal, S.P.; MacDonald, I.M.; Bubela, T.",2015,10.1016/j.ajo.2015.05.026,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939258801&doi=10.1016%2Fj.ajo.2015.05.026&partnerID=40&md5=fdaf1a7223d511416c7b84feed4cf280,scopus,"This literature review identifies challenges and proposes solutions for communication between patients and clinicians regarding ocular gene therapy. It highlights the need for clinicians to provide realistic expectations, address misinformation from media and the internet, and differentiate between research and actual therapy, emphasizing that gene therapies are often years away and may not be cures.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:43:39.835949
280680c0ee200d86,Comparison of non-linear optimization algorithms for yield curve estimation,"The yield curve is a very important financial tool used in investment and policy decisions. Its estimation from market data is essentially a non-linear optimization problem. In this paper, we compare a diversity of non-linear optimization algorithms for estimating yield curves based on actual bond market data and conclude that certain classes of algorithms are more effective due to the nature of the problem. (C) 2007 Elsevier B.V. All rights reserved.","Manousopoulos, Polychronis; Michalopoulos, Michalis",2009,10.1016/j.ejor.2007.09.017,,wos,"This paper compares various non-linear optimization algorithms for estimating yield curves using real bond market data, finding certain algorithms to be more effective for this financial application.",True,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:43:47.096074
c2146119cca81457,"Complex systems and ‘‘Spatio ‐Temporal Anti‐Compliance Coordination’’ In cyber‐physical networks: A critique of the Hipster Effect, bankruptcy prediction and alternative risk premia","The Hipster Effect is a group of evolutionary ‘‘Diffusive Learning’’ processes of networks of individuals and groups (and their communication devices) that form Cyber‐Physical Systems; and the Hipster Effect theory has potential applications in many fields of research. This study addresses decision‐making parameters in machine‐learning algorithms, and more specifically, critiques the explanations for the Hipster Effect, and discusses the implications for portfolio management and corporate bankruptcy prediction (two areas where AI has been used extensively). The methodological approach in this study is entirely theoretical analysis. The main findings are as follows: (i) the Hipster Effect theory and associated mathematical models are flawed; (ii) some decision‐making and learning models in machine‐learning algorithms are flawed; (iii) but regardless of whether or not the Hipster Effect theory is correct, it can be used to develop portfolio management models, some of which are summarised herein; (iv) the [1] corporate bankruptcy prediction model can also be used for portfolio‐selection (stocks and bonds).",,2021,10.1049/ccs2.12029,,proquest,"This theoretical analysis critiques the ""Hipster Effect"" in cyber-physical networks and its applications in machine learning, portfolio management, and bankruptcy prediction. The study argues that the Hipster Effect theory and some machine learning models are flawed, but suggests potential applications for portfolio management and corporate bankruptcy prediction models.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:43:49.176514
8c2778fc6d16b718,"Complexity, nonlinearity and high frequency financial data modeling: lessons from computational approaches","This editorial introduces the special issue Complexity, Nonlinearity and High Frequency Financial Data Modeling: Lessons from Computational Approaches in Annals of Operations Research, which brings together 19 contributions exploring advanced methods and applications in the analysis of financial markets. The collected works reflect the growing importance of complexity and nonlinear dynamics in understanding modern financial systems, marked by high volatility, interdependence, and structural shifts. The papers are organized thematically into five main areas: (i) complexity and nonlinearity in financial markets, (ii) advanced forecasting and econometric modeling, (iii) network theory, causality, and information flows, (iv) banking, credit risk, and economic growth, and (v) continuous-time and structural model reviews. There is an additional section on methodological innovations, which include time–frequency and multi-scale analysis, recent developments of nonlinear and regime-switching models, machine learning, and complex network approaches. A heartfelt tribute is dedicated to the late Marco Tucci, co-editor of this special issue, whose vision and scholarly contributions significantly shaped its content. Sadly, Marco passed away while we were in the process of compiling this special issue. The editorial concludes by highlighting common methodological threads, synthesizing key insights, and outlining promising avenues for future research in complexity-informed financial modeling.",,2025,10.1007/s10479-025-06809-z,,proquest,"This editorial introduces a special issue focused on complexity, nonlinearity, and high-frequency financial data modeling using computational approaches. It highlights 19 contributions covering various themes such as market complexity, advanced forecasting, network theory, banking risk, and model reviews, with a focus on methodological innovations like machine learning and time-frequency analysis. The editorial also pays tribute to co-editor Marco Tucci and outlines future research directions.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:44:01.790958
807b76db734ed6fd,Conditional Skewness in Asset Pricing: 25 Years of Out-of-Sample Evidence,"Much attention is paid to portfolio variance, but skewness is also important for both portfolio design and asset pricing. We revisit the empirical research on systematic skewness that we initiated 25 years ago. We analyze the out-of-sample evidence for the skewness risk premium presented in the literature including the recent work of Anghel et al. (2023). We also conduct an out-of-sample test and focus on the sensitivity of the risk premium estimate to different research choices. Overall, we find that the risk premium associated with systematic skewness is similar to the one reported in our original paper. © 2024 Elsevier B.V., All rights reserved.","Harvey, C.R.; Siddique, A.",2023,10.1561/104.00000134,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192779205&doi=10.1561%2F104.00000134&partnerID=40&md5=bf020e243fcc8a2f732bf1780d4d5319,scopus,"This paper revisits the empirical evidence for the skewness risk premium in asset pricing, building on 25 years of research. It analyzes out-of-sample data, including recent studies, and finds that the risk premium associated with systematic skewness remains consistent with earlier findings. The study also examines the sensitivity of the risk premium estimate to various research choices.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:44:03.641557
980a360dd8f36999,Conditioning information and variance bounds on pricing kernels,"Gallant, Hansen, and Tauchen (1990) show how to use conditioning information optimally to construct a sharper unconditional variance bound (the GHT bound) on pricing kernels. The literature predominantly resorts to a simple but suboptimal procedure that scales returns with predictive instruments and computes standard bounds using the original and scaled returns. This article provides a formal bridge between the two approaches. We propose an optimally scaled bound that coincides with the GHT bound when the first and second conditional moments are known. When these moments are misspecified, our optimally scaled bound yields a valid lower bound for the standard deviation of pricing kernels, whereas the GHT bound does not. We illustrate the behavior of the bounds using a number of linear and nonlinear models for consumption growth and bond and stock returns. We also illustrate how the optimally scaled bound can be used as a diagnostic for the specification of the first two conditional moments of asset returns.","Bekaert, G; Liu, J",2004,10.1093/rfs/hhg052,,wos,"This article bridges the gap between the optimal conditioning information approach (GHT bound) and a common suboptimal procedure for bounding pricing kernels. It proposes an ""optimally scaled bound"" that matches the GHT bound when conditional moments are known and provides a valid lower bound for the standard deviation of pricing kernels even when these moments are misspecified. The authors demonstrate the bounds' behavior with linear and nonlinear models for consumption growth and asset returns, and suggest the optimally scaled bound as a diagnostic tool for moment specification.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:44:08.998549
9b6490fe957a02c8,Constructing Risk Analysis for Changes in China’s Local Government Bond System Based on SSP,"The local government bond system of China has experienced a series of changes from its initial creation to its abolition and then to a recovery again. During the period, the central government always dominated the changing direction of the local government bond system. However, as fiscal decentralization reform has progressed, the institutional needs of local governments and investors have gradually gained attention. As a result, the size and variety of local government bonds are expanding. Through the introduction of analysis of system change based on situation structure performance (SSP), this paper uses Machine Learning (ML) approaches to predict the risk of government debt of China in the context of changing the local government bond system. Besides, this research work includes the comprehensive weight assignment for government debt hazard, fiscal revenue forecasting, default risk calculation, and finally an analysis of the validity of government debt hazard. The system may provide financial signal advice and strategy reference for dealing with hazards in early payment, organizing debt repayment significance order, optimizing fiscal revenue and cost structure, and so on.",,2022,10.1155/2022/4606905,,proquest,"This paper analyzes risks associated with changes in China's local government bond system using a Situation Structure Performance (SSP) framework and Machine Learning (ML) approaches. It aims to predict government debt risk, considering factors like fiscal revenue forecasting and default risk, to provide financial signal advice and strategy references.",True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:44:27.493681
1914bca37c18fad2,Corporate bond pricing and the effects of endogenous default and call options,"Acharya and Carpenter [2002] model the value of a defaultable and callable bond. Focusing on the interaction between the call and default options, they note that the existence of one option delays the exercise of the other. This has strong empirical implications for duration and for the determinants of yield spread changes. For duration, when considered separately, both default and call shorten duration. However, when the interaction between the two risks is also accounted for, it is shown to lengthen duration. Since duration is equivalent to the interest-rate elasticity of the bond, the above analysis is also important for the interest-rate sensitivity of the bond yield spread. Sarkar and Hong [2004] also derive a structural model for pricing a fixed-price callable and defaultable bond, and their model has similar implications to that of Acharya and Carpenter [2002]. In this article we test the main implications of these theoretical models. In particular, we test the interest-rate elasticity of the call spread and that of the default spread, allowing for interaction between both spreads. We also examine the impact of both risks and their interaction on the effective duration of corporate bonds. In our tests we examine both bonds carrying a fixed-price call option and those carrying the newer and more popular makewhole option. We find evidence supporting the predictions of the two models. First, our data show a statistically significant interaction between default spreads and call spreads. Second, as the theory predicts, we show that, when considered separately, both default and call risks shorten duration for fixed-price callable bonds. When both risks are considered together, in general we find that the interaction term lengthens the effective duration of callable defaultable bonds. This finding is in agreement with the theory. The implication of our findings is that portfolio managers of callable corporate bonds, who use duration as an immunization tool or practice active rate anticipation strategies, must pay attention to the maturity of the bond. For example, it is clear that no risk adjustment is necessary when one manages A rated make-whole callable bonds with short and medium maturities. However, it is important to consider not only the impacts of default and call risks on estimating duration but also the interaction between these variables when one manages callable debt of all other maturities. We repeat our tests for bond portfolios stratified by ratings. In general, the results for the portfolios are also in agreement with the implications of the Acharya and Carpenter [2002] model. Another prediction made by Acharya and Carpenter is that both the noncallable and callable bonds' sensitivity to firm value decrease. For noncallable bonds, we find that the default spread's sensitivity to equity return is much greater for BBB rated bonds compared with AA and A rated bonds. Acharya and Carpenter [2002] also argue that callable bond's sensitivity to firm value is lower than that of a noncallable bond. In general, our results support this theoretical prediction, especially for make-whole bonds. The models of Acharya and Carpenter [2002] and Sarkar and Hong [2004] focus on pricing fixed-price callable bonds. However, the make-whole call provision became the standard for callable bonds in recent years, both in the U.S. and Canada. Powers and Tsyplakov [2004] derive a structural model for pricing make-whole callable bonds. Their model does not allow the study of duration similar to that of Acharya and Carpenter [2002] and Sarkar and Hong [2004]. Given the popularity of make-whole bonds, a possible direction for future research is the introduction of a structural bond-pricing model that allows studying the duration and yield-spread elasticity of these bonds. © 2010 Elsevier B.V., All rights reserved.","Jacoby, G.; Shiller, I.",2010,10.3905/jfi.2010.20.2.080,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957848360&doi=10.3905%2Fjfi.2010.20.2.080&partnerID=40&md5=93545e73cc89f0bfc89b6c60f801f529,scopus,"This study empirically tests the implications of theoretical models by Acharya and Carpenter (2002) and Sarkar and Hong (2004) on corporate bond pricing, focusing on the interaction between default and call options. The research examines how these risks affect bond duration and yield spreads, considering both fixed-price and make-whole call provisions. Findings support the models, indicating that the interaction between default and call risks generally lengthens effective duration, contrary to their individual effects. The study also investigates the sensitivity of bond prices to firm value and equity returns across different credit ratings, finding support for theoretical predictions.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:44:30.546093
446b6410f6523443,Counting the investor vote: Political business cycle effects on sovereign bond spreads in developing countries,"International business research has paid scant attention to whether and how electoral politics and economic policies affect foreign investment risk assessment, particularly in developing countries, where the last decade has seen both considerable foreign investment and domestic progress toward democratization and electoral competitiveness. We respond with development and testing of a framework using partisan and opportunistic political business cycle (PBC) theory to predict the investment risk perceived by investors holding sovereign bonds during 19 presidential elections in 12 developing countries from 1994 to 2000. Consistent with our framework, we find that bondholders perceive higher (lower) investment risk in the form of higher (lower) credit spreads on their sovereign bonds as right-wing (left-wing) political incumbents appear more likely to be replaced by left-wing (right-wing) challengers. For international business research, our findings illustrate the promise of PBC theory in explaining the election-period behavior of sovereign bondholders and, perhaps, other investors who also 'vote' in developing country elections and can substantially influence the price and availability of capital there. For developing country investors and states, our findings highlight the financial effects of democracy in action, and underscore the importance of state communication with investors during election periods. © 2005 Palgrave Macmillan Ltd. All rights reserved. © 2024 Elsevier B.V., All rights reserved.","Vaaler, P.M.; Schrage, B.N.; Block, S.A.",2005,10.1057/palgrave.jibs.8400111,https://www.scopus.com/inward/record.uri?eid=2-s2.0-14944342935&doi=10.1057%2Fpalgrave.jibs.8400111&partnerID=40&md5=682574fa0a86b86aad256960bf9d9a1c,scopus,"This study investigates the impact of electoral politics on sovereign bond spreads in developing countries, using partisan and opportunistic political business cycle theory. It finds that bondholders perceive higher investment risk (wider credit spreads) when left-wing challengers are likely to replace right-wing incumbents, and vice versa. The research suggests that political business cycle theory can explain investor behavior during elections and highlights the financial implications of democracy in developing nations.",False,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:44:32.473704
c924d5a2b4699539,Creating investment scheme with state space modeling,"This paper proposes a unified approach to creating investment strategies with various desirable properties for investors. Particularly, we provide a new interpretation and the resulting formulations for state space models to attain our investment objectives, which are possibly specified as generating additional returns over benchmark stock indexes or achieving target risk-adjusted returns.Our state space models with particle filtering algorithm are employed to develop expert systems for investment strategies in highly complex financial markets. More concretely, in our state space framework, we apply a system model to representing portfolio weight processes with various constraints, as well as the standard underlying state variables such as volatility processes, Further, we formulate an observation model to stand for target value processes with non-linear functions of observed and latent variables.Numerical experiments demonstrate the effectiveness of our methodology through creating excess returns over S&P 500 and generating investment portfolios with fine risk-return profiles. (C) 2017 Elsevier Ltd. All rights reserved.","Nakano, Masafumi; Takahashi, Akihiko; Takahashi, Soichiro",2017,10.1016/j.eswa.2017.03.045,,wos,This paper presents a unified approach using state space models and particle filtering to create investment strategies. The models aim to generate excess returns over benchmarks or achieve target risk-adjusted returns by representing portfolio weights and underlying variables like volatility. Numerical experiments show the methodology's effectiveness in outperforming the S&P 500 and creating portfolios with good risk-return profiles.,True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:44:51.726883
983a23dd8bacff46,Credit Debt Default Risk Assessment Based on the XGBoost Algorithm: An Empirical Study from China,"The bond market is an important part of China’s capital market. However, defaults have become frequent in the bond market in recent years, and consequently, the default risk of Chinese credit bonds has become increasingly prominent. Therefore, the assessment of default risk is particularly important. In this paper, we utilize 31 indicators at the macroeconomic level and the corporate microlevel for the prediction of bond defaults, and we conduct principal component analysis to extract 10 principal components from them. We use the XGBoost algorithm to analyze the importance of variables and assess the credit debt default risk based on the XGBoost prediction model through the calculation of evaluation indicators such as the area under the ROC curve (AUC), accuracy, precision, recall, and F1-score, in order to evaluate the classification prediction effect of the model. Finally, the grid search algorithm and k-fold cross-validation are used to optimize the parameters of the XGBoost model and determine the final classification prediction model. Existing research has focused on the selection of bond default risk prediction indicators and the application of XGBoost algorithm in default risk prediction. After optimization of the parameters, the optimized XGBoost algorithm is found to be more accurate than the original algorithm. The grid search and k-fold cross-validation algorithms are used to optimize the XGBoost model for predicting the default risk of credit bonds, resulting in higher accuracy of the proposed model. Our research results demonstrate that the optimized XGBoost model has a significantly improved prediction accuracy, compared to the original model, which is beneficial to improving the prediction effect for practical applications.",,2022,10.1155/2022/8005493,,proquest,"This study uses the XGBoost algorithm to assess credit debt default risk in China's bond market, employing 31 indicators reduced to 10 principal components. The model's performance is evaluated using AUC, accuracy, precision, recall, and F1-score, with parameters optimized via grid search and k-fold cross-validation. The optimized XGBoost model shows improved prediction accuracy compared to the original.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:44:59.586340
fe9ab6866fa2b491,"Credit Spreads, Leverage and Volatility: A Cointegration Approach","This work documents the existence of a cointegration relationship between credit spreads, leverage and equity volatility for a large set of US companies. It is shown that accounting for the long-run equilibrium dynamic between these variables is essential to correctly explain credit spread changes. Using a novel structural model in which equity is modeled as a compound option on the firm's assets, a new methodology for estimating the unobservable market value of the firm's assets and volatility is developed. The proposed model allows to significantly reduce the pricing errors in predicting credit spreads when compared with several structural models. In terms of correlation analysis, it is shown that not accounting for the long-run equilibrium equation embedded in an error correction mechanism (ECM) results into a misspecification problem when regressing a set of explanatory variables onto the spread changes. Once credit spreads, leverage and volatility are correctly modeled, thus allowing for a long-run equilibrium, the fit of the regressions sensibly increases if compared to the results of previous research. It is further shown that most of the cross-sectional variation of the spreads appears to be more driven by firm-specific characteristics rather than systematic factors.","Maglione, Federico",2022,10.3390/computation10090155,,wos,"This study investigates the cointegration relationship between credit spreads, leverage, and equity volatility for US companies. It proposes a structural model treating equity as a compound option on firm assets, enabling estimation of unobservable market values and volatility. The model improves credit spread prediction accuracy compared to existing structural models and highlights the importance of accounting for long-run equilibrium dynamics via an error correction mechanism. The findings suggest firm-specific characteristics are primary drivers of credit spread variation.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:45:04.777868
c93e69f76880bb85,"Credit growth, the yield curve and financial crisis prediction: Evidence from a machine learning approach","We develop early warning models for financial crisis prediction applying machine learning techniques on macrofinancial data for 17 countries over 1870–2016. Most nonlinear machine learning models outperform logistic regression in out-of-sample predictions and forecasting. We identify economic drivers of our machine learning models by applying a novel framework based on Shapley values, uncovering nonlinear relationships between the predictors and crisis risk. Throughout, the most important predictors are credit growth and the slope of the yield curve, both domestically and globally. A flat or inverted yield curve is of most concern when nominal interest rates are low and credit growth is high. © 2023 Elsevier B.V., All rights reserved.","Bluwstein, K.; Buckmann, M.; Joseph, A.; Kapadia, S.; Şimşek, Ö.",2023,10.1016/j.jinteco.2023.103773,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153873224&doi=10.1016%2Fj.jinteco.2023.103773&partnerID=40&md5=abb66d3fa385d12f8957f0e53e487141,scopus,"This study develops early warning models for financial crisis prediction using machine learning techniques on macrofinancial data from 17 countries spanning 1870-2016. The models, particularly nonlinear ones, demonstrate superior out-of-sample predictive performance compared to logistic regression. Key predictors identified through Shapley values include credit growth and the yield curve slope, with a flat or inverted yield curve being particularly concerning when combined with low nominal interest rates and high credit growth.",True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:45:18.262905
c54779c722cfa540,Cross-sectional regression of returns on betas and portfolio grouping procedures,"This paper shows that the deviation of the estimated coefficient of beta from the market risk premium in cross-sectional regression of returns on betas is a direct consequence of the cross-sectional relation between the estimated alphas and betas. Therefore, the portfolio grouping procedure results in systematic cross-sectional relationship between the alphas and betas, causing a deviation in the estimated coefficient of beta in either direction. When firm size is used as the only portfolio grouping variable (Table AI in Fama and French, 1992), the estimated alphas and betas across portfolios are positively related, causing the estimated coefficient of beta to be upwardly biased. However, when beta is used as the only portfolio grouping variable (Table 2 in Kothari et al., 1995), the estimated alphas and betas across portfolios are negatively related, causing the estimated coefficient of beta to be downward biased. We show that forming portfolios on alphas and betas independently can adequately control for this deviation. © 2014 Inderscience Enterprises Ltd. © 2020 Elsevier B.V., All rights reserved.","Hur, J.; Kumar, R.; Singh, V.",2014,10.1504/ijbsr.2014.058005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901001561&doi=10.1504%2FIJBSR.2014.058005&partnerID=40&md5=6c9b7fb0ff1849da3457d63fab3a0b8b,scopus,"This paper investigates the deviation of the estimated coefficient of beta from the market risk premium in cross-sectional regression of returns on betas. It attributes this deviation to the cross-sectional relationship between estimated alphas and betas, which is influenced by portfolio grouping procedures. The study demonstrates how using firm size or beta as grouping variables can lead to upward or downward bias in the estimated beta coefficient, respectively. The authors propose forming portfolios on alphas and betas independently to control for this deviation.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:45:21.523810
17468938c29ef9d9,Currency Value,"We assess the properties of currency value strategies based on real exchange rates. We find that real exchange rates have predictive power for the cross-section of currency excess returns. However, adjusting real exchange rates for key country-specific fundamentals (productivity, the quality of export goods, net foreign assets, and output gaps) better isolates information related to the currency risk premium. In turn, the resultant measure of currency value displays considerably stronger predictive power for currency excess returns. Finally, the predictive information content in our currency value measure is distinct from that embedded in popular currency strategies, such as carry and momentum.","Menkhoff, Lukas; Sarno, Lucio; Schmeling, Maik; Schrimpf, Andreas",2017,10.1093/rfs/hhw067,,wos,This study investigates currency value strategies using real exchange rates and country-specific fundamentals. It finds that adjusting real exchange rates for factors like productivity and net foreign assets enhances their predictive power for currency excess returns. The proposed currency value measure is distinct from existing strategies like carry and momentum.,False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:45:22.785667
1aeb51a3f8d0fb1b,DEEP NEURAL NETWORKS METHODS FOR ESTIMATING MARKET MICROSTRUCTURE AND SPECULATIVE ATTACKS MODELS: THE CASE OF GOVERNMENT BOND MARKET,"A sovereign bond market offers a wide range of opportunities for public and private sector financing and has drawn the interest of both scholars and professionals as they are the main instrument of most fixed-income asset markets. Numerous works have studied the behavior of sovereign bonds at the microeconomic level, given that a domestic securities market can enhance overall financial stability and improve financial market intermediation. Nevertheless, they do not deepen methods that identify liquidity risks in bond markets. This study introduces a new model for predicting unexpected situations of speculative attacks in the government bond market, applying methods of deep learning neural networks, which proactively identify and quantify financial market risks. Our approach has a strong impact in anticipating possible speculative actions against the sovereign bond market and liquidity risks, so the aspect of the potential effect on the systemic risk is of high importance.",,2025,10.1142/s0217590822480034,,proquest,This study introduces a novel deep learning neural network model to predict speculative attacks and quantify liquidity risks in the government bond market. The model aims to proactively identify potential financial market risks and their impact on systemic risk.,True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:45:29.153277
8cc0fdcc0a19bb4a,DETECTING BUSINESS CYCLES FOR HUNGARIAN LEADING AND COINCIDENT INDICATORS WITH A MARKOV SWITCHING DYNAMIC MODEL TO IMPROVE SUSTAINABILITY IN ECONOMIC GROWTH,"This paper applies the hidden Markov switching dynamic regression (MSDR) model to estimate transition probabilities of the Hungarian GDP between recessionary and expansionary periods. The transition probabilities are then compared to the OECD Hungarian binary business cycle indicator to assess the predictive power of the model. The paper proposes a linear model with a mean and a homoscedastic component. The level of symmetricity between the GDP and business cycles is explained by the panel data variables (Unemployment rate, IPI index, Inflation, BUX year-on-year change, and 10-3 Year sovereign bond yield spreads). It is assumed in this paper that by extending the model to encompass an exogenous variable listed in the panel data, essentially making the model bivariate, the maximum likelihood function would capture the business cycle more accurately. The results show that by plugging the unemployment rate as the exogenous variable in the regression, our model’s accuracy is 70%. © 2023 Elsevier B.V., All rights reserved.","Molnár, A.; Vasa, L.; Csiszárik-Kocsir, Á.",2023,10.31181/dmame060120032023m,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164272002&doi=10.31181%2Fdmame060120032023m&partnerID=40&md5=47f287a47b3f2d769b9970cd41c0662f,scopus,"This paper utilizes a hidden Markov switching dynamic regression (MSDR) model to analyze Hungarian GDP, distinguishing between recessionary and expansionary periods. It compares the model's transition probabilities with the OECD's business cycle indicator and explores the relationship between GDP and business cycles using panel data variables like unemployment rate and inflation. The study suggests that incorporating the unemployment rate as an exogenous variable improves the model's accuracy to 70% in capturing business cycles.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:45:34.950747
45985ece0e559f1f,Data-Driven and Mechanistic Soil Modeling for Precision Fertilization Management in Cotton,"This study introduces a novel methodology for predicting cotton yield by integrating machine learning (ML) with mechanistic soil modeling. This hybrid approach enhances yield prediction by combining data-driven ML techniques with soil process modeling. Using the developed yield model, yield curves for various nitrogen (N) levels can be constructed to identify the optimal N dose that maximizes yield. Estimating cotton N requirements is crucial, as growers often apply excessive N, exceeding the amount needed for maximum yield. By comparing the Mean Absolute Error (MAE) between predicted and observed cotton yield values across three ML algorithms, i.e., Random Forest (RF), XGBoost, and LightGBM, the RF model achieved the lowest error (422.6 kg/ha), outperforming XGBoost (446 kg/ha) and LightGBM (449 kg/ha). Additionally, the RF model exhibited high sensitivity to N fertilization, ranking N as the most influential variable in feature importance analysis. Furthermore, phosphorus (P) availability in the soil model was found to be a significant factor influencing the RF yield model, highlighting P’s crucial role in cotton growth and productivity. © 2025 Elsevier B.V., All rights reserved.","Iatrou, M.; Tziachris, P.; Bilias, F.; Kekelis, P.; Pavlakis, C.; Theofilidou, A.; Papadopoulos, I.; Strouthopoulos, G.; Giannopoulos, G.; Arampatzis, D.; Vergos, E.; Karydas, C.; Beslemes, D.; Aschonitis, V.",2025,10.3390/nitrogen6020029,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009251145&doi=10.3390%2Fnitrogen6020029&partnerID=40&md5=b1c1d36c3eaba2b309b31ebf8aaad2ca,scopus,"This study presents a hybrid approach combining machine learning (ML) with mechanistic soil modeling to predict cotton yield and optimize nitrogen (N) fertilization. The Random Forest (RF) model demonstrated the best performance in yield prediction, identifying N as the most influential variable and highlighting the importance of phosphorus (P) availability. This method aims to reduce excessive N application by growers.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:45:36.900751
cc76b502f40b1012,Day trading profit maximization with multi-task learning and technical analysis,"Stock price movements are claimed to be chaotic and unpredictable, and mainstream theories of finance refute the possibility of realizing risk-free profit through predictive modelling. Despite this, a large body of technical analysis work maintains that price movements can be predicted solely from historical market data, i.e., markets are not completely efficient. In this paper we seek to test this claim empirically by developing a novel stochastic trading algorithm in the form of a linear model with a profit maximization objective. Using this method we show improvements over the competitive buy-and-hold baseline over a decade of stock market data for several companies. We further extend the approach to allow for non-stationarity in time, and using multi-task learning to modulate between individual companies and the overall market. Both approaches further improve the predictive profit. Overall this work shows that market movements do exhibit predictable patterns as captured through technical analysis.","Bitvai, Zsolt; Cohn, Trevor",2015,10.1007/s10994-014-5480-x,,wos,"This paper proposes a novel stochastic trading algorithm using multi-task learning and technical analysis to maximize profits in day trading. The algorithm, framed as a linear model with a profit maximization objective, is tested against historical stock market data and shows improvements over the buy-and-hold strategy. Extensions to handle non-stationarity and incorporate market-wide data further enhance predictive profit, suggesting that predictable patterns exist in market movements.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:45:39.375747
e83cfb6b1bda5312,Decision-making during the credit crisis: Did the Treasury let commercial banks fail?,"Limited attention has been paid to the comparative fate of banks benefiting from Capital Purchase Program (CPP) funding and less fortunate banks subject to FDIC resolution. We address this omission by investigating two core issues. One is whether commercial banks that ended up being subject to FDIC resolution received CPP funds. The other is whether the non-allocation of CPP funds made FDIC receivership more likely for viable commercial banks. Our findings show almost no overlap between CPP-funded and FDIC-resolved commercial banks, but we provide evidence that a significant number of FDIC-resolved banks could have avoided receivership if they had been allocated CPP funding. By comparing estimated funding and resolution costs we also show that bailing out more banks would have been cost-efficient. While our results do not allow for any policy suggestion on the optimality of bailouts per se, they suggest that once a bailout program is already on the table, it is better to err on the side of rescuing too many rather than too few banks. © 2016 Elsevier B.V., All rights reserved.","Croci, E.; Hertig, G.; Nowak, E.",2016,10.1016/j.jempfin.2016.01.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962173589&doi=10.1016%2Fj.jempfin.2016.01.001&partnerID=40&md5=85c4a94fa1e40a43f82f5e97c5786c32,scopus,"This study investigates whether commercial banks that received Capital Purchase Program (CPP) funding were treated differently from those subject to FDIC resolution during the credit crisis. The findings indicate minimal overlap between CPP-funded and FDIC-resolved banks, but suggest that some FDIC-resolved banks might have avoided resolution with CPP funding. The analysis also implies that rescuing more banks would have been cost-efficient, advocating for a more inclusive bailout approach.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:45:40.920893
3237c5fa565c89c2,Decomposing the yield curve with linear regressions and survey information,"The decomposition of bond yields into term premiums and average expected future short rates is impaired by the limited availability of information about the dynamics of the expectations component. Therefore, many studies require the model-implied average expected future short rates to be close to short rate expectations from surveys. In this paper, I restrict the variance of changes in model-implied average expected future short rates to match the variance of changes in short rate expectations from surveys. The variance of changes in survey expectations is relatively similar across markets and thus provides a reliable source of additional information about the expectation formation of investors. Technically, I impose a nonlinear restriction to the term structure model of Adrian, Crump, and Moench (2013). I show that typical small sample problems of term structure estimations can be mitigated if the restriction on the variance of changes is imposed. However, the analysis also makes a case for unrestricted estimations if they are based on a dataset with a typical sample length in macro finance. © 2023 Elsevier B.V., All rights reserved.","Halberstadt, A.",2023,10.1016/j.qref.2023.07.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166646520&doi=10.1016%2Fj.qref.2023.07.002&partnerID=40&md5=dfd4e947091892f6484d399c6125a8f4,scopus,This paper proposes a method to decompose bond yields into term premiums and expected future short rates by incorporating survey information to constrain the variance of changes in expected future short rates. This approach aims to mitigate small sample issues in term structure estimations and improve the accuracy of yield curve models.,False,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:46:24.294972
29abf8e325a84dfb,Deep treasury management for banks,"Retail banks use Asset Liability Management (ALM) to hedge interest rate risk associated with differences in maturity and predictability of their loan and deposit portfolios. The opposing goals of profiting from maturity transformation and hedging interest rate risk while adhering to numerous regulatory constraints make ALM a challenging problem. We formulate ALM as a high-dimensional stochastic control problem in which monthly investment and financing decisions drive the evolution of the bank's balance sheet. To find strategies that maximize long-term utility in the presence of constraints and stochastic interest rates, we train neural networks that parametrize the decision process. Our experiments provide practical insights and demonstrate that the approach of Deep ALM deduces dynamic strategies that outperform static benchmarks. © 2023 Elsevier B.V., All rights reserved.","Englisch, H.; Krabichler, T.; Müller, K.J.; Schwarz, M.",2023,10.3389/frai.2023.1120297,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152561995&doi=10.3389%2Ffrai.2023.1120297&partnerID=40&md5=b02754657e30e9df825a73854a19d290,scopus,"This paper presents a novel approach called Deep ALM, which uses neural networks to solve the complex Asset Liability Management (ALM) problem for retail banks. ALM aims to hedge interest rate risk while managing the balance sheet and adhering to regulations. The Deep ALM method formulates ALM as a high-dimensional stochastic control problem and trains neural networks to derive dynamic investment and financing strategies that outperform traditional methods.",True,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:46:27.588745
bf5e5a0f65403ae9,DeepClue: Visual Interpretation of Text-Based Deep Stock Prediction,"The recent advance of deep learning has enabled trading algorithms to predict stock price movements more accurately. Unfortunately, there is a significant gap in the real-world deployment of this breakthrough. For example, professional traders in their long-term careers have accumulated numerous trading rules, the myth of which they can understand quite well. On the other hand, deep learning models have been hardly interpretable. This paper presents DeepClue, a system built to bridge text-based deep learning models and end users through visually interpreting the key factors learned in the stock price prediction model. We make three contributions in DeepClue. First, by designing the deep neural network architecture for interpretation and applying an algorithm to extract relevant predictive factors, we provide a useful case on what can be interpreted out of the prediction model for end users. Second, by exploring hierarchies over the extracted factors and displaying these factors in an interactive, hierarchical visualization interface, we shed light on how to effectively communicate the interpreted model to end users. Specially, the interpretation separates the predictables from the unpredictables for stock prediction through the use of intercept model parameters and a risk visualization design. Third, we evaluate the integrated visualization system through two case studies in predicting the stock price with financial news and company-related tweets from social media. Quantitative experiments comparing the proposed neural network architecture with state-of-the-art models and the human baseline are conducted and reported. Feedbacks from an informal user study with domain experts are summarized and discussed in details. The study results demonstrate the effectiveness of DeepClue in helping to complete stock market investment and analysis tasks.",L. Shi; Z. Teng; L. Wang; Y. Zhang; A. Binder,2019,10.1109/tkde.2018.2854193,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8408524,ieeexplore,"This paper introduces DeepClue, a system designed to visually interpret text-based deep learning models for stock price prediction. It addresses the interpretability gap between complex models and end-users by extracting and visualizing key predictive factors in an interactive, hierarchical interface. The system separates predictable from unpredictable elements and is evaluated through case studies and user feedback, demonstrating its effectiveness in investment and analysis tasks.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:46:29.238645
fbf80399751a0d02,"Default, liquidity, and crises: An econometric framework","This article presents a general discrete-time affine framework aimed at jointly modeling yield curves associated with different debtors. The underlying fixed-income securities may differ in terms of credit quality and/or in terms of liquidity. The risk factors follow conditionally Gaussian processes, with drifts and covariance matrices that are subject to regime shifts described by a Markov chain with (historical) non-homogenous transition probabilities. Bond prices are given by quasi-explicit formulas. The tractability of the framework is illustrated by the estimation of a term-structure model of the spreads between U.S. BBB-rated corporate bonds and Treasuries. Alternative applications are proposed, including a sector-contagion model as well as the explicit modeling of credit-rating transitions. © The Author, 2012. © 2013 Elsevier B.V., All rights reserved.","Monfort, A.; Renne, J.-P.",2013,10.1093/jjfinec/nbs020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875255758&doi=10.1093%2Fjjfinec%2Fnbs020&partnerID=40&md5=4142c53787af3e844a2eb0e5d250da50,scopus,"This article introduces a discrete-time affine framework to model yield curves for different debtors, considering credit quality and liquidity. It uses conditionally Gaussian risk factors with Markov-switching regime shifts and provides quasi-explicit bond pricing formulas. The framework is demonstrated by estimating a term-structure model for U.S. BBB-rated corporate bond spreads against Treasuries, with potential applications in sector contagion and credit-rating transitions.",False,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:46:31.359448
eb29f049a6d970f8,Departures from Rational Expectations and Asset Pricing Anomalies,"We investigate the potential of the consumption CAPM with pessimism, doubt, and the availability heuristic in the agent's beliefs to resolve the equity premium and risk-free rate puzzles. Using the nonlinear GMM estimation techniques, we find that doubt and the availability heuristic play an important role in explaining the cross-section of asset returns. However, when taken alone, these deviations from rational expectations cannot resolve the equity premium and risk-free rate puzzles. This result is robust to the assumption that the expected value of an uncertain prospect is nonlinear in the subjective outcome probabilities.","Semenov, Andrei",2009,10.1080/15427560903373245,,wos,"This study explores how deviations from rational expectations, specifically pessimism, doubt, and the availability heuristic, can explain asset pricing anomalies like the equity premium and risk-free rate puzzles within the consumption CAPM framework. While doubt and the availability heuristic are found to be significant in explaining asset returns, they do not fully resolve these puzzles on their own. The findings remain consistent even when considering nonlinear subjective probabilities.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:46:33.099672
f1cadf4bf4556874,Determinants of cryptocurrency returns: A LASSO quantile regression approach,"We consider a relatively large set of predictors and investigate the determinants of cryptocurrency returns at different quantiles. Our analysis exclusively focuses on the highly volatile period of COVID-19. The innovation in the paper stems from the fact that we employ the LASSO penalty in a quantile regression framework to select informative variables. We find that US government bond indices and small company stock returns, a new predictor introduce in this study, significantly impact the tail behavior of the cryptocurrency returns. © 2022 Elsevier B.V., All rights reserved.","Ciner, C.; Lucey, B.; Yarovaya, L.",2022,10.1016/j.frl.2022.102990,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133923555&doi=10.1016%2Fj.frl.2022.102990&partnerID=40&md5=01a12ed6ee35436c208a8c486eac384a,scopus,This study uses LASSO quantile regression to analyze the determinants of cryptocurrency returns during the COVID-19 pandemic. It identifies US government bond indices and small company stock returns as significant factors influencing the tail behavior of cryptocurrency returns.,True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:46:41.493009
e1c46f7c4c1f6375,Developing a Composite Measure to Represent Information Flows in Networks: Evidence from a Stock Market,"There is increasing interest in information systems research to model information flows from different sources (e.g., social media, news) associated with a network of assets (e.g., stocks, products) and to study the economic impact of such information flows. This paper employs a design science approach and proposes a new composite metric, eigen attention centrality (EAC), as a proxy for information flows associated with a node that considers both attention to a node and coattention with other nodes in a network. We apply the EAC metric in the context of financial market where nodes are individual stocks and edges are based on coattention relationships among stocks. Composite information from different channels is used to measure attention and coattention. To evaluate the effectiveness of the EAC metric on predicting outcomes, we conduct an in-depth performance evaluation of the EAC metric by (1) using multiple linear and nonlinear prediction methods and (2) comparing EAC with a benchmark model without EAC and models with a set of alternative network metrics. Our analysis shows that EAC significantly outperforms other measures in predicting the direction and magnitude of abnormal returns of stocks. Besides, our EAC specification has better predictive performance than alternative specifications, and EAC outperforms direct attention in predicting abnormal returns. Using the EAC metric, we derive a stock portfolio and develop a trading strategy that provides significant and positive excess returns. Lastly, we find that composite information has significantly better predictive performance than separate information sources, and such superior performance owes to information from social media instead of traditional media.",,2022,10.1287/isre.2021.1066,,proquest,"This paper introduces a new composite metric, eigen attention centrality (EAC), to represent information flows in networks, specifically applied to stock markets. EAC considers both attention to and coattention with other nodes. The metric's effectiveness is evaluated using various prediction methods, showing it outperforms benchmark and alternative network metrics in predicting stock returns. A trading strategy based on EAC yields significant excess returns, and composite information, particularly from social media, proves more predictive than individual sources.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:46:45.504828
19d08c4e33e25dd9,Development of Adaptive Neuro-Fuzzy Inference System for Assessing Industry Leadership in Accident Situations,"Petroleum activity is characterized as a high-risk activity due to the probability of accidents with material and human losses. The leaders of this segment assume, besides the complex routine tasks, the challenge of making assertive decisions during an accident. This study aims to present an evaluation model of the Industry Leadership Index for Emergencies Situations (ILIE), using the Adaptive Neuro-Fuzzy System (ANFIS). The model was composed of 4 input variables, namely: knowledge, behavior, skill, and attitude; and one output variable, Industry Leadership. The data collection took place in petroleum production units in Brazil, with a sample of 151 respondents through the application of a survey. The observed data were treated in an Excel tabulator and used in the development of the ANFIS model. From this model, it was possible to carry out simulations to predict the impact, which the increase or decrease in the value of each input variable can influence the leader’s profile. The model performed satisfactorily in the Root of the Mean Square Error (RMSE) analysis, being 0.199 in data training and 1.217 in data verification. The results suggest that the ANFIS method can be successfully applied to establish a model to analyze industry leaders prepared for assertive responses in crisis scenarios.",I. C. D. S. Cerqueira; P. P. S. Carvalho; J. L. Moya Rodríguez; S. Á. Filho; F. G. M. Freires,2022,10.1109/access.2022.3206766,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9893569,ieeexplore,"This study developed an Adaptive Neuro-Fuzzy Inference System (ANFIS) to create an Industry Leadership Index for Emergencies Situations (ILIE) in the petroleum industry. The model uses four input variables (knowledge, behavior, skill, attitude) to predict the output variable (Industry Leadership). Data from 151 respondents in Brazilian petroleum units were used to train and verify the ANFIS model, which showed satisfactory performance. The results indicate ANFIS can effectively model industry leaders' preparedness for crisis response.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:46:48.011310
19c5432dca3228fd,Diffusion copulas: Identification and estimation,"We propose a new semiparametric approach for modelling nonlinear univariate diffusions, where the observed process is a nonparametric transformation of an underlying parametric diffusion (UPD). This modelling strategy yields a general class of semiparametric Markov diffusion models with parametric dynamic copulas and nonparametric marginal distributions. We provide primitive conditions for the identification of the UPD parameters together with the unknown transformations from discrete samples. Likelihood-based estimators of both parametric and nonparametric components are developed and we analyse their asymptotic properties. Kernel-based drift and diffusion estimators are also proposed and shown to be normally distributed in large samples. A simulation study investigates the finite sample performance of our estimators in the context of modelling US short-term interest rates. We also present a simple application of the proposed method for modelling the CBOE volatility index data. (C) 2020 Elsevier B.V. All rights reserved.","Bu, Ruijun; Hadri, Kaddour; Kristensen, Dennis",2021,10.1016/j.jeconom.2020.06.004,,wos,"This paper introduces a novel semiparametric method for modeling nonlinear univariate diffusions, where the observed process is a nonparametric transformation of an underlying parametric diffusion. This approach results in a class of semiparametric Markov diffusion models with parametric dynamic copulas and nonparametric marginal distributions. The authors establish conditions for identifying UPD parameters and unknown transformations from discrete samples, develop likelihood-based estimators for both parametric and nonparametric components, and analyze their asymptotic properties. Kernel-based drift and diffusion estimators are also proposed and shown to have normal distributions in large samples. The study includes a simulation to assess the finite sample performance of these estimators in modeling US short-term interest rates and an application to CBOE volatility index data.",True,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:46:49.727842
0866f9d53f9c17c0,Digital transformation: statistical evaluation of success factors of an ICO-campaign,"High rates of growth of the ICO market and its excess returns stipulate a significant interest of investors to projects which use initial token allocation (ICO) for attracting investments. This work takes into account the fact that even a potentially profitable project may fail to collect the required amount of money and to start placing tokens on the stock exchange. We are speaking about success of an ICO-campaign for fund raising. In order to estimate the influence of factors and check the suggested research hypotheses, logistic regression was used. The selection included 672 projects. As a dependent variable, the proportion of the amount collected in the ICO process from the required value is selected. Depending on the tested hypothesis the influencing variables took into account the presence of a pre-sale stage and the bounty program and also the price of the token, the upper limit of fund raising, the duration of the ICO-campaign and the number of team members. The work results allow token emitters to substantiate managing the success of the ICO-campaign of the project and the investors to see whether it deserves their attention. Besides, the obtained materials can be useful for specialists in forming the legal framework of token transactions.",,2019,10.1088/1757-899x/497/1/012087,,proquest,"This study uses logistic regression to evaluate success factors for Initial Coin Offering (ICO) campaigns, analyzing 672 projects. It considers variables like pre-sale stages, bounty programs, token price, fundraising limits, campaign duration, and team size to predict the proportion of funds raised relative to the target. The findings aim to help token issuers manage campaign success, investors assess project viability, and policymakers understand token transactions.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:46:51.573125
01ee1f71747e58fc,Discrete sine transform for multi-scale realized volatility measures,"In this study we present a new realized volatility estimator based on a combination of the multi-scale regression and discrete sine transform (DST) approaches. Multi-scale estimators similar to that recently proposed by Zhang (2006) can, in fact, be constructed within a simple regression-based approach by exploiting the linear relation existing between the market microstructure bias and the realized volatilities computed at different frequencies. We show how such a powerful multi-scale regression approach can also be applied in the context of the Zhou [Nonlinear Modelling of High Frequency Financial Time Series, pp. 109-123, 1998] or DST orthogonalization of the observed tick-by-tick returns. Providing a natural orthonormal basis decomposition of observed returns, the DST permits the optimal disentanglement of the volatility signal of the underlying price process from the market microstructure noise. The robustness of the DST approach with respect to the more general dependent structure of the microstructure noise is also shown analytically. The combination of the multi-scale regression approach with DST gives a multi-scale DST realized volatility estimator similar in efficiency to the optimal Cramer-Rao bounds and robust against a wide class of noise contamination and model misspecification. Monte Carlo simulations based on realistic models for price dynamics and market microstructure effects show the superiority of DST estimators over alternative volatility proxies for a wide range of noise-to-signal ratios and different types of noise contamination. Empirical analysis based on six years of tick-by-tick data for the S&P 500 index future, FIB 30, and 30 year U.S. Treasury Bond future confirms the accuracy and robustness of DST estimators for different types of real data. © 2012 Taylor and Francis Group, LLC. © 2012 Elsevier B.V., All rights reserved.","Curci, G.; Corsi, F.",2012,10.1080/14697688.2010.490561,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857188308&doi=10.1080%2F14697688.2010.490561&partnerID=40&md5=2c4e6b57d7f22d1af8754ae1cd5f005c,scopus,"This study introduces a novel realized volatility estimator by integrating multi-scale regression with the discrete sine transform (DST). The DST provides an orthonormal basis for decomposing returns, effectively separating volatility signals from microstructure noise. The proposed multi-scale DST estimator demonstrates high efficiency, robustness against noise and model misspecification, and outperforms alternative volatility proxies in Monte Carlo simulations. Empirical analysis on S&P 500 index futures, FIB 30, and U.S. Treasury Bond futures confirms its accuracy and robustness.",True,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:47:06.842652
ed308ed2ce635f75,Distributional modeling and forecasting of natural gas prices,"We examine the problem of modeling and forecasting European day‐ahead and month‐ahead natural gas prices. For this, we propose two distinct probabilistic models that can be utilized in risk and portfolio management. We use daily pricing data ranging from 2011 to 2020. Extensive descriptive data analysis shows that both time series feature heavy tails and conditional heteroscedasticity and show asymmetric behavior in their differences. We propose state‐space time series models under skewed, heavy‐tailed distributions to capture all stylized facts of the data. They include the impact of autocorrelation, seasonality, risk premia, temperature, storage levels, the price of European Emission Allowances, and related fuel prices of oil, coal, and electricity. We provide rigorous model diagnostics and interpret all model components in detail. Additionally, we conduct a probabilistic forecasting study with significance tests and compare the predictive performance against literature benchmarks. The proposed day‐ahead (month‐ahead) model leads to a 13% (9%) reduction in out‐of‐sample continuous ranked probability score (CRPS) compared with the best performing benchmark model, mainly due to adequate modeling of the volatility and heavy tails.",,2022,10.1002/for.2853,,proquest,"This paper proposes two probabilistic state-space time series models for forecasting European day-ahead and month-ahead natural gas prices, incorporating factors like temperature, storage levels, and related fuel prices. The models utilize skewed, heavy-tailed distributions to capture the stylized facts of the data, including heavy tails and conditional heteroscedasticity. The proposed models demonstrate improved predictive performance compared to benchmark models, particularly in modeling volatility and heavy tails.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:47:10.691121
dfa20513b40f2c55,Diving into recession: the collective knowledge of online users as an early warning system for recessionary expectations,"As concerns about economic downturns manifest in online discussions, we investigate whether sentiment extracted from social media can serve as an early warning signal for recessionary pressures. Using a dataset of Twitter (X) posts related to economic prospects, we apply a range of sentiment analysis techniques, including a lexicon and rule-based method (VADER) and deep learning approaches (GPT and BERT). We assess the relationship between online sentiment and key recession indicators, such as the yield curve and GDPNow forecasts, using a combination of econometric and machine learning methods. In addition, we perform a comparative evaluation of sentiment classification techniques, incorporating both traditional models and deep learning architectures. Our results confirm that Twitter discussions precede changes in recessionary indicators and can thus provide forward-looking insights into economic sentiment. Furthermore, the comparative analysis reveals variations in sentiment detection across different methodologies, emphasizing the importance of selecting appropriate approaches in economic forecasting. © 2025 Elsevier B.V., All rights reserved.","Hayawi, K.; Shahriar, S.; Samuel Mathew, S.S.; Polyzos, E.; Ganguli, K.K.",2026,10.1016/j.im.2025.104252,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105015563853&doi=10.1016%2Fj.im.2025.104252&partnerID=40&md5=a7e2b0bfc6dee3e9b87a6eddcf573d18,scopus,"This study explores the use of online user sentiment, specifically from Twitter (X) posts, as an early warning system for recessionary expectations. It employs various sentiment analysis techniques (VADER, GPT, BERT) and econometric/machine learning methods to correlate online sentiment with recession indicators like the yield curve and GDPNow forecasts. The findings suggest that Twitter discussions can precede changes in these indicators, offering forward-looking economic insights, and highlight the importance of choosing appropriate sentiment analysis methodologies.",True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:48:28.790075
bb9ac1ae6fbd153b,Do Post-Corona European Economic Policies Lift Growth Prospects? Exploring an ML-Methodology,"This article explores the determinants of people’s growth prospects in survey data as well as the impact of the European recovery fund to future growth. The focus is on the aftermath of the Corona pandemic, which is a natural limit to the sample size. We use Eurobarometer survey data and macroeconomic variables, such as GDP, unemployment, public deficit, inflation, bond yields, and fiscal spending data. We estimate a variety of panel regression models and develop a new simulation-regression methodology due to limitation of the sample size. We find the major determinant of people’s growth prospect is domestic GDP per capita, while European fiscal aid does not significantly matter. In addition, we exhibit with the simulation-regression method novel scientific insights, significant outcomes, and a policy conclusion alike.",,2022,10.3390/jrfm15030120,,proquest,"This article investigates the determinants of individual growth prospects and the impact of the European recovery fund on future growth in the post-Corona era. Using Eurobarometer survey data and macroeconomic variables, the study employs panel regression models and a novel simulation-regression methodology due to sample size limitations. Key findings indicate that domestic GDP per capita is the primary driver of growth prospects, while European fiscal aid shows no significant impact. The simulation-regression method yields novel scientific insights and policy conclusions.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:48:32.037921
4b6d64e1256f6eb5,Do cross-border investors benchmark commercial real estate markets?: Evidence from relative yields and risk premia for a European investment horizon,"Purpose: The purpose of this study is to introduce a new perspective on determinants of cross-border investments in commercial real estate, namely, the relative attractiveness of a target market. So far, the literature has analyzed only absolute measures of investment attractiveness as determinants of cross-border investment flows. Design/methodology/approach: The empirical study uses a classic ordinary least squares estimation for a European panel data set containing 28 cities in 18 countries, with quarterly observations from Q1/2008 to Q3/2018. After controlling for empirically proven explanatory covariates, the model is extended by the new relative measurement based on relative yields/cap rates and relative risk premia. Additionally, the study applies a generalized additive mixed model (GAMM) to investigate a potentially nonlinear relationship. Findings: The study finds on average a ceteris paribus, statistically significant lagged influence of the proxy for relative attractiveness. Nonetheless, a differentiation is needed; relative risk premia are statistically significant, whereas relative yields are not. Moreover, the GAMM confirms a nonlinear relationship for relative risk premia and cross-border transaction volumes. Practical implications: The results are of interest for both academia and market participants as a means of explaining cross-border capital flows. The existing knowledge on determinants is expanded by relative market attractiveness, as well as an awareness of nonlinear relationships. Both insights help to comprehend the underlying transaction dynamics in commercial real estate markets. Originality/value: Whereas the existing body of literature focuses on absolute attractiveness to explain cross-border transaction activity, this study introduces relative attractiveness as an explanatory variable. © 2021 Elsevier B.V., All rights reserved.","Oertel, C.; Willwersch, J.; Cajias, M.",2020,10.1108/jerer-10-2019-0032,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078991288&doi=10.1108%2FJERER-10-2019-0032&partnerID=40&md5=96b180a1e0163261b87854a8d492aa3e,scopus,"This empirical study investigates the influence of relative market attractiveness on cross-border commercial real estate investments in Europe, using panel data from 2008-2018. It introduces relative yields and risk premia as new determinants, finding that relative risk premia significantly impact cross-border transaction volumes, with a nonlinear relationship confirmed by a generalized additive mixed model (GAMM).",True,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:48:34.498807
e8d85a38742f329b,Do leading indicators forecast US recessions? A nonlinear re-evaluation using historical data,"This paper analyses to what extent a selection of leading indicators is able to forecast U.S. recessions, by means of both dynamic probit models and Support Vector Machine (SVM) models, using monthly data from January 1871 to June 2016. The results suggest that the probit models predict U.S. recession periods more accurately than SVM models up to six months ahead, while the SVM models are more accurate over longer horizons. Furthermore, SVM models appear to distinguish between recessions and tranquil periods better than probit models do. Finally, the most accurate forecasting models are those that include oil, stock returns and the term spread as leading indicators.","Plakandaras, Vasilios; Cunado, Juncal; Gupta, Rangan; Wohar, Mark E.",2017,10.1111/infi.12111,,wos,"This paper evaluates the forecasting ability of leading indicators for US recessions using dynamic probit and Support Vector Machine (SVM) models from 1871 to 2016. Probit models are more accurate for short-term forecasts (up to six months), while SVM models perform better for longer horizons. SVM models also better distinguish recessions from non-recessionary periods. The study identifies oil, stock returns, and the term spread as the most effective leading indicators.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:48:39.095462
9ed23b41c26184ea,Does a search attention index explain portfolio returns in India?,"Employing asset-pricing models over the period 2012 to 2017, this study examines whether a search attention index (SAI) explains the variation in the weekly excess return of stocks. The study finds that the estimated abnormal return of a portfolio based on search intensity is significantly high for stocks with higher search intensity and low for stocks with lower search intensity. Further, the study observes that, when the SAI is high, the excess returns are high for stocks with a high value, high volatility, and high sensitivity. Interestingly, the study documents that in the Indian market investor attention is irrelevant for stocks with extremely high risk. This study finds that the SAI in India explains the variation in the excess return of stocks as well as the market, size, value, and momentum factors. © 2022 Elsevier B.V., All rights reserved.","Dharani, M.; Hassan, M.K.; Abedin, M.Z.; Ismail, M.A.",2022,10.1016/j.bir.2021.04.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105775150&doi=10.1016%2Fj.bir.2021.04.003&partnerID=40&md5=2e803c6ed55f99bd21aa2e550c14959d,scopus,"This study investigates whether a search attention index (SAI) can explain stock returns in India from 2012 to 2017. It finds that SAI significantly explains variations in weekly excess stock returns, particularly for stocks with high value, volatility, and sensitivity when SAI is high. However, investor attention appears irrelevant for extremely high-risk stocks in the Indian market. The SAI also explains variations related to market, size, value, and momentum factors.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:48:43.849671
d3d12a9e1655b3b2,Does conditioning information matter in estimating continuous time interest rate diffusions?,"We examine an important aspect of empirical estimation of term structure models; the role of conditioning information in dynamic term structure models. The use of both real world or simulated data implicitly incorporates conditioning information. We examine the bias created in estimating the drift by a specific form of conditioning, namely truncation. Using the theory of enlargement of filtrations we provide estimates of the extent of this truncation bias for commonly used short rate models. We find that this truncation bias causes the drift of these models to have a nonlinear structure. © 2018 Elsevier B.V., All rights reserved.","Abhyankar, A.; Basu, D.",2001,10.2307/2676286,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035612944&doi=10.2307%2F2676286&partnerID=40&md5=659a2f593a7c8bb3bb31fd10ef82c8f5,scopus,"This paper investigates the impact of conditioning information on estimating term structure models for interest rates. It analyzes the truncation bias in estimating the drift of short rate models using the theory of enlargement of filtrations, finding that this bias introduces a nonlinear structure to the drift. The study uses both real-world and simulated data.",False,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:48:46.536445
066db9df3dc96c98,Does extreme climate concern drive equity premiums? Evidence from China,"We construct an extreme climate concern indicator (ECC) on the basis of the coverage of the extreme climate news reports. First, ECC significantly negatively forecasts stock market returns in subsequent months. The predictability of ECC returns outperforms alternative confidence indicators and economic predictors over both in-sample and out-of-sample periods. Second, relative to before the Paris Agreement entered into force, extreme climate concerns prominently enhanced the forecasting capabilities after the signing of the Paris Agreement. Third, the return prediction accuracy of ECC in periods of low climate concern is significantly greater than that in periods of high climate concern, which is also consistent with the limited attention of investors. Finally, ECC substantially brings appreciable economic gains to investors, and the relevant empirical results pass a series of robustness tests.",,2024,10.1057/s41599-024-03705-y,,proquest,"This study develops an ""extreme climate concern"" (ECC) indicator using news reports to predict stock market returns. The ECC negatively forecasts returns, outperforming other predictors. Its predictive power increased after the Paris Agreement and is stronger during periods of low climate concern, suggesting investor attention limits. The ECC also generates economic gains for investors.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:49:01.536137
296118b61e9a3e48,"Does high frequency trading affect technical analysis and market efficiency? And if so, how?","In this paper we investigate how high frequency trading affects technical analysis and market efficiency in the foreign exchange (FX) market by using a special adaptive form of the Strongly Typed Genetic Programming (STGP)-based learning algorithm. We use this approach for real one-minute high frequency data of the most traded currency pairs worldwide: EUR/USD, USD/JPY, GBP/USD, AUD/USD, USD/CHF, and USD/CAD. The STGP performance is compared with that of parametric and non-parametric models and validated by two formal empirical tests. We perform in-sample and out-of-sample comparisons between all models on the basis of forecast performance and investment return. Furthermore, our paper shows the relative strength of these models with respect to the actual trading profit generated by their forecasts. Empirical experiments suggest that the STGP forecasting technique significantly outperforms the traditional econometric models. We find evidence that the excess returns are both statistically and economically significant, even when appropriate transaction costs are taken into account. We also find evidence that HFT has a beneficial role in the price discovery process. © 2013 Elsevier B.V. © 2013 Elsevier B.V., All rights reserved.","Manahov, V.; Hudson, R.; Gebka, B.",2014,10.1016/j.intfin.2013.11.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888777057&doi=10.1016%2Fj.intfin.2013.11.002&partnerID=40&md5=c39d0f295369ac49ddb0508cf7876015,scopus,"This study investigates the impact of high-frequency trading (HFT) on technical analysis and market efficiency in the foreign exchange (FX) market. Using a specialized adaptive Strongly Typed Genetic Programming (STGP) algorithm on real one-minute FX data for major currency pairs, the research compares STGP performance against parametric and non-parametric models. The findings indicate that STGP significantly outperforms traditional econometric models in terms of forecast performance and investment return, even after accounting for transaction costs. The study also suggests that HFT plays a positive role in price discovery.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:49:07.506990
c657958f06d1abb7,Dynamic portfolio insurance strategy: A robust machine learning approach,"In this paper, we propose a robust genetic programming (RGP) model for a dynamic strategy of stock portfolio insurance. With portfolio insurance strategy, we divide the money in a risky asset and a risk-free asset. Our applied strategy is based on a constant proportion portfolio insurance strategy. For determining the amount for investing in the risky asset, a critical parameter is a constant risk multiplier that is calculated in our proposed model using RGP to reflect market dynamics. Our model includes four main steps: (1) Selecting the best stocks for constructing a portfolio using a density-based clustering strategy. (2) Enhancing the robustness of our proposed model with an application of the Adaptive Neuro-Fuzzy Inference Systems (ANFIS) for forecasting the future prices of the selected stocks. The findings show that using ANFIS, instead of a regular multi-layer artificial neural network improves the prediction accuracy and our model’s robustness. (3) Implementing the RGP model for calculating the risk multiplier. Risk variables are used to generate equation trees for calculating the risk multiplier. (4) Determining the optimal portfolio weights of the assets using the well-known Markowitz portfolio optimization model. Experimental results show that our proposed strategy outperforms our previous model. © 2021 Elsevier B.V., All rights reserved.","Dehghanpour, S.; Esfahanipour, A.",2018,10.1080/24751839.2018.1431447,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065718194&doi=10.1080%2F24751839.2018.1431447&partnerID=40&md5=5a81c0b59c02568f89020e86bab8376a,scopus,"This paper introduces a robust genetic programming (RGP) model for dynamic stock portfolio insurance. The strategy involves dividing funds between risky and risk-free assets, with a constant proportion portfolio insurance strategy. The model uses RGP to determine a risk multiplier reflecting market dynamics. It includes steps for stock selection via clustering, price forecasting using Adaptive Neuro-Fuzzy Inference Systems (ANFIS) for enhanced robustness, RGP for risk multiplier calculation, and Markowitz optimization for portfolio weights. Experimental results indicate the proposed strategy outperforms previous models.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:49:08.758769
04924b384fc50756,Dynamic relationship of volatility of returns across different markets: evidence from selected next 11 countries,"Purpose: This paper aims to examine whether the volatility of returns in commodity (gold, oil), bond and forex markets is related over time to the volatility of returns in equity markets of Bangladesh, Indonesia, Pakistan, Philippines, Turkey and Vietnam. In addition, the authors analyze the integration of the commodity, bond, forex and equity markets across these markets. Design/methodology/approach: The dynamic conditional correlation GARCH (DCC-GARCH) model is used to capture the time-varying conditional correlation among markets. The authors use daily data of stock prices, oil prices, gold prices, exchange rates and 10 years' bond yields of the six countries from Datastream and investing.com from January 2001 to April 2021. Findings: Findings reveal that the parameters of dynamic correlation are statistically significant which indicates the importance of time-varying co-movements. Estimation of the DCC-GARCH model suggests that the stock market is significantly correlated with bond, forex, gold and oil markets in all six countries. Practical implications: This study has practical implications for policymakers and investment professionals. A better understanding of dynamic linkages among the markets would help in constructing effective hedging and portfolio diversification strategies. Policy makers can get insight to build proper strategies in order to insulate the economy from factors that cause volatility. Originality/value: Several studies have investigated the linkage between commodity and stock markets and the volatility spillover effect, but very little attention is given to study the interrelationship between groups of market segments of different economies. No study has comparatively examined the dynamic relationship of multiple markets of a group of emerging countries simultaneously. © 2024 Elsevier B.V., All rights reserved.","Shafiq, S.; Qureshi, S.S.; Akbar, M.",2024,10.1108/jeas-09-2022-0216,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197194276&doi=10.1108%2FJEAS-09-2022-0216&partnerID=40&md5=16a64a068d49b634b8f06b98b9336458,scopus,"This study investigates the dynamic relationships between the volatility of returns in equity, commodity (gold, oil), bond, and forex markets across six emerging economies (Bangladesh, Indonesia, Pakistan, Philippines, Turkey, and Vietnam). Using daily data from 2001 to 2021 and the DCC-GARCH model, the research finds significant time-varying correlations, indicating that stock markets are correlated with bond, forex, gold, and oil markets in these countries. The findings offer practical implications for hedging, portfolio diversification, and policy-making.",True,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:49:17.722880
cfa12aa6640fe808,Dynamic semiparametric factor models in risk neutral density estimation,"Dynamic semiparametric factor models (DSFM) simultaneously smooth in space and are parametric in time, approximating complex dynamic structures by time invariant basis functions and low dimensional time series. In contrast to traditional dimension reduction techniques, DSFM allows the access of the dynamics embedded in high dimensional data through the lower dimensional time series. In this paper, we study the time behavior of risk assessments from investors facing random financial payoffs. We use DSFM to estimate risk neutral densities from a dataset of option prices on the German stock index DAX. The dynamics and term structure of risk neutral densities are investigated by Vector Autoregressive (VAR) methods applied on the estimated lower dimensional time series.","Giacomini, Enzo; Haerdle, Wolfgang; Kraetschmer, Volker",2009,10.1007/s10182-009-0115-4,,wos,"This paper introduces Dynamic Semiparametric Factor Models (DSFM) for estimating risk-neutral densities from option prices. DSFM models complex dynamics using time-invariant basis functions and low-dimensional time series, allowing insights into high-dimensional data. The study applies DSFM to DAX index option prices to analyze the dynamics and term structure of risk-neutral densities using Vector Autoregressive (VAR) methods on the estimated time series.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:49:32.151590
116d6c5d4db5f1b9,Dynamics of Money Market Interest Rates in Ghana: Time-Frequency Analysis of Volatility Spillovers,"As the second longest practicing inflation targeting economy in Africa, it is of interest to investigate the degree to which policy interest rate influences other money market rates so as to gauge the overall effectiveness of monetary policy transmission in Ghana. This study evaluates the degree of connectedness among money market rates and also determines the most dominant money market rate(s) in Ghana. The basic finding is that the monetary policy rate has a low-to-moderate influence on volatility dynamics of other money market rates in Ghana across historical time-interval and time-frequency domains. This is a reflection of a generally weak capability of policy interest rate to drive other market rates in Ghana. Both monetary policy rate and Treasury bill rate are net transmitters of shocks, while interbank, lending and saving rates are net receivers of shocks in the money market. However, the Treasury bill emerges as the largest shock transmitter in the money market, across all forecast horizons and analytical domains. The lending rate is the largest shock recipient in the money market, largely from the Treasury bill rate which suggests ample evidence of fiscal dominance in Ghana. The study accentuates the exigency for monetary and fiscal policies to expeditiously address the domestic structural bottlenecks, especially in the financial sector and the fragile fiscal profile, in order to strengthen policy transmission in Ghana.","Akosah, Nana Kwame; Alagidede, Imhotep Paul; Schaling, Eric",2021,10.1111/saje.12287,,wos,"This study analyzes the influence of Ghana's policy interest rate on other money market rates using time-frequency analysis of volatility spillovers. It finds a low-to-moderate influence, indicating weak monetary policy transmission. The Treasury bill rate is the largest shock transmitter, while the lending rate is the largest recipient, suggesting fiscal dominance. The study emphasizes the need for policy reforms to strengthen transmission.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:49:33.811098
d116e668a6034e16,"ECONOMIC INJURY LEVEL, ACTION THRESHOLD, AND A YIELD-LOSS MODEL FOR THE PEA APHID, ACYRTHOSIPHON-PISUM (HOMOPTERA, APHIDIDAE), ON GREEN PEAS, PISUM-SATIVUM","Economic injury level, action threshold, and population development studies with the pea aphid (PA), Acyrthosiphon pisum (Harris), were conducted during 1983-85 [Washington, USA]. Pea aphid densities, simulating those in commercial pea fields, were established using insecticides to manipulate infestation levels. Three experiments, incorporating 12 treatments and six replications, were analyzed. A generalized, nonlinear equation relating pea yield to accumulated aphid feeding days (AFD) is described. The model approximates two phases of a sigmoid infestation-yield curve. An upper maximum plateau and a region of rapidly decreasing yield are approximate. Beyond 1,800 aFD, a lower minimum yield plateau is hypothesized. Economic injury levels calculated for the 3 years'' experiments using the generalized model were 22.2, 18.2, and 12.2 AFD, respectively. Action threshold estimates were determined from linear regression estimates of yield versus aphids per plant at bloom. Action thresholds were 3.6, 0.3, and 0.3 aphids per plant for the years 1983, 1984, 1985, respectively. The pea quality components, tenderometer (TD) and sieve size, were altered by maximum PA densities. High AFD levels increased TD and decreased sieve size significantly when compared with aphid-free controls. Protein content of green peas was not significantly altered by PA feeding.","YENCHO, GC; GETZIN, LW; LONG, GE",1986,10.1093/jee/79.6.1681,,wos,"This study establishes economic injury levels and action thresholds for pea aphids on green peas by developing a yield-loss model based on accumulated aphid feeding days (AFD). The model accounts for different phases of infestation-yield response, and calculated economic injury levels varied across the study years. Action thresholds were also determined, and high aphid densities were found to affect pea quality components like tenderometer readings and sieve size, but not protein content.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:49:35.549404
4e1455a476507192,ESG and corporate credit spreads,"Purpose: The authors investigate the implications of environmental, social and governance (ESG) practices of firms for the pricing of their credit default swaps (CDS). In doing so, the authors compare European and US firms and consider nonlinear and indirect effects. This complements the previous literature focusing on linear and direct effects using bond yields and credit ratings of US firms. Design/methodology/approach: For this purpose, the authors apply fixed effects regressions on a comprehensive panel data set of US and European firms. Further, nonlinear and indirect effects are investigated utilizing quantile regressions and a path analysis. Findings: The evidence indicates that higher ESG ratings mitigate credit risks of US and European firms from 2007 to 2019. The risk mitigation effect is U-shaped across ESG quantiles, which is consistent with opposing effects of growing stakeholder influence capacity and diminishing marginal returns on ESG investments. The authors further reveal a mediating indirect volatility channel that substantially amplifies the direct effect of ESG on credit risk. A one-standard-deviation improvement in ESG ratings is estimated to reduce CDS spreads of low, medium and high ESG firms by approximately 4%, 8% and 3%, respectively. Originality/value: This is the first study to examine whether credit markets reflect regional differences between Europe and the US with regard to the ESG-CDS-relationship. In addition, this paper contributes to the existing literature by investigating differences in the response of CDS spreads across ESG quantiles and to study potential indirect channels connecting ESG and CDS spreads using structural credit risk variables. © 2022 Elsevier B.V., All rights reserved.","Barth, F.; Hübel, B.; Scholz, H.",2022,10.1108/jrf-03-2021-0045,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124343222&doi=10.1108%2FJRF-03-2021-0045&partnerID=40&md5=17a558b0787d2116f8797fc6df2d9467,scopus,"This study investigates the impact of ESG practices on credit default swap (CDS) spreads for US and European firms from 2007 to 2019. It finds that higher ESG ratings mitigate credit risks, with a U-shaped effect across ESG quantiles, suggesting a balance between stakeholder influence and diminishing marginal returns on ESG investments. The research also identifies an indirect volatility channel that amplifies the ESG-credit risk relationship. The study utilizes fixed effects regressions, quantile regressions, and path analysis to explore these effects, differentiating itself by examining regional differences between Europe and the US and investigating nonlinear and indirect channels.",True,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:49:37.492321
03cc69643da21feb,ESG in the headlines: Media-driven reputational risk and stock performance,"This study examines the impact of environmental, social, and governance (ESG) reputational risks on stock performance. We use a unique dataset of media-driven ESG reputational risk indicators, covering 4963 Chinese firms from 2009 to 2023. On average, a one-standard-deviation increase in ESG reputational risks is associated with a 4.5 % decrease in simple stock returns, a 14.5 % reduction in excess stock returns relative to the market index, and a 12.2% decline in excess stock returns compared to peer firms of similar size. These negative effects contradict the traditional risk-return relationship predicted by risk premium theory. Further analysis identifies reduced investor confidence and tighter financing constraints as key mechanisms through which ESG reputational risks negatively affect stock returns. Heterogeneity analyses indicate that the negative impact is more pronounced for firms in non-pollution-intensive industries, those facing financing difficulties, and those exposed to environment-related reputational risks.","Zhou, Bole; Ge, Wanjun",2025,10.1016/j.gfj.2025.101127,,wos,"This study investigates how media-driven ESG reputational risks affect stock performance in Chinese firms. It finds that increased ESG risks lead to significant decreases in stock returns, contradicting traditional risk-return theories. The study identifies reduced investor confidence and financing constraints as key mechanisms and notes that the impact is stronger for certain types of firms.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:49:38.737063
5d75205f29fc9e07,ESTIMATING WEIGHTED BANK BETA INDEX UNDER MACRO EFFECTS IN VIETNAM IN INDUSTRY 4.0 AND ROLES OF DIGITAL TRANSFORMATION FOR BETTER RISK MANAGEMENT INFORMATION SYSTEM,"The applications of mathematics in finance have been developed widely in recent years. In our study, authors aim to propose weighted beta index formula in banking industry and then, factors that affect bank sustainable development, as well as risk management policies and strategies for commercial banks in Vietnam financial market. This study mainly use combination of quantitative methods (statistics, calculation formulas) and qualitative methods including synthesis, inductive and explanatory methods. Our study results show that first, because mean value of weighted beta in period 2011-2020 higher than beta in 2011-2016 time, we need to pay attention to risk management solution in bank system. Second, as CPI has negative impact while G, IM and R has positive impact on weighted beta: and Risk free rate have higher effects on market risks of banks, Ministry of Finance, State bank of Vietnam and relevant agencies need to control GDP growth as well as rates of Treasury bonds and lending rate (not increase so much) toward benefits for managing risk. And do not need to reduce CPI too much. Then we also mention the roles of digital data and transformation to help us to build better risk model and management information system at banks. © 2025 Elsevier B.V., All rights reserved.","Xuan Que, H.; Tran Ngoc Huy, D.; Duc Thang, T.; Van Thanh, T.; Hoang, H.",2022,10.31407/ijees12.338,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172739866&doi=10.31407%2Fijees12.338&partnerID=40&md5=9077207f47188466c73640dbdb313952,scopus,"This study proposes a weighted beta index formula for the Vietnamese banking industry, analyzing factors affecting bank development and risk management. It uses quantitative and qualitative methods, finding that the weighted beta increased from 2011-2016 to 2011-2020, necessitating risk management attention. CPI negatively impacts weighted beta, while GDP, imports, and interest rates positively impact it. The study also highlights the role of digital transformation in improving risk management information systems.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:49:41.053714
2058d64637af6983,Early Warning of Systemic Financial Risk of Local Government Implicit Debt Based on BP Neural Network Model,"In recent years, local governments have boosted their local economies by raising large amounts of debt. Even though the state further strictly controls local government debt, the hidden debt formed by the local government borrowing in disguised form can infect systemic financial risks, creating an urgent need to carry out risk warning based on local government hidden debt. The paper uses the macro indicators of local government implicit debt risk at the prefecture-level city level, and introduces the micro indicators of PPP projects, financing platform bank debt, and urban investment debt to establish a BP neural network model. We not only study the contagion effect of local government hidden debt on systemic financial risks, but also predict the systemic financial risks in 2019 and construct an early warning risk system based on the prefecture-level city data from 2015 to 2018. In addition, the early warning effect of local government implicit debt on systemic financial risk under different stress scenarios is investigated. The study found that the implicit debt risk of local governments, the scale of financing platform bank debt, the scale of PPP, and the scale of urban investment bonds have a significant impact on systemic financial risks. The neural network model constructed by introducing these four variables at the same time can better predict the level of systemic financial risk. The model can also accurately predict the changes in systemic financial risks under the stress test of the increase in hidden debt of different local governments, and has a good early warning effect.",,2022,10.3390/systems10060207,,proquest,"This paper develops a BP neural network model to predict systemic financial risks stemming from local government implicit debt. It incorporates macro indicators and micro indicators like PPP projects, financing platform bank debt, and urban investment debt. The model demonstrates an ability to predict risks and their contagion effects, showing a good early warning effect under various stress scenarios.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:49:42.802452
7b3ea689637650bb,Early warning strategies for corporate operational risk: A study by an improved random forest algorithm using FCM clustering,"To enhance the accuracy and response speed of the risk early warning system, this study develops a novel early warning system that combines the Fuzzy C-Means (FCM) clustering algorithm and the Random Forest (RF) model. Firstly, based on operational risk theory, market risk, research and development risk, financial risk, and human resource risk are selected as the primary indicators for enterprise risk assessment. Secondly, the Criteria Importance Through Intercriteria Correlation (CRITIC) weight method is employed to determine the importance of these risk indicators, thereby enhancing the model's prediction ability and stability. Following this, the FCM clustering algorithm is utilized for pre-processing sample data to improve the efficiency and accuracy of data classification. Finally, an improved RF model is constructed by optimizing the parameters of the RF algorithm. The data selected is mainly from RESSET/DB, covering the issuance, trading, and rating data of fixed-income products such as bonds, government bonds, and corporate bonds, and provides basic information, net value, position, and performance data of funds. The experimental results show that the model achieves an F1 score of 87.26%, an accuracy of 87.95%, an Area under the Curve (AUC) of 91.20%, a precision of 89.29%, and a recall of 87.48%. They are respectively 6.45%, 4.45%, 5.09%, 4.81%, and 3.83% higher than the traditional RF model. In this study, an improved RF model based on FCM clustering is successfully constructed, and the accuracy of risk early warning models and their ability to handle complex data are significantly improved.To enhance the accuracy and response speed of the risk early warning system, this study develops a novel early warning system that combines the Fuzzy C-Means (FCM) clustering algorithm and the Random Forest (RF) model. Firstly, based on operational risk theory, market risk, research and development risk, financial risk, and human resource risk are selected as the primary indicators for enterprise risk assessment. Secondly, the Criteria Importance Through Intercriteria Correlation (CRITIC) weight method is employed to determine the importance of these risk indicators, thereby enhancing the model's prediction ability and stability. Following this, the FCM clustering algorithm is utilized for pre-processing sample data to improve the efficiency and accuracy of data classification. Finally, an improved RF model is constructed by optimizing the parameters of the RF algorithm. The data selected is mainly from RESSET/DB, covering the issuance, trading, and rating data of fixed-income products such as bonds, government bonds, and corporate bonds, and provides basic information, net value, position, and performance data of funds. The experimental results show that the model achieves an F1 score of 87.26%, an accuracy of 87.95%, an Area under the Curve (AUC) of 91.20%, a precision of 89.29%, and a recall of 87.48%. They are respectively 6.45%, 4.45%, 5.09%, 4.81%, and 3.83% higher than the traditional RF model. In this study, an improved RF model based on FCM clustering is successfully constructed, and the accuracy of risk early warning models and their ability to handle complex data are significantly improved.",,2025,10.1371/journal.pone.0318491,,proquest,"This study proposes an improved early warning system for corporate operational risk by integrating Fuzzy C-Means (FCM) clustering with a Random Forest (RF) model. It identifies key risk indicators (market, R&D, financial, human resource), uses the CRITIC method for weighting, and employs FCM for data pre-processing. The improved RF model demonstrates enhanced accuracy and performance compared to the traditional RF model, with reported metrics including an F1 score of 87.26% and AUC of 91.20%. Data sources include RESSET/DB for fixed-income products.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:49:45.120192
7b6e0566cefac712,Econometric Model Using Arbitrage Pricing Theory and Quantile Regression to Estimate the Risk Factors Driving Crude Oil Returns,"This work presents a novel approach to determining the risk and return of crude oil stocks by employing Arbitrage Pricing Theory and Quantile Regression. Arbitrage Pricing Theory identifies the risk factors likely to impact crude oil returns. Subsequently, Quantile Regression estimates the relationship between the selected factors and the returns across different distribution quantiles. The West Texas Intermediate (WTI) crude oil price is used in this study as a benchmark for crude oil prices. WTI’s price fluctuations can significantly impact the performance of global crude oil stocks and, subsequently, the global economy. Various statistical measures are used in this study to determine the proposed model's stability. The results show that changes in WTI returns can have varying effects depending on market conditions and levels of volatility. This study emphasizes the influence of structural discontinuities on returns. These are likely generated by changes in the global economy and the unpredictable demand for crude oil during the pandemic. The inclusion of pandemic, geopolitical, and inflation-related explanatory variables adds uniqueness to the study as it considers current global events that can affect crude oil returns. Findings show that the key factors that pose significant risks to returns are industrial production, inflation, the global price of energy, the shape of the yield curve, and global economic policy uncertainty. This implies that while making investment decisions in WTI futures, investors should pay particular attention to these elements. © 2024 Elsevier B.V., All rights reserved.","Maitra, S.; Mishra, V.; Kundu, S.; Chopra, M.",2024,10.62527/joiv.8.1.2268,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189629613&doi=10.62527%2Fjoiv.8.1.2268&partnerID=40&md5=91a6c326b4237fac4db0eac49a9b571c,scopus,"This study uses Arbitrage Pricing Theory and Quantile Regression to analyze crude oil stock returns, focusing on the West Texas Intermediate (WTI) benchmark. It identifies risk factors like industrial production, inflation, energy prices, yield curve shape, and economic policy uncertainty, highlighting their varying impact across different market conditions and the influence of structural discontinuities such as the pandemic. The model incorporates pandemic, geopolitical, and inflation variables.",True,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:49:48.773939
f801db2ab5d662e6,Economic Evaluation and Risk Premium Estimation of Rainfed Soybean under Various Planting Practices in a Semi-Humid Drought-Prone Region of Northwest China,"Economic benefits and risk premiums significantly affect the production system decision making of farmers and government departments. This study evaluated the economic feasibility and estimated the risk premium of 12 rainfed soybean production systems with various planting densities, fertilization rates and planting patterns by considering the impact of soybean price fluctuation. There were two planting densities (D1: 160,000 plants ha−1 and D2: 320,000 plants ha−1), two fertilization rates (F1: 20 kg ha−1 N, 30 kg ha−1 P, 30 kg ha−1 K; F2: 40 kg ha−1 N, 60 kg ha−1 P, 60 kg ha−1 K) and three planting patterns (F+W0: flat cultivation with no irrigation; R+W0: plastic-mulched ridge-furrow cultivation (PMRF) with no irrigation; R+W1: PMRF with supplemental irrigation of 30 mm at the pod-filling stage). Based on the two-year (2019–2020) field data in a semi-humid drought-prone region of northwest China and soybean price fluctuation from January 2014 to June 2021, the net income (NI) was calculated by considering the impact of soybean price fluctuation and assuming constant soybean production costs. The net present value (NPV) method and the stochastic efficiency with respect to a function (SERF) method were used to evaluate the profitability of protective alternatives and the risk of these alternatives. The results showed that the 12 proposed soybean production systems were economically feasible. Reducing the fertilization rate reduced the input costs, but it did not necessarily result in a decrease in soybean yield and NI. The payback period of all production systems was within two years for farmers investing through loans. High-fertilizer and high-density production systems made personal investment obtain the highest economic benefit in this study, which was not the best investment strategy from the perspective of production-to-investment ratio and environmental protection departments. The preferences of farmers with various risk aversion and environmental protection departments in terms of risk premium were also proposed. The economic and risk assessment framework of this study can enhance the understanding of the adjustment of production systems from different perspectives, and provide strategies for promoting the protection of economic, environmental and socially sustainable agricultural systems.",,2023,10.3390/agronomy13112840,,proquest,"This study evaluates the economic feasibility and risk premium of 12 rainfed soybean production systems in a drought-prone region of China, considering various planting densities, fertilization rates, and planting patterns. It uses net income, net present value, and stochastic efficiency with respect to a function to assess profitability and risk under soybean price fluctuations. The findings suggest that while high-input systems may offer higher immediate returns, they are not always the most beneficial from a broader economic or environmental perspective. The research provides a framework for understanding production system adjustments and promoting sustainable agriculture.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:49:50.123431
593bb4ca69c5d955,Empirical Asset Pricing via Machine Learning,"We perform a comparative analysis of machine learning methods for the canonical problem of empirical asset pricing: measuring asset risk premiums. We demonstrate large economic gains to investors using machine learning forecasts, in some cases doubling the performance of leading regression-based strategies from the literature. We identify the best-performing methods (trees and neural networks) and trace their predictive gains to allowing nonlinear predictor interactions missed by other methods. All methods agree on the same set of dominant predictive signals, a set that includes variations on momentum, liquidity, and volatility.Authors have furnished an Internet Appendix, which is available on the Oxford University Press Web site next to the link to the final published paper online.",,2020,10.1093/rfs/hhaa009,,proquest,"This paper compares machine learning methods for empirical asset pricing, specifically measuring asset risk premiums. It shows significant economic gains for investors using machine learning forecasts, outperforming traditional regression-based strategies. The study identifies trees and neural networks as top performers, attributing their success to capturing nonlinear predictor interactions. Key predictive signals identified include momentum, liquidity, and volatility.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:49:58.864051
e504e29bd4408ea9,Empirical asset pricing with nonlinear risk premia,"We introduce a new model for the joint dynamics of the S&P 100 index and the VXO implied volatility index. The nonlinear specification of the variance process is designed to simultaneously accommodate extreme persistence and strong mean reversion. This grants superior forecasting power over the standard (linear) specifications for implied variance forecasting.We obtain statistically significant predictions in an out-of-sample exercise spanning several market crashes starting 1986 and including the recent subprime crisis. The model specification is possible through a simple continuous-time no-arbitrage asset pricing framework that combines semi-analytic pricing with a nonlinear specification for the market price of risk. © The Author, 2013. Published by Oxford University Press. All rights reserved. © 2014 Elsevier B.V., All rights reserved.","Mijatović, A.; Schneider, P.",2014,10.1093/jjfinec/nbt018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902659724&doi=10.1093%2Fjjfinec%2Fnbt018&partnerID=40&md5=1106d2afa715a821243ef162a47eabd7,scopus,"This paper proposes a new model for the joint dynamics of the S&P 100 index and the VXO implied volatility index. The model features a nonlinear variance process that captures both extreme persistence and mean reversion, leading to improved out-of-sample forecasts for implied variance, particularly during market crashes. The framework is based on a no-arbitrage asset pricing model with a nonlinear market price of risk.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:50:06.136862
f010f2916ddce7c3,Empirical causal analysis of flood risk factors on U.S. flood insurance payouts:Implications for solvency and risk reduction,"This paper presents a regression model that quantifies the causal relationship between flood risk factors and the flood insurance payout in the U.S. The flood risk factors that have been considered in this research are flood exposure, infrastructure vulnerability, social vulnerability, and the number of mobile homes. Historical data for the annual flood insurance payout, flood risk factors, and other control variables were collected for six years between 2016 and 2021 and used in a Mixed Effects Regression model to derive the empirical relationships. The regression model expressed the natural logarithm of the annual flood insurance payout in a county based on the flood risk factors and control variables. The paper presents the regression coefficients that quantify the causal influence. It has been found that all four flood risk factors have statistically significant positive influence on the flood insurance payout in a county. However, the extent of the influence is different for different flood risk factors. Among them, flood exposure has the highest influence on the flood insurance payout, which is followed by the number of mobile homes, infrastructure vulnerability, and social vulnerability. Since the federal flood insurance program in the U.S. has a large debt to the U.S. treasury, the government should plan for effective risk reduction that can reduce the flood insurance payout in future to keep the program solvent. The outcomes of this research are expected to facilitate that decision-making process by providing the empirical relationship between flood risk factors and flood insurance payout. © 2024 Elsevier B.V., All rights reserved.","Bhattacharyya, A.; Hastak, M.",2024,10.1016/j.jenvman.2024.120075,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182725293&doi=10.1016%2Fj.jenvman.2024.120075&partnerID=40&md5=b9178af6b532ea7a3bb7c2201e439a3b,scopus,"This study uses a regression model to analyze the causal impact of flood risk factors (exposure, infrastructure vulnerability, social vulnerability, mobile homes) on U.S. flood insurance payouts between 2016-2021. All factors significantly increase payouts, with flood exposure having the largest effect. The findings aim to inform risk reduction strategies for the solvency of the federal flood insurance program.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:50:07.949760
de032d8bca7f4579,Empirical evidence on the Euler equation for consumption in the US,"Recently developed econometric methods, that are robust to weak instruments and exploit information in possible structural changes, are applied to study the Euler equation for consumption using aggregate US post-war data. Several extensions to the baseline Euler equation model are investigated. The results are insensitive to using linear versus nonlinear specifications, different instruments or different consumption data, but they are very sensitive to asset returns. With risk-free returns, the elasticity of intertemporal substitution is tightly estimated around zero, while with stock market returns, it is significantly positive but very imprecisely estimated. There is no evidence of parameter instability. © 2021 Elsevier B.V., All rights reserved.","Ascari, G.; Magnusson, L.M.; Mavroeidis, S.",2021,10.1016/j.jmoneco.2019.12.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076864764&doi=10.1016%2Fj.jmoneco.2019.12.004&partnerID=40&md5=977ffefb7cfe3b8791af31afbd50e27f,scopus,This study applies advanced econometric methods to US post-war aggregate data to analyze the Euler equation for consumption. It explores various model extensions and finds that results are robust to specification choices but sensitive to asset returns. The elasticity of intertemporal substitution is estimated near zero with risk-free returns and imprecisely positive with stock returns. No parameter instability was detected.,True,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:50:09.637645
d434a775f2150cee,Empirical reverse engineering of the pricing kernel,"This paper proposes an econometric procedure that allows the estimation of the pricing kernel without either any assumptions about the investors preferences or the use of the consumption data. We propose a model of equity price dynamics that allows for (i) simultaneous consideration of multiple stock prices, (ii) analytical formulas for derivatives such as futures, options and bonds, and (iii) a realistic description of all of these assets. The analytical specification of the model allows us to infer the dynamics of the pricing kernel. The model, calibrated to a comprehensive dataset including the S&P 500 index, individual equities, T-bills and gold futures, yields the conditional filter of the unobservable pricing kernel. As a result we obtain the estimate of the kernel that is positive almost surely (i.e. precludes arbitrage), consistent with the equity risk premium, the risk-free discounting, and with the observed asset prices by construction. The pricing kernel estimate involves a highly nonlinear function of the contemporaneous and lagged returns on the S&P 500 index. This contradicts typical implementations of CAPM that use a linear function of the market proxy return as the pricing kernel. Hence, the S&P 500 index does not have to coincide with the market portfolio if it is used in conjunction with nonlinear asset pricing models. We also find that our best estimate of the pricing kernel is not consistent with the standard time-separable utilities, but potentially could be cast into the stochastic habit formation framework of Campbell and Cochrane (J. Political Economy 107 (1999) 205). © 2003 Elsevier B.V. All rights reserved. © 2008 Elsevier B.V., All rights reserved.","Chernov, M.",2003,10.1016/s0304-4076(03)00111-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346937478&doi=10.1016%2FS0304-4076%2803%2900111-8&partnerID=40&md5=887c4b2dc8caa87597889e2ab5275885,scopus,"This paper presents an econometric method to estimate the pricing kernel without assuming investor preferences or using consumption data. It models equity price dynamics, allowing for multiple stock prices, analytical derivative formulas, and realistic asset descriptions. The model infers the pricing kernel's dynamics, calibrated to data including the S&P 500 index, individual equities, T-bills, and gold futures. The estimated kernel is positive, consistent with equity risk premium and observed asset prices, and is a nonlinear function of S&P 500 returns, contradicting typical CAPM implementations. The findings suggest the S&P 500 index may not always represent the market portfolio in nonlinear asset pricing models and that the estimated kernel is not consistent with standard time-separable utilities but might fit a stochastic habit formation framework.",True,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:50:10.973119
9d3cef205fa28fe7,End-to-end transport network digital twins with cloud-native SDN controllers and generative AI [Invited],"This paper explores the potential of network digital twins (NDTs) in networking (both IP Ethernet networks and optical transport networks), highlighting their integration with cloud-native software-defined networking (SDN) controllers and intent-based networking enabled by generative artificial intelligence (GenAI). The proposed framework represents an approach that combines advanced virtualization, real-time analytics, and GenAI. The use of NDTs enables a comprehensive and dynamic digital representation of the physical network, capturing critical aspects, such as topology, traffic patterns, and performance metrics, which permits data-driven decision-making to lead to more efficient networking operations. The incorporation of cloud-native SDN controllers along with an NDT ensures that the system remains scalable, flexible, and responsive to dynamic network conditions. Intent-based networking, powered by GenAI, allows the network to interpret high-level objectives from operators and autonomously translate them into actionable configurations that are enforced by orchestrators and SDN controllers. This eliminates manual intervention, minimizes errors, accelerates the deployment of network services, and provides a means for easier network management. The presented framework significantly enhances automation, enabling predictive maintenance by identifying potential issues before they impact network performance. It optimizes network design by simulating various configurations and testing their feasibility in a risk-free environment. These capabilities collectively improve operational efficiency, reduce downtime, and ensure optimal resource utilization.",A. Abishek; D. Adanza; P. Alemany; L. Gifre; R. Casellas; R. Martinez; R. Munoz; R. Vilalta,2025,10.1364/jocn.550864,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10974926,ieeexplore,"This paper proposes a framework for end-to-end transport network digital twins (NDTs) integrated with cloud-native SDN controllers and generative AI (GenAI). NDTs provide a dynamic digital representation of the physical network, enabling data-driven decisions for efficient operations. Cloud-native SDN controllers ensure scalability and responsiveness, while GenAI-powered intent-based networking translates high-level operator objectives into automated network configurations. This approach enhances automation, enables predictive maintenance, optimizes network design through simulation, reduces downtime, and improves resource utilization.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:50:12.937410
c7ed342d298e889b,Endogenous inattention and risk-specific price underreaction in corporate bonds,"Corporate bond prices are slow to respond to default risk and interest rate shocks, as proxied by firm-level stock returns and Treasury returns, respectively. Furthermore, the underreaction is risk-specific: bonds with better credit quality underreact more to default risk, while those with worse quality underreact more to interest rates. The underreactions imply substantial out-of-sample return predictability, and investors appear to be leaving too much money on the table. The results are consistent with behavioral inattention models in which investors endogenously allocate more attention to payoff-relevant (or salient) risks, and they are not explained by traditional trading friction mechanisms. © 2022 Elsevier B.V., All rights reserved.","Li, J.",2022,10.1016/j.jfineco.2021.09.025,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117362116&doi=10.1016%2Fj.jfineco.2021.09.025&partnerID=40&md5=c050948826a3bbc8282b4b542ccd0387,scopus,"This study investigates the slow response of corporate bond prices to default risk and interest rate shocks, suggesting investors underreact to these risks. The underreaction is risk-specific, with better credit quality bonds underreacting more to default risk and worse quality bonds underreacting more to interest rates. This implies significant out-of-sample return predictability and is explained by behavioral inattention models rather than trading frictions.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:50:14.941182
b0057b5ec8318381,Equity returns of financial institutions and the pricing of interest rate risk,"This study investigates the issue of whether financial intermediaries' common stock returns incorporate a risk premium for their inherent exposure to unexpected changes in interest rates. A wide range of financial institutions is employed to test the hypothesis that the interest rate risk is priced by capital markets. In addition, the above sample is extended by incorporating firms from the non-financial sector. A two-factor model with the market portfolio and the changes in market yields, as exogenously specified risk variables, is employed. The model is estimated via a seemingly unrelated regression estimation (SURE) framework with both cross-equation restrictions and within equation nonlinear constraints on the parameters. The findings indicate that financial institutions' equity returns incorporate a risk premium for their exposure to market yields' surprises. The return generating function of the insurance business could be further explained by an additional factor such as currency movements. It is also empirically supported that the market premium drops out from the estimation process. When commercial and industrial firms are included in the estimation process, the findings unveil a reduction in the magnitude of the interest rate risk premium. © 2005 Taylor & Francis Group Ltd. © 2008 Elsevier B.V., All rights reserved.","Staikouras, S.K.",2005,10.1080/09603100500039557,https://www.scopus.com/inward/record.uri?eid=2-s2.0-17744384695&doi=10.1080%2F09603100500039557&partnerID=40&md5=081d75e8784219025c52e20bc5c362a1,scopus,"This study examines if financial intermediaries' stock returns reflect a risk premium for unexpected interest rate changes. Using a diverse sample of financial institutions and extending it to non-financial firms, the research employs a two-factor model (market portfolio and market yield changes) estimated with SURE. Results show that financial institutions' equity returns do include a risk premium for market yield surprises, with insurance firms potentially influenced by currency movements. The market premium is found to be insignificant. Including commercial and industrial firms reduces the magnitude of the interest rate risk premium.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:50:16.410623
06fcd3a9f651e0dc,Estimating Latent Factors Based on Statistical Data Analysis,"In recent years, statistical methods have been widely used to estimate latent risk factors that affect the prices of financial assets. This paper develops new estimators for asset pricing factors by introducing dependence measure--distance covariance, that can identify nonlinear dependence. We combined distance covariance with Principal Component Analysis (PCA) and Risk-Premium PCA (RPPCA) and made contrast analysis based on Chinese market data. RPPCA, as a new method, shows strong applicability and detects factors with high Sharpe-ratio efficiently. Moreover, distance covariance produces better performance than covariance in PCA as a factor estimator, which illustrates the superiority of the distance covariance. Finally, the most striking results revealed by the study is that RPPCA including distance covariance of residuals outperforms others with a smaller pricing error and a significantly large Sharpe-ratio.",,2021,10.1088/1742-6596/1995/1/012065,,proquest,"This paper proposes new methods for estimating latent risk factors in asset pricing by incorporating distance covariance, a measure of nonlinear dependence, into Principal Component Analysis (PCA) and Risk-Premium PCA (RPPCA). Empirical analysis on Chinese market data demonstrates that RPPCA with distance covariance is effective in identifying factors with high Sharpe ratios and achieves smaller pricing errors compared to other methods.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:50:41.477605
cade2431f60ef1f8,Estimating market liquidity from daily data: Marrying microstructure models and machine learning,"We apply machine learning to estimate daily measures of market liquidity by combining microstructure models with low-frequency daily data only, in stock markets in the United States and China. Boosting trees and neural networks significantly improve the performance across different liquidity measures. Our machine learning models are interpretable and improvements are due to (a) more information from raw data that microstructure models do not capture; and (b) better use of information from learned nonlinear and non-monotonic relationships. We further demonstrate two applications of our trained machine learning models in estimating the illiquidity risk premium and systematic liquidity risk. © 2025 Elsevier B.V., All rights reserved.","Dai, Y.; Shi, C.; Zhang, R.",2025,10.1016/j.finmar.2025.101019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017326098&doi=10.1016%2Fj.finmar.2025.101019&partnerID=40&md5=3e7a6b83920c50e3f9a006efbbac3e9e,scopus,"This study uses machine learning techniques, specifically boosting trees and neural networks, to estimate daily market liquidity measures using only low-frequency daily data from US and Chinese stock markets. The models improve performance by capturing additional information from raw data and learning complex relationships, offering interpretability and applications in estimating illiquidity risk premium and systematic liquidity risk.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:50:47.777025
49367fe67a852c41,Estimating the Equity Premium,"Existing empirical research investigating the size of the equity premium has largely consisted of a series of innovations around a common theme: producing a better estimate of the equity premium by using better data or a better estimation technique. The equity premium estimate that emerges from most of this work matches one moment of the data alone: the mean difference between an estimate of the return to holding equity and a risk-free rate. We instead match multiple moments of U.S. market data, exploiting the joint distribution of the dividend yield, return volatility, and realized excess returns, and find that the equity premium lies within 50 basis points of 3.5%, a range much narrower than was achieved in previous studies. Additionally, statistical tests based on the joint distribution of these moments reveal that only those models of the conditional equity premium that embed time variation, breaks, and/or trends are supported by the data. In order to develop the joint distribution of the dividend yield, return volatility, and excess returns, we need a model of price and return fundamentals. We document that even recently developed analytically tractable models that permit autocorrelated dividend growth rates and discount rates impose restrictions that are rejected by the data. We therefore turn to a wider range of models, requiring numerical solution methods and parameter estimation by the simulated method of moments.","Donaldson, R. Glen; Kamstra, Mark J.; Kramer, Lisa A.",2010,10.1017/s0022109010000347,,wos,"This study estimates the equity premium by matching multiple moments of U.S. market data, including the dividend yield, return volatility, and realized excess returns. The authors find a narrower range for the equity premium (within 50 basis points of 3.5%) than previous studies and suggest that models supporting time variation, breaks, and/or trends are supported by the data. They also explore various models, including those requiring numerical solution methods and simulated method of moments estimation, due to limitations of existing analytically tractable models.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:50:56.710580
42d8ee05b390a701,Estimating the risk-return profile of new venture investments using a risk-neutral framework and 'thick' models,"This study proposes cascade neural networks to estimate the model parameters of the Cox-Ross-Rubinstein risk-neutral approach, which, in turn, explain the risk-return profile of firms at venture capital and initial public offering (IPO)financing rounds. Combining the two methods provides better estimation accuracy than risk-adjusted valuation approaches, conventional neural networks, and linear benchmark models. The findings are persistent across in-sample and out-of-sample tests using 3926 venture capital and 1360 US IPO financing rounds between January 1989 and December 2008. More accurate estimates of the risk-return profile are due to less heterogeneous risk-free rates of return from the risk-neutral framework. Cascade neural networks nest both the linear and nonlinear functional estimation form in addition to taking account of variable interaction effects. Better estimation accuracy of the risk-return profile is desirable for investors so they can make a more informed judgement before committing capital at different stages of development and various financing rounds. Reprinted by permission of Routledge, Taylor and Francis Ltd.",,2014,10.1080/1351847x.2012.708471,,proquest,"This study uses cascade neural networks within a risk-neutral framework to estimate the risk-return profile of new venture investments at venture capital and IPO stages. The proposed method shows improved estimation accuracy compared to traditional risk-adjusted valuation, conventional neural networks, and linear models, based on analysis of US venture capital and IPO financing rounds from 1989-2008. The authors suggest this enhanced accuracy aids investor decision-making.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:50:59.613505
817d1aea113ecade,Estimating the spot rate curve using the Nelson-Siegel model. A ridge regression approach,"The Nelson-Siegel model is widely used in practice for fitting the term structure of interest rates. Due to the ease in linearizing the model, a grid search or an OLS approach using a fixed shape parameter are popular estimation procedures. The estimated grid search parameters, however, have been reported (1) to behave erratically over time, and (2) to have relatively large variances. On the other hand, parameter estimates based on a fixed shape parameter, while avoiding multicollinearity, turn out to be too smooth. We show that the Nelson-Siegel model can become heavily collinear depending on the estimated/fixed shape parameter. A simple procedure based on ridge regression can remedy the reported problems significantly. © 2013 Elsevier Inc. © 2013 Elsevier B.V., All rights reserved.","Annaert, J.; Claes, A.G.P.; Ceuster, M.J.K.; Zhang, H.",2013,10.1016/j.iref.2013.01.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876326729&doi=10.1016%2Fj.iref.2013.01.005&partnerID=40&md5=4bbf46f64bd8757be4fa5c077b100dfb,scopus,"This paper proposes a ridge regression approach to estimate the Nelson-Siegel model for the term structure of interest rates, addressing issues of erratic parameter behavior and large variances associated with grid search methods, and excessive smoothness from fixed shape parameter methods. The authors demonstrate that the Nelson-Siegel model can suffer from multicollinearity and that ridge regression offers a significant improvement.",True,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:51:25.060996
786cba4cf35e5a45,Estimating time-varying risk premia in UK long-term government bonds,"Simple models of time-varying risk premia are used to measure the risk premia in long-term UK government bonds. The parameters of the models can be estimated using nonlinear seemingly unrelated regression (NL-SUR), which permits efficient use of information across the entire yield curve and facilitates the testing of various cross-sectional restrictions. The estimated time-varying premia are found to be substantially different to those estimated using models that assume constant risk premia. © 2004 Taylor and Francis Ltd. © 2008 Elsevier B.V., All rights reserved.","Steeley, J.M.",2004,10.1080/0960310042000211632,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1542639767&doi=10.1080%2F0960310042000211632&partnerID=40&md5=19ff6b7366f9cad45ca9caab1717eb0d,scopus,This paper estimates time-varying risk premia in UK long-term government bonds using nonlinear seemingly unrelated regression (NL-SUR). The method allows for efficient use of information across the yield curve and testing of cross-sectional restrictions. The estimated time-varying premia differ significantly from those derived from models assuming constant risk premia.,False,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:51:33.670579
65a824924d8fbb16,Estimating yield curves of the U.S. Treasury securities: An interpolation approach,"Following the approach of interpolation, this paper proposes the multiple exponential decay model to fit yield curves for both the U.S. TIPS market and the conventional Treasury security market. Several estimation methods, including the unconstrained/constrained nonlinear minimization, quadratic programming, and the iterative linear least squares, are applied to estimate the unknown parameters according to different curve-fitting purposes. Comparisons between the proposed model and the alternatives show that the multiple exponential decay successfully (1) adapts to a variety of shapes associated with yield curves, (2) (partially) keeps in line with the economic interpretations of Nelson–Siegel summarized by Diebold and Li (), and (3) dominates the competing models in curve-fitting performance measured by mean fitted-price errors over the sample period. In addition, the exact specification of a nonparametric interpolation model is pinned down by applying three statistical tools, which enable us to jointly take into account validity, optimality, and parsimoniousness of the proposed model. © 2020 Elsevier B.V., All rights reserved.","Guo, F.",2019,10.1002/rfe.1039,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055113189&doi=10.1002%2Frfe.1039&partnerID=40&md5=04461965ab8433b380f1e433878baeda,scopus,"This paper proposes a multiple exponential decay model for fitting yield curves of U.S. Treasury securities and TIPS, using various estimation methods. The model demonstrates adaptability to different yield curve shapes, aligns with economic interpretations, and outperforms competing models in fitting accuracy.",True,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:51:41.419213
204aa7a29bdc5b45,"Estimation of the concurrent radiological dosage to humans due to the transfer of 226Ra, 228Ra, and 40K from soil-to-Malaysian traditional medicinal plants","Medicinal plants have been incorporated into various traditional medicine systems worldwide to reduce disease risk, treat illnesses, and provide medicinal remedies. Today, the pharmaceutical industry uses the most active plant compounds in drug synthesis. Possible high levels of naturally occurring radionuclides in medicinal plants have raised public concern about the consequent radiological impact on the consumption of medicinal plants and herbs. This paper reports the first study of soil-to-plant mobilities of natural radionuclides in native medicinal plants in Malaysia. Representative samples of soils and organically grown traditional medicinal plants from western Malaysia were collected and studied using HPGe gamma-ray spectrometry. Average activity concentrations for 226Ra, 228Ra, and 40K in the soils are respectively 57, 84, and 520 Bq/kg, and the respective values in the medicinal plants are 10, 4, and 498 Bq/kg. The respective transfer factors (TFs) for the medicinal leaves are 0.18, 0.05, and 1.18. The TFs of 40K were higher than others due to higher uptake and its essentiality in plant growth. These findings indicate that plant growth habits greatly influenced the radionuclides' uptake. The radioactivities in soils and their corresponding mobilities are in accordance with literature data. To discard any radiological hazards to human health, the estimated threshold consumption rate is found to be approximately 46 kg/y. Annual effective doses and excess lifetime cancer risk for adult members of the public due to the consumption of medicinal plants are found to be negligible. It is suggested that the use of traditional medicinal plants may provide a risk-free and safe means of maintaining public health. © 2025 Elsevier B.V., All rights reserved.","Shuaibu, H.K.; Mohamed, F.; Khandaker, M.U.; Ismail, A.F.; Osman, H.",2024,10.1016/j.radphyschem.2024.111982,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197039271&doi=10.1016%2Fj.radphyschem.2024.111982&partnerID=40&md5=9e5010be32b9a982b5a0f2752b665351,scopus,"This study investigates the transfer of natural radionuclides (226Ra, 228Ra, and 40K) from soil to Malaysian traditional medicinal plants and estimates the resulting radiological dosage to humans. Radionuclide concentrations in soil and plants were measured, and transfer factors were calculated. The findings suggest that the consumption of these medicinal plants poses negligible radiological risks to human health, with an estimated threshold consumption rate of 46 kg/y.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:51:53.299940
aa64a2822074ac0d,Ethical and regulatory challenges in machine learning-based healthcare systems: A review of implementation barriers and future directions,"Machine learning significantly enhances clinical decision-making quality, directly impacting patient care with early diagnosis, personalized treatment, and predictive analytics. Nonetheless, the increasing proliferation of such ML applications in practice raises potential ethical and regulatory obstacles that may prevent their widespread adoption in healthcare. Key issues concern patient data privacy, algorithmic bias, absence of transparency, and ambiguous legal liability. Fortunately, regulations like the General Data Protection Regulation (GDPR), the Health Insurance Portability and Accountability Act (HIPAA), and the FDA AI/ML guidance have raised important ways of addressing things like fairness, explainability, legal compliance, etc.; however, the landscape is far from risk-free. AI liability is another one of the gray areas approaching black, worrying about who is liable for an AI medical error — the developers, the physicians, or the institutions. The study reviews ethical risks and potential opportunities, as well as regulatory frameworks and emerging challenges in AI-driven healthcare. It proposes solutions to reduce bias, improve transparency, and enhance legal accountability. This research addresses these challenges to support the safe, fair, and effective deployment of ML-based systems in clinical practice, guaranteeing that patients can trust, regulators can approve, and healthcare can use them. © 2025 Elsevier B.V., All rights reserved.","Mohammed, S.; Malhotra, N.",2025,10.1016/j.tbench.2025.100215,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007763586&doi=10.1016%2Fj.tbench.2025.100215&partnerID=40&md5=e0577e0306751fa9b17e4ec0fb7e7501,scopus,"This review examines the ethical and regulatory challenges associated with implementing machine learning (ML) in healthcare. It discusses barriers such as data privacy, algorithmic bias, lack of transparency, and legal liability, while also highlighting existing regulations (GDPR, HIPAA, FDA guidance) and proposing solutions for bias reduction, improved transparency, and enhanced accountability to ensure safe and effective deployment of ML systems in clinical practice.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:51:55.961628
b07e605b9567222c,European Union Allowance price forecasting with Multidimensional Uncertainties: A TCN-iTransformer Approach for Interval Estimation,"In response to the research demand for forecasting European Union Allowance (EUA) prices, this paper proposes a probabilistic forecasting framework based on a spatiotemporal convolutional neural network. This framework innovatively integrates multidimensional external uncertainty indicators, captures the long-term dependencies of carbon prices through a spatiotemporal convolutional structure, and combines quantile regression with conformal prediction to effectively estimate prediction intervals. Empirical studies demonstrate that the proposed TCN-iTransformer model outperforms existing methods in both point prediction and interval prediction, exhibiting excellent prediction interval coverage probability and normalized average width at different confidence intervals. The Diebold–Mariano (DM) test and ordinary least squares (OLS) regression analysis further validate the predictive advantages of the proposed model. Furthermore, SHAP analysis reveals that the U.S. Treasury yield spread has the most significant impact on EUA price forecasting, while geopolitical risks predominantly exert negative effects. The research findings provide important references for constructing risk mitigation strategies in the European Union carbon emissions market under complex market environments. © 2025 Elsevier B.V., All rights reserved.","Wu, R.; Abedin, M.; Zeng, H.; Lucey, B.",2025,10.1002/for.70024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105016613972&doi=10.1002%2Ffor.70024&partnerID=40&md5=9305158e398c871b8bd5e60c86d2c3c3,scopus,"This paper proposes a probabilistic forecasting framework using a spatiotemporal convolutional neural network (TCN-iTransformer) to forecast European Union Allowance (EUA) prices. It integrates external uncertainty indicators and uses quantile regression with conformal prediction for interval estimation. The model outperforms existing methods in point and interval prediction, with SHAP analysis identifying the U.S. Treasury yield spread as a significant factor. The findings offer insights for risk mitigation in the EU carbon emissions market.",True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:52:14.133377
12936628b2b341f7,European spreads at the interest rate lower bound,"This paper analyzes the effect of the interest rate lower bound on long-term sovereign bond spreads in the euro area. We specify a joint shadow rate term structure model for the risk-free, the German, and the Italian sovereign yield curves. In our model, the behavior of long-term spreads becomes strongly nonlinear in the underlying factors when interest rates are close to the lower bound, which occurs in the data since the beginning of 2012. We fit the model via Quasi-Maximum Likelihood and show three consequences of the nonlinear behavior of sovereign spreads: (i) they are asymmetrically distributed, (ii) they are affected by (possibly exogenous) changes in the lower bound, and (iii) they become less informative about sovereign risk than when interest rates are far from the lower bound. Shadow spreads, however, still provide reliable information. (C) 2020 The Author(s). Published by Elsevier B.V.","Coroneo, Laura; Pastorello, Sergio",2020,10.1016/j.jedc.2020.103979,,wos,"This paper examines how the interest rate lower bound impacts long-term sovereign bond spreads in the euro area using a joint shadow rate term structure model for risk-free, German, and Italian sovereign yield curves. The model reveals nonlinear behavior of spreads near the lower bound, leading to asymmetric distributions, sensitivity to lower bound changes, and reduced informativeness about sovereign risk. Shadow spreads, however, remain reliable.",True,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:52:26.016115
7612e5acd7dc712d,Evaluating the 'Fed Model' of Stock Price Valuation: An out-of-sample forecasting perspective,"The ""Fed Model"" postulates a cointegrating relationship between the equity yield on the S&P 500 and the bond yield. We evaluate the Fed Model as a vector error correction forecasting model for stock prices and for bond yields. We compare out-of-sample forecasts of each of these two variables from a univariate model and various versions of the Fed Model including both linear and nonlinear vector error correction models. We find that for stock prices the Fed Model improves on the univariate model for longer-horizon forecasts, and the nonlinear vector error correction model performs even better than its linear version. © 2006 Elsevier Ltd. All rights reserved. © 2008 Elsevier B.V., All rights reserved.","Jansen, D.W.; Wang, Z.",2006,10.1016/s0731-9053(05)20026-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645856784&doi=10.1016%2FS0731-9053%2805%2920026-9&partnerID=40&md5=6ec05711fd88e6ea0dcf87377d48d09f,scopus,"This study evaluates the 'Fed Model' for stock price valuation using a vector error correction forecasting approach. It compares the Fed Model's out-of-sample forecasts for stock prices and bond yields against a univariate model, considering both linear and nonlinear versions. The findings suggest that the Fed Model enhances longer-horizon stock price forecasts, with nonlinear models showing superior performance.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:52:28.902324
a8cf076865ff9282,Evaluating the combined forecasts of the dynamic factor model and the artificial neural network model using linear and nonlinear combining methods,"The paper evaluates the advantages of combined forecasts from the dynamic factor model (DFM) and the artificial neural networks (ANN). The analysis was based on three financial variables namely the Johannesburg Stock Exchange Return Index, Government Bond Return Index and the Rand/Dollar Exchange Rate in South Africa. The forecasts were based on the out-of-sample period from January 2006 to December 2011. Compared to benchmark autoregressive (AR) models, both the DFM and ANN offer more accurate forecasts with reduced root-mean-square error (RMSE) of around 2–12 % for all variables and over all forecasting horizons. The ANN as a nonlinear combining method outperforms all linear combining methods for all variables and over all forecasting horizons. The results suggest that the ANN combining method can be used as an alternative to linear combining methods to achieve greater forecasting accuracy. The ANN combining method produces out-of-sample forecasts that are substantially more accurate with a sizeable reduction in RMSE of both the AR benchmark model and the best individual forecasting model. We attribute the superiority of the ANN combining method to its ability to capture any existing nonlinear relationship between the individual forecasts and the actual forecasting values. © 2017 Elsevier B.V., All rights reserved.","Babikir, A.; Mwambi, H.",2016,10.1007/s00181-015-1049-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954512445&doi=10.1007%2Fs00181-015-1049-1&partnerID=40&md5=11ab9cf5dd4d813d51fc5ba1afb4d024,scopus,"This study compares the forecasting accuracy of a dynamic factor model (DFM) and an artificial neural network (ANN) model using financial data from South Africa. Both models outperformed benchmark autoregressive models. The ANN, as a nonlinear combining method, demonstrated superior forecasting accuracy compared to linear methods, attributed to its ability to capture nonlinear relationships.",True,True,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T09:52:58.266451
c11bf27ee96c2868,Evolving fuzzy modelling for yield curve forecasting,"Forecasting the term structure of interest rates plays a crucial role in portfolio management, household finance decisions, business investment planning, and policy formulation. This paper aims to address yield curve forecasting and evolving fuzzy systems modelling using data from US and Brazilian fixed income markets. Evolving fuzzy models provide a high level of system adaptation and learn the system dynamic continuously, which is essential for uncertain environments as interest rate markets. Computational experiments show that the evolving fuzzy modelling approaches describe the interest rate behaviour accurately, outperforming traditional econometric techniques in terms of error measures and statistical tests. Moreover, evolving models provide promising results for short and long-term maturities and for both fixed income markets evaluated, highlighting its potential to forecast complex nonlinear dynamics in uncertain environments. © 2020 Elsevier B.V., All rights reserved.","Maciel, L.; Ballini, R.; Gomide, F.",2018,10.1504/ijebr.2018.091047,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045552146&doi=10.1504%2FIJEBR.2018.091047&partnerID=40&md5=7727d894acc5a836f5878e176fffd734,scopus,"This paper explores the use of evolving fuzzy models for yield curve forecasting, applying them to US and Brazilian fixed income markets. The study demonstrates that these adaptive models accurately capture interest rate behavior and outperform traditional econometric methods, showing promise for both short and long-term forecasting in uncertain financial environments.",True,True,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T09:53:04.205849
30458d1834570aa3,Exchange rate parities and Taylor rule deviations,"This paper investigates the PPP and UIP conditions by taking into account possible nonlinearities as well as the role of Taylor rule deviations under alternative monetary policy frameworks. The analysis is conducted using monthly data from January 1993 to December 2020 for five inflation-targeting countries (the UK, Canada, Australia, New Zealand and Sweden) and three non-targeting ones (the USA, the Euro Area and Switzerland). Both a benchmark linear VECM and a nonlinear Threshold VECM are estimated; the latter includes Taylor rule deviations as the threshold variable. The results can be summarized as follows. First, the nonlinear specification provides much stronger evidence for the PPP and UIP conditions, the estimated adjustment speed towards equilibrium being twice as fast. Second, Taylor rule deviations play an important role: the adjustment speed is twice as fast when deviations are small and the credibility of the central bank is higher. Third, inflation targeting tends to generate a higher degree of credibility for the monetary authorities, thereby reducing deviations of the exchange rate from the PPP- and UIP-implied equilibrium.",,2022,10.1007/s00181-021-02192-3,,proquest,"This paper examines Purchasing Power Parity (PPP) and Uncovered Interest Parity (UIP) conditions, considering nonlinearities and deviations from the Taylor rule under different monetary policy regimes. Using monthly data from 1993-2020 for eight countries, the study employs both linear and nonlinear Vector Error Correction Models (VECM). The findings indicate that nonlinear models offer stronger support for PPP and UIP, with faster adjustment speeds. Taylor rule deviations significantly impact adjustment speed, which is faster when deviations are smaller and central bank credibility is higher. Inflation targeting appears to enhance central bank credibility, leading to reduced exchange rate deviations from PPP and UIP.",True,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:53:06.237769
6e8a4c15b74abc41,"External shocks, cross-border flows and macroeconomic risks in emerging market economies","We study the relationship between cross-border flows and risks to macroeconomic stability for a sample of ten major emerging market economies (EMEs) from 2000 to 2017 in the presence of external shocks. We examine this relationship with a focus on two key channels of cross-border flows, namely external debt securities (EDS) and cross-border loans (CBLs). Our analysis focuses on the transition in cross-border flows post-global financial crisis 2008 (GFC) termed as the second phase of global liquidity (Shin in Keynote address at Federal Reserve Bank of San Francisco Asia economic policy conference, 2013). Panel vector autoregression estimations show that volatility in global risk perception affects cross-border flows to EMEs more as compared to the effect of the US monetary policy stance. Post-GFC, EDS flows rise with shocks in global risk perception, while CBL flows register a decline. CBL flows are also associated with larger risks post-GFC compared to the pre-GFC period, which is in contrast to the result for EDS flows. Second, a panel threshold estimation confirms a nonlinear association between EDS/CBL flows and macroeconomic risks largely dependent upon global uncertainty. US GDP growth also affects the nonlinearity, but US federal funds rate have insignificant threshold effects. Our results conclude that global uncertainty is a significant driver of cross-border flows to EMEs post-GFC and that it is a strong signal in determining riskiness of EDS flows and CBL flows for EMEs.","Goyal, Ashima; Verma, Akhilesh K.; Sengupta, Rajeswari",2022,10.1007/s00181-021-02099-z,,wos,"This study investigates the link between cross-border flows (external debt securities and cross-border loans) and macroeconomic stability risks in emerging market economies (EMEs) from 2000-2017, considering external shocks. The analysis reveals that global risk perception volatility impacts EME cross-border flows more than US monetary policy. Post-2008 financial crisis, external debt securities flows increased with global risk perception shocks, while cross-border loans declined and became riskier. Panel threshold estimations confirm a nonlinear relationship between these flows and macroeconomic risks, influenced by global uncertainty and US GDP growth.",True,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:53:07.641522
f62c257dbca918f1,FINITE-SAMPLE PROPERTIES OF THE GENERALIZED-METHOD OF MOMENTS IN TESTS OF CONDITIONAL ASSET PRICING-MODELS,"We develop evidence on the finite sample properties of the Generalized Method of Moments (GMM) in an asset pricing context. The models imply nonlinear, cross-equation restrictions on predictive regressions for security returns. We find that a two-stage GMM approach produces goodness-of-fit statistics that reject the restrictions too often. An iterated GMM approach has superior finite sample properties. The coefficient estimates are approximately unbiased in simpler models, but their asymptotic standard errors are understated. Simple adjustments for the standard errors are partially successful in correcting the bias. In more complex models the coefficients and their standard errors can be highly unreliable. The power of the tests to reject a single-premium model is higher against a two-premium, fixed-beta alternative than against a conditional Capital Asset Pricing Model with time-varying betas.","FERSON, WE; FOERSTER, SR",1994,10.1016/0304-405x(94)90029-9,,wos,"This paper investigates the finite sample properties of the Generalized Method of Moments (GMM) when applied to conditional asset pricing models. It finds that a two-stage GMM approach tends to reject restrictions too frequently, while an iterated GMM approach shows better finite sample performance. Coefficient estimates can be biased in simpler models, and their standard errors are often understated, though adjustments can help. In more complex models, both coefficients and standard errors can be unreliable. The study also compares the power of tests against different asset pricing model alternatives.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:53:10.063160
99d45debc2b13a50,FORECASTING SERIES CONTAINING OFFSETTING BREAKS: OLD SCHOOL AND NEW SCHOOL METHODS OF FORECASTING TRANSNATIONAL TERRORISM,"Transnational terrorism data are difficult to forecast because they contain an unknown number of structural breaks of unknown functional form. The rise of religious fundamentalism, the demise of the Soviet Union, and the rise of al Qaeda have changed the nature of transnational terrorism. 'Old School' forecasting methods simply smooth or difference the data. 'New School' methods use estimated break dates to control for regime shifts when forecasting. We compare the various forecasting methods using a Monte Carlo study with data containing different types of breaks. The study's results are used to forecast various types of transnational terrorist incidents.","Enders, Walter; Liu, Yu; Prodan, Ruxandra",2009,10.1080/10242690802425772,,wos,"This paper compares 'old school' (smoothing/differencing) and 'new school' (estimated break dates) forecasting methods for transnational terrorism data, which are characterized by structural breaks. A Monte Carlo study evaluates these methods, and the results are applied to forecast terrorist incidents.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:53:11.706300
23c2359624d6bbdc,Factor Investment or Feature Selection Analysis?,"This study has made significant findings in A-share market data processing and portfolio management. Firstly, by adopting the Lasso method and CPCA framework, we effectively addressed the problem of multicollinearity among feature indicators, with the Lasso method demonstrating superior performance in handling this issue, thus providing a new method for financial data processing. Secondly, Deep Feedforward Neural Networks (DFN) exhibited exceptional performance in portfolio management, significantly outperforming other evaluated machine learning methods, and achieving high levels of out-of-sample performance and Sharpe ratios. Additionally, we consistently identified price changes, earnings per share, net assets per share, and excess returns as key factors influencing predictive signals. Finally, this study combined the Lasso method with DFN, providing a new perspective and methodological support for asset pricing measurement in the financial field.",,2025,10.3390/math13010009,,proquest,"This study applies the Lasso method and CPCA framework to address multicollinearity in A-share market data, with Lasso showing superior performance. Deep Feedforward Neural Networks (DFN) were found to be highly effective for portfolio management, outperforming other machine learning methods. Key influencing factors identified include price changes, earnings per share, net assets per share, and excess returns. The research combines Lasso and DFN for asset pricing measurement.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:53:34.567507
1783e8e530cbe052,"Factor Models, Machine Learning, and Asset Pricing","We survey recent methodological contributions in asset pricing using factor models and machine learning. We organize these results based on their primary objectives: estimating expected returns, factors, risk exposures, risk premia, and the stochastic discount factor as well as model comparison and alpha testing. We also discuss a variety of asymptotic schemes for inference. Our survey is a guide for financial economists interested in harnessing modern tools with rigor, robustness, and power to make new asset pricing discoveries, and it highlights directions for future research and methodological advances.","Giglio, Stefano; Kelly, Bryan; Xiu, Dacheng",2022,10.1146/annurev-financial-101521-104735,,wos,"This article surveys recent methodological advancements in asset pricing that utilize factor models and machine learning. It categorizes these advancements by their main goals, including estimating expected returns, factors, risk exposures, risk premia, and the stochastic discount factor, as well as model comparison and alpha testing. The survey also covers various asymptotic schemes for inference and aims to guide financial economists in applying modern, rigorous, and powerful tools for asset pricing research, while also pointing towards future research directions.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:53:39.696891
aae98e7d411abd8a,Financial Stock Investment Management Using Deep Learning Algorithm in the Internet of Things,"This paper aims to explore a new model to study financial stock investment management (SIM) and obtain excess returns. Consequently, it proposes a financial SIM model using deep Q network (DQN) as reinforcement earning (RL) algorithm and Long Short-Term Memory (LSTM) as deep neural network (DNN). Then, after training and optimization, the proposed model is back-tested. The research findings are as follows: the LSTM neural network (NN)-based model will import the observation of the market at each time and the change of transaction information over time. The LSTM network can find and learn the potential relationship between time series data. There are two hidden layers and one output layer in the model. The hidden layer is an LSTM structure and the output layer is the fully connected NN. DQN algorithm first stores the experience sample data of the agent-environment interaction into the experience pool. It then randomly selects a small batch of data from the experience pool to train the network. Doing so removes the correlation and dependence between samples so that the DNN model can better learn the value function in the RL task. The model can predict the future state according to historical information and decide which actions to take in the next step. Meanwhile, five stocks of Chinese A-shares are selected to form an asset pool. The initial 500,000 amount of the account is divided into five equal shares, which are invested and traded. Overall, the model account’s rate of return (RoR) during the back-test is 32.12%. The Shanghai Stock Exchange (SSI) has risen by 19.157% in the same period. Thus, the model’s performance has exceeded the SSI’s in the same period. E stock has the maximum RoR of 78.984%. The RoR of A, B, and C stocks is 54.129%, 11.594%, and 9.815%, respectively. B stock presents a minimum RoR of 6.084%. All these stocks have got positive returns. Therefore, the proposed financial SIM based on the DL algorithm is scientific and feasible. The research content has certain significant reference for the DL-based financial SIM.",,2022,10.1155/2022/4514300,,proquest,"This paper proposes a financial stock investment management model using Deep Q Network (DQN) and Long Short-Term Memory (LSTM) deep learning algorithms. The model, trained and back-tested on five Chinese A-shares, achieved a 32.12% rate of return, outperforming the Shanghai Stock Exchange's 19.157% increase over the same period. The research suggests this deep learning-based approach is scientific, feasible, and offers significant reference for financial investment management.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:53:42.869552
76f6fcb6934e9db1,Financial conditions and nonlinearities in the European Central Bank (ECB) reaction function: In-sample and out-of-sample assessment,"Our purpose is to investigate how the European Central Bank (ECB) sets interest rates in the context of both linear and nonlinear policy reaction functions. This work contributes to the current debate on central banks having additional objectives over and above control of inflation and output. Three findings emerge. First, the ECB takes financial conditions into account when setting interest rates. Second, amongst Taylor rule models, linear and nonlinear models are empirically indistinguishable within sample, and model specifications with real-time data provide the best description of in-sample ECB interest rate setting behaviour. Third, the 2007-2009 financial crisis witnessed a shift from inflation targeting to output stabilization, and a shift from an asymmetric policy response to financial conditions at high inflation rates to a more symmetric response regardless of the state of inflation. Finally, guidance is provided as regards models for forecasting interest rates in the Eurozone area. Without imposing an a priori choice of the parametric functional form, semiparametric models and autoregressive processes forecast the out-of-sample ECB interest rate setting behaviour better than linear and nonlinear Taylor rule models. (C) 2011 Elsevier B.V. All rights reserved.","Milas, Costas; Naraidoo, Ruthira",2012,10.1016/j.csda.2011.06.032,,wos,"This study examines the European Central Bank's (ECB) interest rate setting behavior using both linear and nonlinear policy reaction functions. It finds that the ECB considers financial conditions, that linear and nonlinear models are empirically similar within sample, and that model specifications with real-time data best describe in-sample behavior. The 2007-2009 financial crisis marked a shift from inflation targeting to output stabilization and a change in the response to financial conditions. For out-of-sample forecasting, semiparametric models and autoregressive processes outperform linear and nonlinear Taylor rule models.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:53:45.126326
ce476a18889360d0,"Financial conditions, macroeconomic factors and disaggregated bond excess returns","Bond excess returns can be predicted by macro factors, however, large parts remain still unexplained. We apply a novel term structure model to decompose bond excess returns into expected excess returns (risk premia) and the innovation part. In order to explore these risk premia and innovations, we complement macro variables by financial condition variables as possible determinants of bond excess returns. We find that the expected part of bond excess returns is driven by macro factors, whereas innovations seem to be mainly influenced by financial conditions, before and after the financial crisis. Thus, financial conditions, such as financial stress, deserve attention when analyzing bond excess returns. © 2015 Elsevier B.V., All rights reserved.","Fricke, C.; Menkhoff, L.",2015,10.1016/j.jbankfin.2015.03.015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930672701&doi=10.1016%2Fj.jbankfin.2015.03.015&partnerID=40&md5=cca2912022d65911e1c5b4829a830f9e,scopus,"This study decomposes bond excess returns into expected excess returns (risk premia) and innovations using a novel term structure model. It investigates the influence of macroeconomic and financial condition variables on these components, finding that macro factors drive expected returns while financial conditions influence innovations, both before and after the financial crisis. The authors suggest that financial conditions, like financial stress, are important for analyzing bond excess returns.",False,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:54:01.207025
5842551f94fabb30,Firm-level productivity and stock return: New evidence from China,"This study provides novel insights into the stock return predictability related to the firm-level total factor productivity (TFP) in the China's stock market, covering the period from 1999 to 2020. Contrary to studies on other countries, our research identifies a positive correlation between the productivity of Chinese firms and their future stock returns. We suggest that this correlation is driven by mispricing rather than risk premium. Specifically, we argue that investors' difficulties in accurately assessing firm-level productivity and limited attention result in the inadequate incorporation of this information into stock prices. Furthermore, our analysis explores additional factors contributing to the observed productivity-related market anomaly, including investor overconfidence, positive feedback trading, lottery preference, information uncertainty, and limits to arbitrage, indicating investors' irrationality and behavioral biases. Importantly, our study also demonstrates that firm-level productivity serves as a reliable predictor of a firm's future profitability. © 2024 Elsevier B.V., All rights reserved.","Tang, N.; Gao, M.; Zhou, Y.; Zhou, F.; Zhu, J.",2024,10.1016/j.iref.2024.103557,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203455085&doi=10.1016%2Fj.iref.2024.103557&partnerID=40&md5=766099b5385b7c429057e195b3e80ea1,scopus,"This study investigates the relationship between firm-level total factor productivity (TFP) and stock returns in China from 1999 to 2020. It finds a positive correlation, suggesting mispricing due to investor difficulties in assessing TFP, limited attention, investor overconfidence, and other behavioral biases. The study also confirms TFP as a predictor of future profitability.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:54:02.536191
897cf60508a18c31,Fiscal multipliers in South Africa after the global financial crisis,"Background: South Africa’s fiscal position has deteriorated considerably over the last 10 years, with debt levels reaching historical highs in the post-apartheid period. National Treasury’s intentions for fiscal consolidation have again drawn attention to the fiscal multiplier literature.Aim: The aim in the study is to calculate the size of fiscal expenditure multipliers over the period 2009 to 2019, taking into account the specific economic conditions and the funding choices of government.Setting: In the study fiscal policy is considered at a time when the debt to gross domestic product (GDP) ratio was rising rapidly.Methods: We use an econometric model to calculate the fiscal multipliers over the past decade. Our estimates take account of the specific fiscal conditions for each year, in particular the changing relationship between debt and the sovereign risk premia as well as the impact of tax increases.Results: The model suggests that the fiscal multiplier declined from 1.5 in 2010 to around zero in 2019 as the debt levels became progressively more unsustainable and large tax increases muted the aggregate demand effects from higher government expenditure.Conclusion: The low fiscal multipliers suggest that fiscal consolidation will be less costly in terms of growth forgone than generally perceived.JEL classification: C50, E62, H62, H63",,2022,10.4102/sajems.v25i1.4191,,proquest,"This study calculates fiscal expenditure multipliers in South Africa from 2009 to 2019 using an econometric model. It finds that multipliers declined from 1.5 in 2010 to near zero in 2019 due to rising debt levels and tax increases, suggesting fiscal consolidation may be less costly than anticipated.",True,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:54:03.900615
8ae5fb614411eb93,Forecasting CDS Term Structure Based on Nelson–Siegel Model and Machine Learning,"In this study, we analyze the term structure of credit default swaps (CDSs) and predict future term structures using the Nelson–Siegel model, recurrent neural network (RNN), support vector regression (SVR), long short-term memory (LSTM), and group method of data handling (GMDH) using CDS term structure data from 2008 to 2019. Furthermore, we evaluate the change in the forecasting performance of the models through a subperiod analysis. According to the empirical results, we confirm that the Nelson–Siegel model can be used to predict not only the interest rate term structure but also the CDS term structure. Additionally, we demonstrate that machine-learning models, namely, SVR, RNN, LSTM, and GMDH, outperform the model-driven methods (in this case, the Nelson–Siegel model). Among the machine learning approaches, GMDH demonstrates the best performance in forecasting the CDS term structure. According to the subperiod analysis, the performance of all models was inconsistent with the data period. All the models were less predictable in highly volatile data periods than in less volatile periods. This study will enable traders and policymakers to invest efficiently and make policy decisions based on the current and future risk factors of a company or country.",,2020,10.1155/2020/2518283,,proquest,"This study forecasts the term structure of credit default swaps (CDSs) using the Nelson-Siegel model and various machine learning techniques (RNN, SVR, LSTM, GMDH). Empirical results show that machine learning models, particularly GMDH, outperform the Nelson-Siegel model. Performance varies with data volatility, with less predictability during highly volatile periods. The findings aim to aid traders and policymakers in investment and policy decisions.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:54:11.108757
bd71e4915db6a904,Forecasting Government Bond Yields with Neural Networks Considering Cointegration,"This paper discusses techniques that might be helpful in predicting interest rates and tries to evaluate a new hybrid forecasting approach. Results of examining government bond yields in Germany and France reported in this study indicate that a hybrid forecasting approach which combines techniques of cointegration analysis with neural network (NN) forecasting models can produce superior results to the use of NN forecasting models alone. The findings documented in this paper could be a consequence of the fact that examining differenced data under certain conditions will lead to a loss of information and that the inclusion of the error correction term from the cointegration model can help to cope with this problem. The paper also discusses some possibly interesting directions for further research. Copyright © 2015 John Wiley & Sons, Ltd.",,2016,10.1002/for.2385,,proquest,"This paper evaluates a hybrid forecasting approach for government bond yields by combining cointegration analysis with neural networks (NN). The study, focusing on German and French bond yields, suggests this hybrid method outperforms NN models alone, potentially by retaining information lost in differenced data through the inclusion of an error correction term.",True,True,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T09:54:13.800576
2dbf7991c4f5dbaf,Forecasting Key Macroeconomic Variables of the South African Economy using Bayesian Variable Selection,"This study analyzed the forecasting performances of various multivariate models in predicting 1-8-quarters-ahead of the growth rate of GDP, the consumer price index inflation rate and the three months Treasury bill rate for South Africa over an out-of-sample period of 2000:Q1-2011:Q2, using an in-sample period of 1960:Ql-1999:Q4. The study compared the forecasting performances of the classical and the Minnesota-type Bayesian vector autoregressive (VAR) models with those of linear (fixed-parameter) and nonlinear (time-varying parameter) VARs involving a stochastic search algorithm for variable selection, estimated using Markov Chain Monte Carlo methods. In general, the study finds that variable selection, whether imposed on a time-varying VAR or a fixed parameter VAR, and non-linearity in VARs, play an important part in improving predictions when compared to the linear fixed coefficients classical VAR. However, the results does not indicate marked gains in forecasting power across the different Bayesian models, as well as, over the classical VAR model, possibly because the problem of over parameterization in the classical VAR is not that acute in our three-variable system. Hence, future research would aim to look at VAR models that include over 10 variables.",,2012,10.3923/jas.2012.645.652,,proquest,"This study evaluates the forecasting accuracy of various multivariate models, including Bayesian and classical Vector Autoregressive (VAR) models with time-varying parameters and variable selection, for key South African macroeconomic variables (GDP growth, inflation, T-bill rate). It found that variable selection and non-linearity improve predictions compared to classical VAR, but marked gains across Bayesian models were not observed, possibly due to the small system size. Future research is suggested for larger systems.",True,False,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T09:54:33.915980
5afe5ce80e830918,Forecasting Stock Market Crashes via Machine Learning,"This paper uses a comprehensive set of predictor variables from the five largest Eurozone countries to compare the performance of simple univariate and machine learning-based multivariate models in forecasting stock market crashes. In terms of statistical predictive performance, a support vector machine-based crash prediction model outperforms a random classifier and is superior to the average univariate benchmark as well as a multivariate logistic regression model. Incorporating nonlinear and interactive effects is both imperative and foundation for the outperformance of support vector machines. Their ability to forecast stock market crashes outof-sample translates into substantial value-added to active investors. From a policy perspective, the use of machine learning-based crash prediction models can help activate macroprudential tools in time.","Dichtl, Hubert; Drobetz, Wolfgang; Otto, Tizian",2023,10.1016/j.jfs.2022.101099,,wos,"This paper compares univariate and machine learning models (specifically Support Vector Machines) for forecasting stock market crashes in the five largest Eurozone countries. The SVM model demonstrated superior predictive performance compared to simpler models and a random classifier, highlighting the importance of capturing nonlinear and interactive effects. The findings suggest significant value for investors and potential policy applications for macroprudential tools.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:54:37.628493
d93a58f741b11b3d,Forecasting WTI crude oil futures returns: Does the term structure help?,"Nelson-Siegel (NS) factors extracted from the term structure of WTI oil futures are shown to predict subsequent WTI holding period returns in-sample. This in-sample predictability is not diminished by augmenting with macroeconomic indicators or oil market specific predictors. Allowing the decay factor in the Nelson-Siegel model to vary over time improves in-sample predictions at medium horizon return forecasts. We conduct out-of-sample forecasting exercises on models that use NS factors, such as a simple two factor model that uses a composite leading indicator along with the NS decay factor, and a LASSO model that combines NS factors with macroeconomic indicators and oil market specific predictors. These models significantly reduce forecast errors relative to a no change benchmark across a range of return horizons and futures contract maturities. We also find consistent evidence that models that use the NS factors result in trading strategies with higher Sharpe ratios and better skewness properties than buy and hold strategies and historical mean strategies.",,2021,10.1016/j.eneco.2021.105350,,proquest,"This study investigates whether the term structure of WTI crude oil futures can aid in forecasting returns. The Nelson-Siegel (NS) factors extracted from the term structure demonstrate in-sample predictability for subsequent returns, even when augmented with macroeconomic and oil-specific predictors. Varying the decay factor in the NS model further enhances medium-horizon forecasts. Out-of-sample tests using models incorporating NS factors, including a composite leading indicator and a LASSO model, significantly reduce forecast errors compared to a no-change benchmark. Furthermore, trading strategies based on NS factors yield higher Sharpe ratios and improved skewness compared to benchmark strategies.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:54:39.271673
4f0fd761748ec99f,Forecasting interest rates: a comparative assessment of some second-generation nonlinear models,"Modeling and forecasting of interest rates has traditionally proceeded in the framework of linear stationary methods such as ARMA and VAR, but only with moderate success. We examine here three methods, which account for several specific features of the real world asset prices such as nonstationarity and nonlinearity. Our three candidate methods are based, respectively, on a combined wavelet artificial neural network (WANN) analysis, a mixed spectrum (MS) analysis and nonlinear ARMA models with Fourier coefficients (FNLARMA). These models are applied to weekly data on interest rates in India and their forecasting performance is evaluated vis--vis three GARCH models [GARCH (1,1), GARCH-M (1,1) and EGARCH (1,1)] as well as the random walk model. Both the WANN and MS methods show marked improvement over other benchmark models, and may thus hold out several potentials for real world modeling and forecasting of financial data.","Nachane, Dilip; Clavel, Jose G.",2008,10.1080/02664760701835243,,wos,"This study compares the forecasting performance of three nonlinear models (wavelet artificial neural network, mixed spectrum, and nonlinear ARMA with Fourier coefficients) against GARCH models and a random walk model for Indian interest rates. The wavelet artificial neural network and mixed spectrum methods demonstrated significant improvements over the benchmarks.",True,False,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T09:55:08.439381
019c24267ee95568,Forecasting the term structure of crude oil futures prices with neural networks,"The paper contributes to the limited literature modelling the term structure of crude oil markets. We explain the term structure of crude oil prices using the dynamic Nelson-Siegel model and propose to forecast oil prices using a generalized regression framework based on neural networks. The newly proposed framework is empirically tested on 24. years of crude oil futures prices covering several important recessions and crisis periods. We find 1-month-, 3-month-, 6-month- and 12-month-ahead forecasts obtained from a focused time-delay neural network to be significantly more accurate than forecasts from other benchmark models. The proposed forecasting strategy produces the lowest errors across all times to maturity. © 2017 Elsevier B.V., All rights reserved.","Baruník, J.; Malinská, B.",2016,10.1016/j.apenergy.2015.11.051,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951017088&doi=10.1016%2Fj.apenergy.2015.11.051&partnerID=40&md5=48247872417761781a5c770a7d5dcee8,scopus,"This paper models and forecasts the term structure of crude oil futures prices using a dynamic Nelson-Siegel model and a neural network framework. The neural network approach, specifically a focused time-delay neural network, significantly outperforms benchmark models in forecasting 1- to 12-month-ahead prices, demonstrating its effectiveness across various economic conditions.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:55:11.867245
04e2f27a592dcab7,Forecasting the term structure of government bond yields,"Despite powerful advances in yield curve modeling in the last 20 years, comparatively little attention has been paid to the key practical problem of forecasting the yield curve. In this paper we do so. We use neither the no-arbitrage approach nor the equilibrium approach. Instead, we use variations on the Nelson-Siegel exponential components framework to model the entire yield curve, period-by-period, as a three-dimensional parameter evolving dynamically. We show that the three time-varying parameters may be interpreted as factors corresponding to level, slope and curvature, and that they may be estimated with high efficiency. We propose and estimate autoregressive models for the factors, and we show that our models are consistent with a variety of stylized facts regarding the yield curve. We use our models to produce term-structure forecasts at both short and long horizons, with encouraging results. In particular, our forecasts appear much more accurate at long horizons than various standard benchmark forecasts. © 2008 Elsevier B.V., All rights reserved.","Diebold, F.X.; Li, C.",2006,10.1016/j.jeconom.2005.03.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-31344448314&doi=10.1016%2Fj.jeconom.2005.03.005&partnerID=40&md5=c1160704fad68807ebd914e62fa33324,scopus,"This paper forecasts the term structure of government bond yields using a dynamic three-dimensional parameter model based on the Nelson-Siegel framework. The time-varying parameters, interpreted as level, slope, and curvature, are estimated efficiently and modeled using autoregressive processes. The proposed models produce encouraging term-structure forecasts, particularly at long horizons, outperforming standard benchmarks.",True,True,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T09:55:17.373134
d9e813cb1f5afe0d,Forecasting the term structure of interest rates using integrated nested laplace approximations,"This article discusses the use of Bayesian methods for inference and forecasting in dynamic term structure models through integrated nested Laplace approximations (INLA). This method of analytical approximation allows accurate inferences for latent factors, parameters and forecasts in dynamic models with reduced computational cost. In the estimation of dynamic term structure models it also avoids some simplifications in the inference procedures, such as the inefficient two-step ordinary least squares (OLS) estimation. The results obtained in the estimation of the dynamic Nelson-Siegel model indicate that this method performs more accurate out-of-sample forecasts compared to the methods of two-stage estimation by OLS and also Bayesian estimation methods using Markov chain Monte Carlo (MCMC). These analytical approaches also allow efficient calculation of measures of model selection such as generalized cross-validation and marginal likelihood, which may be computationally prohibitive in MCMC estimations. Copyright © 2014 John Wiley & Sons, Ltd. Copyright © 2014 John Wiley & Sons, Ltd. © 2022 Elsevier B.V., All rights reserved.","Laurini, M.P.; Hotta, L.K.",2014,10.1002/for.2288,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897012994&doi=10.1002%2Ffor.2288&partnerID=40&md5=a252763e08afa540fa05096fdebc11ac,scopus,"This article explores the use of Integrated Nested Laplace Approximations (INLA), a Bayesian method, for forecasting the term structure of interest rates within dynamic term structure models. The study demonstrates that INLA provides more accurate out-of-sample forecasts compared to traditional OLS and MCMC methods, while also offering computational advantages for model selection metrics.",True,True,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T09:55:22.210849
3341cd39f6436146,Forecasting time series subject to multiple structural breaks,"This paper provides a new approach to forecasting time series that are subject to discrete structural breaks. We propose a Bayesian estimation and prediction procedure that allows for the possibility of new breaks occurring over the forecast horizon, taking account of the size and duration of past breaks (if any) by means of a hierarchical hidden Markov chain model. Predictions are formed by integrating over the parameters from the meta-distribution that characterizes the stochastic break-point process. In an application to U.S. Treasury bill rates, we find that the method leads to better out-of-sample forecasts than a range of alternative methods. © 2006 The Review of Economic Studies Limited. © 2008 Elsevier B.V., All rights reserved.","Pesaran, M.; Pettenuzzo, D.; Timmermann, A.",2006,10.1111/j.1467-937x.2006.00408.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749045516&doi=10.1111%2Fj.1467-937X.2006.00408.x&partnerID=40&md5=5ea9b46ce031cdf0af74247798ef4184,scopus,This paper introduces a Bayesian approach using a hierarchical hidden Markov chain model to forecast time series with multiple structural breaks. The method accounts for potential new breaks and uses past break information. An application to U.S. Treasury bill rates shows improved out-of-sample forecasts compared to other methods.,True,True,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T09:55:29.094842
400f23285a075f7c,Forward contracts for the operation of an electricity industry under spot pricing,A study is reported of the use of forward contracts as risk instruments for electricity industries operating under spot pricing. Forward contracts involve financial transactions or commitments which relate to a physical trade at a later time instance. Price setting and appropriate participant responses are discussed. Simulation studies are used to demonstrate that forward contracts offer participants an opportunity to reduce their risk exposure without removing the incentive to respond to higher spot prices.<>,R. J. Kaye; H. R. Outhred; C. H. Bannister,1990,10.1109/59.49085,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=49085,ieeexplore,"This study investigates the use of forward contracts as risk management tools in electricity industries that operate under spot pricing. It discusses price setting, participant responses, and uses simulations to show that forward contracts can reduce risk exposure without diminishing the incentive to respond to high spot prices.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:55:31.479547
8413a03397041cf2,From Man vs. Machine to Man plus Machine: The art and AI of stock analyses,"An AI analyst trained to digest corporate disclosures, industry trends, and macroeconomic indicators surpasses most analysts in stock return predictions. Nevertheless, humans win Man vs. Machinewhen institutional knowledge is crucial, e.g., involving intangible assets and financial distress. AI wins when information is transparent but voluminous. Humans provide significant incremental value in Man + Machine, which also substantially reduces extreme errors. Analysts catch up with machines after alternative databecome available if their employers build AI capabilities. Documented synergies between humans and machines inform how humans can leverage their advantage for better adaptation to the growing AI prowess.","Cao, Sean; Jiang, Wei; Wang, Junbo; Yang, Baozhong",2024,10.1016/j.jfineco.2024.103910,,wos,"This study explores the synergy between human analysts and AI in stock market analysis. While AI excels at processing large volumes of transparent data for stock return predictions, human analysts remain crucial for understanding intangible assets and financial distress. The combination of human and machine intelligence (Man + Machine) significantly improves predictions and reduces extreme errors, with humans catching up to AI when alternative data becomes available and AI capabilities are integrated.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:55:36.917260
e7deb740efeb41c6,Gaussian Weighting Reversion Strategy for Accurate Online Portfolio Selection,"In this paper, we design and implement a new on-line portfolio selection strategy based on reversion mechanism and weighted on-line learning. Our strategy, called “Gaussian Weighting Reversion” (GWR), improves the reversion estimator to form optimal portfolios and effectively overcomes the shortcomings of existing on-line portfolio selection strategies. Firstly, GWR uses Gaussian function to weight data in a sliding window to exploit the “time validity” of historical market data. It means that the more recent data are more valuable for market prediction than the earlier. Secondly, the self-learning for various sliding windows is created to make our strategy adaptive to different markets. In addition, double estimations are first proposed to be made at each time point, and the average of double estimations is obtained to alleviate the influence of noise and outliers. Extensive evaluation on six public datasets shows the advantages of our strategy compared with other nine competing strategies, including the state-of-the-art ones. Finally, the complexity analysis of GWR shows its availability in large-scale real-life online trading.",X. Cai; Z. Ye,2019,10.1109/tsp.2019.2941067,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8834832,ieeexplore,"This paper introduces the Gaussian Weighting Reversion (GWR) strategy for online portfolio selection. GWR utilizes a weighted online learning approach with a Gaussian function to prioritize recent market data. It also incorporates self-learning for adaptive window sizes and double estimations to reduce noise. Empirical evaluations on six datasets demonstrate GWR's superiority over nine other strategies, and complexity analysis confirms its suitability for real-time trading.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:55:39.414270
7ff2a7cb36ec3f22,Generating currency trading rules from the term structure of forward foreign exchange premia,"The quality of an exchange rate forecasting model has typically been judged relative to a random-walk in terms of out-of-sample forecast errors. The difficulty of outperforming this benchmark is well documented, although Clarida and Taylor have demonstrated how the random walk can be beaten in this metric by exploiting information embedded within the term structure of forward exchange rate premia. But this achievement does not guarantee success within an investment context. We therefore assess whether the Clarida-Taylor framework can be used to generate significant trading profits in combination with an acceptable degree of risk in a realistic investment portfolio context. (C) 2013 Published by Elsevier Ltd.","Sager, Michael; Taylor, Mark P.",2014,10.1016/j.jimonfin.2013.03.005,,wos,"This paper evaluates whether the Clarida-Taylor framework, which uses information from the term structure of forward exchange rate premia to forecast exchange rates, can generate significant trading profits in a realistic investment portfolio context, going beyond simply outperforming a random-walk benchmark.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:55:41.111095
77d74bafaf54a565,Gold Price Prediction Using Two-layer Decomposition and XGboost Optimized by the Whale Optimization Algorithm,"Gold price prediction is of great importance in big data computing and economic sphere. This paper aims to contribute to the study of hybrid models that can be used to forecast the price of gold. In this study, The Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN) is employed to decompose a residual term containing complex information following the variational modal decomposition (VMD) and an extreme gradient boosting tree (XGBoost) optimized by the Whale Optimization Algorithm (WOA) is combined to construct the VMD-RES.-CEEMDAN-WOA-XGBoost model. The closing price data of COMEX gold futures from 1 October 2018 to 20 November 2023 were selected as examples of gold futures price. A variety of factors that can affect the price of gold are considered in the research. This study indicates that the combined forecasting model proposed in this paper has superior performance when compared to the other comparison forecasting models evaluated. Furthermore, it has been found through SHAP analysis that the Nasdaq index, silver price, and the yield of US 10-year Treasury bonds are most closely related to the prediction of gold price.",,2025,10.1007/s10614-024-10736-9,,proquest,"This paper proposes a hybrid model (VMD-RES.-CEEMDAN-WOA-XGBoost) for gold price prediction, combining Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN) and an extreme gradient boosting tree (XGBoost) optimized by the Whale Optimization Algorithm (WOA). The model was tested on COMEX gold futures data from 2018-2023 and showed superior performance compared to other models. SHAP analysis identified the Nasdaq index, silver price, and US 10-year Treasury bond yield as key factors influencing gold price.",True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:55:52.607990
0871d13199519d67,Graph-based stock correlation and prediction for high-frequency trading systems,"In this paper, we have implemented a high-frequency quantitative system that can obtain stable returns for the Chinese A-share market, which has been running for more than 3 months (from March 27, 2020 to June 30, 2020) with the expected results. A number of rules and barriers exist in the Chinese A-share market such as trading restrictions and high fees, as well as scarce and expensive hedging tools. It is difficult to achieve stable absolute returns in such a market. Stock correlation analysis and price prediction play an important role to achieve any profitable trading. The portfolio management and subsequent trading decisions highly depend on the results of stock correlation analysis and price prediction. However, it is nontrivial to analyze and predict any stocks, being time-varying and affected by unlimited factors in a given market. Traditional methods only take some certain factors into consideration but ignore others that may be changed dynamically. In this paper, we propose a novel machine learning model named Graph Attention Long Short-Term Memory (GALSTM) to learn the correlations between stocks and predict their future prices automatically. First, a multi-Hawkes Process is used to initial a correlation graph between stocks. This procedure provides a good training start as the multi-Hawkes Processes will be studied on the most saint feature fluctuations with any correlations being statistically significant. Then an attention-based LSTM is built to learn the weighting matrix underlying the dynamic graph. In addition, we also build matching data process plus portfolio management modules to form a complete system. The proposed GALSTM enables us to expand the scope of stock selection under the premise of controlling risks with limited hedging tools in the A-share market, thereby effectively increasing high-frequency excess returns. We then construct a long and short positions combination, select long positions in the A shares of the entire market, and use stock index futures to short. With GALSTM model, the products managed by our fully automatic quantitative trading system achieved an absolute annual return rate of 44.71% and the standard deviation of daily returns is only 0.42% in three months of operation. Only 1 week loss in 13 weeks of running time. © 2021 Elsevier B.V., All rights reserved.","Yin, T.; Liu, C.; Ding, F.; Feng, Z.; Yuan, B.; Zhang, N.",2022,10.1016/j.patcog.2021.108209,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113573353&doi=10.1016%2Fj.patcog.2021.108209&partnerID=40&md5=971f0fc98ae32339efd70adf33e67ef9,scopus,"This paper introduces a novel machine learning model, GALSTM, for high-frequency stock trading in the Chinese A-share market. The system uses a multi-Hawkes Process to establish stock correlations and an attention-based LSTM to predict future prices. The implemented system achieved a 44.71% annual return rate over three months, demonstrating its effectiveness in managing risk and increasing returns despite market constraints.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:56:23.815902
1943ea8afc4b3783,"Green bonds forecasting: evidence from pre-crisis, Covid-19 and Russian–Ukrainian crisis frameworks","PurposeWithout precedent, green bonds confront, for the first time since their emergence, a twofold crisis context, namely the Covid-19-Russian–Ukrainian crisis period. In this context, this paper aims to investigate the connectedness between the two pioneering bond market classes that are conventional and treasury, with the green bonds market.Design/methodology/approachIn their forecasting target, authors use a Support Vector Regression model on daily S&P 500 Green, Conventional and Treasury Bond Indexes for a year from 2012 to 2022.FindingsAuthors argue that conventional bonds could better explain and predict green bonds than treasury bonds for the three studied sub-periods (pre-crisis period, Covid-19 crisis and Covid-19-Russian–Ukrainian crisis period). Furthermore, conventional and treasury bonds lose their forecasting power in crisis framework due to enhancements in market connectedness relationships. This effect makes spillovers in bond markets more sensitive to crisis and less predictable. Furthermore, this research paper indicates that even if the indicators of the COVID-19 crisis have stagnated and the markets have adapted to this rather harsh economic framework, the forecast errors persist higher than in the pre-crisis phase due to the Russian–Ukrainian crisis effect not yet addressed by the literature.Originality/valueThis study has several implications for the field of green bond forecasting. It not only illuminates the market participants to the best market forecasters, but it also contributes to the literature by proposing an unadvanced investigation of green bonds forecasting in Crisis periods that could help market participants and market policymakers to anticipate market evolutions and adapt their strategies to period specificities.",,2025,10.1108/jes-01-2024-0061,,proquest,"This study investigates the connectedness between conventional, treasury, and green bond markets during pre-crisis, Covid-19, and Russian-Ukrainian crisis periods using a Support Vector Regression model. It finds that conventional bonds are better predictors of green bonds, but both conventional and treasury bonds lose forecasting power during crises due to enhanced market interconnectedness. The Russian-Ukrainian crisis significantly impacts forecast errors even after market adaptation to Covid-19.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:56:36.606239
1ffdcdc331372c74,Have trend-following signals in commodity futures markets become less reliable in recent years?,"Various trend-following trading rules have been shown to be valuable for predicting market directions and thus the formulation of investment strategies. However, recent equity market research has provided striking evidence that the predictive power of such rules appears to diminish over time due to increased investor attention and lowered arbitrage barriers. Given that trend-following rules are also very successful and have been widely used in futures markets, we analyze whether a similar effect can be observed for commodity futures contracts. Using a trend regression approach based on time-varying success ratios, we detect significantly higher predictive accuracy for cross-sectional than for time-series strategies. In addition, with the exception of a few commodities, we find no significant trending behavior in trading rule reliability. These results, which are robust in a variety of settings, indicate strong momentum stability in futures markets and justify the application of this class of trading rules in commodity futures investing.","Auer, Benjamin R.",2021,10.1007/s11408-021-00385-5,,wos,"This study investigates whether trend-following signals in commodity futures markets have become less reliable recently, similar to observations in equity markets. The authors analyze trend-following rules using a time-varying success ratio approach. They find that cross-sectional strategies are more accurate than time-series strategies, but overall, there is no significant decline in the reliability of trading rules for most commodities, suggesting stable momentum in futures markets.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:56:38.510401
0cdd1e39bd843201,Hawkes-diffusion process and the conditional probability of defaults in the Eurozone,"This study examines market information embedded in the European sovereign CDS (credit default swap) market by analyzing the sovereign CDSs of 13 Eurozone countries from January 1, 2008, to February 29, 2012, which includes the recent Eurozone debt crisis period. We design the conditional probability of defaults for the CDS prices based on the Hawkes-diffusion process and obtain the theoretical prices of CDS indexes. To estimate the model parameters, we calibrate the model prices to empirical prices obtained from individual sovereign CDS term structure data. The estimated parameters clearly explain both cross-sectional and time-series data. Our empirical results show that the probability of a huge loss event sharply increased during the Eurozone debt crisis, indicating a contagion effect. Even countries with strong and stable economies, such as Germany and France, suffered from the contagion effect. We also find that the probability of small events is sensitive to the state of the economy, spiking several times due to the global financial crisis and the Greek government debt crisis. (C) 2015 Elsevier B.V. All rights reserved.","Kim, Jungmu; Park, Yuen Jung; Ryu, Doojin",2016,10.1016/j.physa.2015.12.087,,wos,"This study uses a Hawkes-diffusion process to analyze sovereign credit default swap (CDS) prices for 13 Eurozone countries from 2008 to 2012, a period encompassing the Eurozone debt crisis. The model estimates the conditional probability of defaults and theoretical CDS index prices, calibrated against empirical data. Findings indicate a contagion effect during the crisis, impacting even strong economies, and that the probability of smaller events is sensitive to economic conditions.",True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:56:43.369393
4242f44ee4893257,Heterogeneous Demand and Supply for an Insurance‐linked Credit Product in Kenya: A Stated Choice Experiment Approach,"We employ a discrete choice experiment to elicit demand and supply side preferences for insurance‐linked credit, a promising market‐based tool for managing agricultural weather risks and providing access to credit for farmers. We estimate preference heterogeneity using primary data from smallholder farmers and managers of lenders/insurers combined with household socio‐economic survey data in Kenya. We analyse the choice data using maximum simulated likelihood and Hierarchical Bayes estimation of a mixed logit model. Although there are some similarities, we find that there is conflicting demand and supply side preferences for credit terms, collateral requirements, and loan use flexibility. We also analyse willingness to buy and willingness to offer for farmers and suppliers, respectively, for the risk premium for different attributes and their levels. Identifying the preferred attributes and levels for both farmers and financial institutions can guide optimal packaging of insurance and credit providing market participation and adoption motivation for insurance‐bundled credit product.",,2021,10.1111/1477-9552.12401,,proquest,"This study uses a discrete choice experiment to understand the demand and supply preferences for an insurance-linked credit product in Kenya, aimed at managing agricultural weather risks for farmers. It analyzes data from both smallholder farmers and lenders/insurers, employing mixed logit models to identify heterogeneous preferences regarding credit terms, collateral, and loan use. The findings reveal conflicting preferences between demand and supply sides, and the study assesses willingness to buy and offer for different product attributes to guide optimal product design and market adoption.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:56:45.657555
7711efe41665e70e,Hidden persistent disasters and asset prices,"This study analyzes the effects of agents' learning about hidden persistent economic disasters on asset prices. In this study, it is assumed that aggregate consumption follows a hidden Markov regime-switching process and a representative agent infers the current regime, normal regime, or disaster regime, sequentially from the realized path of the past consumption process. In this setting, the fluctuation in the agent's posterior probabilities of the disaster regime augments the volatility of equity returns. By utilizing the stochastic differential utility, this study demonstrates that the current model can help resolve many asset pricing puzzles including the equity premium puzzle, equity volatility puzzle, and risk-free rate puzzle simultaneously. Further, the current model predicts the counter-cyclical pattern in the equity premium and equity-return volatility on the normal regime, although asset returns are negative and highly volatile during disasters. The study also demonstrates that, if the agent's preferences are restricted to time-additive power utility, the consideration of hidden persistent disasters deepens the asset pricing puzzles. © 2013 Springer-Verlag Berlin Heidelberg. © 2014 Elsevier B.V., All rights reserved.","Suzuki, M.",2014,10.1007/s10436-013-0226-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905013524&doi=10.1007%2Fs10436-013-0226-5&partnerID=40&md5=bbf69d9f4aeb9390f5e08124536c8c25,scopus,"This study investigates how agents' learning about hidden persistent economic disasters influences asset prices. It models aggregate consumption with a hidden Markov regime-switching process, where agents infer the current regime (normal or disaster) from past consumption data. The model shows that fluctuating posterior probabilities of the disaster regime increase equity return volatility. The study suggests this model can address asset pricing puzzles like the equity premium, equity volatility, and risk-free rate puzzles. It also predicts counter-cyclical patterns in equity premium and volatility during normal times, contrasting with the negative and volatile returns during disasters. The paper notes that if agents have time-additive power utility, hidden persistent disasters exacerbate asset pricing puzzles.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:56:47.741499
33f5d072b20c21b6,Hierarchical Bayesian Models for Regularization in Sequential Learning,"We show that a hierarchical Bayesian modeling approach allows us to perform regularization in sequential learning. We identify three inference levels within this hierarchy: model selection, parameter estimation, and noise estimation. In environments where data arrive sequentially, techniques such as cross validation to achieve regularization or model selection are not possible. The Bayesian approach, with extended Kalman filtering at the parameter estimation level, allows for regularization within a minimum variance framework. A multilayer perceptron is used to generate the extended Kalman filter nonlinear measurements mapping. We describe several algorithms at the noise estimation level that allow us to implement on-line regularization. We also show the theoretical links between adaptive noise estimation in extended Kalman filtering, multiple adaptive learning rates, and multiple smoothing regularization coefficients.",J. F. G. d. Freitas; M. Niranjan; A. H. Gee,2000,10.1162/089976600300015655,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6789428,ieeexplore,"This paper proposes a hierarchical Bayesian modeling approach for regularization in sequential learning, addressing the limitations of traditional methods like cross-validation in dynamic environments. It outlines three inference levels: model selection, parameter estimation (using extended Kalman filtering), and noise estimation. The authors demonstrate theoretical links between adaptive noise estimation, multiple learning rates, and smoothing regularization coefficients, suggesting on-line regularization capabilities.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:56:56.516819
ef6ce888ac066f40,Hierarchical Bayesian collective risk model: an application to health insurance,"This paper deals with the main statistical steps involved in building an insurance plan, with special emphasis on an application to health insurance. The pure premium is predicted based on the available past information concerning the number and the amount of losses, and also the population exposed to risk. Both the size and the number of losses are treated in a stochastic manner. The claims are assumed to follow a Poisson process and the claim sizes are independent and identically distributed non-negative random variables. The model proposed is a generalization of the collective risk model, usually applied in practice. The evolution of the population at risk is also stochastically described via a nonlinear hierarchical growth model. Furthermore, a theoretical decision framework is adopted for evaluating the premium. Model selection and premium calculation are obtained from the predictive distribution, incorporating all the uncertainties involved.",,2005,10.1016/j.insmatheco.2004.11.006,,proquest,"This paper presents a hierarchical Bayesian collective risk model for health insurance. It predicts pure premiums by stochastically modeling the number and amount of losses, as well as the exposed population. The model generalizes the standard collective risk model and incorporates a hierarchical growth model for the population. Premium evaluation uses a theoretical decision framework and predictive distributions to account for uncertainties.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:56:59.015574
2c4a4637c32b86cb,High frequency online inflation and term structure of interest rates: Evidence from China,"In the digital era, the information value of online prices, characterized by weak price stickiness and high sensitivity to economic shocks, deserves more attention. This paper integrates the high-frequency online inflation rate into the dynamic Nelson-Siegel (DNS) model to explore its relationship with the term structure of interest rates. The empirical results show that the weekly online inflation significantly predicts the yield curve, especially the slope factor, whereas the monthly official inflation cannot predict the yield curve and is instead predicted by the yield curve factors. The mechanism analysis reveals that, due to low price stickiness, online inflation is more sensitive to short-term economic fluctuations and better reflects money market liquidity, thereby having significant predictive power for short-term interest rates and the slope factor. Specifically, online inflation for non-durable goods and on weekdays shows stronger predictive power for the slope factor. The heterogeneity in price stickiness across these categories explains the varying impacts on the yield curve. © 2025 Elsevier B.V., All rights reserved.","Zhang, T.; Tang, K.; Liu, T.; Jiang, T.",2025,10.1016/j.jempfin.2025.101626,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006742365&doi=10.1016%2Fj.jempfin.2025.101626&partnerID=40&md5=3f0e5cbafd7d26ef438cc757b09558ad,scopus,"This paper integrates high-frequency online inflation into the dynamic Nelson-Siegel model to analyze its impact on China's interest rate term structure. Empirical findings indicate that weekly online inflation significantly predicts the yield curve, particularly its slope, while monthly official inflation does not. The study suggests that online inflation's sensitivity to economic fluctuations and its reflection of money market liquidity contribute to its predictive power for short-term interest rates and the slope factor, with non-durable goods and weekday online inflation showing stronger effects.",True,True,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T09:57:02.239753
13680c9aa7a8de08,How Do Macroeconomic Fundamentals Affect Sovereign Bond Yields? New Evidence from European Forecasts,"Macroeconomic fundamentals impact the long-term insolvency problem of a country. This article empirically assesses the role played by both macroeconomic and fiscal fundamentals, proxied by a set of European Commission's forecasts, in affecting sovereign bond yields. We look at a large panel of 25 European countries between 1992 and 2015. By means panel and time-series approaches, we find that lower short-term interest rates and better fiscal institutions tend to lower bond yields. The better the economic and fiscal outlooks going forward, the lower the yields demanded in international markets. Timing also matters: investors seem to pay more attention to forecasts the shorter the forecast horizon, and they started carrying more weight since the Global Financial Crisis. Finally, the impact of yields' determinants is different across countries, being more prominent in those characterized by economic hardship conditions (Greece, Ireland, Spain, and Portugal). (JEL codes: C23, E44, H68) © 2019 Elsevier B.V., All rights reserved.","Tovar Jalles, J.T.",2019,10.1093/cesifo/ify025,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064515400&doi=10.1093%2Fcesifo%2Fify025&partnerID=40&md5=7535ceeeb760f8019d70ad6693a12d0a,scopus,"This study investigates how macroeconomic and fiscal fundamentals, using European Commission forecasts, influence sovereign bond yields across 25 European countries from 1992 to 2015. Using panel and time-series methods, it finds that lower short-term interest rates and stronger fiscal institutions reduce bond yields. Positive economic and fiscal outlooks also lower yields, with investors paying more attention to shorter forecast horizons, especially since the Global Financial Crisis. The impact varies by country, being more significant in nations facing economic difficulties like Greece, Ireland, Spain, and Portugal.",False,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:57:35.106882
339904381e96023a,Hybrid LSTM–Transformer Architecture with Multi-Scale Feature Fusion for High-Accuracy Gold Futures Price Forecasting,"Amidst global economic fluctuations and escalating geopolitical risks, gold futures, as a pivotal safe-haven asset, demonstrate price dynamics that directly impact investor decision-making and risk mitigation effectiveness. Traditional forecasting models face significant limitations in capturing long-term trends, addressing abrupt volatility, and mitigating multi-source noise within complex market environments characterized by nonlinear interactions and extreme events. Current research predominantly focuses on single-model approaches (e.g., ARIMA or standalone neural networks), inadequately addressing the synergistic effects of multimodal market signals (e.g., cross-market index linkages, exchange rate fluctuations, and policy shifts) and lacking the systematic validation of model robustness under extreme events. Furthermore, feature selection often relies on empirical assumptions, failing to uncover non-explicit correlations between market factors and gold futures prices. A review of the global literature reveals three critical gaps: (1) the insufficient integration of temporal dependency and global attention mechanisms, leading to imbalanced predictions of long-term trends and short-term volatility; (2) the neglect of dynamic coupling effects among cross-market risk factors, such as energy ETF-metal market spillovers; and (3) the absence of hybrid architectures tailored for high-frequency noise environments, limiting predictive utility for decision support. This study proposes a three-stage LSTM–Transformer–XGBoost fusion framework. Firstly, XGBoost-based feature importance ranking identifies six key drivers from thirty-six candidate indicators: the NASDAQ Index, S&P 500 closing price, silver futures, USD/CNY exchange rate, China’s 1-year Treasury yield, and Guotai Zhongzheng Coal ETF. Second, a dual-channel deep learning architecture integrates LSTM for long-term temporal memory and Transformer with multi-head self-attention to decode implicit relationships in unstructured signals (e.g., market sentiment and climate policies). Third, rolling-window forecasting is conducted using daily gold futures prices from the Shanghai Futures Exchange (2015–2025). Key innovations include the following: (1) a bidirectional LSTM–Transformer interaction architecture employing cross-attention mechanisms to dynamically couple global market context with local temporal features, surpassing traditional linear combinations; (2) a Dynamic Hierarchical Partition Framework (DHPF) that stratifies data into four dimensions (price trends, volatility, external correlations, and event shocks) to address multi-driver complexity; (3) a dual-loop adaptive mechanism enabling endogenous parameter updates and exogenous environmental perception to minimize prediction error volatility. This research proposes innovative cross-modal fusion frameworks for gold futures forecasting, providing financial institutions with robust quantitative tools to enhance asset allocation optimization and strengthen risk hedging strategies. It also provides an interpretable hybrid framework for derivative pricing intelligence. Future applications could leverage high-frequency data sharing and cross-market risk contagion models to enhance China’s influence in global gold pricing governance.",,2025,10.3390/math13101551,,proquest,"This study proposes a hybrid LSTM–Transformer–XGBoost fusion framework for gold futures price forecasting, addressing limitations of traditional models in capturing long-term trends, volatility, and multi-source noise. The framework integrates XGBoost for feature importance, a dual-channel deep learning architecture (LSTM and Transformer) for temporal memory and implicit relationship decoding, and rolling-window forecasting. Key innovations include a bidirectional LSTM–Transformer interaction architecture, a Dynamic Hierarchical Partition Framework (DHPF), and a dual-loop adaptive mechanism. The research aims to provide financial institutions with robust quantitative tools for asset allocation and risk hedging.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:57:38.143752
f85dca55e6510587,"Identifying and Exploiting Alpha in Linear Asset Pricing Models with Strong, Semi-Strong, and Latent Factors","The risk premia of traded factors are the sum of factor means and a parameter vector, we denote by ϕ, which is identified from the cross-sectional regression of α<inf>i</inf> on the vector of factor loadings, β<inf>i</inf>. If ϕ is non-zero, then α<inf>i</inf> are non-zero and one can construct “phi-portfolios” which exploit the systematic components of non-zero alpha. We show that for known values of β<inf>i</inf> and when ϕ is non-zero, there exist phi-portfolios that dominate mean–variance (MV) portfolios. This article then proposes a two-step bias corrected estimator of ϕ and derives its asymptotic distribution allowing for idiosyncratic pricing errors, weak missing factors, and weak error cross-sectional dependence. Small sample results from extensive Monte Carlo experiments show that the proposed estimator has the correct size with good power properties. This article also provides an empirical application to a large number of U.S. securities with risk factors selected from a large number of potential risk factors according to their strength and constructs phi-portfolios and compares their Sharpe ratios to MV and S&P portfolios. © 2025 Elsevier B.V., All rights reserved.","Pesaran, M.H.; Smith, R.P.",2025,10.1093/jjfinec/nbae029,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003408796&doi=10.1093%2Fjjfinec%2Fnbae029&partnerID=40&md5=f1f288926de6bb2574a79c557b5caa70,scopus,"This article proposes a bias-corrected estimator for factor risk premia (phi) in linear asset pricing models, allowing for idiosyncratic pricing errors and weak dependencies. It demonstrates that 'phi-portfolios' can outperform mean-variance portfolios when phi is non-zero. An empirical application to U.S. securities is presented, comparing the performance of these portfolios.",True,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:57:40.904375
b6709facf4548a31,Identifying and measuring the contagion channels at work in the European financial crises,"We investigate the phenomenon of contagion with a special focus on the recent financial crisis, distinguishing four alternative channels, namely the flight-to-quality, flight-to-liquidity, risk premium, and correlated information channels. Specifically, we employ the differences among estimates and impulse response functions across linear and nonlinear models to identify and measure cross-asset contagion. An application to weekly Eurozone data for a 2007–2014 sample reveals that a two-state Markov switching model shows economically weak, though accurately estimated, contagion effects in a crisis regime. These findings are mainly explained by a flight-to-quality channel. Furthermore, we extend our analysis to explore whether European markets may or may not have been subject to contagion when exposed to external shocks, such as those originated from the US subprime crisis. © 2017 Elsevier B.V., All rights reserved.","Guidolin, M.; Pedio, M.",2017,10.1016/j.intfin.2017.01.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009812010&doi=10.1016%2Fj.intfin.2017.01.001&partnerID=40&md5=28b192b2ae5c94f358f7816372c12690,scopus,"This paper investigates contagion channels during the European financial crises, distinguishing between flight-to-quality, flight-to-liquidity, risk premium, and correlated information channels. Using linear and nonlinear models applied to Eurozone data from 2007-2014, the study finds weak contagion effects in a crisis regime, primarily attributed to a flight-to-quality channel. The analysis also explores contagion from external shocks, such as the US subprime crisis.",True,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:57:42.614623
1ff0c33a84a29bac,Implied volatility sentiment: a tale of two tails,"We propose a sentiment measure jointly derived from out-of-the-money index puts and single stock calls: implied volatility (IV-) sentiment. In contrast to implied correlations, our measure uses information from the tails of the risk-neutral densities from these two markets rather than across their entire moneyness structures. We find that IV-sentiment measure adds value over and above traditional factors in predicting the equity risk premium out-of-sample. Forecasting results are superior when constrained ensemble models are used vis-à-vis unregularized machine learning techniques. In a mean-reversion strategy, our IV-sentiment measure delivers economically significant results, with limited exposure to a set of cross-sectional equity factors, including Fama and French's five factors, the momentum factor and the low-volatility factor, and seems valuable in preventing momentum crashes. Our novel measure reflects overweight of tail events, which we interpret as a behavioral bias. However, we cannot rule out a risk-compensation rationale. © 2020 Elsevier B.V., All rights reserved.","Félix, L.; Kräussl, R.; Stork, P.",2020,10.1080/14697688.2019.1696018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078861813&doi=10.1080%2F14697688.2019.1696018&partnerID=40&md5=411cec64c69eb2f534d55dd481c69d0b,scopus,"This paper introduces a novel sentiment measure, IV-sentiment, derived from out-of-the-money index puts and single stock calls. This measure utilizes information from the tails of risk-neutral densities. The study demonstrates that IV-sentiment enhances out-of-sample predictions of the equity risk premium, particularly when using constrained ensemble models. It also proves effective in a mean-reversion strategy, showing economic significance and limited exposure to common equity factors, while helping to prevent momentum crashes. The authors interpret the measure as reflecting a behavioral bias related to tail events, though a risk-compensation explanation is also considered.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:57:44.969483
345281eb8dc19585,Incorporating economic indicators and market sentiment effect into US Treasury bond yield prediction with machine learning,"Accurate prediction of US Treasury bond yields is crucial for investment strategies and economic policymaking. This paper explores the application of advanced machine learning techniques, specifically Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM) models, in forecasting these yields. By integrating key economic indicators and policy changes, our approach seeks to enhance the precision of yield predictions. Our study demonstrates the superiority of LSTM models over traditional RNNs in capturing the temporal dependencies and complexities inherent in financial data. The inclusion of macroeconomic and policy variables significantly improves the models’ predictive accuracy. This research underscores a pioneering movement for the legacy banking industry to adopt artificial intelligence (AI) in financial market prediction. In addition to considering the conventional economic indicator that drives the fluctuation of the bond market, this paper also optimizes the LSTM to handle situations when rate hike expectations have already been priced-in by market sentiment. © 2024 Elsevier B.V., All rights reserved.","Li, Z.; Wang, B.; Chen, Y.",2024,10.24294/jipd.v8i9.7671,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204238837&doi=10.24294%2Fjipd.v8i9.7671&partnerID=40&md5=7603a9906c2d14dd96d306253e3a6399,scopus,"This paper investigates the use of RNN and LSTM models to predict US Treasury bond yields, incorporating economic indicators and market sentiment. The study finds that LSTM models outperform traditional RNNs and that including macroeconomic and policy variables improves prediction accuracy, highlighting the potential of AI in financial market forecasting.",True,True,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T09:58:21.668697
716bd56fb97759d4,Inference in Bayesian additive vector autoregressive tree models,"Vector autoregressive (VAR) models assume linearity between the endogenous variables and their lags. This assumption might be overly restrictive and could have a deleterious impact on forecasting accuracy. As a solution we propose combining VAR with Bayesian additive regression tree (BART) models. The resulting Bayesian additive vector autoregressive tree (BAVART) model is capable of capturing arbitrary nonlinear relations between the endogenous variables and the covariates without much input from the researcher. Since controlling for heteroscedasticity is key for producing precise density forecasts, our model allows for stochastic volatility in the errors. We apply our model to two datasets. The first application shows that the BAVART model yields highly competitive forecasts of the U.S. term structure of interest rates. In a second application we estimate our model using a moderately sized Eurozone dataset to investigate the dynamic effects of uncertainty on the economy. © 2023 Elsevier B.V., All rights reserved.","Huber, F.; Rossini, L.",2022,10.1214/21-aoas1488,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127778413&doi=10.1214%2F21-AOAS1488&partnerID=40&md5=742157785b372051b99fe7333df16fb0,scopus,"This paper introduces the Bayesian additive vector autoregressive tree (BAVART) model, which combines Vector Autoregressive (VAR) models with Bayesian Additive Regression Trees (BART) to capture nonlinear relationships between endogenous variables and covariates. The model also incorporates stochastic volatility for density forecasts. The authors demonstrate its effectiveness in forecasting the U.S. term structure of interest rates and in analyzing the dynamic effects of uncertainty on the Eurozone economy.",True,True,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T09:58:28.501873
65827861f45c0f78,Inflation Prediction Method Based on Deep Learning,"Forward-looking forecasting of the inflation rate could help the central bank and other government departments to better use monetary policy to stabilize prices and prevent the impact of inflation on market entities, especially for low- and middle-income groups. It can also help financial institutions and investors better make investment decisions. In this sense, the forecast of inflation rate is of great significance. The existing literature mainly uses linear models such as autoregressive (AR) and vector autoregressive (VAR) models to predict the inflation rate. The nonlinear relationship between variables and the mining of historical data information are relatively lacking. Therefore, the prediction strategies and accuracy of the existing literature need to be improved. The predictive model designed in deep learning can fully mine the nonlinear relationship between variables and process complex long-term time series dynamic information, thereby making up for the deficiencies of existing research. Therefore, this paper employs the recurrent neural networks with gated recurrent unit (GRU-RNN) model to train and analyze the Consumer Price Index (CPI) indicators to obtain inflation-related prediction results. The experimental results on historical data show that the GRU-RNN model has good performance in predicting China's inflation rate. In comparison, the performance of the proposed method is significantly better than some traditional models, showing its superior effectiveness.","Yang, Cheng; Guo, Shuhua",2021,10.1155/2021/1071145,,wos,"This paper proposes a deep learning model, specifically a Gated Recurrent Unit Recurrent Neural Network (GRU-RNN), for predicting China's inflation rate using Consumer Price Index (CPI) data. The authors argue that deep learning models can better capture nonlinear relationships and long-term dynamics compared to traditional linear models like AR and VAR, leading to improved prediction accuracy. Experimental results demonstrate that the GRU-RNN model outperforms traditional methods.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:58:31.912786
475fb32ab8e4049d,Informed Trading and Return Predictability in China: Research Based on Ensemble Neural Network,"We construct a new informed trading index based on the high-frequency trading data of the Chinese A-share market using the ensemble neural network algorithm. We find that the informed trading index is a strong negative predictor of future aggregate stock market returns, with monthly in-sample and out-of-sample (Formula presented.) of 5.45% and 3.53%, respectively, which is far greater than the predictive power of other previously studied informed trading indicators and macroeconomic variables. The asset allocation strategy based on our index can generate large economic gains for the mean-variance investors, with annualized CER (certain equivalent return) gains ranging from 10.91% to 7.80% as the investor’s risk appetite varies. The driving force of the predictive power appears to stem from the liquidity provider role that informed traders play, which decreases the market’s illiquidity risk and lowers the risk premium of equity. Our analysis complements the returns predictability study by adding a new predictor on the one hand and informs market timing strategies on the other. © 2024 Elsevier B.V., All rights reserved.","Li, P.; Yang, L.",2025,10.1080/1540496x.2024.2379471,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200154249&doi=10.1080%2F1540496X.2024.2379471&partnerID=40&md5=45cabd8cde831077ff35ce5455b1167b,scopus,"This study develops an informed trading index for the Chinese A-share market using an ensemble neural network and high-frequency trading data. The index is found to be a significant negative predictor of future aggregate stock market returns, outperforming existing indicators and macroeconomic variables. An asset allocation strategy based on this index yields substantial economic gains for investors. The predictive power is attributed to informed traders' role in reducing market illiquidity risk and equity risk premium. The research contributes to return predictability studies and informs market timing strategies.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:58:45.136389
46aa043b290272aa,Inter-Factor Determinants of Return Reversal Effect with Dynamic Bayesian Network Analysis: Empirical Evidence from Pakistan,"Bayesian Networks are multivariate probabilistic factor graphs that are used to assess underlying factor relationships. From January 2005 to December 2018, the study examines how Dynamic Bayesian Networks can be utilized to estimate portfolio risk and return as well as determine inter-factor relationships among reversal profit-generating components in Pakistan's emerging market (PSX). The goal of this article is to uncover the factors that cause reversal profits in the Pakistani stock market. In visual form, Bayesian networks can generate causal and inferential probabilistic relationships. Investors might update their stock return values in the network simultaneously with fresh market information, resulting in a dynamic shift in portfolio risk distribution across the networks. The findings show that investments in low net profit margin, low investment, and high volatility-based designed portfolios yield the biggest dynamical reversal profits. The main triggering aspects related to generation reversal profits in the Pakistan market, in the long run, are net profit margin, market risk premium, investment, size, and volatility factor. Investors should invest in and build portfolios with small companies that have a low price-to-earnings ratio, small earnings per share, and minimal volatility, according to the most likely explanation.","Haque, Abdul; Rao, Marriam; Qamar, Muhammad Ali Jibran",2022,10.13106/jafeb.2022.vol9.no3.0203,,wos,"This study uses Dynamic Bayesian Networks to analyze inter-factor relationships and estimate portfolio risk and return in Pakistan's stock market from 2005 to 2018. It identifies factors contributing to reversal profits, suggesting that portfolios with low net profit margin, low investment, and high volatility yield the largest reversal profits. Key long-term triggering factors include net profit margin, market risk premium, investment, size, and volatility. The study recommends investing in small companies with low P/E ratios, small EPS, and minimal volatility.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:58:48.719779
b5ff09ccbbe072af,Interest rate fluctuations and the UK financial services industry,"The article explores the relationship between short-term interest rates and the equity returns of the UK financial services industry. Based on the arbitrage pricing theory, the present study seeks to answer the sensitivity and pricing questions. The former is tested with a linear two-index model attempting to identify any interest rate risk exposure of these stock returns. The latter, however, is examined using a nonlinear multivariate analysis based on the Seemingly Unrelated Regression Equations (SURE) model by imposing cross- and within-equation constraints on the estimated parameters. The econometric analysis unveils a significant negative interest rate effect and the existence of a risk premium incorporated in the expected returns of portfolios consisting of these stocks. © 2007 Elsevier B.V., All rights reserved.","Artikis, P.; Kalotychou, E.; Staikouras, S.K.",2007,10.1080/17446540601118319,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548570513&doi=10.1080%2F17446540601118319&partnerID=40&md5=519a8765fc7ddc63aca544b961ed56b8,scopus,"This study investigates the impact of short-term interest rate fluctuations on the equity returns of the UK financial services industry, utilizing arbitrage pricing theory. It employs a linear two-index model to assess interest rate risk exposure and a nonlinear multivariate SURE model to examine pricing questions, revealing a significant negative interest rate effect and a risk premium in expected returns.",True,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:59:01.006366
471849c4c2e8e8d5,Interest rate models on Lie groups,"This paper examines an alternative approach to interest rate modeling, in which the nonlinear and random behavior of interest rates is captured by a stochastic differential equation evolving on a curved state space. We consider as candidate state spaces the matrix Lie groups; these offer not only a rich geometric structure, butunlike general Riemannian manifoldsalso allow for diffusion processes to be constructed easily without invoking the machinery of stochastic calculus on manifolds. After formulating bilinear stochastic differential equations on general matrix Lie groups, we then consider interest rate models in which the short rate is defined as linear or quadratic functions of the state. Stochastic volatility is also augmented to these models in a way that respects the Riemannian manifold structure of symmetric positive-definite matrices. Methods for numerical integration, parameter identification, pricing, and other practical issues are addressed through examples.","Park, F. C.; Chun, C. M.; Han, C. W.; Webber, N.",2011,10.1080/14697680903468963,,wos,"This paper proposes an alternative approach to interest rate modeling using stochastic differential equations on matrix Lie groups, which offer a rich geometric structure. The models capture nonlinear and random interest rate behavior, with the short rate defined as linear or quadratic functions of the state. Stochastic volatility is incorporated while respecting the manifold structure of symmetric positive-definite matrices. Practical aspects like numerical integration, parameter identification, and pricing are discussed with examples.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:59:03.548752
7a23cbb917fcc583,"Interest rate next-day variation prediction based on hybrid feedforward neural network, particle swarm optimization, and multiresolution techniques","Multiresolution analysis techniques including continuous wavelet transform, empirical mode decomposition, and variational mode decomposition are tested in the context of interest rate next-day variation prediction. In particular, multiresolution analysis techniques are used to decompose interest rate actual variation and feedforward neural network for training and prediction. Particle swarm optimization technique is adopted to optimize its initial weights. For comparison purpose, autoregressive moving average model, random walk process and the naive model are used as main reference models. In order to show the feasibility of the presented hybrid models that combine multiresolution analysis techniques and feedforward neural network optimized by particle swarm optimization, we used a set of six illustrative interest rates; including Moody's seasoned Aaa corporate bond yield, Moody's seasoned Baa corporate bond yield, 3-Month, 6-Month and 1-Year treasury bills, and effective federal fund rate. The forecasting results show that all multiresolution-based prediction systems outperform the conventional reference models on the criteria of mean absolute error, mean absolute deviation, and root mean-squared error. Therefore, it is advantageous to adopt hybrid multiresolution techniques and soft computing models to forecast interest rate daily variations as they provide good forecasting performance. © 2021 Elsevier B.V., All rights reserved.","Lahmiri, S.",2016,10.1016/j.physa.2015.09.061,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945945078&doi=10.1016%2Fj.physa.2015.09.061&partnerID=40&md5=68bb4b64f208c7540f7135a221279046,scopus,"This study proposes a hybrid model combining multiresolution analysis techniques (continuous wavelet transform, empirical mode decomposition, variational mode decomposition) with a feedforward neural network optimized by particle swarm optimization for predicting next-day interest rate variations. The model was tested on six different interest rates and demonstrated superior performance compared to conventional models like ARMA, random walk, and naive models, based on MAE, MAD, and RMSE metrics.",True,True,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T09:59:30.047497
770dcddf8d700ba0,Interest rate term structure modeling using free-knot splines,"In this article a new methodology for estimating the term structure of interest rates is developed. Using polynomial splines, a reliable approximation to term structure may depend crucially upon intelligent selection of numbers and position of spline knots, which can be a combinatorially very complex task. A different approach based on heuristic optimization techniques called genetic algorithms is presented. The optimal spline function takes into account the goodness of fit of the spline function. The new methodology was applied to estimating the term structure using data on zero-coupon Euro market bonds. © 2006 by The University of Chicago. All rights reserved. © 2011 Elsevier B.V., All rights reserved.","Fernández-Rodríguez, F.",2006,10.1086/508009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847039555&doi=10.1086%2F508009&partnerID=40&md5=8d8b9389b44d5169d8cc6a2819340341,scopus,This article proposes a new method for estimating the term structure of interest rates using free-knot splines and genetic algorithms. It addresses the complexity of traditional spline knot selection by employing heuristic optimization to find an optimal spline function that fits the data well. The methodology was applied to Euro market zero-coupon bond data.,True,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T09:59:39.264508
e1ac05598dcf5552,Interest rates mapping,"The present study deals with the analysis and mapping of Swiss franc interest rates. Interest rates depend on time and maturity, defining term structure of the interest rate curves (IRC). In the present study IRC are considered in a two-dimensional feature space-time and maturity. Exploratory data analysis includes a variety of tools widely used in econophysics and geostatistics. Geostatistical models and machine learning algorithms (multilayer perceptron and Support Vector Machines) were applied to produce interest rate maps. IR maps can be used for the visualisation and pattern perception purposes, to develop and to explore economical hypotheses, to produce dynamic asset-liability simulations and for financial risk assessments. The feasibility of an application of interest rates mapping approach for the IRC forecasting is considered as well. © 2008 Elsevier Ltd. All rights reserved. © 2008 Elsevier B.V., All rights reserved.","Kanevski, M.; Maignan, M.; Pozdnoukhov, A.; Timonin, V.",2008,10.1016/j.physa.2008.02.069,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42649101653&doi=10.1016%2Fj.physa.2008.02.069&partnerID=40&md5=3d37fefbc596802a878a5487263c92be,scopus,"This study analyzes and maps Swiss franc interest rates, considering their dependence on time and maturity to define the term structure of interest rate curves (IRC). It applies geostatistical models and machine learning algorithms (multilayer perceptron and Support Vector Machines) to create interest rate maps. These maps can be used for visualization, hypothesis exploration, financial simulations, and risk assessment. The study also explores the feasibility of using this mapping approach for IRC forecasting.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:00:06.009760
e259919b32e771b4,International evidence on the predictability of return to securitized real estate assets: Econometric models versus neural networks,"The performance of various statistical models and commonly used financial indicators for forecasting securitised real estate returns are examined for five European countries: the UK, Belgium, the Netherlands, France and Italy. Within a VAR framework, it is demonstrated that the gilt-equity yield ratio is in most cases a better predictor of securitized returns than the term structure or the dividend yield. In particular, investors should consider in their real estate return models the predictability of the gilt-equity yield ratio in Belgium, the Netherlands and France, and the term structure of interest rates in France. Predictions obtained from the VAR and univariate time-series models are compared with the predictions of an artificial neural network model. It is found that, whilst no single model is universally superior across all series, accuracy measures and horizons considered, the neural network model is generally able to offer the most accurate predictions for 1-month horizons. For quarterly and half-yearly forecasts, the random walk with a drift is the most successful for the UK, Belgian and Dutch returns and the neural network for French and Italian returns. Although this study underscores market context and forecast horizon as parameters relevant to the choice of the forecast model, it strongly indicates that analysts should exploit the potential of neural networks and assess more fully their forecast performance against more traditional models. © 2004 Elsevier Science B.V., Amsterdam. All rights reserved.","Brooks, C.; Tsolacos, S.",2003,10.1080/0959991032000109517,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0141671708&doi=10.1080%2F0959991032000109517&partnerID=40&md5=551008a304241f4cd4b10fcc00d8a1bf,scopus,"This study examines the performance of econometric models and neural networks in forecasting securitized real estate returns across five European countries. It finds that the gilt-equity yield ratio is often a better predictor than the term structure or dividend yield. While no single model is universally superior, neural networks generally offer the most accurate predictions for 1-month horizons. The study suggests that market context and forecast horizon are important, and analysts should explore the potential of neural networks.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:00:08.104638
84ac3579499de299,Interval forecasting. An analysis based upon ARCH-quantile estimators,"In this paper we explore techniques for obtaining interval forecasts based on estimated time-series models for processes which may exhibit autoregressive conditional heteroskedasticity (ARCH). To deal with the available variety of possible interval forecasts, we propose a method for combining these forecasts based on quantile regression techniques. Our approach is practical rather than theoretical, with attention focused directly on obtaining interval forecasts for two U.S. time series: a measure of unemployment and a Treasury bill rate. We evaluate the performance of our procedures using a variety of diagnostics. We find interval estimates which perform reasonably well, judged by both in-sample and out-of-sample criteria. Our experience suggests that a certain amount of care is required in order to obtain useful forecasts. © 1989. © 2014 Elsevier B.V., All rights reserved.","Granger, C.W.J.; White, H.; Kamstra, M.",1989,10.1016/0304-4076(89)90031-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38249026075&doi=10.1016%2F0304-4076%2889%2990031-6&partnerID=40&md5=38e979d6b16ecea739e87e183cf50b40,scopus,"This paper proposes a practical method for combining interval forecasts from ARCH-quantile estimators for time-series models, focusing on applications to U.S. unemployment and Treasury bill rates. The performance is evaluated using in-sample and out-of-sample criteria, suggesting careful implementation is needed for useful forecasts.",True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:00:23.552949
57b422147c25c2e6,Investor attention and FX market volatility,"We study the relationship between investors' active attention, measured by a Google search volume index (SVI), and the dynamics of currency prices. Investor attention is correlated with the trading activities of large FX market participants. Investor attention comoves with contemporaneous FX market volatility and predicts subsequent FX market volatility, after controlling for macroeconomic fundamentals. In addition, investor attention is related to the currency risk premium. Our results suggest that investor attention is a priced source of risk in FX markets. © 2015 Elsevier B.V., All rights reserved.","Goddard, J.; Kita, A.; Wang, Q.",2015,10.1016/j.intfin.2015.05.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931270574&doi=10.1016%2Fj.intfin.2015.05.001&partnerID=40&md5=74e9e7e91322306d1e91a40836ecb27b,scopus,"This study investigates the link between investor attention, proxied by Google search volume, and foreign exchange (FX) market volatility. The findings indicate that investor attention correlates with trading by large FX participants, moves with current FX volatility, and can predict future volatility, even when accounting for macroeconomic factors. Furthermore, attention is associated with currency risk premiums, suggesting it's a priced risk factor in FX markets.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:00:26.958667
26003fee30aeb702,Investor attention and Google Search Volume Index: Evidence from an emerging market using quantile regression analysis,"This study investigates whether the investor attention measured by the Google Search Volume Index (GSVI) is effective in forecasting stock returns. The evolving literature on investor attention suggests that higher GSVI can predict higher returns for the first one or two weeks, but with a subsequent price reversal. We use a more recent dataset that covers S&P BSE 500 companies listed on the Indian stock exchange for 2012-2017 and employ the quantile regression approach because it alleviates the statistical problems arising from biased distribution data. The results suggest that a higher GSVI predicts positive and significant returns in the subsequent first and second weeks. Higher quantiles of GSVI experience higher excess returns. The panel cointegration test results support the findings regarding the cointegration of the GSVI and stock returns. Our empirical evidence shows that our model is robust when using a trading strategy based on the Fama-French four-factor model. Thus, the model with GSVI acts as a better predictor of both the direction and magnitude of the excess returns than the model without GSVI.","Swamy, Vighneswara; Dharani, M.; Takeda, Fumiko",2019,10.1016/j.ribaf.2019.04.010,,wos,"This study examines the predictive power of Google Search Volume Index (GSVI) for stock returns in the Indian emerging market (S&P BSE 500, 2012-2017). Using quantile regression, it finds that higher GSVI predicts positive and significant returns in the first two weeks, with higher quantiles of GSVI yielding higher excess returns. Panel cointegration tests support the GSVI-stock return relationship, and a trading strategy based on the Fama-French four-factor model confirms the model's robustness. The GSVI model is shown to be a better predictor of excess returns than models without it.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:00:28.984333
60a0da6f58c3e011,Investor flows and the 2008 boom/bust in oil prices,"This paper explores the impact of investor flows and financial market conditions on returns in crude oil futures markets. I argue that informational frictions and the associated speculative activity may induce prices to drift away from ""fundamental"" values, and may result in price booms and busts. Particular attention is given to the interplay between imperfect information about real economic activity, including supply, demand, and inventory accumulation, and speculative activity in oil markets. Furthermore, I present new evidence that there were economically and statistically significant effects of investor flows on futures prices, after controlling for returns in the United States and emerging-economy stock markets, a measure of the balance sheet flexibility of large financial institutions, open interest, the futures/spot basis, and lagged returns on oil futures. The largest impacts on futures prices were from intermediate-term growth rates of index positions and managed-money spread positions. Moreover, my findings suggest that these effects were through risk or informational channels distinct from changes in convenience yield. Finally, the evidence suggests that hedge fund trading in spread positions in futures impacted the shape of term structure of oil futures prices. © 2014 INFORMS. © 2014 Elsevier B.V., All rights reserved.","Singleton, K.J.",2014,10.1287/mnsc.2013.1756,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894362096&doi=10.1287%2Fmnsc.2013.1756&partnerID=40&md5=7afb35ff36a50061f0a050888e34e1ed,scopus,"This paper investigates how investor flows and financial market conditions influence crude oil futures prices, suggesting that informational frictions and speculation can lead to price booms and busts. It provides evidence that investor flows significantly impact futures prices, distinct from changes in convenience yield, and that hedge fund trading affects the term structure of oil futures prices.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:00:40.444953
2842d62ce44c72c3,Is BRCA Mutation Testing Cost Effective for Early Stage Breast Cancer Patients Compared to Routine Clinical Surveillance? The Case of an Upper Middle-Income Country in Asia,"Objective Previous studies showed that offering BRCA mutation testing to population subgroups at high risk of harbouring the mutation may be cost effective, yet no evidence is available for low- or middle-income countries (LMIC) and in Asia. We estimated the cost effectiveness of BRCA mutation testing in early-stage breast cancer patients with high pre-test probability of harbouring the mutation in Malaysia, an LMIC in Asia. Methods We developed a decision analytic model to estimate the lifetime costs and quality-adjusted life-years (QALYs) accrued through BRCA mutation testing or routine clinical surveillance (RCS) for a hypothetical cohort of 1000 early-stage breast cancer patients aged 40 years. In the model, patients would decide whether to accept testing and to undertake risk-reducing mastectomy, oophorectomy, tamoxifen, combinations or neither. We calculated the incremental cost-effectiveness ratio (ICER) from the health system perspective. A series of sensitivity analyses were performed. Results In the base case, testing generated 11.2 QALYs over the lifetime and cost US$4815 per patient whereas RCS generated 11.1 QALYs and cost US$4574 per patient. The ICER of US$2725/QALY was below the cost-effective thresholds. The ICER was sensitive to the discounting of cost, cost of BRCA mutation testing and utility of being risk-free, but the ICERs remained below the thresholds. Probabilistic sensitivity analysis showed that at a threshold of US$9500/QALY, 99.9% of simulations favoured BRCA mutation testing over RCS. Conclusions Offering BRCA mutation testing to early-stage breast cancer patients identified using a locally-validated risk-assessment tool may be cost effective compared to RCS in Malaysia.",,2018,10.1007/s40258-018-0384-8,,proquest,"This study evaluated the cost-effectiveness of BRCA mutation testing versus routine clinical surveillance for early-stage breast cancer patients in Malaysia, an upper middle-income Asian country. Using a decision analytic model, the study found that BRCA mutation testing was cost-effective, generating more quality-adjusted life-years (QALYs) at an incremental cost-effectiveness ratio (ICER) below the established thresholds. Sensitivity analyses confirmed the robustness of these findings.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:00:42.357241
d398e1bb9cf578bc,Iterative and Recursive Estimation in Structural Nonadaptive Models,"An inference method, called latent backfitting, is proposed. This method appears well suited for econometric models where the structural relationships of interest define the observed endogenous variables as a known function of unobserved state variables and unknown parameters. This nonlinear state-space specification paves the way for iterative or recursive EM-like strategies. In the E steps, the state variables are forecasted given the observations and a value of the parameters. In the M steps, these forecasts are used to deduce estimators of the unknown parameters from the statistical model of latent variables. The proposed iterative/recursive estimation is particularly useful for latent regression models and for dynamic equilibrium models involving latent state variables. Practical implementation issues are discussed through the example of term structure models of interest rates. © 2020 Elsevier B.V., All rights reserved.","Pastorello, S.; Patilea, V.; Renault, E.",2003,10.1198/073500103288619124,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0242318897&doi=10.1198%2F073500103288619124&partnerID=40&md5=89bd95f11924d290104a4169ffefdf21,scopus,"This paper introduces latent backfitting, an inference method for econometric models where structural relationships define observed endogenous variables as a function of unobserved state variables and parameters. It uses iterative or recursive EM-like strategies for forecasting state variables and estimating parameters. The method is applicable to latent regression and dynamic equilibrium models, with an example of term structure models of interest rates.",True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:01:02.173274
c35e1a92cfdc2c3d,Jump and variance risk premia in the S&P 500,"We analyze the risk premia embedded in the S&P 500 spot index and option markets. We use a long time series of spot prices and a large panel of option prices to jointly estimate the diffusive stock risk premium, the price jump risk premium, the diffusive variance risk premium and the variance jump risk premium. The risk premia are statistically and economically significant and move over time. Investigating the economic drivers of the risk premia, we are able to explain up to 63% of these variations. (C) 2016 Elsevier B.V. All rights reserved.","Neumann, Maximilian; Prokopczuk, Marcel; Simen, Chardin Wese",2016,10.1016/j.jbankfin.2016.03.013,,wos,"This study analyzes risk premia in the S&P 500 index and its options market. It jointly estimates diffusive and jump risk premia for both stock and variance, finding them to be significant and time-varying. The research also identifies economic drivers that explain a substantial portion of these variations.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:01:04.310340
84a95360c6c72c18,Kernel-Based Aggregating Learning System for Online Portfolio Optimization,"Recently, various machine learning techniques have been applied to solve online portfolio optimization (OLPO) problems. These approaches typically explore aggressive strategies to gain excess returns due to the existence of irrational phenomena in financial markets. However, existing aggressive OLPO strategies rarely consider the downside risk and lack effective trend representation, which leads to poor prediction performance and large investment losses in certain market environments. Besides, prediction with a single model is often unstable and sensitive to the noises and outliers, and the subsequent selection of optimal parameters also become obstacles to accurate estimation. To overcome these drawbacks, this paper proposes a novel kernel-based aggregating learning (KAL) system for OLPO. It includes a two-step price prediction scheme to improve the accuracy and robustness of the estimation. Specifically, a component price estimator is built by exploiting additional indicator information and the nonstationary nature of financial time series, and then an aggregating learning method is presented to combine multiple component estimators following different principles. Next, this paper conducts an enhanced tracking system by introducing a kernel-based increasing factor to maximize the future wealth of next period. At last, an online learning algorithm is designed to solve the system objective, which is suitable for large-scale and time-limited situations. Experimental results on several benchmark datasets from diverse real markets show that KAL outperforms other state-of-the-art systems in cumulative wealth and some risk-adjusted metrics. Meanwhile, it can withstand certain transaction costs.","Wang, Xin; Sun, Tao; Liu, Zhi",2020,10.1155/2020/6595329,,wos,"This paper introduces a novel kernel-based aggregating learning (KAL) system for online portfolio optimization (OLPO) to address limitations of existing aggressive strategies, such as neglecting downside risk and poor trend representation. The KAL system features a two-step price prediction scheme using component estimators and an aggregating learning method, enhanced by a kernel-based increasing factor for wealth maximization. An online learning algorithm is designed for efficiency. Experiments show KAL outperforms state-of-the-art systems in cumulative wealth and risk-adjusted metrics, and can handle transaction costs.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:01:07.317219
e63ce5fd8a54f209,LAND OF ADDICTS? AN EMPIRICAL INVESTIGATION OF HABIT-BASED ASSET PRICING MODELS,"This paper Studies the ability of a general class of habit-based asset pricing models to match the conditional moment restrictions implied by asset pricing theory. We treat the functional form of the habit as unknown, and estimate it along with the rest of the model's finite dimensional parameters. Using quarterly data on consumption growth, assets returns and instruments, our empirical results indicate that the estimated habit function is nonlinear, that habit formation is better described as internal rather than external, and the estimated time-preference parameter and the power utility parameter are sensible. In addition, the estimated habit function generates a positive stochastic discount factor (SDF) proxy and performs well in explaining cross-sectional stock return data. We find that an internal habit SDF proxy can explain a cross-section of size and book-market sorted portfolio equity returns better than (i) the Fama and French (1993) three-factor model, (ii) the Lettau and LUdvigson (2001b) scaled consumption CAPM model, (iii) an external habit SDF proxy, (iv) the classic CAPM, and (v) the classic consumption CAPM. Copyright (C) 2009 John Wiley & Sons, Ltd.","Chen, Xiaohong; Ludvigson, Sydney C.",2009,10.1002/jae.1091,,wos,"This empirical study investigates habit-based asset pricing models by estimating an unknown habit function using quarterly data. The findings suggest a nonlinear, internal habit formation, with sensible parameter estimates. The model effectively explains cross-sectional stock returns, outperforming several benchmark models including the Fama-French three-factor model and scaled consumption CAPM.",True,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:01:09.072282
c7ba5d09c7bc1893,LSTM Framework Design and Volatility Research on Intelligent Forecasting Model for Solving the Parallel Dislocation Problem,"The yield of treasury bonds is the benchmark interest rate in the financial market which is worth predicting and judging. Based on the Long Short-Term Memory (LSTM) neural network model in deep learning, combined with the vector autoregression method (VAR), this paper creatively constructs the VAR-LSTM framework and uses the predicted values of macroeconomic variables and lagged value of the time sequence as input factors to solve the problem of “parallel dislocation” of the fitting results of the traditional LSTM model which significantly improves the prediction accuracy. In order to meet the requirements of active quantitative investment for high precision prediction of stock market index, adaptive noise complete ensemble empirical mode decomposition (EMD) is introduced into the modeling of stock market index prediction. Combined with the efficient modeling ability of long-term and short-term memory network for medium- and long-term dependence of complex series, using the idea of “decomposition-reorganization-prediction-integration”, an integrated prediction method of stock market index CEEMDAN-LSTM is proposed. CEEMDAN is used to decompose and reconstruct the index to obtain its high and low frequency components and trend items. The LSTM prediction models of each component are constructed respectively and the IMF reorganization mode of high frequency subseries is optimized. Then the overall predicted value of the index is obtained by adding and integrating the predicted values of each component. Taking five representative stock market indexes as test data, the prediction results of CEEMDAN-LSTM and mainstream financial time series machine learning modeling methods are compared systematically. The results show that for treasury bond yield series, the prediction accuracy of ARIMA model is higher than that of general LSTM method, while VAR-LSTM model is better than ARIMA model. The prediction error in the training set and the test set is reduced by about 55% and 50% respectively, and the prediction accuracy of the change direction is improved by about 5% and 8% respectively, which has higher application value. The prediction performance of CEEMDAN-LSTM is consistently better than that of existing modeling methods, and has less prediction error and lower lag.",,2021,10.1088/1742-6596/1982/1/012028,,proquest,"This paper proposes two novel forecasting models, VAR-LSTM and CEEMDAN-LSTM, to improve the prediction accuracy of financial time series, specifically treasury bond yields and stock market indexes. The VAR-LSTM model combines Vector Autoregression with Long Short-Term Memory (LSTM) networks to address the ""parallel dislocation"" issue in traditional LSTM models. The CEEMDAN-LSTM model utilizes adaptive noise complete ensemble empirical mode decomposition (CEEMDAN) to decompose and reconstruct stock market indexes, followed by LSTM prediction on individual components. Empirical results demonstrate that both VAR-LSTM and CEEMDAN-LSTM outperform existing methods like ARIMA and general LSTM in terms of prediction accuracy and error reduction.",True,True,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T10:01:51.351355
0fda4c9a6ad9595b,LSTM–GARCH Hybrid Model for the Prediction of Volatility in Cryptocurrency Portfolios,"In the present work, the volatility of the leading cryptocurrencies is predicted through generalised autoregressive conditional heteroskedasticity (GARCH) models, multilayer perceptron (MLP), long short-term memory (LSTM), and hybrid models of the type LSTM and GARCH, where parameters of the GARCH family are included as features of LSTM models. The study period covered the scenario of the World Health Organization pandemic declaration around March 2020 at hourly frequency. We have found that the different variants of deep neural network models outperform those of the GARCH family in the sense of the hetorerocedastic error, and absolute and squared error (HSE). Under the sharpe ratio, the volatility forecasting of a uniform portfolio at long horizons systematically outperforms the stablecoin Tether, which is considered here as the risk-free asset. Also, including transaction volume helps reduce the value at risk or loss probability for the uniform portfolio. Moreover, in a minimum variance portfolio, it is observed that before the pandemic declaration, a large proportion of the capital was allocated to bitcoin (BTC). In contrast, after March 2020, the portfolio is more diversified with short positions for BTC. Moreover, the MLP models give the best predictive results, although not statistically different in accuracy compared to the LSTM and LSTM–GARCH versions under the Diebold–Mariano test. In sum, MLP models outperform most stylised financial models and are less computationally expensive than more complex neural networks. Therefore, simple learning models are suggested in highly non-linear time series volatility forecasts as it is the cryptocurrency market.",,2024,10.1007/s10614-023-10373-8,,proquest,"This study compares LSTM, GARCH, MLP, and hybrid LSTM-GARCH models for predicting cryptocurrency portfolio volatility during the COVID-19 pandemic. Deep learning models, particularly MLP, generally outperformed GARCH models. The study also found that volatility forecasting for a uniform portfolio exceeded the performance of Tether as a risk-free asset, and including transaction volume reduced risk. Portfolio allocation shifted significantly after the pandemic declaration. MLP models are recommended for their predictive accuracy and computational efficiency in volatile, non-linear markets.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:01:53.826688
7f77847f3cb759f0,"Labor Links, Comovement, and Predictable Returns","Using firms' online job postings, we identify economically related peer firms in the labor market. Firms' labor peers are vastly different from their industry peers, where the overlap is about 20%. Returns of labor-linked firms strongly comove, suggesting common responses to labor market shocks on average. However, industry shocks can affect firms outside the industry through the labor network, leading to substitution effects between labor peers. Lastly, we show that investors do not promptly incorporate news about labor-linked firms, leading to predictable subsequent returns. A long-short strategy exploiting this delay generates an average annualized excess return of 9%. © 2025 Elsevier B.V., All rights reserved.","Liu, Y.; Wu, X.",2025,10.1017/s0022109025101610,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006901938&doi=10.1017%2FS0022109025101610&partnerID=40&md5=bd128e8bf860437c6f596da829c7b48e,scopus,"This study utilizes firms' online job postings to identify labor-related peer firms, which differ significantly from industry peers. It finds that returns of labor-linked firms comove strongly, indicating shared responses to labor market shocks. The research also reveals that industry shocks can propagate through the labor network, causing substitution effects among labor peers. Furthermore, the study demonstrates that investors are slow to react to news about labor-linked firms, resulting in predictable future returns. A trading strategy based on this delay yields an average annualized excess return of 9%.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:02:02.818474
3b8bb9a74be525f1,Leveraging latent representations for milk yield prediction and interpolation using deep learning,"In this study, we propose a lactation model that estimates the daily milk yield by using autoencoders to generate a latent representation of all milk yields observed during the entire lactation cycle, irrespective of the length of the time interval between the different measurements. More specifically, we propose a sequential autoencoder (SAE) to process the sequential data, extract and decode the low-dimensional representations and generate the milk yield sequences. The SAE is compared with a more traditional multilayer perceptron model (MLP) which uses herd and parity information and lagged milk yields as input. Results show that incorporating the recorded daily milk yields, lactation number, herd statistics as well as reproduction and health events the cow encountered during the lactation cycle results in the most qualitative latent representations. Moreover, by leveraging these low-dimensional encodings, the SAE reconstructed the entire milk yield curve with a higher accuracy than the MLP. Hence, we present a framework that is able to infer missing milk yields along the entire lactation curve which facilitates selection and culling decisions as well as the estimation of future earnings and costs. Furthermore, the model allows farmers to enhance their animal monitoring systems as it incorporates the sequence of health and reproduction events to forecast the cow's future productivity.",,2020,10.1016/j.compag.2020.105600,,proquest,"This study proposes a lactation model using sequential autoencoders (SAE) to predict daily milk yield by creating latent representations of lactation cycles. The SAE, which incorporates milk yields, lactation number, herd statistics, and health/reproduction events, outperforms a traditional multilayer perceptron (MLP) in reconstructing milk yield curves and inferring missing yields. This framework aids in selection, culling, financial estimations, and enhances animal monitoring by forecasting productivity.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:02:24.029559
b43fdd1652b6f7bb,Leveraging multi-time-span sequences and feature correlations for improved stock trend prediction,"Accurate stock trend prediction is critical for informed investment decisions and the stability of financial markets. However, existing methodologies often overlook fine-grained stock price volatility and fail to incorporate a comprehensive spectrum of technical indicators, inadequately capturing the complex interrelationships fundamental to technical analysis. This paper proposes MSFCE, a novel framework for stock market trend prediction that enhances feature correlations across multi-time-span sequences. Specifically, MSFCE designs a multi-scale feature encoder to capture both intraday and daily features, which are processed through a Transformer-based dimensionally adaptive encoder. Furthermore, the framework leverages higher-order interactions among technical indicators via a graph attention network, dynamically modeling their interdependencies to improve prediction robustness in dynamic markets. Extensive experiments on the SSE50 and CSI300 datasets demonstrate that MSFCE significantly outperforms existing state-of-the-art methods, consistently exhibiting superior performance across multiple test periods and market conditions. Its strong prediction accuracy and risk management suggest practical applicability in trading strategies, yielding significant excess returns in empirical backtests. © 2024 Elsevier B.V., All rights reserved.","Li, Y.; Zhuang, M.; Wang, J.; Zhou, J.",2025,10.1016/j.neucom.2024.129218,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213080195&doi=10.1016%2Fj.neucom.2024.129218&partnerID=40&md5=34da531abdfb33b8628e81aa8a89243b,scopus,"This paper introduces MSFCE, a novel framework for stock market trend prediction that improves feature correlations across multi-time-span sequences. It uses a multi-scale feature encoder and a Transformer-based encoder to capture intraday and daily features, and a graph attention network to model interdependencies among technical indicators. Experiments on SSE50 and CSI300 datasets show MSFCE outperforms existing methods, demonstrating practical applicability in trading strategies.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:02:27.204130
97d99bf76069f7fb,Linear and non-linear filtering in mathematical finance: a review,This paper presents a review of time series filtering and its applications in mathematical finance. A summary of results of recent empirical studies with market data are presented for yield curve modelling and stochastic volatility modelling. The paper also outlines different approaches to filtering of non-linear time series.,P. Date; K. Ponomareva,2011,10.1093/imaman/dpq008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8133776,ieeexplore,"This review paper discusses time series filtering and its applications in mathematical finance, covering yield curve and stochastic volatility modeling with empirical results. It also explores methods for filtering non-linear time series.",True,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:02:41.429768
5ab29547914291e7,"Liquidity shocks, business cycles and asset prices","In the aftermath of the Great Recession, macro models that feature financing constraints have attracted increasing attention. Among these, Kiyotaki et al. (2012) is a prominent example. In this paper, we investigate whether the liquidity shocks and financial frictions proposed by Kiyotaki et al. (2012) can improve the asset pricing predictions of the frictionless RBC model. We study the quantitative business cycle and asset pricing properties in an economy in which agents feature recursive preferences, are subject to a liquidity constraint, and suffer liquidity shocks. We find that the model predicts highly nonlinear time variation and levels of risk premia, which are driven by endogenous fluctuations in equity prices. However, the model fails to account for a basic fact: Periods of scarce liquidity are associated with high asset prices and low expected returns. © 2020 Elsevier B.V., All rights reserved.","Bigio, S.; Schneider, A.",2017,10.1016/j.euroecorev.2017.05.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021406771&doi=10.1016%2Fj.euroecorev.2017.05.004&partnerID=40&md5=7f22dc6c370cebd17926a00725e416ae,scopus,"This paper investigates whether liquidity shocks and financial frictions, as proposed by Kiyotaki et al. (2012), can enhance the asset pricing predictions of the frictionless RBC model. The study incorporates recursive preferences, liquidity constraints, and liquidity shocks. While the model generates significant time variation in risk premia driven by equity prices, it fails to explain the observed phenomenon where periods of scarce liquidity coincide with high asset prices and low expected returns.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:02:45.247144
0082a75f3674363a,Local currency bond risk premia: A panel evidence on emerging markets,"This paper investigates the sources of variation in emerging market (EM) local currency bond risk premium. We find that macroeconomic and financial variables contain valuable information in explaining local currency bond excess returns. Additionally, we extend our analysis to investigate how the influence of different factors change depending on the level of global risk appetite. Although macro fundamentals have an important role in explaining the risk premiums during tranquil times, investors pay less attention to changes in inflation forecast in times of high risk aversion. Positive credit rating changes decrease the bond risk premium in both regimes with a different magnitude. Also, the influence of exchange rate volatility is more pronounced during the time of market stress. © 2019 Elsevier B.V., All rights reserved.","Cepni, O.; Güney, I.E.",2019,10.1016/j.ememar.2019.01.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060970321&doi=10.1016%2Fj.ememar.2019.01.002&partnerID=40&md5=13091778147b29cc4c0611f40f380d9f,scopus,"This paper examines the factors influencing emerging market local currency bond risk premiums, utilizing macroeconomic and financial variables. It finds that while macro fundamentals are important during stable periods, inflation forecast changes are less influential during high risk aversion. Credit rating changes consistently reduce risk premiums, and exchange rate volatility has a greater impact during market stress.",False,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:02:52.514886
29399bb2b4b0fe0c,Machine learning algorithms applied to the estimation of liquidity: the 10-year United States treasury bond,"PurposeHaving defined liquidity, the aim is to assess the predictive capacity of its representative variables, so that economic fluctuations may be better understood.Design/methodology/approachConceptual variables that are representative of liquidity will be used to formulate the predictions. The results of various machine learning models will be compared, leading to some reflections on the predictive value of the liquidity variables, with a view to defining their selection.FindingsThe predictive capacity of the model was also found to vary depending on the source of the liquidity, in so far as the data on liquidity within the private sector contributed more than the data on public sector liquidity to the prediction of economic fluctuations. International liquidity was seen as a more diffuse concept, and the standardization of its definition could be the focus of future studies. A benchmarking process was also performed when applying the state-of-the-art machine learning models.Originality/valueBetter understanding of these variables might help us toward a deeper understanding of the operation of financial markets. Liquidity, one of the key financial market variables, is neither well-defined nor standardized in the existing literature, which calls for further study. Hence, the novelty of an applied study employing modern data science techniques can provide a fresh perspective on financial markets.",,2024,10.1108/ejmbe-06-2022-0176,,proquest,This study assesses the predictive capacity of liquidity-representative variables using various machine learning models applied to the 10-year United States Treasury bond. It found that private sector liquidity data was more predictive of economic fluctuations than public sector liquidity data. The research highlights the need for better definition and standardization of liquidity in financial markets and offers a fresh perspective using modern data science techniques.,True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:03:05.792465
fa3ff66abeee79a5,Machine learning portfolio allocation,"We find economically and statistically significant gains when using machine learning for portfolio allocation between the market index and risk-free asset. Optimal portfolio rules for time-varying expected returns and volatility are implemented with two Random Forest models. One model is employed in forecasting monthly excess returns with macroeconomic factors including payout yields. The second is used to estimate the prevailing volatility. Reward-risk timing with machine learning provides substantial improvements over the buy-and-hold in utility, risk-adjusted returns, and maximum drawdowns. This paper presents a unifying framework for machine learning applied to both return- and volatility-timing. © 2021 Elsevier B.V., All rights reserved.","Pinelis, M.; Ruppert, D.",2022,10.1016/j.jfds.2021.12.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122398619&doi=10.1016%2Fj.jfds.2021.12.001&partnerID=40&md5=da9f23a6b807a580ea95847d19e96657,scopus,"This paper explores the use of machine learning, specifically Random Forest models, for portfolio allocation between a market index and a risk-free asset. It demonstrates significant improvements over traditional buy-and-hold strategies by forecasting time-varying expected returns and volatility using macroeconomic factors. The study presents a framework for applying machine learning to both return and volatility timing.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:03:08.176672
aa8f3a5c48986db6,Macroeconomic factors and emerging market equity returns: A Bayesian model selection approach,"Macroeconomics figures prominently in analyses of emerging markets, both as an asset class and for allocations within emerging markets. However, the literature on the drivers of emerging markets equity returns generally pays little attention to macroeconomic factors. This paper investigates the predictive power of several candidate macroeconomic factors for emerging market equity returns using the Bayesian model selection approach developed in Cremers [Cremers, K.J.M., 2002. Stock return predictability: a Bayesian model selection perspective. The Review of Financial Studies 15, 1223-1249]. The results provide strong evidence against all of the macro factors considered with the exception of exchange rate changes and, consistent with the existing literature, provide strong support for several financial factors, but not beta, as significant predictors of excess returns. © 2004 Published by Elsevier B.V. © 2018 Elsevier B.V., All rights reserved.","Hooker, M.A.",2004,10.1016/j.ememar.2004.09.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-10644293253&doi=10.1016%2Fj.ememar.2004.09.001&partnerID=40&md5=a4db5f2ad33213b6bf3c83efe1967361,scopus,"This paper examines the predictive power of macroeconomic factors for emerging market equity returns using a Bayesian model selection approach. It finds that exchange rate changes and certain financial factors are significant predictors, while most other macro factors are not.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:03:10.321950
b92002fe79b6fc1b,Making cost-benefit analysis a practical tool for evaluation,"A cost-benefit evaluation requires precise data on program out-comes. However, such data are unavailable when the analysis is prospective, and expensive and time-consuming to collect when the analysis is retrospective. This problem of uncertain data is partly solved by the revised version of the Treasury Board Benefit-Cost Analysis Guide (Watson & Mallory, 1997), which allows probabilistic estimates of program results to be used in the analysis. There are not yet many examples of this technique in practice. One is Transport Canada's evaluation of alternative requirements for small commercial vessels to carry emergency signaling equipment. This article describes that evaluation and assesses how well the methodology worked. Copyright © 2006 Canadian Evaluation Society. © 2024 Elsevier B.V., All rights reserved.","Watson, K.",2006,10.3138/cjpe.021.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250161650&doi=10.3138%2Fcjpe.021.003&partnerID=40&md5=0a455db4987c1230ae4a280f1d2da0f9,scopus,"This article discusses the challenges of data uncertainty in cost-benefit analysis, particularly for prospective evaluations. It introduces a revised methodology from the Treasury Board Benefit-Cost Analysis Guide that allows for probabilistic estimates of program results. The article then presents a case study of Transport Canada's evaluation of emergency signaling equipment requirements for small commercial vessels to assess the practical application of this methodology.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:03:13.083511
cda1c91e62a7ddb3,Market Returns and a Tale of Two Types of Attention,"We provide novel evidence that aggregate investor attention to stocks predicts marketwide returns, but with a striking difference across investor clienteles. Daily aggregate retail attention (ARA) negatively predicts one-week-ahead market returns, is associated with aggregate retail order imbalance and flows to equity mutual funds, and exhibits a stronger predictability during periods of high marketwide uncertainty, poor liquidity, or more costly short selling. In contrast, aggregate institutional attention (AIA), when observed before major news announcements, positively predict future marketwide returns. In cross-sectional analysis, we show that the predictability is stronger for ARA among illiquid stocks and for AIA among high-beta stocks. The predictability results are robust out-of-sample and correspond to meaningful expected utility gains even for diversified investors. The findings are consistent with the idea that attention-driven retail buying can generate an aggregate price pressure on the stock market, whereas institutional attention precedes the resolution of marketwide uncertainty and the accrual of risk premiums.","Da, Zhi; Hua, Tian; Hung, Tim Chih-Ching; Peng, Lin",2025,10.1287/mnsc.2023.01294,,wos,"This study investigates how aggregate investor attention, categorized into retail (ARA) and institutional (AIA), influences market returns. ARA negatively predicts near-term returns and is linked to retail trading behavior, especially during uncertain or illiquid market conditions. AIA, particularly around news events, positively predicts returns. The findings suggest retail attention can cause price pressure, while institutional attention anticipates market uncertainty resolution and risk premium accrual.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:03:14.670717
00a73739f92c0789,Measuring Granger Causality in Quantiles,"We consider measures of Granger causality in quantiles, which detect and quantify both linear and nonlinear causal effects between random variables. The measures are based on nonparametric quantile regressions and defined as logarithmic functions of restricted and unrestricted expectations of quantile check loss functions. They can consistently be estimated by replacing the unknown expectations of check loss functions by their nonparametric kernel estimates. We derive a Bahadur-type representation for the nonparametric estimator of the measures. We establish the asymptotic distribution of this estimator, which can be used to build tests for the statistical significance of the measures. Thereafter, we show the validity of a smoothed local bootstrap that can be used in finite-sample settings to perform statistical tests. A Monte Carlo simulation study reveals that the bootstrap-based test has a good finite-sample size and power properties for a variety of data-generating processes and different sample sizes. Finally, we provide an empirical application to illustrate the usefulness of measuring Granger causality in quantiles. We quantify the degree of predictability of the quantiles of equity risk premium using the variance risk premium, unemployment rate, inflation, and the effective federal funds rate. The empirical results show that the variance risk premium and effective federal funds rate have a strong predictive power for predicting the risk premium when compared to that of the predictive power of the other two macro variables. In particular, the variance risk premium is able to predict the center, lower, and upper quantiles of the distribution of the risk premium; however, the effective federal funds rate predicts only the lower and upper quantiles. Nevertheless, unemployment and inflation rates have no effect on the risk premium. © 2021 Elsevier B.V., All rights reserved.","Song, X.; Taamouti, A.",2021,10.1080/07350015.2020.1739531,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083679836&doi=10.1080%2F07350015.2020.1739531&partnerID=40&md5=e9977f79be5e9c56482a636bb3b4d94d,scopus,"This paper introduces measures of Granger causality in quantiles, utilizing nonparametric quantile regressions to detect and quantify linear and nonlinear causal effects. The measures are estimated using nonparametric kernel estimates and their asymptotic distribution is derived for statistical testing. A smoothed local bootstrap is validated for finite-sample testing, showing good size and power in simulations. An empirical application demonstrates the usefulness of these measures by quantifying the predictability of equity risk premium quantiles using variance risk premium, unemployment rate, inflation, and the effective federal funds rate. The variance risk premium predicts all quantiles, while the federal funds rate predicts lower and upper quantiles, with unemployment and inflation having no significant effect.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:03:37.756746
f17ea1ec214a9104,Measuring Nonlinear Granger Causality in Mean,"We propose model-free measures for Granger causality in mean between random variables. Unlike the existing measures, ours are able to detect and quantify nonlinear causal effects. The new measures are based on nonparametric regressions and defined as logarithmic functions of restricted and unrestricted mean square forecast errors. They are easily and consistently estimated by replacing the unknown mean square forecast errors by their nonparametric kernel estimates. We derive the asymptotic normality of nonparametric estimator of causality measures, which we use to build tests for their statistical significance. We establish the validity of smoothed local bootstrap that one can use in finite sample settings to perform statistical tests. Monte Carlo simulations reveal that the proposed test has good finite sample size and power properties for a variety of data-generating processes and different sample sizes. Finally, the empirical importance of measuring nonlinear causality in mean is also illustrated. We quantify the degree of nonlinear predictability of equity risk premium using variance risk premium. Our empirical results show that the variance risk premium is a very good predictor of risk premium at horizons less than 6 months. We also find that there is a high degree of predictability at the 1-month horizon, that can be attributed to a nonlinear causal effect. Supplementary materials for this article are available online. © 2018 Elsevier B.V., All rights reserved.","Song, X.; Taamouti, A.",2018,10.1080/07350015.2016.1166118,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018176704&doi=10.1080%2F07350015.2016.1166118&partnerID=40&md5=19c7fae0c027a8832819bbcf997de9c1,scopus,"This paper introduces novel model-free measures for Granger causality in mean, capable of detecting and quantifying nonlinear causal effects. These measures are based on nonparametric regressions and estimated using kernel methods. The study derives the asymptotic normality of the estimators, enabling statistical significance tests, and validates a smoothed local bootstrap for finite sample settings. Monte Carlo simulations demonstrate good size and power properties. The authors illustrate the empirical importance by quantifying nonlinear predictability of equity risk premium using variance risk premium, finding significant nonlinear causality at short horizons.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:44:40.828578
3bf14016cea7e254,Measuring sovereign contagion in Europe,"This paper analyzes sovereign risk shift-contagion, i.e. positive and significant changes in the propagation mechanisms, using bond yield spreads for the major eurozone countries. By emphasizing the use of two econometric approaches based on quantile regressions (standard quantile regression and Bayesian quantile regression with heteroskedasticity) we find that the propagation of shocks in euro's bond yield spreads shows almost no presence of shift-contagion in the sample periods considered (2003-2006, Nov. 2008-Nov. 2011, Dec. 2011-Apr. 2013). Shock transmission is no different on days with big spread changes and small changes. This is the case even though a significant number of the countries in our sample have been extremely affected by their sovereign debt and fiscal situations. The risk spillover among these countries is not affected by the size or sign of the shock, implying that so far contagion has remained subdued. However, the US crisis does generate a change in the intensity of the propagation of shocks in the eurozone between the 2003-2006 pre-crisis period and the Nov. 2008-Nov. 2011 post-Lehman one, but the coefficients actually go down, not up! All the increases in correlation we have witnessed over the last years come from larger shocks and the heteroskedasticity in the data, not from similar shocks propagated with higher intensity across Europe. These surprising, but robust, results emerge because this is the first paper, to our knowledge, in which a Bayesian quantile regression approach allowing for heteroskedasticity is used to measure contagion. This methodology is particularly well-suited to deal with nonlinear and unstable transmission mechanisms especially when asymmetric responses to sign and size are suspected. (C) 2017 Elsevier B.V. All rights reserved.","Caporin, Massimiliano; Pelizzon, Loriana; Ravazzolo, Francesco; Rigobon, Roberto",2018,10.1016/j.jfs.2017.12.004,,wos,"This paper investigates sovereign contagion in Europe using bond yield spreads for major eurozone countries. Employing quantile regression techniques, it finds limited evidence of significant shift-contagion, meaning the propagation mechanisms of shocks did not substantially change during the analyzed periods. While the US crisis influenced shock propagation intensity between pre- and post-Lehman periods, the effect was a decrease in coefficient values, not an increase. The study attributes observed increases in correlation to larger shocks and data heteroskedasticity rather than heightened shock intensity. The authors highlight the novelty of their Bayesian quantile regression approach, which is adept at handling nonlinear and unstable transmission mechanisms.",True,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:44:47.939427
2ca7dfdb8bc7cff3,Measuring the Macroeconomic Impact of Monetary Policy at the Zero Lower Bound,"This paper employs an approximation that makes a nonlinear term structure model extremely tractable for analysis of an economy operating near the zero lower bound for interest rates. We show that such a model offers an excellent description of the data compared to the benchmark model and can be used to summarize the macroeconomic effects of unconventional monetary policy. Our estimates imply that the efforts by the Federal Reserve to stimulate the economy since July 2009 succeeded in making the unemployment rate in December 2013 1% lower, which is 0.13% more compared to the historical behavior of the Fed. © 2016 Elsevier B.V., All rights reserved.","Wu, J.C.; Xia, F.D.",2016,10.1111/jmcb.12300,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962845380&doi=10.1111%2Fjmcb.12300&partnerID=40&md5=a52aebcac9facacc69f11196ac83a70d,scopus,"This paper uses a tractable nonlinear term structure model to analyze the macroeconomic impact of monetary policy at the zero lower bound. The model provides a good fit to the data and suggests that Federal Reserve stimulus since July 2009 lowered unemployment by 1% by December 2013, exceeding historical Fed behavior by 0.13%.",True,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:44:58.177742
8991d3ec333a8394,Measuring the risk of a non-linear portfolio with fat-tailed risk factors through a probability conserving transformation,"This paper presents a new heuristic for fast approximation of VaR (Value-at-Risk) and CVaR (conditional Value-at-Risk) for financial portfolios, where the net worth of a portfolio is a non-linear function of possibly non-Gaussian risk factors. The proposed method is based on mapping non-normal marginal distributions into normal distributions via a probability conserving transformation and then using a quadratic, i.e. Delta–Gamma, approximation for the portfolio value. The method is very general and can deal with a wide range of marginal distributions of risk factors, including non-parametric distributions. Its computational load is comparable with the Delta–Gamma–Normal method based on Fourier inversion. However, unlike the Delta–Gamma–Normal method, the proposed heuristic preserves the tail behaviour of the individual risk factors, which may be seen as a significant advantage. We demonstrate the utility of the new method with comprehensive numerical experiments on simulated as well as real financial data.",P. Date; R. Bustreo,2016,10.1093/imaman/dpu015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8146531,ieeexplore,"This paper introduces a novel heuristic for approximating Value-at-Risk (VaR) and conditional Value-at-Risk (CVaR) in financial portfolios with non-linear risk factor dependencies and non-Gaussian distributions. It employs a probability-conserving transformation to map non-normal distributions to normal ones, followed by a Delta-Gamma approximation. This method is versatile, handling various marginal distributions, including non-parametric ones, and is computationally efficient, comparable to the Delta-Gamma-Normal method. A key advantage is its preservation of tail behavior in risk factors, demonstrated through experiments with simulated and real financial data.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:45:03.705059
e16d12b73176b407,Metals: resources or financial assets? A multivariate cross-sectional analysis,"Metals are very important resources for industrial production, but recently they have attracted more and more attention from investors. While certainly industrial producers, consumers, and financial investors do have some influence on metal price development, the role of relevant price factors is not yet quite clear. Therefore, in this paper, we examine the explanatory power of various fundamental factors and characteristics known from financial markets, specifically on the expected returns in a unique data sample of 30 metals. We apply-to our knowledge for the first time in this context-the widely accepted method of characteristic-sorted portfolios, extended by the very recent method of two-way portfolio sorts as an alternative to classical multivariate regressions. This mostly nonparametric approach, combined with portfolio aggregation, provides very robust results. Our major finding is that the financial characteristics value and momentum have a very high predictive power for monthly returns of metal portfolios. Metal-specific fundamental factors like stocks, secondary production, apparent consumption, country concentration, mine production, or reserves perform depending on the interpretation moderately well or rather poorly, regarding some economically interpretable transformations and when using multivariate two-way sorts. Hence, from the perspective of expected returns, metals are predominantly assets, while fundamental metal-specific factors still play a non-negligible role. Thus, to a much lesser extent, metals can still be regarded as resources. Overall, the combination of financial characteristics and metal-specific fundamental factors yields the best results. With these robust results, we hope to contribute to a better understanding of metal prices and their underlying factors.","Lutzenberger, Fabian; Gleich, Benedikt; Mayer, Herbert G.; Stepanek, Christian; Rathgeber, Andreas W.",2017,10.1007/s00181-016-1162-9,,wos,"This paper investigates whether metals are primarily industrial resources or financial assets by analyzing the predictive power of financial and fundamental factors on metal returns. Using characteristic-sorted portfolios and two-way sorts, the study finds that financial characteristics like value and momentum significantly predict metal returns, suggesting metals function more as financial assets. While metal-specific factors have some influence, their predictive power is weaker. The findings contribute to understanding metal price dynamics.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:45:06.630212
c4b4f1784aacf26c,Mind the gap: forecasting euro-area output gaps with machine learning,"In this paper, we use the Eurozone yield curve in an effort to forecast the deviations of the euro-area output (IPI) from its long-run trend. We use various short- and long-term interest rates spanning the period from 2004:9 to 2020:6 in monthly frequency. The interest rates are fed to three machine learning methodologies: Decision Trees, Random Forests, and Support Vector Machines (SVM). These Machine Learning methodologies are then compared to an Elastic-Net Logistic Regression (Logit) model from the area of Econometrics. According to the results, the optimal SVM model coupled with the RBF kernel outperforms the competition reaching an in-sample accuracy of 85.29% and an out-of-sample accuracy of 94.74%.",,2022,10.1080/13504851.2021.1963403,,proquest,"This paper forecasts euro-area output gaps using machine learning models (Decision Trees, Random Forests, SVM) trained on Eurozone yield curve data from 2004 to 2020. The optimal SVM model achieved high accuracy in both in-sample and out-of-sample predictions, outperforming an econometric Elastic-Net Logistic Regression model.",True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:45:12.472900
f296d1ee3108d658,Model risk for European-style stock index options,"In empirical modeling, there have been two strands for pricing in the options literature, namely the parametric and nonparametric models. Often, the support for the nonparametric methods is based on a benchmark such as the Black-Scholes (BS) model with constant volatility. In this paper, we study the stochastic volatility (SV) and stochastic volatility random jump (SVJ) models as parametric benchmarks against feedforward neural network (FNN) models, a class of neural network models. Our choice for FNN models is due to their well-studied universal approximation properties of an unknown function and its partial derivatives. Since the partial derivatives of an option pricing formula are risk pricing tools, an accurate estimation of the unknown option pricing function is essential for pricing and hedging. Our findings indicate that FNN models offer themselves as robust option pricing tools, over their sophisticated parametric counterparts in predictive settings. There are two routes to explain the superiority of FNN models over the parametric models in forecast settings. These are normormality of return distributions and adaptive learning.","Gencay, Ramazan; Gibson, Rajna",2007,10.1109/tnn.2006.883005,,wos,"This paper compares the performance of feedforward neural network (FNN) models against parametric models (stochastic volatility and stochastic volatility random jump) for pricing European-style stock index options. The study finds that FNN models are more robust and superior in predictive settings, attributing this to the non-normality of return distributions and adaptive learning capabilities.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:45:19.050485
08344aa9e15b1cee,Model uncertainty in the cross-section of stock returns,"We develop a transparent Bayesian framework to measure uncertainty in asset pricing models. By assigning a modified class of g-priors to the risk prices of asset pricing factors, our method quantifies the trade-off between mean–variance efficiency and parsimony for asset pricing models to achieve high posterior probabilities. Model uncertainty is defined as the entropy of these model probabilities. We prove the model selection consistency property of our procedure, which is missing from the classic g-priors. Acknowledging the possibility of omitting true asset pricing factors in real applications, we also characterize the maximum degree of contamination that the omitted factors can introduce to our model uncertainty measure. Empirically, we find that model uncertainty escalates during major market events and carries a significantly negative risk premium of approximately half the magnitude of the market. Positive shocks to model uncertainty predict persistent outflows from US equity funds and inflows to Treasury funds. © 2025 Elsevier B.V., All rights reserved.",,2025,10.1016/j.jeconom.2025.106066,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011139721&doi=10.1016%2Fj.jeconom.2025.106066&partnerID=40&md5=b5f82e65537b031a0ee7bbc1e2956ee4,scopus,"This paper introduces a Bayesian framework to quantify uncertainty in asset pricing models by using g-priors on risk prices. It measures model uncertainty as the entropy of model probabilities and demonstrates model selection consistency. The study also assesses the impact of omitted factors and finds that model uncertainty increases during market events, carries a negative risk premium, and predicts fund flows.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:45:22.148542
0ae15ca252dbc816,Modeling Investor Responses to Green Bond Issuance: Multidimensional Perspectives and Evidence From China,"Although green bonds are rapidly growing to be a mature financing tool, the debate over whether there are benefits to be gained by issuers’ stocks has yet to be resolved, especially in the emerging market context. Issuing green bonds, as a financing procedure targeted to green engineering projects and demonstrating the issuers’ environmentally friendly attitude, does and how does it affect the issuers’ stock prices, liquidity, and risk? We address this issue by paying attention to the Chinese green bond issuance events. Utilizing the event study method, research shows that investors react positively to green bond issuance events. However, this reaction is only sensitive to green bond listing events, but not to announcements. Investor responses can be reflected in the abnormal changes in stock prices and liquidity. Both the stock systematic risk and idiosyncratic risk show little change after firms issue green bonds, which illustrates that green bond issuance cannot shape the inherent investors’ value judgments on issuer companies, thereby only producing temporary impacts. This study suggests that the green premium of corporate stocks induced by green bond issuance events may be sourced from investors’ optimistic predictions about green transformation, rather than investors’ subjective willingness to promote environmental sustainability.",T. Su; B. Lin,2025,10.1109/tem.2025.3538945,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10874181,ieeexplore,"This study investigates investor responses to green bond issuance in China using an event study method. It finds that investors react positively to green bond listing events, leading to abnormal changes in stock prices and liquidity, but not to announcements. The issuance of green bonds does not significantly alter stock systematic or idiosyncratic risk, suggesting temporary impacts. The authors propose that the observed 'green premium' stems from optimistic predictions about green transformation rather than a direct willingness to promote environmental sustainability.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:46:06.376600
7333ecab1e4cef3e,Modeling and predicting historical volatility in exchange rate markets,"Volatility modeling and forecasting of currency exchange rate is an important task in several business risk management tasks; including treasury risk management, derivatives pricing, and portfolio risk evaluation. The purpose of this study is to present a simple and effective approach for predicting historical volatility of currency exchange rate. The approach is based on a limited set of technical indicators as inputs to the artificial neural networks (ANN). To show the effectiveness of the proposed approach, it was applied to forecast US/Canada and US/Euro exchange rates volatilities. The forecasting results show that our simple approach outperformed the conventional GARCH and EGARCH with different distribution assumptions, and also the hybrid GARCH and EGARCH with ANN in terms of mean absolute error, mean of squared errors, and Theil's inequality coefficient. Because of the simplicity and effectiveness of the approach, it is promising for US currency volatility prediction tasks. © 2021 Elsevier B.V., All rights reserved.","Lahmiri, S.",2017,10.1016/j.physa.2016.12.061,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007518089&doi=10.1016%2Fj.physa.2016.12.061&partnerID=40&md5=9b4dde2750886471bb75756b62a43d05,scopus,"This study proposes a simple and effective approach for predicting historical volatility in currency exchange rates using technical indicators as inputs to artificial neural networks (ANN). The approach was applied to US/Canada and US/Euro exchange rates and outperformed conventional GARCH and EGARCH models, as well as hybrid GARCH-ANN models, in terms of various error metrics. The authors suggest this approach is promising for currency volatility prediction.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:46:09.063596
7ebc71b7e07716e7,Modeling the Yield Curve of BRICS Countries: Parametric vs. Machine Learning Techniques,"We compare parametric and machine learning techniques (namely: Neural Networks) for in–sample modeling of the yield curve of the BRICS countries (Brazil, Russia, India, China, South Africa). To such aim, we applied the Dynamic De Rezende–Ferreira five–factor model with time–varying decay parameters and a Feed–Forward Neural Network to the bond market data of the BRICS countries. To enhance the flexibility of the parametric model, we also introduce a new procedure to estimate the time varying parameters that significantly improve its performance. Our contribution spans towards two directions. First, we offer a comprehensive investigation of the bond market in the BRICS countries examined both by time and maturity; working on five countries at once we also ensure that our results are not specific to a particular data–set; second we make recommendations concerning modelling and estimation choices of the yield curve. In this respect, although comparing highly flexible estimation methods, we highlight superior in–sample capabilities of the neural network in all the examined markets and then suggest that machine learning techniques can be a valid alternative to more traditional methods also in presence of marked turbulence. © 2022 Elsevier B.V., All rights reserved.","Castello, O.; Resta, M.",2022,10.3390/risks10020036,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124573638&doi=10.3390%2Frisks10020036&partnerID=40&md5=b73ed06482dce2f2b6ac91c5c94bb0ae,scopus,"This study compares parametric and machine learning (Neural Networks) techniques for modeling the yield curve of BRICS countries. It applies the Dynamic De Rezende–Ferreira five–factor model and a Feed–Forward Neural Network to bond market data. The research investigates the bond markets of Brazil, Russia, India, China, and South Africa, offering recommendations on yield curve modeling and estimation choices. The findings suggest that neural networks exhibit superior in-sample capabilities compared to traditional methods, even during periods of market turbulence.",True,True,False,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T10:46:23.605300
68e704365ecd74f1,Modeling the term structure from the on-the-run treasury yield curve,"We propose a new model to estimate the term structure of interest rates using observed on-the-run Treasury yields. The new model is an improvement over models that require a priori knowledge of the shape of the yield curve to estimate the term structure. The general form of the model is an exponential function that depends on the estimation of four parameters fit by nonlinear least squares and has straightforward interpretations. In comparing the proposed model with current yield-curve-smoothing models, we find that, for the data used, the proposed model does best overall in terms of pricing accuracy both in sample and out of sample. JEL classification: E43, G12. © The Southern Finance Association and the Southwestern Finance Association. © 2018 Elsevier B.V., All rights reserved.","Mansi, S.A.; Phillips, J.H.",2001,10.1111/j.1475-6803.2001.tb00830.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037790039&doi=10.1111%2Fj.1475-6803.2001.tb00830.x&partnerID=40&md5=b4245734dc46cc70b7f7dc5a6c901db8,scopus,"This paper introduces a novel model for estimating the term structure of interest rates using on-the-run Treasury yields. The model, an exponential function with four parameters, improves upon existing methods by not requiring prior assumptions about the yield curve's shape. It demonstrates superior pricing accuracy compared to current yield-curve-smoothing models, both in-sample and out-of-sample.",True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:47:23.704817
2cde5aab7ff222dc,Modeling volatility changes in the 10-year Treasury,"This paper examines the daily volatility of changes in the 10-year Treasury note utilizing the iterated cumulative sums of squares algorithm [C. Inclan, G. Tiao, Use of cumulative sums of squares for retrospective detection of changes of variance, J. Am. Stat. Assoc. 89 (1994) 913-923]. The ICSS algorithm can detect regime shifts in the volatility of the interest rate changes. A general model allows for endogenously determined changes in variance while the more restrictive model forces the variance to follow the same process throughout the sample period. A comparison of the out-of-sample volatility forecasting performance of two competing models is made using asymmetric error measures. The asymmetric error statistics penalize models for under- or over-predicting volatility. The results shed light on the importance of ignoring volatility regime shifts when performing out-of-sample forecasts. The findings are important to financial market participants who require accurate forecasts of future volatility in order to implement and evaluate asset performance. (c) 2006 Elsevier B.V. All rights reserved.","Covarrubias, Guillermo; Ewing, Bradley T.; Hein, Scott E.; Thompson, Mark A.",2006,10.1016/j.physa.2006.01.074,,wos,"This paper uses the iterated cumulative sums of squares (ICSS) algorithm to model and detect regime shifts in the daily volatility of 10-year Treasury note changes. It compares the out-of-sample forecasting performance of models that account for volatility regime shifts versus those that do not, using asymmetric error measures. The findings highlight the importance of considering volatility regime shifts for accurate out-of-sample volatility forecasts, which is crucial for financial market participants.",False,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:47:34.534200
c60c32b11706bfaf,Modelling Electricity Swaps with Stochastic Forward Premium Models,"We present a new model for pricing electricity swaps. Two general factors affect contracts but unique risk elements affect each contract. General factors are average swap prices and deterministic trend-seasonal components, and unique elements are forward premiums. Innovations follow MNIG distributions. We estimate the model with data from the European Energy Exchange. The model outperforms four competitors, both in in-sample valuation and in out-of-sample forecasting, and in fitting the term structure of volatilities by market segments. Competitor models are (i) diffusion spot prices, (ii) jump-diffusion spot prices with time dependent volatility, (iii) HJM-based and (iv) Levy multifactor model with NIG distributions. Value-at-Risk measures based on normality strongly underestimate tail risk but our model gives estimates that are more exact.Keywords: Electricity swaps; Stochastic forward premium; Multivariate Normal Inverse Gaussian distribution; Levy processes",,2018,10.5547/01956574.39.2.ibla,,proquest,"This paper introduces a novel model for pricing electricity swaps, incorporating general factors like average swap prices and seasonal trends, alongside unique risk elements represented by forward premiums. The model utilizes Multivariate Normal Inverse Gaussian (MNIG) distributions for innovations and is validated using data from the European Energy Exchange. It demonstrates superior performance compared to five competitor models in valuation, forecasting, and fitting volatility term structures. The model also provides more accurate Value-at-Risk estimates, particularly concerning tail risk, unlike normality-based measures.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:47:35.892383
3e7e2117f4a117be,Modelling dependence structure with Archimedean copulas and applications to the iTraxx CDS index,"In this paper we model the dependence structure between credit default swap (CDS) and jump risk using Archimedean copulas. The paper models and estimates the different relationships that can exist in different ranges of behaviour. It studies the bivariate distributions of CDS index spreads and the kurtosis of equity return distribution. To take into account nonlinear relationships and different structures of dependency, we employ three Archimedean copula functions: Gumbel, Clayton, and Frank. We adopt nonparametric estimation of copula parameters and we find an extreme co-movement of CDS and stock market conditions. In addition, tail dependence indicates the extreme co-movements and the potential for a simultaneous large loss in stock markets and a significant default risk. Ignoring the tail dependence would lead to underestimation of the default risk premium. © 2010 Elsevier B.V. All rights reserved. © 2011 Elsevier B.V., All rights reserved.","Naifar, N.",2011,10.1016/j.cam.2010.10.047,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79251599155&doi=10.1016%2Fj.cam.2010.10.047&partnerID=40&md5=3494ea2f4e142ba65691f60966ea8084,scopus,"This paper models the dependence structure between credit default swap (CDS) and jump risk using Archimedean copulas (Gumbel, Clayton, and Frank). It estimates bivariate distributions of CDS index spreads and equity return kurtosis, employing nonparametric estimation of copula parameters. The study finds extreme co-movement between CDS and stock market conditions, highlighting tail dependence which, if ignored, could lead to underestimation of default risk premium.",True,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:47:38.305985
b1e9b249051157d6,Modelling nonlinearities in equity returns: the mean impact curve analysis,"A time-varying model of equity returns consisting of a volatility factor with time-varying loading, is specified to investigate the dynamical effects of shocks on expected returns. The proposed specification yields a nonlinear relationship between the conditional mean and the news, referred to as the mean impact curve (MIC). Applying this framework to the AORD, Hang Seng and Straits Times equity indexes yields estimated MICs with qualitatively similar nonlinear characteristics for each equity market. An important implication of the empirical results is that the relationship between the conditional mean and the news is found to be dependent upon the size of the shock, a result which is consistent with equity markets displaying mean aversion over short horizons and mean reversion over long horizons.","Martin, Vance L.; Sarkar, Saikat; Kanto, Antti Jaakko",2014,10.1515/snde-2012-0030,,wos,"This paper proposes a time-varying model for equity returns that incorporates a volatility factor with time-varying loading to study the dynamic effects of shocks on expected returns. The model results in a nonlinear relationship between the conditional mean and news, termed the mean impact curve (MIC). Empirical analysis of the AORD, Hang Seng, and Straits Times equity indexes reveals similar nonlinear MIC characteristics across these markets. The findings suggest that the relationship between the conditional mean and news depends on shock size, aligning with equity markets exhibiting mean aversion in the short term and mean reversion in the long term.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:47:40.511596
052c16a20382c1b6,Modelling the dependence structures of Australian iTraxx CDS index,"In contrast to market expectations, the correlation between credit default swap (CDS) spreads and their respective stock prices in Australia was found to be positive. The global financial crisis (GFC) affected the nonlinear association between the two asset classes with firms experiencing financial distress and stock prices plummeting. CDSs issuers reacted to such exogenous shocks by increasing their risk premiums on their spreads, reflecting the increased inherent risk. By splitting the data into pre- and post-GFC contexts and by employing the use of Archimedean copulas, we observe a negative co-movement in the post-GFC period. This finding is robust to several equity indices. Overall, such result is critical for investors engaging in arbitrageur activities. © 2013 Taylor & Francis. © 2013 Elsevier B.V., All rights reserved.","Fenech, J.-P.; Vosgha, H.; Shafik, S.",2014,10.1080/00036846.2013.849378,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887918664&doi=10.1080%2F00036846.2013.849378&partnerID=40&md5=e1bbd0d1c5a92b23deea51cdab6fda29,scopus,"This study models the dependence structures of Australian iTraxx CDS index spreads and stock prices, finding a positive correlation that was affected by the GFC. Using Archimedean copulas, a negative co-movement was observed in the post-GFC period, which is robust across several equity indices and has implications for arbitrage activities.",True,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:47:42.612737
2c7bb54560d01c08,Moment Risk Premia and Stock Return Predictability,"We study the predictive power of option-implied moment risk premia embedded in the conventional variance risk premium. We find that although the second-moment risk premium predicts market returns in short horizons with positive coefficients, the third-moment (fourth-moment) risk premium predicts market returns in medium horizons with negative (positive) coefficients. Combining the higher-moment risk premia with the second-moment risk premium improves the stock return predictability over multiple horizons, both in sample and out of sample. The finding is economically significant in an asset-allocation exercise and survives a series of robustness checks.","Fan, Zhenzhen; Xiao, Xiao; Zhou, Hao",2022,10.1017/s002210902000085x,,wos,"This study investigates the predictive power of option-implied moment risk premia for stock returns. It finds that second-moment risk premia predict returns at short horizons, while third- and fourth-moment risk premia predict at medium horizons. Combining these higher-moment risk premia enhances stock return predictability across multiple horizons, with significant economic implications and robustness.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:47:44.599292
8a16f3a80a9ddf11,"Moments, shocks and spillovers in Markov-switching VAR models","To investigate how economies, financial markets or institutions can deal with stress, we often analyze the effects of shocks conditional on being in a recession or a bear market. MSVAR models are perfectly suited for such analyses because they combine gradual movements with sudden regime switches. In this paper, we develop a comprehensive methodology to conduct these analyses. We derive first and second moments conditional only on the regime distribution and propose impulse response functions for both moments. By formulating the MSVAR as an extended linear non-Gaussian VAR, all results are available in closed-form. We illustrate our methods with an application to stock and bond return predictability. We show how forecasts of means, volatilities and (auto-)correlations depend on the regimes. The effect of shocks becomes highly nonlinear, and they propagate via different channels. During bear markets, shocks have stronger effects on means and volatilities and die out more slowly. & COPY; 2023 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).","Kole, Erik; van Dijk, Dick",2023,10.1016/j.jeconom.2023.105474,,wos,"This paper develops a methodology for analyzing the effects of shocks in Markov-switching VAR (MSVAR) models, which are suitable for capturing both gradual movements and sudden regime switches. The authors derive conditional moments and impulse response functions in closed-form by formulating the MSVAR as an extended linear non-Gaussian VAR. An application to stock and bond returns demonstrates how forecasts of means, volatilities, and autocorrelations are regime-dependent, with shocks having nonlinear and slower-dying effects during bear markets.",True,True,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T10:48:00.545181
fdfb42067bfce0e2,Monetary base and federal government debt in the long‐run: A non‐linear analysis,"Government bonds are usually traded between the financial institutions and the Fed during the open market operations. These operations impact the bank reserves, subsequently influencing the monetary base. The monetary base and government bonds may portray a common trend and government debt could potentially bind the central bank to debt monetization. This paper, using monthly data on federal government debt and the monetary base from 1947:1 to 2018:10, investigates the presence of a long‐run equilibrium relationship between the two variables and as to how the long‐run equilibrium relationship vary in the short‐run. Threshold cointegration tests find evidence of a long‐run equilibrium relationship. Estimates of the threshold vector error‐correction model find statistically significant evidence of contraction in the monetary base growth in the short‐run in regime 1. In regime 2, the growth in the monetary base does not adjust to accommodate faster government debt growth. These estimates find no evidence of debt monetization or otherwise in either of the regimes in the United States. The Fed, by reducing the monetary base, perhaps focuses more on the inflation target. The findings also suggest a potential scenario where the Fed and the fiscal authority are not conjoined with each other in their operations.",,2020,10.1111/boer.12216,,proquest,This paper investigates the long-run equilibrium relationship between the monetary base and federal government debt in the US from 1947 to 2018 using threshold cointegration and a threshold vector error-correction model. The study finds evidence of a long-run relationship but no evidence of debt monetization. It suggests the Fed may prioritize inflation targets and operate independently of fiscal authority.,True,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:48:05.015517
872b565b23f27078,Monetary policy document analysis for prediction of monetary policy board decision,"In terms of market capitalization, the bond market is larger than the stock market, and the bond market is affected by macroeconomic indicators. Despite this, there has been relatively little research, making it a good candidate for the use of data mining techniques. In this paper, a novel approach designed to predict the vote results of the Korean Monetary Policy Committee regarding the base interest rate was proposed. To predict sentence sentiment, prior monetary policy decision text was used as input for classification models. The sentence sentiment prediction model showed 83.7% performance when using a support vector machine. In addition, it was observed that the bigrams extracted from documents provided important descriptions of the Korean economy at the time. Finally, the document sentiment of monetary policy decision was calculated using aggregating sentence sentiment, and the vote results were predicted using this sentiment. As a result, when using the support vector machine to predict the Monetary Policy Committee vote results, the performance improved by 29.5% over the baseline model. Statistical tests confirmed whether there is a difference in document sentiments between unanimous and non-unanimous, and the null hypothesis was rejected at a significance level of 5%.In terms of market capitalization, the bond market is larger than the stock market, and the bond market is affected by macroeconomic indicators. Despite this, there has been relatively little research, making it a good candidate for the use of data mining techniques. In this paper, a novel approach designed to predict the vote results of the Korean Monetary Policy Committee regarding the base interest rate was proposed. To predict sentence sentiment, prior monetary policy decision text was used as input for classification models. The sentence sentiment prediction model showed 83.7% performance when using a support vector machine. In addition, it was observed that the bigrams extracted from documents provided important descriptions of the Korean economy at the time. Finally, the document sentiment of monetary policy decision was calculated using aggregating sentence sentiment, and the vote results were predicted using this sentiment. As a result, when using the support vector machine to predict the Monetary Policy Committee vote results, the performance improved by 29.5% over the baseline model. Statistical tests confirmed whether there is a difference in document sentiments between unanimous and non-unanimous, and the null hypothesis was rejected at a significance level of 5%.",,2023,10.1016/j.heliyon.2023.e20696,,proquest,"This paper proposes a novel approach to predict the vote results of the Korean Monetary Policy Committee regarding the base interest rate by analyzing monetary policy documents. The study uses sentence sentiment analysis, with a support vector machine achieving 83.7% performance in predicting sentence sentiment. Bigrams from documents were found to provide important descriptions of the Korean economy. Aggregating sentence sentiments to calculate document sentiment allowed for the prediction of vote results, improving performance by 29.5% over the baseline model. Statistical tests also indicated a significant difference in document sentiments between unanimous and non-unanimous decisions.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:48:07.234727
56f3840b8e773856,Multi-Period Portfolio Optimization with Investor Views under Regime Switching,"We propose a novel multi-period trading model that allows portfolio managers to perform optimal portfolio allocation while incorporating their interpretable investment views. This model’s significant advantage is its intuitive and reactive design that incorporates the latest asset return regimes to quantitatively solve managers’ question: how certain should one be that a given investment view is occurring? First, we describe a framework for multi-period portfolio allocation formulated as a convex optimization problem that trades off expected return, risk and transaction costs. Using a framework borrowed from model predictive control introduced by Boyd et al., we employ optimization to plan a sequence of trades using forecasts of future quantities, only the first set being executed. Multi-period trading lends itself to dynamic readjustment of the portfolio when gaining new information. Second, we use the Black-Litterman model to combine investment views specified in a simple linear combination based format with the market portfolio. A data-driven method to adjust the confidence in the manager’s views by comparing them to dynamically updated regime-switching forecasts is proposed. Our contribution is to incorporate both multi-period trading and interpretable investment views into one framework and offer a novel method of using regime-switching to determine each view’s confidence. This method replaces portfolio managers’ need to provide estimated confidence levels for their views, substituting them with a dynamic quantitative approach. The framework is reactive, tractable and tested on 15 years of daily historical data. In a numerical example, this method’s benefits are found to deliver higher excess returns for the same degree of risk in both the case when an investment view proves to be correct, but, more notably, also the case when a view proves to be incorrect. To facilitate ease of use and future research, we also developed an open-source software library that replicates our results. © 2023 Elsevier B.V., All rights reserved.","Oprisor, R.; Kwon, R.",2021,10.3390/jrfm14010003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165752842&doi=10.3390%2Fjrfm14010003&partnerID=40&md5=5f2b6dac5d316cc3ac5eb6c02e9b0cb2,scopus,"This paper presents a multi-period portfolio optimization model that integrates investor views and regime switching. It uses a convex optimization framework and a Black-Litterman model to combine market portfolios with manager views, dynamically adjusting confidence in views based on regime-switching forecasts. The model is tested on 15 years of historical data and aims to improve portfolio performance by providing a quantitative approach to view confidence.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:48:17.136049
893895957d5ecc26,Multifractality and the economic value of equity risk premium forecasts,"This paper presents novel evidence on the economic value of equity risk premium forecasts across varying degrees of market multifractality. The degree of multifractality in returns is measured to capture relevant nonlinear long-range autocorrelations. Risk premium forecasts are generated using economic indicators as predictors. Their economic value is assessed through a portfolio strategy that allocates between equities and risk-free assets based on these forecasts. The results show that higher levels of nonlinear long-range autocorrelations are associated with statistically significant out-of-sample risk premium forecasts. These periods also yield greater economic gains, underscoring the importance for investors of monitoring nonlinear dependencies when incorporating risk premium forecasts into portfolio decisions. © 2025 Elsevier B.V., All rights reserved.","Maciel, L.S.",2025,10.1080/13504851.2025.2498060,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003882998&doi=10.1080%2F13504851.2025.2498060&partnerID=40&md5=af289c71f5315456eb43d7eb26633d15,scopus,"This paper investigates the economic value of equity risk premium forecasts by examining the relationship between market multifractality and forecast accuracy. It uses economic indicators to generate forecasts and assesses their value through a portfolio strategy. The study finds that higher levels of nonlinear long-range autocorrelations in returns are linked to statistically significant out-of-sample forecasts and greater economic gains, suggesting investors should consider these nonlinear dependencies.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:48:39.718892
fc81822659444a59,Multiobjective Model Predictive Control for portfolio optimization with cardinality constraint,"Model Predictive Control has been shown to be adequate to solve portfolio optimization problems because of its ability to perform the dynamic readjustment of the portfolio considering the market expectations. To consider both wealth and risk and real issues imposed by the financial market, this work proposes a Multiobjective Model Predictive Control strategy with cardinality constraints, besides transaction costs, self-financing, and upper and lower limits. The objective functions are the expected portfolio wealth and the expected Variance and Conditional Value at Risk as the portfolio risk measures. The optimization is performed by a multiobjective genetic algorithm, with operators proposed to control the number of assets in each portfolio and respect the prediction horizon perspective. Finally, an insightful case study is designed using the Brazilian Stock Exchange data in 2019 and 2020. An in-sample analysis explores the relationship between prediction horizon length, cardinality, optimal portfolio composition, risk-free asset, and objective functions on performance. An out-of-sample analysis considers the cumulative wealth, Sharpe ratio, maximum Drawdown, and the monthly accumulated return. Numerical experiments indicate that the proposed strategy outperforms the myopic portfolio selection, beats the primary Brazilian benchmark, a modified Markowitz model, and some top Brazilian investment funds even in crisis times like during the COVID-19 pandemic. © 2022 Elsevier B.V., All rights reserved.","de Melo, M.K.; Cardoso, R.T.N.; Jesus, T.A.",2022,10.1016/j.eswa.2022.117639,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132322733&doi=10.1016%2Fj.eswa.2022.117639&partnerID=40&md5=826d77fde8a0fb668125cdcf76866e6d,scopus,"This paper proposes a Multiobjective Model Predictive Control (MPC) strategy for portfolio optimization, incorporating cardinality constraints, transaction costs, and other financial market realities. It aims to balance expected portfolio wealth with risk measures like Variance and Conditional Value at Risk. A multiobjective genetic algorithm is used for optimization. The strategy is tested using Brazilian Stock Exchange data and shows superior performance compared to benchmarks and investment funds, even during the COVID-19 pandemic.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:48:42.111602
a2acf588c5e07225,Multiplicative parameters and estimators: applications in economics and finance,"In this paper, we pay our attention to multiplicative parameters of random variables and their estimators. We study multiplicative properties of the multiplicative expectation and multiplicative variation as well as their estimators. For distributions having applications in finance and insurance we provide their multiplicative parameters and their properties. We consider, among others, heavy-tailed distributions such as lognormal and Pareto distributions, applied to the modelling of large losses. We discuss multiplicative models, in which the geometric mean and the geometric standard deviation are more natural than their arithmetic counterparts. We provide two examples from the Warsaw Stock Exchange in 1995-2009 and from a bid of 52-week treasury bills in 1992-2009 in Poland as an illustrative example.",,2016,10.1007/s10479-015-2035-x,,proquest,"This paper investigates multiplicative parameters and their estimators, focusing on their applications in economics and finance. It examines multiplicative expectation and variation, particularly for heavy-tailed distributions like lognormal and Pareto, which are relevant for modeling large losses in finance and insurance. The study highlights the advantages of multiplicative models, using geometric mean and standard deviation, and provides examples from the Warsaw Stock Exchange and Polish treasury bills.",False,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:49:03.606384
c6a858641ba903a5,Multivariate CDS risk premium prediction with SOTA RNNs on MI[N]T countries,"In this study, CDS risk premiums of Mexico, Indonesia and Turkey were predicted by applying state-of-the-art forecasters in deep learning recurrent neural networks architectures which are the most recent ground-breaking predictors in the time series setting. The predictive power of each sota forecaster is compared, and the results are differentiated by country and type of sota predictors. While the long short-term memory model is better to predict Mexico's CDS risk premiums, the nonlinear autoregressive network with exogenous inputs model is found to be more suitable for Indonesia and Turkey. The results of Turkey model reached the highest forecast accuracy. © 2022 Elsevier B.V., All rights reserved.","Kutuk, Y.; Barokas, L.",2022,10.1016/j.frl.2021.102198,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107964009&doi=10.1016%2Fj.frl.2021.102198&partnerID=40&md5=308c6bc5add7af268f4e99f0c972824d,scopus,"This study predicts Credit Default Swap (CDS) risk premiums for Mexico, Indonesia, and Turkey using state-of-the-art deep learning recurrent neural network (RNN) models. It compares the predictive power of different RNN architectures, finding that Long Short-Term Memory (LSTM) is best for Mexico, while Nonlinear Autoregressive Network with Exogenous Inputs (NARX) is more suitable for Indonesia and Turkey. The NARX model achieved the highest forecast accuracy for Turkey.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:49:05.031403
83686a7846be2d41,Multivariate tests of financial models. A new approach,"A variety of financial models are cast as nonlinear parameter restrictions on multivariate regression models, and the framework seems well suited for empirical purposes. Aside from eliminating the errors-in-the-variables problem which has plagued a number of past studies, the suggested methodology increases the precision of estimated risk premiums by as much as 76%. In addition, the approach leads naturally to a likelihood ratio test of the parameter restrictions as a test for a financial model. This testing framework has considerable power over past test statistics. With no additional variable beyond β, the substantive content of the CAPM is rejected for the period 1926-1975 with a significance level less than 0.001. © 1982. © 2014 Elsevier B.V., All rights reserved.","Gibbons, M.R.",1982,10.1016/0304-405x(82)90028-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750176969&doi=10.1016%2F0304-405X%2882%2990028-9&partnerID=40&md5=26284468277d35496a042e65f56a379d,scopus,"This paper proposes a new multivariate approach to testing financial models by casting them as nonlinear parameter restrictions on multivariate regression models. This method addresses errors-in-the-variables issues, improves the precision of risk premium estimates, and introduces a powerful likelihood ratio test for financial models. The Capital Asset Pricing Model (CAPM) is rejected for the period 1926-1975 using this framework.",True,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:49:06.255544
9cf15666ac064bd7,Multivariate time-series modeling with generative neural networks,"Generative moment matching networks (GMMNs) are introduced as dependence models for the joint innovation distribution of multivariate time series (MTS). Following the popular copula–GARCH approach for modeling dependent MTS data, a framework based on a GMMN–GARCH approach is presented. First, ARMA–GARCH models are utilized to capture the serial dependence within each univariate marginal time series. Second, if the number of marginal time series is large, principal component analysis (PCA) is used as a dimension-reduction step. Last, the remaining cross-sectional dependence is modeled via a GMMN, the main contribution of this work. GMMNs are highly flexible and easy to simulate from, which is a major advantage over the copula–GARCH approach. Applications involving yield curve modeling and the analysis of foreign exchange-rate returns demonstrate the utility of the GMMN–GARCH approach, especially in terms of producing better empirical predictive distributions and making better probabilistic forecasts. © 2022 Elsevier B.V., All rights reserved.","Hofert, M.; Prasad, A.; Zhu, M.",2022,10.1016/j.ecosta.2021.10.011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119899540&doi=10.1016%2Fj.ecosta.2021.10.011&partnerID=40&md5=4c2db93c152de7fc06453d4c54c6c943,scopus,"This paper introduces Generative Moment Matching Networks (GMMNs) as a method for modeling the joint innovation distribution of multivariate time series (MTS). It proposes a GMMN-GARCH framework, which first uses ARMA-GARCH models for univariate series, then PCA for dimension reduction if needed, and finally GMMNs to model the remaining cross-sectional dependence. The approach is demonstrated with applications in yield curve modeling and foreign exchange rate returns, showing improved empirical predictive distributions and probabilistic forecasts.",True,True,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T10:49:09.525901
2cf4672afef2a0ae,Navigating the Complexity of Money Laundering: Anti–money Laundering Advancements with AI/ML Insights,"This study explores the fusion of artificial intelligence (AI) and machine learning (ML) methods within anti–money laundering (AML) frameworks using data from the US Treasury’s Financial Crimes Enforcement Network (FinCEN). ML and deep learning (DL) algorithms—such as random forest classifier, elastic net regressor, least absolute shrinkage and selection operator (LASSO) regression, gradient boosting regressor, linear regression, multilayer perceptron (MLP) classifier, convolutional neural network (CNN), random forest regressor, and K-nearest neighbor (KNN)—were used to forecast variables such as state, year, and transaction types (credit card and debit card). Hyperparameter tuning through grid search and randomized search was used to optimize model performance. The results demonstrated the efficacy of AI/ML algorithms in predicting temporal, spatial, and industry-specific money-laundering patterns. The random forest classifier achieved 99.99% average accuracy in state prediction, while the gradient boosting regressor and random forest classifier excelled in predicting year and state simultaneously, and credit card transactions, respectively. MLP and CNN showed promise in the context of debit card transactions. The gradient boosting regressor performed competitively with low mean squared error (MSE) (2.9) and the highest R-squared (R2) value of 0.24, showcasing its pattern-capturing proficiency. Logistic regression and random forest classifier performed well in predicting credit card transactions, with area under the receiver operating characteristic curve (ROC_AUC) scores of 0.55 and 0.53, respectively. For debit card prediction, MLP achieved a precision of 0.55 and recall of 0.42, while CNN showed a precision of 0.6 and recall of 0.54, highlighting their effectiveness. The study recommends interpretability, hyperparameter optimization, specialized models, ensemble methods, data augmentation, and real-time monitoring for improved adaptability to evolving financial crime patterns. Future improvements could include exploring the integration of blockchain technology in AML.",,2024,10.2478/ijssis-2024-0024,,proquest,"This study investigates the application of various AI/ML algorithms (including random forest, elastic net, LASSO, gradient boosting, linear regression, MLP, CNN, and KNN) to anti-money laundering (AML) frameworks using FinCEN data. The models were used to predict transaction types, state, and year, with hyperparameter tuning employed for optimization. The results indicate strong predictive performance for several algorithms in identifying money laundering patterns, with recommendations for further improvements such as interpretability and real-time monitoring.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:49:11.925469
0e8c12740794a640,"Neural Networks, the Treasury Yield Curve, and Recession Forecasting","The authors use neural networks to examine the power of Treasury term spreads and other macro-financial variables to forecast US recessions and compare them with probit regression. They propose a novel three-step econometric method for cross-validating and conducting statistical inference on machine learning classifiers and explaining forecasts. They find that probit regression does not underperform a neural network classifier in the present application, which stands in contrast to a growing body of literature demonstrating that machine learning methods outperform alternative classification algorithms. That said, neural network classifiers do identify important features of the joint distribution of recession over term spreads and other macro-financial variables that probit regression cannot. The authors discuss some possible reasons for their results and use their procedure to study US recessions over the post-Volcker period, analyzing feature importance across business cycles. © 2022 Elsevier B.V., All rights reserved.","Puglia, M.; Tucker, A.",2021,10.3905/jfds.2021.1.061,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127450588&doi=10.3905%2Fjfds.2021.1.061&partnerID=40&md5=abf79b5fec58c545a98983f579b5062c,scopus,"This study employs neural networks and probit regression to forecast US recessions using Treasury term spreads and other macro-financial variables. The authors introduce a novel econometric method for validating and explaining machine learning forecasts. They find that probit regression performs comparably to neural networks in this application, but neural networks reveal important distributional features missed by probit regression. The analysis covers the post-Volcker period, examining feature importance across business cycles.",True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:49:19.845006
c2c778f7c4011a35,Neural forecasting of the Italian sovereign bond market with economic news,"In this paper, we employ economic news within a neural network framework to forecast the Italian 10‐year interest rate spread. We use a big, open‐source, database known as Global Database of Events, Language and Tone to extract topical and emotional news content linked to bond markets dynamics. We deploy such information within a probabilistic forecasting framework with autoregressive recurrent networks (DeepAR). Our findings suggest that a deep learning network based on long short‐term memory cells outperforms classical machine learning techniques and provides a forecasting performance that is over and above that obtained by using conventional determinants of interest rates alone.",,2022,10.1111/rssa.12813,,proquest,This paper utilizes economic news and a neural network framework (DeepAR with LSTM) to forecast the Italian 10-year interest rate spread. The approach incorporates topical and emotional news content from a large database and demonstrates superior performance compared to classical machine learning techniques and conventional determinants of interest rates.,True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:49:33.277565
62c5869d03cd0ee0,Neural network forecasting in prediction Sharpe ratio: Evidence from EU debt market,"This study analyzes a neural networks model that forecast Sharpe ratio. The developed neural networks model is successful to predict the position of the investor who will be rewarded with extra risk premium on debt securities for the same level of portfolio risk or a greater risk premium than proportionate growth risk. The main purpose of the study is to predict highest Sharpe ratio in the future. Study grouped the data on yields of debt instruments in periods before, during and after world crisis. Results shows that neural networks is successful in forecasting nonlinear time lag series with accuracy of 82% on test cases for the prediction of Sharpe-ratio dynamics in future and investor‘s portfolio position. © 2021 Elsevier B.V., All rights reserved.","Vuković, D.; Vyklyuk, Y.; Matsiuk, N.; Maiti, M.",2020,10.1016/j.physa.2019.123331,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078116317&doi=10.1016%2Fj.physa.2019.123331&partnerID=40&md5=15556954c997b409a337691856374b6f,scopus,"This study applies a neural network model to forecast the Sharpe ratio in the EU debt market, aiming to predict future Sharpe ratio peaks and investor portfolio positions. The model demonstrated an 82% accuracy in forecasting nonlinear time-lag series, analyzing debt instrument yields across periods before, during, and after a world crisis.",True,True,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T10:49:48.398499
6b5d1dd465367a4c,Neural network prediction of crude oil futures using B-splines,"We propose two ways to improve the forecasting accuracy of a focused time-delay neural network (FTDNN) that forecasts the term structure of crude oil futures. Our results show that a convergence based FTDNN makes consistently more accurate predictions than the fixed-epoch FTDNN in Barunik and Malinska (2016). Further, we suggest using basis splines (B-splines), instead of Nelson-Siegel functions, to fit the term structure curves. The empirical results show that the B-spline expansions lead to consistently better 1 and 3 months ahead predictions compared to the convergence based FTDNN. We also explore conditions under which the B-spline based approach may be better for longer-term predictions.",,2021,10.1016/j.eneco.2020.105080,,proquest,"This paper proposes two methods to enhance the accuracy of a focused time-delay neural network (FTDNN) for forecasting crude oil futures term structures. The first method involves a convergence-based FTDNN, which outperforms the fixed-epoch FTDNN. The second method suggests using B-splines instead of Nelson-Siegel functions to fit term structure curves, leading to improved 1 and 3-month ahead predictions. The study also investigates the potential for B-splines in longer-term predictions.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:49:50.348613
bfdbb84acb713f76,New results on the predictive value of crude oil for US stock returns,"PurposeThe purpose of this study is to clarify the nature of the predictive relationship between crude oil and the US stock market, with particular attention to whether this relationship is driven by time-varying risk premia.Design/methodology/approachThe authors formulate the predictive regression as a state-space model and estimate the time-varying coefficients via the Kalman filter and prediction-error decomposition.FindingsThe authors find that the nature of the predictive relationship between crude oil and the US stock market changed in the latter half of 2008. After mid-2008, the predictive relationship switched signs and exhibited characteristics which make it much more likely that the predictive relationship is due to time-varying risk premia rather than a market inefficiency.Originality/valueThe authors apply a state-space approach to modeling the predictive relationship. This allows one to watch the evolution of the predictive relationship over time. In particular, the authors identify a dramatic shift in the relationship around August 2008. Prior research has not been able to identify shifts in the relationship.",,2018,10.1108/sef-01-2017-0020,,proquest,"This study investigates the predictive relationship between crude oil and the US stock market, employing a state-space model and Kalman filter to estimate time-varying coefficients. The findings indicate a significant shift in this relationship around August 2008, suggesting it is driven by time-varying risk premia rather than market inefficiency. The methodology allows for tracking the evolution of this predictive relationship over time.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:49:56.904571
99693fcac2d04cb6,Non-invasive evaluation of neonatal cerebral status in the newborns of mothers addicted to alcohol and drugs,"The present study aims to assess the effects of alcohol and drug consumption on the cerebral status of a newborn with risk. Although there is a vast literature on the quality of life in terms of health, there is no uniform point of view, since the well-being of a person implies other elements that consider not only health but also the economic and educational environment in which the individual evolves and often these factors are connected. Besides, there is no valid instrument for measuring the quality of life either for an adult or for a child. In most cases, alcohol consumption intensifies in time, significantly decresing the quality of life for the mother and especially for the conception product. The study focuses on showing the The study focuses on highlighting the psychosocial and pharmacological aspects relevant to the diagnosis and management of neonatal cerebral status. The study participants, whose responses were the base for the quantitative analyzes, were individually interviewed using a standardized interview protocol. The interviews were conducted between October 2015 and September 2017. The interview protocol included three sections, in this chapter focusing our attention on the following sections: a) socio-demographic characteristics: age of gestation, sex of the newborn; b) clinical data: Presentation, Weight at Birth, Apgar Score, Cerebral Saturation (rSO<inf>2</inf>), Peripheral saturation (SpO<inf>2</inf>), The extraction fraction (FTOE), Parameters harvested from the umbilical cord at birth (pH, Base excess (BE), pCO<inf>2</inf>, pO<inf>2</inf>, MetHb, COHb), c) risk profile: mother’s alcohol consumption, including during pregnancy and drug use. The study group consisted of 90 infants born full term in Elena Doamna Maternity Hospital in Iasi, between 2015-2017, included in the programme of follow-up of the newborn with risk with the purpose of performing an non-invasive assessment of the fetal and neonatal cerebral status, in order to prevent and establish treatment methods for perinatal asphyxia. Based on the information obtained through the preliminary documentation, 30 newborns with alcohol and / or drug-consuming mothers and 60 neonates with risk-free mothers were selected - the control batch, who accepted to participate in the study.The cases studied showed the homogeneity of the groups depending on the mother’s age and gestational age, as well as the sex of the newborn and the weight at birth (p>0.05). In neonates from mothers at risk, the under-reference level of 1-minute brain saturation, combined with a lower gestational age and the 62.5% probability of performing a caesarean section at low levels of cerebral saturation was noted in 66.7% of newborns.The cut off value of SpO<inf>2</inf>, was established at 70 mL/ 100g/1 min, with a sensitivity of 50.9% and a specificity of 51.3%, after reading the coordinates of ROC curve, but the prediction was not significant from the statistical point of view (p=0.670). The mean level of base excess was al excesului de baze was slightly lower in newborns with the extraction fraction below the cut off value (-4.64 vs -4.18; p=0.560). According to the cases studied, 1 min after birth, 23.3% of the newborns showed an increased level of pCO<inf>2</inf> associated with a reduced level of peripheral saturation (r= -0.231; p=0.05). The correlation between the pO<inf>2</inf> level and the cerebral saturation, recorded 1 min after birth, was direct, but reduced as intensity (r= +0.295; p=0.049). About 27% of the newborns associated increased values of pO<inf>2</inf> with reduced values of the extraction fraction (r=-0.272; p=0.047). The newborns with an extraction fraction over the cut off value had a level of COHb below 1% (p=0.756) more frequently. Newborns from mothers who have consumed alcohol and / or drugs, including during pregnancy, show a reduced level of cerebral saturation and peripheral saturation 1 minute after birth. In 16.7% of newborns, the extraction limit was below the baseline 1 minute after birth. © 2020 Elsevier B.V., All rights reserved.","Crauciuc, D.V.; Crauciuc, E.G.; Iov, C.J.; Furnica, C.; Iov, T.",2018,10.37358/rc.18.11.6708,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062692954&doi=10.37358%2Frc.18.11.6708&partnerID=40&md5=f4546e0e9b4122f43f1e0a1afd543e0f,scopus,"This study non-invasively assesses the cerebral status of newborns exposed to maternal alcohol and drug use. It compares 30 exposed infants with 60 control infants, collecting socio-demographic, clinical (including cerebral saturation, peripheral saturation, and umbilical cord gases), and maternal risk factor data. Results indicate reduced cerebral and peripheral saturation in exposed newborns, with correlations observed between saturation levels and various clinical parameters. The study highlights the impact of maternal substance use on neonatal brain status.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:49:58.689623
3227b5dbadd1f5b3,Non-invasive prenatal testing: A diagnostic innovation shaped by commercial interests and the regulation conundrum,"Non-invasive prenatal testing (NIPT) is grounded in the analysis of free circulating fetal DNA (cfDNA) in pregnant women's blood. The rolling out of this screening method was in large part driven by commercial firms, which hoped to reach a huge potential market by offering a test that was expected to be risk-free, reliable, inexpensive, and able to detect a wide range of genetic traits of the future child. To date, most predictions about the scope and uses of NIPT have not materialized: in 2020 NIPT detects only a limited number of genetic anomalies, while results have to be confirmed by amniocentesis. NIPT has become a commercial success. Nevertheless the implementation of NIPT has tended to diverge across different national settings. In countries that already have state-sponsored screening for Down risk, NIPT has been offered by the state health insurance to women defined as “high risk”, using a variant of the test that detects only three autosomal aneuploidies: trisomy 21, 13 and 18. These countries effectively regulate the supply of NIPT on grounds of cost-effectiveness and reliability. In countries without state-sponsored screening for Down risk, in contrast, multiple versions of NIPT covering a wider range of birth defects are commonly available on the free market, and purchased by women at low as well as high risk of having an affected child. Market-based healthcare systems tend to present women who can afford to pay for NIPT with a largely unregulated choice of technologies – though reimbursement rules imposed by private insurance providers may serve in effect to regulate use by those consumers who cannot afford to pay for tests from their own pockets. This regulatory divergence is shaped by the presence or absence of prior state-sponsored screening programs for Down risk. © 2022 Elsevier B.V., All rights reserved.","Löwy, I.",2022,10.1016/j.socscimed.2020.113064,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086363099&doi=10.1016%2Fj.socscimed.2020.113064&partnerID=40&md5=d7d7b2854549e2fa4d78b2fdc0ca34d0,scopus,"This article discusses the development and implementation of Non-invasive Prenatal Testing (NIPT), a diagnostic innovation based on fetal DNA analysis. It highlights how commercial interests initially drove its rollout with optimistic predictions that have not fully materialized. The implementation of NIPT varies significantly across countries, influenced by existing state-sponsored screening programs and regulatory approaches, leading to different availability and scope of testing in state-funded versus market-based healthcare systems.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:50:00.518715
3304d99c523318c7,Nonlinear Kalman Filtering in Affine Term Structure Models,"The extended Kalman filter, which linearizes the relationship between security prices and state variables, is widely used in fixed-income applications. We investigate whether the unscented Kalman filter should be used to capture nonlinearities and compare the performance of the Kalman filter with that of the particle filter. We analyze the cross section of swap rates, which are mildly nonlinear in the states, and cap prices, which are highly nonlinear. When caps are used to filter the states, the unscented Kalman filter significantly outperforms its extended counterpart. The unscented Kalman filter also performs well when compared with the much more computationally intensive particle filter. These findings suggest that the unscented Kalman filter may be a good approach for a variety of problems in fixed-income pricing.","Christoffersen, Peter; Dorion, Christian; Jacobs, Kris; Karoui, Lotfi",2014,10.1287/mnsc.2013.1870,,wos,"This paper compares the performance of the extended Kalman filter, unscented Kalman filter, and particle filter in affine term structure models. It finds that the unscented Kalman filter outperforms the extended Kalman filter, especially for highly nonlinear instruments like cap prices, and performs well compared to the particle filter. The authors suggest the unscented Kalman filter as a suitable approach for fixed-income pricing problems.",True,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:50:32.469243
c362129692c13803,Nonlinear equilibrium adjustment dynamics and predictability of the term structure of interest rates,"We analyze money market dynamics under a long-run equilibrium framework where commonly-monitored spreads serve as error correction terms, derived from a structural model incorporating autocorrelated risk premia, interest rate smoothing and monetary policy feedback. Using a dataset of monthly observations of the spot next and four-, thirteen-, twenty six- and fifty two-week Treasury Bills rates for the United States, Germany and United Kingdom from January 1999 to April 2016, we investigate the power of the expectations hypothesis theory of interest rates taking into account long-run deviations from equilibrium and inherent nonlinearities. We reveal short-run dynamic adjustments for the term structure of the USA, Germany and the UK, which are subject to regime switches. When forecastability is tested during May 2016–October 2017, the MSIH-VECM outperforms systematically the VECM. This is the first attempt to explore the possibility of parameter instability as a crucial factor in deriving the rejection of the restricted version of the cointegration space. Moreover, we investigate the dynamic out-of-sample forecasts of the term structure to assess the effectiveness of nonlinear MS-VECM modeling in capturing the after-effects of the global crisis. Overall, our results suggest that regime shifts in the mean and variance of the term structure may be intertwined with changes in fundamentals, that play a role in driving interest rate regimes, in particular business cycle and inflation fluctuations. © 2017 Elsevier B.V., All rights reserved.","Bekiros, S.; Avdoulas, C.; Hassapis, C.",2018,10.1016/j.irfa.2017.11.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037364609&doi=10.1016%2Fj.irfa.2017.11.009&partnerID=40&md5=fbbb976cfd76f77641d8202840001708,scopus,"This study analyzes the dynamics and predictability of the term structure of interest rates using a long-run equilibrium framework and a structural model. It incorporates autocorrelated risk premia, interest rate smoothing, and monetary policy feedback. The research uses monthly data for US, German, and UK Treasury Bills from 1999 to 2016 to examine the expectations hypothesis, considering long-run deviations and nonlinearities. The findings indicate short-run dynamic adjustments in the term structure, subject to regime switches. A nonlinear MS-VECM model outperforms a standard VECM in forecasting, especially after the global crisis. The study suggests that regime shifts in the term structure are linked to changes in economic fundamentals like business cycles and inflation.",True,True,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T10:51:07.096768
0b74a66f5bfe27e3,Nonlinear support vector machines can systematically identify stocks with high and low future returns,"This paper investigates the profitability of a trading strategy based on training a model to identify stocks with high or low predicted returns. A tail set is defined to be a group of stocks whose volatility-adjusted price change is in the highest or lowest quantile, for example the highest or lowest 5%. Each stock is represented by a set of technical and fundamental features computed using CRSP and Compustat data. A classifier is trained on historical tail sets and tested on future data. The classifier is chosen to be a nonlinear support vector machine (SVM) due to its simplicity and effectiveness. The SVM is trained once per month, in order to adjust to changing market conditions. Portfolios are formed by ranking stocks using the classifier output. The highest ranked stocks are used for long positions and the lowest ranked ones for short sales. The Global Industry Classification Standard is used to build a model for each sector such that a total of 8 long-short portfolios for Energy, Materials, Industrials, Consumer Discretionary, Consumer Staples, Health Care, Financials, and Information Technology are formed. The data range from 1981 to 2010. Without measuring trading costs, but using 91 day holding periods to minimize these, the strategy leads to annual excess returns (Jensen alpha) of 15% with volatilities under 8% using the top 25% of the stocks of the distribution for training long positions and the bottom 25% for the short ones. © 2015 Elsevier B.V., All rights reserved.","Huerta, R.; Corbacho, F.; Elkan, C.",2013,10.3233/af-13016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921366265&doi=10.3233%2FAF-13016&partnerID=40&md5=2deab8efb093476e3de638f71535fbd5,scopus,"This paper explores a trading strategy using nonlinear Support Vector Machines (SVMs) to predict stocks with high and low future returns. The SVM is trained monthly on historical data, using technical and fundamental features, to identify 'tail sets' (top/bottom 5% volatility-adjusted returns). Portfolios are constructed based on SVM rankings, forming long-short positions within industry sectors. The strategy, tested from 1981-2010 with 91-day holding periods, reportedly yields annual excess returns of 15% with under 8% volatility, excluding trading costs.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:51:10.563476
62ca100ade73914d,"Nonlinear term structure dependence: Copula functions, empirics, and risk implications","This paper documents nonlinear cross-sectional dependence in the term structure of US-Treasury yields and points out risk management implications. The analysis is based on a Kalman filter estimation of a two-factor affine model which specifies the yield curve dynamics. We then apply a broad class of copula functions for modeling dependence in factors spanning the yield curve. Our sample of monthly yields in the 1982-2001 period provides evidence of upper tail dependence in yield innovations; i.e., large positive interest rate shocks tend to occur under increased dependence. In contrast, the best-fitting copula model coincides with zero lower tail dependence. This asymmetry has substantial risk management implications. We give an example in estimating bond portfolio loss quantiles and report the biases which result from an application of the normal dependence model. © 2005 Elsevier B.V. All rights reserved. © 2006 Elsevier B.V., All rights reserved.","Junker, M.; Szimayer, A.; Wagner, N.",2006,10.1016/j.jbankfin.2005.05.014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645876764&doi=10.1016%2Fj.jbankfin.2005.05.014&partnerID=40&md5=32ab47b0aeff8bdd8f432aa07d529247,scopus,"This paper investigates nonlinear dependence in the term structure of US Treasury yields using a Kalman filter and copula functions. It finds evidence of upper tail dependence in yield innovations, suggesting that large positive interest rate shocks tend to occur together. This asymmetry has significant implications for risk management, particularly in estimating bond portfolio loss quantiles, and highlights biases from using normal dependence models.",True,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:51:12.370646
35cf911ff0798991,Nonlinear time‐series analysis of stock volatilities,"The absolute value of the mean‐corrected excess return is used in this paper to measure the volatility of stock returns. We apply various nonlinearity tests available in the literature to show that such volatility series are strongly nonlinear. We then explore the use of threshold autoregressive (TAR) models in describing monthly volatility series. The models built suggest that the volatility series exhibit significant lower‐order serial correlations when the volatility is large, indicating certain volatility clustering in stock returns. Out‐of‐sample forecasts are used to compare the TAR models with linear ARMA models and nonlinear GARCH and EGARCH models. Based on mean squared error and average absolute deviation, the comparisons show that (a) the TAR models consistently outperform the linear ARMA models in multi‐step ahead forecasts for large stocks, (b) the TAR models provide better forecasts than the GARCH and EGARCH models also for the volatilities of large stock returns, and (c) the EGARCH model gives the best long‐horizon volatility forecasts for small stock returns. Copyright © 1992 John Wiley & Sons, Ltd. © 2016 Elsevier B.V., All rights reserved.","Cao, C.Q.; Tsay, R.S.",1992,10.1002/jae.3950070512,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986414605&doi=10.1002%2Fjae.3950070512&partnerID=40&md5=ccecaf8680144f56020bdb3e20ff479f,scopus,"This paper analyzes stock volatilities using nonlinear time-series methods, specifically threshold autoregressive (TAR) models. It demonstrates that stock volatility series are nonlinear and exhibit volatility clustering. The study compares TAR models with linear ARMA and nonlinear GARCH/EGARCH models for out-of-sample forecasting, finding TAR models superior for large stocks, while EGARCH performs best for small stocks at long horizons.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:51:14.074678
ff621dc65c3048e4,Nonlinear trading models through Sharpe Ratio maximization,"While many trading strategies are based on price prediction, traders in financial markets are typically interested in optimizing risk-adjusted performance such as the Sharpe Ratio, rather than the price predictions themselves. This paper introduces an approach which generates a nonlinear strategy that explicitly maximizes the Sharpe Ratio. It is expressed as a neural network model whose output is the position size between a risky and a risk-free asset. The iterative parameter update rules are derived and compared to alternative approaches. The resulting trading strategy is evaluated and analyzed on both computer-generated data and real world data (DAX, the daily German equity index). Trading based on Sharpe Ratio maximization compares favorably to both profit optimization and probability matching (through cross-entropy optimization). The results show that the goal of optimizing out-of-sample risk-adjusted profit can indeed be achieved with this nonlinear approach.While many trading strategies are based on price prediction, traders in financial markets are typically interested in optimizing risk-adjusted performance such as the Sharpe Ratio, rather than the price predictions themselves. This paper introduces an approach which generates a nonlinear strategy that explicitly maximizes the Sharpe Ratio. It is expressed as a neural network model whose output is the position size between a risky and a risk-free asset. The iterative parameter update rules are derived and compared to alternative approaches. The resulting trading strategy is evaluated and analyzed on both computer-generated data and real world data (DAX, the daily German equity index). Trading based on Sharpe Ratio maximization compares favorably to both profit optimization and probability matching (through cross-entropy optimization). The results show that the goal of optimizing out-of-sample risk-adjusted profit can indeed be achieved with this nonlinear approach.",,1997,10.1142/s0129065797000410,,proquest,"This paper presents a nonlinear trading strategy using a neural network to maximize the Sharpe Ratio, optimizing risk-adjusted performance rather than just price prediction. The strategy is evaluated on simulated and real-world data (DAX) and shows favorable comparisons to profit optimization and probability matching methods, achieving optimized out-of-sample risk-adjusted profit.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:51:15.572456
335a8fbd8ef67621,Nonlinearities in the black market zloty-dollar exchange rate: Some further evidence,"This study reappraises the evidence for nonlinear dependence in the monthly black market exchange returns of the Polish zloty, 1955-1990. Predictive asymmetry is reported in conditional variance such that depreciatory shocks have a greater impact on subsequent volatility than appreciatory shocks, jointly with conditional mean nonlinearity of smooth transition between regimes which suggests a simple trading strategy capable of generating positive profit over the sample period. However, support is also found for a competing variance-in-mean model consistent with a time-varying risk premium that is able to rationalize the presence of unexploited profit opportunities, particularly over the latter half of the sample. © 2007 Elsevier B.V., All rights reserved.","McMillan, D.G.; Speight, A.E.H.",2001,10.1080/096031001750071604,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035084341&doi=10.1080%2F096031001750071604&partnerID=40&md5=167b5291532c6c9ef646e89ed01d7817,scopus,"This study examines nonlinearities in the black market zloty-dollar exchange rate from 1955-1990. It finds evidence of predictive asymmetry in conditional variance, where depreciatory shocks have a larger impact on volatility than appreciatory shocks. Additionally, conditional mean nonlinearity suggests a profitable trading strategy. A competing variance-in-mean model, consistent with a time-varying risk premium, also explains unexploited profit opportunities.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:51:17.486450
d67898e84b4d8939,Nonlinearities in the relation between the equity risk premium and the term structure,"This paper investigates the relation between the conditional expected equity risk premium and the slope of the term structure of interest rates. Theoretically, these variables are linked, the relation may be nonlinear, and negative risk premiums are consistent with equilibrium. Given these implications, we employ a nonparametric estimation technique to document the empirical relation between the risk premium and the slope of the term structure using almost two hundred years of data. Of particular interest, the risk premium is increasing in the term structure slope; however, for either small or negative slopes, the risk premium is much more sensitive to changes in interest rates. In addition, the empirical results imply negative expected equity risk premiums for some inverted term structures. Finally, variations in the risk premium do not appear to be related to variations in the variance of equity returns. We illustrate these features in a stylized consumption-based model, and provide the economic intuition behind the results. © 2017 Elsevier B.V., All rights reserved.","Boudoukh, J.; Richardson, M.; Whitelaw, R.F.",1997,10.1287/mnsc.43.3.371,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031096836&doi=10.1287%2Fmnsc.43.3.371&partnerID=40&md5=f1a811b5a39a8ccd4a9ef7c511499e9a,scopus,"This paper examines the nonlinear relationship between the equity risk premium and the term structure slope using nonparametric estimation on nearly 200 years of data. It finds that the risk premium increases with the term structure slope, but is more sensitive to interest rate changes at small or negative slopes, consistent with negative expected equity risk premiums for inverted term structures. The study also illustrates these findings in a stylized consumption-based model.",True,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:51:49.159488
9d6021375ec7e4f4,Nonparametric long term prediction of stock returns with generated bond yields,"Recent empirical approaches in forecasting equity returns or premiums found that dynamic interactions among the stock and bond are relevant for long term pension products. Automatic procedures to upgrade or downgrade risk exposure could potentially improve long term performance for such products. The risk and return of bonds is more easy to predict than the risk and return of stocks. This and the well known stock-bond correlation motivates the inclusion of the current bond yield in a model for the prediction of excess stock returns. Here, we take the actuarial long term view using yearly data, and focus on nonlinear relationships between a set of covariates. We employ fully nonparametric models and apply for estimation a local-linear kernel smoother. Since the current bond yield is not known, it is predicted in a prior step. The structure imposed this way in the final estimation process helps to circumvent the curse of dimensionality and reduces bias in the estimation of excess stock returns. Our validated stock prediction results show that predicted bond returns improve stock prediction significantly. © 2018 Elsevier B.V., All rights reserved.","Scholz, M.; Sperlich, S.; Nielsen, J.P.",2016,10.1016/j.insmatheco.2016.04.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969264273&doi=10.1016%2Fj.insmatheco.2016.04.007&partnerID=40&md5=17bf2ebd89d43d3a897d4e7079a14d3f,scopus,"This paper proposes a nonparametric model using local-linear kernel smoothing to predict excess stock returns by incorporating generated bond yields. The model leverages the predictability of bond returns and the stock-bond correlation to improve long-term stock market forecasting, especially for pension products. A two-step approach predicts bond yields first, then uses these predictions to estimate stock returns, mitigating the curse of dimensionality and bias.",True,True,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T10:52:19.203810
c2c226c46dd988d9,Nonparametric modeling and analysis of association between huntington's disease onset and CAG repeats,"Huntington's disease (HD) is a neurodegenerative disorder with a dominant genetic mode of inheritance caused by an expansion of CAG repeats on chromosome 4. Typically, a longer sequence of CAG repeat length is associated with increased risk of experiencing earlier onset of HD. Previous studies of the association between HD onset age and CAG length have favored a logistic model, where the CAG repeat length enters the mean and variance components of the logistic model in a complex exponential-linear form. To relax the parametric assumption of the exponential-linear association to the true HD onset distribution, we propose to leave both mean and variance functions of the CAG repeat length unspecified and perform semiparametric estimation in this context through a local kernel and backfitting procedure. Motivated by including family history of HD information available in the family members of participants in the Cooperative Huntington's Observational Research Trial (COHORT), we develop the methodology in the context of mixture data, where some subjects have a positive probability of being risk free. We also allow censoring on the age at onset of disease and accommodate covariates other than the CAG length. We study the theoretical properties of the proposed estimator and derive its asymptotic distribution. Finally, we apply the proposed methods to the COHORT data to estimate the HD onset distribution using a group of study participants and the disease family history information available on their family members. © 2013 John Wiley & Sons, Ltd. © 2013 John Wiley & Sons, Ltd. © 2014 Elsevier B.V., All rights reserved.","Ma, Y.; Wang, Y.",2014,10.1002/sim.5971,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896717351&doi=10.1002%2Fsim.5971&partnerID=40&md5=8093cb412464361671de561844e3824e,scopus,"This study proposes a semiparametric method using local kernel and backfitting to model the association between Huntington's disease onset and CAG repeat length, relaxing previous parametric assumptions. It incorporates family history, allows for risk-free individuals, handles censored data, and includes other covariates. The method is applied to the COHORT data.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:52:22.558427
3b332c3efa34f2b7,"Nutritional value, elemental bioaccumulation and antioxidant activity of fruiting bodies and mycelial cultures of an unrecorded wild Lactarius hatsudake from Nanyue mountainous region in China","An unrecorded wild mushroom Lactarius hatsudake from Nanyue mountainous region in China was identified. Subsequently, comparative investigation on the nutritional value, elemental bioaccumulation, and antioxidant activity was performed in the fruiting body (FB) and mycelium (MY) samples of this species. It revealed that the contents of moisture (87.66 ± 0.16 g/100 g fw) and ash (6.97 ± 0.16 g/100 g dw) were significantly higher in FB, and the total carbohydrate, fat, and protein concentrations of FB were similar to those in MY. Among nutritionally important elements, FB possessed higher concentrations of potassium (37808.61 ± 1237.38 mg/kg dw), iron (470.69 ± 85.54 mg/kg dw), and zinc (136.13 ± 5.16 mg/kg dw), whereas MY was a better source of magnesium (1481.76 ± 18.03 mg/kg dw), calcium (2203.87 ± 69.61 mg/kg dw), and sodium (277.44 ± 22.93 mg/kg dw). According to the health risk estimation, FB might pose an aluminum-related health problem when a prolonged period of exposure, while MY was risk-free for consumers. The results of antioxidant capacity (1,1-diphenyl-2-picrylhydrazyl (DPPH) and 2,2'-Azino-bis (3-ethylbenzothiazoline-6-sulfonic acid) diammonium salt (ABTS) assays) in FB and MY were within the range of 104.19 ± 5.70 mg ascorbic acid equivalents (AAE)/g to 169.50 ± 4.94 mg AAE/g, and half maximal effective concentration EC50 values ranged from 0.23 ± 0.01 mg/mL to 0.62 ± 0.05 mg/mL. The aqueous extracts of MY demonstrated a strong ABTS radical scavenging capacity with the highest AAE value.An unrecorded wild mushroom Lactarius hatsudake from Nanyue mountainous region in China was identified. Subsequently, comparative investigation on the nutritional value, elemental bioaccumulation, and antioxidant activity was performed in the fruiting body (FB) and mycelium (MY) samples of this species. It revealed that the contents of moisture (87.66 ± 0.16 g/100 g fw) and ash (6.97 ± 0.16 g/100 g dw) were significantly higher in FB, and the total carbohydrate, fat, and protein concentrations of FB were similar to those in MY. Among nutritionally important elements, FB possessed higher concentrations of potassium (37808.61 ± 1237.38 mg/kg dw), iron (470.69 ± 85.54 mg/kg dw), and zinc (136.13 ± 5.16 mg/kg dw), whereas MY was a better source of magnesium (1481.76 ± 18.03 mg/kg dw), calcium (2203.87 ± 69.61 mg/kg dw), and sodium (277.44 ± 22.93 mg/kg dw). According to the health risk estimation, FB might pose an aluminum-related health problem when a prolonged period of exposure, while MY was risk-free for consumers. The results of antioxidant capacity (1,1-diphenyl-2-picrylhydrazyl (DPPH) and 2,2'-Azino-bis (3-ethylbenzothiazoline-6-sulfonic acid) diammonium salt (ABTS) assays) in FB and MY were within the range of 104.19 ± 5.70 mg ascorbic acid equivalents (AAE)/g to 169.50 ± 4.94 mg AAE/g, and half maximal effective concentration EC50 values ranged from 0.23 ± 0.01 mg/mL to 0.62 ± 0.05 mg/mL. The aqueous extracts of MY demonstrated a strong ABTS radical scavenging capacity with the highest AAE value.",,2023,10.1016/j.foodres.2023.113358,,proquest,"This study identifies an unrecorded wild mushroom, Lactarius hatsudake, from China and compares the nutritional value, elemental bioaccumulation, and antioxidant activity of its fruiting bodies (FB) and mycelium (MY). FB had higher moisture and ash content, while nutrient concentrations were similar. FB showed higher levels of potassium, iron, and zinc, whereas MY was richer in magnesium, calcium, and sodium. FB consumption might pose an aluminum-related health risk, unlike MY. Both FB and MY exhibited antioxidant activity, with MY extracts showing stronger ABTS radical scavenging capacity.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:52:23.960410
e0d700e7e0724a0d,Oil price dynamics and speculation: A multivariate financial approach,"This paper assesses empirically whether speculation affects oil price dynamics. The growing presence of financial operators in the oil markets has led to the diffusion of trading techniques based on extrapolative expectations. Strategies of this kind foster feedback trading that may cause considerable departures of prices from their fundamental values. We investigate this hypothesis using a modified CAPM following Shiller (1984) and Sentana and Wadhwani (1992). First, a univariate GARCH(1,1)-M is estimated assuming the risk premium to be a function of the conditional oil price volatility. The single factor model, however, is outperformed by the multifactor ICAPM (Merton, 1973), which takes into account a larger investment opportunity set. Analysis is then carried out using a trivariate CCC GARCH-M model with complex nonlinear conditional mean equations where oil price dynamics are associated with both stock market and exchange rate behavior. We find strong evidence that oil price shifts are negatively related to stock price and exchange rate changes and that a complex web of time-varying first and second order conditional moment interactions affects both the CAPM and feedback trading components of the model. Despite the difficulties, we identify a significant role played by speculation in the oil market, which is consistent with the observed large daily upward and downward shifts in prices - a clear evidence that it is not a fundamental-driven market. Thus, from a policy point of view - given the impact of volatile oil prices on global inflation and growth - actions that monitor speculative activities on commodity markets more effectively are to be welcomed. [Copyright Elsevier B.V.]",,2010,10.1016/j.eneco.2009.08.014,,proquest,"This paper empirically investigates the impact of speculation on oil price dynamics using a multivariate financial approach. It employs modified CAPM and GARCH-M models, including a trivariate CCC GARCH-M model incorporating stock market and exchange rate behavior. The findings suggest that speculation plays a significant role, leading to price volatility not solely driven by fundamentals, and advocates for enhanced monitoring of speculative activities in commodity markets.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:52:25.790773
0a36a7f19f6f627b,On LASSO for predictive regression,"Explanatory variables in a predictive regression typically exhibit low signal strength and various degrees of persistence. Variable selection in such a context is of great importance. In this paper, we explore the pitfalls and possibilities of the LASSO methods in this predictive regression framework. In the presence of stationary, local unit root, and cointegrated predictors, we show that the adaptive LASSO cannot asymptotically eliminate all cointegrating variables with zero regression coefficients. This new finding motivates a novel post-selection adaptive LASSO, which we call the twin adaptive LASSO (TAlasso), to restore variable selection consistency. Accommodating the system of heterogeneous regressors, TAlasso achieves the well-known oracle property. In contrast, conventional LASSO fails to attain coefficient estimation consistency and variable screening in all components simultaneously. We apply these LASSO methods to evaluate the short- and long-horizon predictability of S&P 500 excess returns.",,2022,10.1016/j.jeconom.2021.02.002,,proquest,"This paper investigates the use of LASSO methods for predictive regression, particularly when explanatory variables have low signal strength and persistence. It introduces a novel method, the twin adaptive LASSO (TAlasso), to improve variable selection consistency in the presence of stationary, local unit root, and cointegrated predictors. The study applies these methods to evaluate the predictability of S&P 500 excess returns.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:52:30.237057
01c47e420fd781a1,On a constrained mixture vector autoregressive model,"A mixture vector autoregressive model has recently been introduced to the literature. Although this model is a promising candidate for nonlinear multiple time series modeling, high dimensionality of the parameters and lack of method for computing the standard errors of estimates limit its application to real data. The contribution of this paper is threefold. First, a form of parameter constraints is introduced with an efficient EM algorithm for estimation. Second, an accurate method for computing standard errors is presented for the model with and without parameter constraints. Lastly, a hypothesis-testing approach based on likelihood ratio tests is proposed, which aids in the selection of unnecessary parameters and leads to the greater efficiency at the estimation. A case study employing U.S. Treasury constant maturity rates illustrates the applicability of the mixture vector autoregressive model with parameter constraints, and the importance of using a reliable method to compute standard errors. © 2013 IMACS. Published by Elsevier B.V. All rights reserved. © 2017 Elsevier B.V., All rights reserved.","Wong, C.S.",2013,10.1016/j.matcom.2013.05.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027926931&doi=10.1016%2Fj.matcom.2013.05.001&partnerID=40&md5=5e71ec085d88b327a249856779217765,scopus,"This paper introduces a constrained mixture vector autoregressive (MVAR) model to address limitations of existing MVAR models, such as high dimensionality and lack of standard error computation methods. It proposes an EM algorithm for estimation with parameter constraints, an accurate method for computing standard errors, and a likelihood ratio test for parameter selection. The model's applicability is demonstrated using U.S. Treasury constant maturity rates.",True,True,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T10:52:56.248031
2a9a49ec44da48fb,On multicollinearity and the value of the shape parameter in the term structure Nelson-Siegel model,"This paper investigates the sensitivity of the dynamic Nelson-Siegel factor loadings to the value of the shape parameter, λ. It also analyses the multicollinearity problem and addresses how to mitigate this issue in the estimation process. First, we find that the selection of a fixed λ is not optimal due to the collinearity problems. Second, we observe a substantial difference between the forecasting performance of the traditional estimation procedures and that of the ridge regression approach. Finally, we implement a Monte Carlo simulation exercise in order to study the statistical distribution of the estimates of the model parameters and thus determine the extent to which they differ from the real values. Furthermore, we find that multicollinearity between the factors of the NS model can, in the case of ordinary least squares estimation with a fixed parameter λ, result in greater differences between the estimates and the actual parameter values. Ridge regression corrects such differences and produces more stable estimates than the ordinary linear and nonlinear least squares methods.",,2018,10.5605/ieb.16.1,,proquest,"This paper examines how the shape parameter (λ) affects the dynamic Nelson-Siegel factor loadings and addresses multicollinearity in the model. It finds that fixing λ is suboptimal and that ridge regression outperforms traditional methods in forecasting. A Monte Carlo simulation confirms that multicollinearity leads to less stable estimates with OLS, which ridge regression mitigates.",False,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:53:05.556637
c2c3870c724130ae,"On the construction of monthly term structures of U.S. interest rates, 1919-1930","This paper presents the methodology used to construct reliable estimates of the term structure of interest rates for the United States during 1919-1930. These monthly term structures are based on individual corporate bonds' price quotations for the majority of U.S. railroad corporations' issues of that era. McCulloch's cubic spline methodology, coupled with Nelson and Siegel's parsimonious estimator, is used to derive curves for three investment-grade risk classes. These estimates compare favorably with Durand's hand-smoothed estimates as well as earlier annual estimates generated by Thies. They provide a consistent basis for a wide range of monetary and financial research on this period. © 1992 Kluwer Academic Publishers. © 2007 Elsevier B.V., All rights reserved.","Baum, C.F.; Thies, C.F.",1992,10.1007/bf00426761,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34249832297&doi=10.1007%2FBF00426761&partnerID=40&md5=29e06ba6e17845fda2889a492cb90335,scopus,"This paper details the construction of monthly U.S. interest rate term structures from 1919-1930, utilizing corporate bond prices and a combination of McCulloch's cubic spline and Nelson-Siegel methodologies. The resulting curves for three investment-grade risk classes are validated against existing estimates and offer a consistent dataset for financial research.",True,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:53:16.176437
84a97f8c3afab555,On the role of liquidity in emerging markets stock prices,"This paper investigates the impact of liquidity on emerging markets' stock prices. Particular attention is given to the estimation of Jensen's alpha and the quantity of risk. Our empirical analysis gives rise to two main issues. The first is related to the presence of an extra premium, i.e. ""alpha puzzle"". The second is the time-varying component of the quantity of risk, i.e. ""beta puzzle"". We find that local liquidity factors do not explain the presence of positive and statistically significant alphas. This puzzle is solved by means of transaction costs. In addition, we show that global liquidity factors, such as VIX and Open Interest, statistically affect the market price of risk. Our empirical finding proves the time varying nature of the global risk factors. Finally, we argue that standard asset pricing models cannot solve the two puzzles simultaneously. © 2012 University of Venice. © 2012 Elsevier B.V., All rights reserved.","Donadelli, M.; Prosperi, L.",2012,10.1016/j.rie.2012.06.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867668206&doi=10.1016%2Fj.rie.2012.06.001&partnerID=40&md5=c2b54628c2c2ef4bebe943d2271295e9,scopus,"This paper examines how liquidity affects stock prices in emerging markets, focusing on Jensen's alpha and risk quantity. It addresses the 'alpha puzzle' (excess premium) and 'beta puzzle' (time-varying risk). The study finds that transaction costs explain the alpha puzzle, while global liquidity factors like VIX and Open Interest influence the market price of risk, demonstrating their time-varying nature. Standard asset pricing models struggle to resolve both puzzles simultaneously.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:53:17.271880
8c483dd1af8aab26,On the specification of the drift and diffusion functions for continuous-time models of the spot interest rate,This paper explores the specification of drift and diffusion functions for continuous-time short-term interest rate models. Various forms for the drift and diffusion of 7-day Eurodollar rates are proposed and then estimated by discrete maximum-likelihood. The results suggest that a nonparametric specification of drift and volatility in terms of orthogonal polynomial expansions is effective in eliminating problems of parameter identification encountered previously. Some evidence is found to support the claim that the drift of the short term interest rate is nonlinear.,"Hurn, AS; Lindsay, KA",2002,10.1111/1468-0084.00277,,wos,This paper investigates the specification of drift and diffusion functions for continuous-time models of short-term interest rates. It proposes and estimates various forms for the drift and diffusion of 7-day Eurodollar rates using discrete maximum-likelihood. The findings indicate that a nonparametric specification using orthogonal polynomial expansions effectively addresses parameter identification issues and suggests a nonlinear drift for short-term interest rates.,True,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:53:19.529456
e4db19deb8251edb,On the time-varying relation between monetary policy uncertainty and bond risk premia,"This paper examines the time-varying relationship between monetary policy uncertainty (MPU) and bond excess returns. To do so, we introduce a nonparametric time-varying coefficient predictive regression model for bond returns, and employ a kernel-based two-step method to estimate the time-varying coefficients. Next, we apply the methodologies to analyze the dynamic forecasting relationship between zero-coupon bond returns and MPU from 1985 to 2022. We find that MPU significantly and positively predicts bond returns in over 75% of the sample period, with the strongest effect observed in 2005. Thus, the expectations hypothesis is only transiently valid. After controlling for the shape of the yield curve, MPU still retains its ability to predict bond returns in 50% to 80% of the sample period. Our conclusions are robust to the so-called embedded endogeneity. Additionally, we find that bond excess returns are less responsive to MPU during periods of high economic activities and are more responsive during periods of low economic activities.","Li, Luyang; Yin, Ximing; Yu, Deshui",2025,10.1016/j.irfa.2025.104465,,wos,"This paper investigates the dynamic relationship between monetary policy uncertainty (MPU) and bond excess returns using a nonparametric time-varying coefficient model. The study finds that MPU significantly predicts bond returns for a substantial portion of the 1985-2022 period, with varying effects depending on economic activity levels. The findings suggest the expectations hypothesis is only temporarily valid.",True,True,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T10:53:42.585763
fe0b869162d09c8f,One idea of portfolio risk control for absolute return strategy risk adjustments by signals from correlation behavior,"Absolute return strategy provided from fund of funds (FOFs) investment schemes is the focus in Japanese Financial Community. FOFs investment mainly consists of hedge fund investment and it has two major characteristics which are low correlation against benchmark index and little impact from various external changes in the environment given maximizing return. According to the historical track record of survival hedge funds in this business world, they maintain a stable high return and low risk. However, one must keep in mind that low risk would not be equal to risk free. The failure of Long-term capital management (LTCM) that took place in the summer of 1998 was a symbolized phenomenon. The summer of 1998 exhibited a certain limitation of traditional value at risk (VaR) and some possibility that traditional VaR could be ineffectual to the nonlinear type of fluctuation in the market. In this paper, I try to bring self-organized criticality (SOC) into portfolio risk control. SOC would be well known as a model of decay in the natural world. I analyzed nonlinear type of fluctuation in the market as SOC and applied SOC to capture complicated market movement using threshold point of SOC and risk adjustments by scenario correlation as implicit signals. Threshold becomes the control parameter of risk exposure to set downside floor and forecast extreme nonlinear type of fluctuation under a certain probability. Simulation results would show synergy effect of portfolio risk control between SOC and absolute return strategy. © 2001 Elsevier Science B.V. All rights reserved. © 2004 Elsevier Science B.V., Amsterdam. All rights reserved.","Nishiyama, N.",2001,10.1016/s0378-4371(01)00411-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035576026&doi=10.1016%2FS0378-4371%2801%2900411-3&partnerID=40&md5=d03382ca37a1fbaacc14f9cfe5da4686,scopus,"This paper proposes using self-organized criticality (SOC) to manage portfolio risk for absolute return strategies, particularly in light of the limitations of traditional Value at Risk (VaR) demonstrated by events like the Long-Term Capital Management failure. The authors analyze nonlinear market fluctuations as SOC and apply it to capture market movements using a threshold point and scenario correlation signals. Simulations suggest a synergistic effect between SOC and absolute return strategies for risk control.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:53:44.826991
7cae5492184c7382,Online Investor Sentiment via Machine Learning,"In this paper, we propose utilizing machine learning methods to determine the expected aggregated stock market risk premium based on online investor sentiment and employing the multifold forward-validation method to select the relevant hyperparameters. Our empirical studies provide strong evidence that some machine learning methods, such as extreme gradient boosting or random forest, show significant predictive ability in terms of their out-of-sample performances with high-dimensional investor sentiment proxies. They also outperform the traditional linear models, which shows a possible unobserved nonlinear relationship between online investor sentiment and risk premium. Moreover, this predictability based on online investor sentiment has a better economic value, so it improves portfolio performance for investors who need to decide the optimal asset allocation in terms of the certainty equivalent return gain and the Sharpe ratio.",,2024,10.3390/math12203192,,proquest,"This paper proposes using machine learning (ML) methods, specifically extreme gradient boosting and random forest, to predict stock market risk premium based on online investor sentiment. The ML models demonstrated significant out-of-sample predictive ability, outperforming traditional linear models and showing potential economic value for portfolio performance.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:53:49.531379
a6849cec0ad130d5,"Option-implied preferences adjustments, density forecasts, and the equity risk premium","The main objective of this paper is to analyse the value of information contained in prices of options on the IBEX 35 index at the Spanish Stock Exchange Market. The forward looking information is extracted using implied risk-neutral density functions estimated by a mixture of two-lognormals and several alternative risk adjustments. Our results show that, between October 1996 and March 2000, we can reject the hypothesis that the risk-neutral densities provide accurate predictions of the distributions of future realisations of the IBEX 35 index at 4- and 8-week horizons. When forecasting through risk-adjusted densities the performance of this period is statistically improved and we no longer reject that hypothesis. We show that risk adjustments based on a power specification for the stochastic discount factor-which is the approach used so far in the literature that derives the objective density function from option prices- generates an excessive volatility of risk premia. We use alternative risk adjustments and find that the forecasting performance of the distribution improves slightly in some cases when risk aversion is allowed to be time-varying. Finally, from October 1996 to December 2004, the ex-ante risk premium perceived by investors and that are embedded in option prices is between 12 and 18% higher than the premium required to compensate the same investors for the realised volatility in stock market returns.","Alonso, Francisco; Blanco, Roberto; Rubio, Gonzalo",2009,10.1007/s10108-008-9049-3,,wos,"This paper analyzes the information in IBEX 35 index options to extract implied risk-neutral densities. It finds that these densities do not accurately predict future index distributions but that risk adjustments improve forecasting performance. The study also critiques existing risk adjustment methods and explores time-varying risk aversion, concluding that implied risk premiums are higher than those compensating for realized volatility.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:53:51.053050
9c03ca737f0a4eb9,Out-of-Sample Predictability of the Equity Risk Premium,"A large set of macroeconomic variables have been suggested as equity risk premium predictors in the literature. Acknowledging the different predictability of the equity premium in expansions and recessions, this paper proposes an approach that combines equity premium forecasts from two-state regression models using an agreement technical indicator as the observable state variable. A comprehensive out-of-sample forecast evaluation exercise based on statistical and economic loss functions demonstrates the superiority of the proposed approach versus combined forecasts from linear models or Markov switching models and forecasts from machine learning methods such as random forests and gradient boosting. The parsimonious state-dependent aspect of risk premium forecasts delivers large improvements in forecast accuracy. The results are robust to sub-period analyses and different investors’ risk aversion levels.",,2025,10.3390/math13020257,,proquest,"This paper proposes a novel approach to forecasting the equity risk premium by combining forecasts from two-state regression models, using an agreement technical indicator as the observable state variable. This method demonstrates superior out-of-sample performance compared to linear models, Markov switching models, and machine learning techniques like random forests and gradient boosting, showing significant improvements in forecast accuracy, particularly in a state-dependent manner. The findings are robust across different sub-periods and investor risk aversion levels.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:53:54.586163
b9e208171342ca97,Out-of-sample forecasts and nonlinear model selection with an example of the term structure of interest rates,"It is well known that goodness-of-fit measures lead to overfilling. We compare the small-sample properties of linear and several nonlinear models using a Monte Carlo study. A large number of linear series are generated and conventional methods of fitting nonlinear models are applied to each. The best linear and nonlinear models are compared using in-sample and out-of-sample criteria. Out-of-sample forecasts are shown to be superior for selecting the proper specification. The experiment is repeated using a nonlinear model and the in-sample fit and forecasts of the various models are compared. An example is provided using the term structure of interest rates. © 2018 Elsevier B.V., All rights reserved.","Liu, Y.; Enders, W.",2003,10.2307/1061692,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037280360&doi=10.2307%2F1061692&partnerID=40&md5=d77d1d9208071e7118c7356f32f054cf,scopus,"This paper compares linear and nonlinear models for forecasting, using Monte Carlo simulations and an example of the term structure of interest rates. It demonstrates that out-of-sample forecasts are superior for model specification selection compared to in-sample measures. The study highlights the issue of overfitting with goodness-of-fit measures and provides evidence for the effectiveness of nonlinear models in certain scenarios.",True,True,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T10:55:09.460561
9e28d710498e479b,PHANGS-JWST First Results: Dust-embedded Star Clusters in NGC 7496 Selected via 3.3 μm PAH Emission,"The earliest stages of star formation occur enshrouded in dust and are not observable in the optical. Here we leverage the extraordinary new high-resolution infrared imaging from JWST to begin the study of dust-embedded star clusters in nearby galaxies throughout the Local Volume. We present a technique for identifying dust-embedded clusters in NGC 7496 (18.7 Mpc), the first galaxy to be observed by the PHANGS-JWST Cycle 1 Treasury Survey. We select sources that have strong 3.3 mu m PAH emission based on a F300M - F335M color excess and identify 67 candidate embedded clusters. Only eight of these are found in the PHANGS-HST optically selected cluster catalog, and all are young (six have SED fit ages of similar to 1 Myr). We find that this sample of embedded cluster candidates may significantly increase the census of young clusters in NGC 7496 from the PHANGS-HST catalog; the number of clusters younger than similar to 2 Myr could be increased by a factor of 2. Candidates are preferentially located in dust lanes and are coincident with the peaks in the PHANGS-ALMA CO (2-1) maps. We take a first look at concentration indices, luminosity functions, SEDs spanning from 2700 angstrom to 21 mu m, and stellar masses (estimated to be between similar to 10(4) and 10(5) M (circle dot)). The methods tested here provide a basis for future work to derive accurate constraints on the physical properties of embedded clusters, characterize the completeness of cluster samples, and expand analysis to all 19 galaxies in the PHANGS-JWST sample, which will enable basic unsolved problems in star formation and cluster evolution to be addressed.","Rodriguez, M. Jimena; Lee, Janice C.; Whitmore, B. C.; Thilker, David A.; Maschmann, Daniel; Chandar, Rupali; Deger, Sinan; Boquien, Mederic; Dale, Daniel A.; Larson, Kirsten L.; Williams, Thomas G.; Kim, Hwihyun; Schinnerer, Eva; Rosolowsky, Erik; Leroy, Adam K.; Emsellem, Eric; Sandstrom, Karin M.; Kruijssen, J. M. Diederik; Grasha, Kathryn; Watkins, Elizabeth J.; Barnes, Ashley. T.; Sormani, Mattia C.; Kim, Jaeyeon; Anand, Gagandeep S.; Chevance, Melanie; Bigiel, F.; Klessen, Ralf S.; Hassani, Hamid; Liu, Daizhong; Faesi, Christopher M.; Cao, Yixian; Belfiore, Francesco; Pessa, Ismael; Kreckel, Kathryn; Groves, Brent; Pety, Jerome; Indebetouw, Remy; Egorov, Oleg V.; Blanc, Guillermo A.; Saito, Toshiki; Hughes, Annie",2023,10.3847/2041-8213/aca653,,wos,"This study uses JWST infrared imaging to identify dust-embedded star clusters in NGC 7496 by analyzing 3.3 μm PAH emission. The technique identified 67 candidate clusters, significantly increasing the census of young clusters compared to previous optical catalogs. These candidates are often found in dust lanes and CO emission peaks. The study also examines cluster properties like concentration, luminosity, SEDs, and stellar masses, laying the groundwork for future analysis of embedded clusters in star formation and evolution.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:55:11.282138
80d155e7f0df34e2,PREDICTING STOCK RETURNS AND VOLATILITY WITH INVESTOR SENTIMENT INDICES: A RECONSIDERATION USING A NONPARAMETRIC CAUSALITY-IN-QUANTILES TEST,"Evidence of monthly stock returns predictability based on popular investor sentiment indices, namely SBW and SPLS as introduced by Baker and Wurgler (2006, 2007) and Huang et al. (2015) respectively are mixed. While, linear predictive models show that only SPLS can predict excess stock returns, nonparametric models (which accounts for misspecification of the linear frameworks due to nonlinearity and regime changes) finds no evidence of predictability based on either of these two indices for not only stock returns, but also its volatility. However, in this paper, we show that when we use a more general nonparametric causality-in-quantiles model of Balcilar et al., (forthcoming), in fact, both SBW and SPLS can predict stock returns and its volatility, with SPLS being a relatively stronger predictor of excess returns during bear and bull regimes, and SBW being a relatively powerful predictor of volatility of excess stock returns, barring the median of the conditional distribution. © 2018 Elsevier B.V., All rights reserved.","Balcilar, M.; Gupta, R.; Kyei, C.",2018,10.1111/boer.12119,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018969715&doi=10.1111%2Fboer.12119&partnerID=40&md5=ad35656b372c4c8565d3b24a65a4d8f0,scopus,"This paper investigates the predictability of stock returns and volatility using investor sentiment indices (SBW and SPLS). While previous linear and some nonparametric models yielded mixed or no results, this study employs a more general nonparametric causality-in-quantiles model. The findings indicate that both SBW and SPLS can predict stock returns and volatility, with SPLS showing stronger prediction for excess returns in different market regimes and SBW predicting volatility, except for the median.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:55:15.812692
93fd092bafa0fe9d,PROPERTY AND NUMERICAL SIMULATION OF THE AIT-SAHALIA-RHO MODEL WITH NONLINEAR GROWTH CONDITIONS,"The Ait-Sahalia-Rho model is an important tool to study a number of financial problems, including the term structure of interest rate. However, since the functions of this model do not satisfy the linear growth condition, we cannot study the properties for the solution of this model by using the traditional techniques. In this paper we overcome the mathematical difficulties due to the nonlinear growth condition by using numerical simulation. Thus we first discuss analytical properties of the model and the convergence property of numerical solutions in probability for the Ait-Sahalia-Rho model. Finally, an example for option pricing is given to illustrate that the numerical solution is an effective method to estimate the expected payoffs.","Jiang, Feng; Yang, Hua; Tian, Tianhai",2017,10.3934/dcdsb.2017005,,wos,"This paper analyzes the Ait-Sahalia-Rho model, commonly used for interest rate term structure, overcoming mathematical challenges posed by its nonlinear growth conditions through numerical simulation. It discusses analytical and convergence properties of the model's solutions and provides an example of option pricing to demonstrate the effectiveness of numerical estimation for expected payoffs.",False,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:55:29.658520
9a9d7a03be6464dd,Pairs selection and outranking: An application to the S&P 100 index,Pairs trading is a popular quantitative speculation strategy. This article proposes a general and flexible framework for pairs selection. The method uses multiple return forecasts based on bivariate information sets and multi-criteria decision techniques. Our approach can be seen as a sort of forecast combination but the output of the method is a ranking. It helps to detect potentially under- and overvalued stocks. A first application with S&P 100 index stocks provides promising results in terms of excess return and directional forecasting. (C) 2008 Elsevier B.V. All rights reserved.,"Huck, Nicolas",2009,10.1016/j.ejor.2008.03.025,,wos,"This article proposes a general framework for pairs selection in pairs trading, utilizing multiple return forecasts from bivariate information sets and multi-criteria decision techniques. The method generates a ranking to identify potentially under- and overvalued stocks, and an application to the S&P 100 index shows promising results for excess return and directional forecasting.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:55:30.619549
f7928e8fe95d86ae,"Particle filtering, learning, and smoothing for mixed-frequency state-space models","A particle filter approach for general mixed-frequency state-space models is considered. It employs a backward smoother to filter high-frequency state variables from low-frequency observations. Moreover, it preserves the sequential nature of particle filters, allows for non-Gaussian shocks and nonlinear state-measurement relation, and alleviates the concern over sample degeneracy. Simulation studies show that it outperforms the commonly used state-augmented approach for mixed-frequency data for filtering and smoothing. In an empirical exercise, predictive mixed-frequency regressions are employed for Treasury bond and US dollar index returns with quarterly predictors and monthly stochastic volatility. Stochastic volatility improves model inference and forecasting power in a mixed-frequency setup but not for quarterly aggregate models. © 2019 Elsevier B.V., All rights reserved.","Leippold, M.; Yang, H.",2019,10.1016/j.ecosta.2019.07.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070197669&doi=10.1016%2Fj.ecosta.2019.07.001&partnerID=40&md5=4f3f599d8db72b3c54cacc8133757f86,scopus,"This paper proposes a particle filter method for mixed-frequency state-space models, utilizing a backward smoother to handle high-frequency state variables with low-frequency observations. The method supports non-Gaussian shocks and nonlinear relationships, and it is shown to outperform the state-augmented approach in simulations. An empirical application to Treasury bond and US dollar index returns demonstrates that stochastic volatility improves inference and forecasting in a mixed-frequency context, but not in quarterly aggregate models.",True,True,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T10:55:37.871701
7b3f1098ac1d9f11,Penalized Averaging of Parametric and Non-Parametric Quantile Forecasts,"We propose a hybrid penalized averaging for combining parametric and non-parametric quantile forecasts when faced with a large number of predictors. This approach goes beyond the usual practice of combining conditional mean forecasts from parametric time series models with only a few predictors. The hybrid methodology adopts the adaptive LASSO regularization to simultaneously reduce predictor dimension and obtain quantile forecasts. Several recent empirical studies have considered a large set of macroeconomic predictors and technical indicators with the goal of forecasting the S&P 500 equity risk premium. To illustrate the merit of the proposed approach, we extend the mean-based equity premium forecasting into the conditional quantile context. The application offers three main findings. First, combining parametric and non-parametric approaches adds quantile forecast accuracy over and above the constituent methods. Second, a handful of macroeconomic predictors are found to have systematic forecasting power. Third, different predictors are identified as important when considering lower, central and upper quantiles of the equity premium distribution. © 2020 Elsevier B.V., All rights reserved.","De Gooijer, J.G.; Zerom, D.",2020,10.1515/jtse-2019-0021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077739820&doi=10.1515%2Fjtse-2019-0021&partnerID=40&md5=55add6393cbc0da4186bc41ac893f6c4,scopus,"This paper introduces a hybrid penalized averaging method to combine parametric and non-parametric quantile forecasts, particularly useful when dealing with a large number of predictors. It utilizes adaptive LASSO regularization for dimension reduction and quantile forecasting. The method is applied to forecast the S&P 500 equity risk premium, demonstrating that combining parametric and non-parametric methods improves accuracy, identifying key macroeconomic predictors, and revealing that different predictors are important for different quantiles of the equity premium distribution.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:55:42.858666
2614c15f385e0a14,Positive forward rates in the maximum smoothness framework,In this paper we present a nonlinear dynamic programming algorithm for the computation of forward rates within the maximum smoothness framework. The algorithm implements the forward rate positivity constraint for a one-parametric family of smoothness measures and it handles price spreads in the constraining data set. We investigate the outcome of the algorithm using the Swedish Bond market showing examples where the absence of the positive constraint leads to negative interest rates. Furthermore we investigate the predictive accuracy of the algorithm as we move along the family of smoothness measures. Among other things we observe that the inclusion of spreads not only improves the smoothness of forward curves but also significantly reduces the predictive error.,"Manzano, J; Blomvall, J",2004,10.1088/1469-7688/4/2/011,,wos,"This paper introduces a nonlinear dynamic programming algorithm to compute forward rates within the maximum smoothness framework, incorporating a positivity constraint for forward rates and handling price spreads. The algorithm is tested on the Swedish Bond market, demonstrating that the absence of the positivity constraint can lead to negative interest rates. The study also examines the predictive accuracy across different smoothness measures, finding that including spreads improves curve smoothness and reduces predictive error.",True,True,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T10:56:03.177282
8a75482548bbd8de,Post-deregulation bank-deposit-rate pricing: The multivariate dynamics,"The relationship between wholesale and retail interest rates since deregulation is of substantial interest to economists and policymakers, because the predictability of the monetary aggregates and their relationship to bank reserves depend on adjustment patterns in the wholesale and retail money markets. We provide evidence on the nature of wholesale-retail interest rate relationships by examining the dynamic interactions among two wholesale interest rates (federal funds and six-month treasury bills) and three retail deposit rates (six-month consumer certificates of deposit, money market deposit accounts, and super NOW’s). We perform a multivariate time series analysis, with particular attention paid to causal patterns and the shapes of impulse- response functions. A number of stylized facts, related to size of adjustment, speed of adjustment, and pattern of adjustment, are established for the response of retail rates to unanticipated shocks in wholesale rates. © 1990 American Statistical Association. © 2016 Elsevier B.V., All rights reserved.","Diebold, F.X.; Sharpe, S.A.",1990,10.1080/07350015.1990.10509799,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002423521&doi=10.1080%2F07350015.1990.10509799&partnerID=40&md5=8d5caf65bff06c5f7bf019341d859000,scopus,"This study investigates the dynamic relationship between wholesale and retail interest rates post-deregulation using multivariate time series analysis. It examines the interactions among federal funds, treasury bills, and various retail deposit rates, focusing on causal patterns and impulse-response functions to understand how retail rates adjust to wholesale rate shocks.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:56:08.593222
1d531db1659c660e,Practical Bayesian support vector regression for financial time series prediction and market condition change detection,"Support vector regression (SVR) has long been proven to be a successful tool to predict financial time series. The core idea of this study is to outline an automated framework for achieving a faster and easier parameter selection process, and at the same time, generating useful prediction uncertainty estimates in order to effectively tackle flexible real-world financial time series prediction problems. A Bayesian approach to SVR is discussed, and implemented. It is found that the direct implementation of the probabilistic framework of Gao et al. returns unsatisfactory results in our experiments. A novel enhancement is proposed by adding a new kernel scaling parameter μ to overcome the difficulties encountered. In addition, the multi-armed bandit Bayesian optimization technique is applied to automate the parameter selection process. Our framework is then tested on financial time series of various asset classes (i.e. equity index, credit default swaps spread, bond yields, and commodity futures) to ensure its flexibility. It is shown that the generalization performance of this parameter selection process can reach or sometimes surpass the computationally expensive cross-validation procedure. An adaptive calibration process is also described to allow practical use of the prediction uncertainty estimates to assess the quality of predictions. It is shown that the machine-learning approach discussed in this study can be developed as a very useful pricing tool, and potentially a market condition change detector. A further extension is possible by taking the prediction uncertainties into consideration when building a financial portfolio. © 2017 Elsevier B.V., All rights reserved.","Law, T.; Shawe- Taylor, J.",2017,10.1080/14697688.2016.1267868,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014617588&doi=10.1080%2F14697688.2016.1267868&partnerID=40&md5=c7c6bf60d97eacfc0bafb852768ddf4a,scopus,"This study proposes an automated Bayesian Support Vector Regression (SVR) framework for financial time series prediction and market condition change detection. It introduces a novel kernel scaling parameter and uses multi-armed bandit Bayesian optimization for automated parameter selection. The framework is tested on various asset classes and demonstrates competitive generalization performance compared to cross-validation. The prediction uncertainty estimates can be used for adaptive calibration, pricing tools, and market condition detection.",True,False,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T10:56:32.601807
a316a8dab4798b82,Predictability and Financial Sufficiency of Health Insurance in Colombia: An Actuarial Analysis With a Bayesian Approach,"Every year, the Colombian government provides a prospective premium, known as the capitation payment unit (CPU), for each affiliated person (according to sex, region, and age) to each health insurance company, in order to manage the corresponding risk in health. This article studies the prediction capacity for the health expenditure for the more than 20 million affiliates to the contributory regime of health, as well as the CPU's financial sufficiency, using an actuarial approach. Using the pure risk premium method and generalized linear models, both classic and Bayesian, the CPU is estimated; these results are compared to actual expenditure by an index of forecasting ability. It is concluded that the use of historical information about expenditure on health, as well as the Bayesian inference, among the other methodological innovations developed, provides an advantage for obtaining more accurate prospective values. These technical recommendations seek to support an improvement in the public budget allocation of more than 6 billion dollars per year to the Colombian health system.","Espinosa, Oscar; Bejarano, Valeria; Ramos, Jeferson",2024,10.1080/10920277.2023.2197475,,wos,"This actuarial analysis investigates the predictive power and financial sufficiency of Colombia's capitation payment unit (CPU) for health insurance. Using both classic and Bayesian generalized linear models, the study estimates the CPU and compares it to actual health expenditures. The findings suggest that incorporating historical expenditure data and employing Bayesian inference leads to more accurate prospective values, potentially improving public budget allocation for the Colombian health system.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:56:40.768396
559a1b5288affd81,Predictability of bull and bear markets: A new look at forecasting stock market regimes (and returns) in the US,"The empirical literature of stock market predictability mainly suffers from model uncertainty and parameter instability. To meet this challenge, we propose a novel approach that combines dimensionality reduction, regime-switching models, and forecast combination to predict excess returns on the S&P 500. First, we aggregate the weekly information of 146 popular macroeconomic and financial variables using different principal component analysis techniques. Second, we estimate Markov-switching models with time-varying transition probabilities using the principal components as predictors. Third, we pool the models in forecast clusters to hedge against model risk and to evaluate the usefulness of different specifications. Our weekly forecasts respond to regime changes in a timely manner to participate in recoveries or to prevent losses. This is also reflected in an improvement of risk-adjusted performance measures as compared to several benchmarks. However, when considering stock market returns, our forecasts do not outperform common benchmarks. Nevertheless, they do add statistical and, in particular, economic value during recessions or in declining markets. © 2023 Elsevier B.V., All rights reserved.","Haase, F.; Neuenkirch, M.",2023,10.1016/j.ijforecast.2022.01.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125423203&doi=10.1016%2Fj.ijforecast.2022.01.004&partnerID=40&md5=2a9da8940804b1ea4ce1e3e635eb98ee,scopus,"This study proposes a novel approach to predict stock market excess returns by combining dimensionality reduction, regime-switching models, and forecast combination. The method uses principal components from macroeconomic and financial variables to estimate Markov-switching models with time-varying transition probabilities. While the forecasts show timely responses to regime changes and improve risk-adjusted performance against benchmarks, they do not outperform benchmarks for stock market returns but add value during recessions or declining markets.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:57:28.987210
c167176d7031a0a1,Predicting EU energy industry excess returns on EU market index via a constrained genetic algorithm,"This article introduces an automated procedure to simultaneously select variables and detect outliers in a dynamic linear model using information criteria as objective functions and diagnostic tests as constraints for the distributional properties of errors. A robust scaling method is considered to take into account the sensitiveness of estimates to abnormal data. A genetic algorithm is developed to these purposes. Two examples are presented where models are designed to produce short-term forecasts for the excess returns of the MSCI Europe Energy sector on the MSCI Europe index and a recursive estimation-window is used to shed light on their predictability performances. In the first application the data-set is obtained by a reduction procedure from a very large number of leading macro indicators and financial variables stacked at various lags, while in the second the complete set of 1-month lagged variables is considered. Results show a promising capability to predict excess sector returns through the selection, using the proposed methodology, of most valuable predictors. Reprinted by permission of Springer",,2009,10.1007/s10614-009-9176-4,,proquest,"This article presents an automated procedure for variable selection and outlier detection in dynamic linear models, utilizing information criteria and diagnostic tests. A genetic algorithm is employed for this purpose, with a robust scaling method to handle sensitive estimates. The methodology is applied to forecast excess returns of the MSCI Europe Energy sector on the MSCI Europe index, demonstrating promising predictive capabilities by selecting valuable predictors.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:57:39.276831
a8ce067fd156e398,Predicting Interest Rate Volatility Using Information on the Yield Curve,"This study examines whether information on the yield curve is useful for predicting volatility of the yield curve. The information is used within dynamic models by specifying the covariance matrix of changes in yield factors as nonlinear functions of the factors. Using such models, it is found that the information (i) is useful for predicting volatility of the slope factor, achieving the accuracy comparable with the GARCH model; (ii) has incremental value for predicting volatility of the curvature factor when combined with a volatility-specific factor; and (iii) does not much improve prediction of volatility of the level factor once the volatility-specific factor is introduced. © 2021 Elsevier B.V., All rights reserved.","Takamizawa, H.",2015,10.1111/irfi.12053,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940795801&doi=10.1111%2Firfi.12053&partnerID=40&md5=401dea9b3f3e7e9a0708eba14f080be1,scopus,"This study investigates the predictive power of yield curve information for yield curve volatility. It employs dynamic models where the covariance matrix of yield factor changes is modeled as a nonlinear function of these factors. The findings indicate that yield curve information is effective in predicting the volatility of the slope factor, rivaling GARCH model accuracy. It also offers incremental predictive value for the curvature factor when integrated with a volatility-specific factor, but shows limited improvement for the level factor after accounting for the volatility-specific factor.",True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:57:41.743833
88b65f3a8cabb4d8,Predicting bond risk premiums with machine learning: Evidence from China,"This study evaluates the ability of machine-learning algorithms to forecast bond risk premiums in the Chinese market. Using a comprehensive set of macro-, firm- and bond-level predictors, we find that machine learning, especially neural network, delivers markedly higher out-of-sample performance than traditional linear benchmarks. The local per-capita fiscal expenditure (EXPEND), bond credit ratings (CREDIT), and profitability- and intangible-related firm characteristics emerge as the most informative variables. Predictive gains are especially pronounced for low-rated issues, non-state-owned enterprises, and periods of heightened economic policy uncertainty. Incorporating machine-learning-based forecasts also helps to enhance credit rating accuracy. Collectively, our findings highlight the value of non-linear machine learning modeling techniques for bond pricing in emerging markets. © 2025 Elsevier B.V., All rights reserved.","Chai, B.; Jiang, F.; Lin, Y.; You, T.",2025,10.1016/j.pacfin.2025.102882,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011378841&doi=10.1016%2Fj.pacfin.2025.102882&partnerID=40&md5=e8cc6284881f9db8972654ad641eaf0d,scopus,"This study investigates the effectiveness of machine learning (ML) algorithms, particularly neural networks, in predicting bond risk premiums in China. The ML models, utilizing macro-, firm-, and bond-level data, significantly outperform traditional linear models in out-of-sample predictions. Key predictors include fiscal expenditure, credit ratings, and firm characteristics. The predictive gains are most substantial for lower-rated bonds, non-state-owned enterprises, and during periods of high economic policy uncertainty. The study also notes that ML-based forecasts improve credit rating accuracy, underscoring the utility of non-linear ML for bond pricing in emerging markets.",True,False,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T10:57:52.281682
b789cc7bea10aacd,Predicting equity premium using news-based economic policy uncertainty: Not all uncertainty changes are equally important,"This study contributes to the growing research that uses the news-based measure of U.S. economic policy uncertainty (EPU) suggested in Baker et al. (2016) to predict economic variables out-of-sample. Using simple predictive regressions à la Goyal and Welch (2008), we evaluate the predictive power afforded by various nonlinear transformations of the U.S. EPU measure suggested in Baker et al. (2016) to predict excess returns on the S&P 500 index one-month ahead. Using data from 1985m1 through 2020m12, we find that not all EPU movements are equally important for obtaining point prediction improvements relative to the historical average benchmark at the population level. Particularly, we document that the one-year net EPU increase, defined as EPU increases beyond the peak over the last year, otherwise zero delivers the most consistent pattern of prediction improvements relative to the benchmark. Conversely, other nonlinear specifications as well as the linear models using log-EPU and the first difference of log-EPU do not deliver the same performance. Overall, this study documents that the predictive impact of the U.S. EPU index suggested in Baker et al. (2016) on equity premium is nonlinear in that EPU increases matter only to the extent that they exceed the maximum value over the last twelve months. In other words, there is evidence of threshold nonlinearity. The statistical predictive power afforded by the one-year net EPU increase also translates into economic gains. © 2021 Elsevier B.V., All rights reserved.","Nonejad, N.",2021,10.1016/j.irfa.2021.101818,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108704036&doi=10.1016%2Fj.irfa.2021.101818&partnerID=40&md5=de440df2664db41659ed758b9ca17144,scopus,"This study investigates the predictive power of the news-based U.S. Economic Policy Uncertainty (EPU) measure for the S&P 500 equity premium. It finds that not all EPU changes are equally important, with a one-year net EPU increase (exceeding the past year's peak) showing the most consistent predictive improvements. The study suggests a threshold nonlinearity in the relationship between EPU and equity premium, indicating that EPU increases only matter when they surpass a recent maximum.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:57:56.856753
101def3e12ba19c0,Predicting failure risk using financial ratios: Quantile hazard model approach,"This study examines the role of financial ratios in predicting companies’ default risk using the quantile hazard model (QHM) approach and compares its results to the discrete hazard model (DHM). We adopt the LASSO method to select essential predictors among the variables mentioned in the literature. We show the preeminence of our proposed QHM through the fact that it presents a different degree of financial ratios’ effect over various quantile levels. While DHM only confirms the aftermaths of “stock return volatilities” and “total liabilities” and the positive effects of “stock price” “stock excess return” and “profitability” on businesses, under high quantile levels QHM is able to supplement “cash and short-term investment to total assets” “market capitalization” and “current liabilities ratio” into the list of factors that influence a default. More interestingly, “cash and short-term investment to total assets” and “market capitalization” switch signs in high quantile levels, showing their different influence on companies with different risk levels. We also discover evidence for the distinction of default probability among different industrial sectors. Lastly, our proposed QHM empirically demonstrates improved out-of-sample forecasting performance. © 2018 Elsevier B.V., All rights reserved.","Dong, M.C.; Tian, S.; Chen, C.W.S.",2018,10.1016/j.najef.2018.01.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042146925&doi=10.1016%2Fj.najef.2018.01.005&partnerID=40&md5=b61603138dc1c51707d88463ab14c1a7,scopus,"This study uses a quantile hazard model (QHM) and LASSO to predict corporate default risk using financial ratios, comparing it to a discrete hazard model (DHM). The QHM reveals varying effects of financial ratios across different risk levels, identifying additional predictors and sign changes for certain ratios at high quantiles. The model also shows improved out-of-sample forecasting performance and highlights differences in default probability across industrial sectors.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:58:06.202719
2589459f5fd7391f,Predicting real growth and the probability of recession in the Euro area using the yield spread,"Although the spread has been established as a leading indicator of economic activity, recent studies in US and European Union (EU) countries have documented, theoretically and empirically, that the term spread-output growth relationship may not be stable over time and it may be subjected to nonlinearities. Using aggregate data for the Euro area over the period 1970:1-2000:4, we applied linear regression as well as nonlinear models to examine the predictive accuracy of the term spread-output growth relationship. Our results confirm the ability of the yield curve as a leading indicator. Moreover, significant nonlinearity with respect to time and past annual growth is detected, outperforming the linear model in out-of-sample forecasts of 1-year-ahead annual growth. Furthermore, probit models that use the EMU and US yield spreads are successful in predicting EMU recessions. © 2004 International Institute of Forecasters. Published by Elsevier B.V. All rights reserved. © 2005 Elsevier B.V., All rights reserved.","Duarte, A.; Venetis, I.A.; Paya, I.",2005,10.1016/j.ijforecast.2004.09.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-14844296381&doi=10.1016%2Fj.ijforecast.2004.09.008&partnerID=40&md5=1b54c1e1fff7743dddb40cdaf2e3bfbb,scopus,"This study investigates the predictive accuracy of the term spread for economic growth and recession probability in the Euro area using both linear and nonlinear models. The findings confirm the yield curve's role as a leading indicator, with nonlinear models showing superior out-of-sample forecasting performance for annual growth. Probit models utilizing yield spreads also proved successful in predicting Euro area recessions.",True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:58:10.201582
c77ed07cec4b8ecf,Predicting recessions using trends in the yield spread,"The yield spread, measured as the difference between long- and short-term interest rates, is widely regarded as one of the strongest predictors of economic recessions. In this paper, we propose an enhanced recession prediction model that incorporates trends in the value of the yield spread. We expect our model to generate stronger recession signals because a steadily declining value of the yield spread typically indicates growing pessimism associated with the reduced future business activity. We capture trends in the yield spread by considering both the level of the yield spread at a lag of 12 months as well as its value at each of the previous two quarters leading up to the forecast origin, and we evaluate its predictive abilities using both logit and artificial neural network models. Our results indicate that models incorporating information from the time series of the yield spread correctly predict future recession periods much better than models only considering the spread value as of the forecast origin. Furthermore, the results are strongest for our artificial neural network model and logistic regression model that includes interaction terms, which we confirm using both a blocked cross-validation technique as well as an expanding estimation window approach.","Kozlowski, Steven E.; Sim, Thaddeus",2019,10.1080/02664763.2018.1537364,,wos,"This paper proposes an enhanced recession prediction model using trends in the yield spread, which is the difference between long- and short-term interest rates. The model incorporates historical yield spread values (12 months and previous two quarters) to capture trends, hypothesizing that a declining spread indicates pessimism about future business activity. The predictive abilities are evaluated using logit and artificial neural network models. Results show that models incorporating time series information of the yield spread predict recessions better than those only considering the current spread value, with the artificial neural network model and logistic regression with interaction terms performing strongest.",True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:58:16.889367
0f8cf13e7b3aab14,Predicting the milk yield curve of dairy cows in the subsequent lactation period using deep learning,"Existing lactation models predict milk yields based on a fixed amount of observed milk production in early lactation. In contrast, this study proposes a model to predict the entire lactation curve of dairy cows by leveraging historical milk yield information observed in the preceding cycle. More specifically, we present a deep learning framework to encode the model inputs, predict the latent representation of the milk yield sequences and generate the corresponding lactation curves. Results show that the proposed framework outperforms the baseline models and that during the first 26 days of lactation, the model's predictions are more accurate than those of a state-of-the-art lactation model which is able to leverage the observed milk yields. As a result, the framework presented in this study allows farmers to increase their forecast horizon with respect to predicting its herd's total production and hence facilitates optimal herd management. Additionally, the model can be used to compare a cow's actual and expected milk yield over the entire course of the lactation cycle. This in turn can help to accelerate disease detection and enhance current animal monitoring systems. Finally, as the model incorporates the impact of health and reproduction events as well as herd management on the cow's productivity, future earnings and costs can be estimated more accurately.","Liseune, Arno; Salamone, Matthieu; Van den Poel, Dirk; van Ranst, Bonifacius; Hostens, Miel",2021,10.1016/j.compag.2020.105904,,wos,"This study introduces a deep learning framework to predict the entire lactation curve of dairy cows using historical milk yield data from the previous lactation cycle. The model outperforms baseline methods and shows higher accuracy in early lactation compared to a state-of-the-art model. It aids in optimizing herd management, accelerating disease detection, enhancing animal monitoring, and improving financial forecasting by incorporating health, reproduction, and management events.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:58:20.318544
e9d4cb3cc9f23a8f,Prediction-Based Portfolio Optimization Models Using Deep Neural Networks,"Portfolio optimization is a hot research topic, which has attracted many researchers in recent decades. Better portfolio optimization model can help investors earn more stable profits. This paper uses three deep neural networks (DNNs), i.e., deep multilayer perceptron (DMLP), long short memory (LSTM) neural network and convolutional neural network (CNN) to build prediction-based portfolio optimization models which own the advantages of both deep learning technology and modern portfolio theory. These models first use DNNs to predict each stock's future return. Then, predictive errors of DNNs are applied to measure the risk of each stock. Next, the portfolio optimization models are built by integrating the predictive returns and semi-absolute deviation of predictive errors. These models are compared with three equal weighted portfolios, where their stocks are selected by DMLP, LSTM neural network and CNN respectively. Also, two prediction-based portfolio models built with support vector regression are used as benchmarks. This paper applies component stocks of China securities 100 index in Chinese stock market as experimental data. Experimental results present that the prediction-based portfolio model based on DMLP performs the best among these models under different desired portfolio returns, and high desired portfolio return can further improve the performance of this model. This paper presents the promising performance of DNNs in building prediction-based portfolio models.",Y. Ma; R. Han; W. Wang,2020,10.1109/access.2020.3003819,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9121212,ieeexplore,"This paper proposes portfolio optimization models using Deep Neural Networks (DNNs) including DMLP, LSTM, and CNN. These models predict stock returns and use predictive errors to measure risk, integrating these with semi-absolute deviation for optimization. The models are compared against equal-weighted portfolios and support vector regression benchmarks using data from the China Securities 100 index. The DMLP-based model showed the best performance, especially with high desired returns, highlighting DNNs' potential in portfolio modeling.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:58:29.655455
fba39a36693c57bd,Pricing Asian and Barrier Options Using a Combined Heston Model and Monte Carlo Simulation Approach with Artificial Intelligence,"The computation of fair values for exotic options often necessitates complex pricing techniques, which remain sparsely addressed in academic literature. Predominantly, the assessment of fair value for vanilla options relies on methodologies such as the Black-Scholes model or Monte Carlo simulations. This study proposes an innovative, dynamic approach to pricing, leveraging artificial intelligence in conjunction with the Heston model and a Monte Carlo simulation engine. This approach aims to furnish estimates of the prices for Barrier and Asian options. To enhance the accuracy of the model, calibration was performed employing a supervised machine learning algorithm, a continuous risk-free curve, and a dynamic implied volatility surface, derived from the current market data of vanilla options on S&P 500 futures. The amalgamation of these models yields instantaneous pricing for exotic option derivatives, contingent on the investor's determination of time to maturity and barrier levels. The efficacy of the model was evaluated by comparing the output prices to theoretical model predictions and a selection of over-the-counter traded options. Our findings indicate that the proposed dynamic, integrated approach substantially reduces the disparity between the theoretical models and current market prices. The prices calculated by our model demonstrate a marginal error of merely 0.33% in comparison to market prices, a significant improvement over the considerably larger error of 3.12% exhibited by traditional models. © 2023 Elsevier B.V., All rights reserved.","Khalife, D.; Yammine, J.; Rahal, S.; Freiha, S.",2023,10.18280/mmep.100519,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175234385&doi=10.18280%2Fmmep.100519&partnerID=40&md5=dd008174b8ed9190c2398219abc45cbb,scopus,"This study introduces a novel method for pricing exotic options like Barrier and Asian options by combining the Heston model, Monte Carlo simulation, and artificial intelligence. The model is calibrated using supervised machine learning, a risk-free curve, and a dynamic implied volatility surface derived from S&P 500 futures data. The approach aims to provide instantaneous pricing and was validated by comparing its output to theoretical predictions and over-the-counter traded options, showing a significant reduction in pricing error compared to traditional models.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:58:32.094198
7e7371ae9f087279,Pricing Climate Change Exposure,"We estimate the risk premium for firm-level climate change exposure among S&P 500 stocks and its time-series evolution between 2005 to 2020. Exposure reflects the attention paid by market participants in earnings calls to a firm's climate-related risks and opportunities. When extracted from realized returns, the unconditional risk premium is insignificant but exhibits a period with a positive risk premium before the financial crisis and a steady increase thereafter. Forward-looking expected return proxies deliver an unconditionally positive risk premium with maximum values of 0.5%–1% p.a., depending on the proxy, between 2011 and 2014. The risk premium has been lower since 2015, especially when the expected return proxy explicitly accounts for the higher opportunities and lower crash risks that characterize high-exposure stocks. This finding arises as the priced part of the risk premium primarily originates from uncertainty about climate-related upside opportunities. In the time series, the risk premium is negatively associated with green innovation; Big Three holdings; and environmental, social, and governance fund flows and positively associated with climate change adaptation programs.",,2023,10.1287/mnsc.2023.4686,,proquest,"This study estimates the risk premium associated with firm-level climate change exposure in S&P 500 stocks from 2005-2020. The risk premium, derived from market participants' attention to climate risks and opportunities in earnings calls, shows varying patterns over time. While unconditional premiums are often insignificant, forward-looking proxies reveal a positive premium between 2011-2014. The premium is influenced by factors like green innovation, fund flows, and adaptation programs, with uncertainty about climate-related upside opportunities being a key driver.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:58:33.488823
488001700e2f9649,Pricing currency options with support vector regression and stochastic volatility model with jumps,"This paper presents an efficient currency option pricing model based on support vector regression (SVR). This model focuses on selection of input variables of SVR. We apply stochastic volatility model with jumps to SVR in order to account for sudden big changes in exchange rate volatility. We use forward exchange rate as the input variable of SVR, since forward exchange rate takes interest rates of a basket of currencies into account. Therefore, the inputs of SVR will include moneyness (spot rate/strike price), forward exchange rate, volatility of the spot rate, domestic risk-free simple interest rate, and the time to maturity. Extensive experimental studies demonstrate the ability of new model to improve forecast accuracy. © 2010 Elsevier Ltd. All rights reserved. © 2011 Elsevier B.V., All rights reserved.","Wang, P.",2011,10.1016/j.eswa.2010.05.037,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956613198&doi=10.1016%2Fj.eswa.2010.05.037&partnerID=40&md5=c18e5dcbda6e97cf6ca8e0968802b1d1,scopus,"This paper proposes a currency option pricing model using Support Vector Regression (SVR) combined with a stochastic volatility model that incorporates jumps. The model utilizes forward exchange rate, moneyness, spot rate volatility, domestic risk-free interest rate, and time to maturity as inputs for SVR. The authors claim improved forecast accuracy through experimental validation.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:58:36.706292
da32f56a08ba4a8f,Pricing participating longevity-linked life annuities: a Bayesian Model Ensemble approach,"Participating longevity-linked life annuities (PLLA) in which benefits are updated periodically based on the observed survival experience of a given underlying population and the performance of the investment portfolio are an alternative insurance product offering consumers individual longevity risk protection and the chance to profit from the upside potential of financial market developments. This paper builds on previous research on the design and pricing of PLLAs by considering a Bayesian Model Ensemble of single population generalised age-period-cohort stochastic mortality models in which individual forecasts are weighted by their posterior model probabilities. For the valuation, we adopt a longevity option decomposition approach with risk-neutral simulation and investigate the sensitivity of results to changes in the asset allocation by considering a more aggressive lifecycle strategy. We calibrate models using Taiwanese (mortality, yield curve and stock market) data from 1980 to 2019. The empirical results provide significant valuation and policy insights for the provision of a cost effective and efficient risk pooling mechanism that addresses the individual uncertainty of death, while providing appropriate retirement income and longevity protection.Participating longevity-linked life annuities (PLLA) in which benefits are updated periodically based on the observed survival experience of a given underlying population and the performance of the investment portfolio are an alternative insurance product offering consumers individual longevity risk protection and the chance to profit from the upside potential of financial market developments. This paper builds on previous research on the design and pricing of PLLAs by considering a Bayesian Model Ensemble of single population generalised age-period-cohort stochastic mortality models in which individual forecasts are weighted by their posterior model probabilities. For the valuation, we adopt a longevity option decomposition approach with risk-neutral simulation and investigate the sensitivity of results to changes in the asset allocation by considering a more aggressive lifecycle strategy. We calibrate models using Taiwanese (mortality, yield curve and stock market) data from 1980 to 2019. The empirical results provide significant valuation and policy insights for the provision of a cost effective and efficient risk pooling mechanism that addresses the individual uncertainty of death, while providing appropriate retirement income and longevity protection.",,2022,10.1007/s13385-021-00279-w,,proquest,"This paper develops a Bayesian Model Ensemble approach to price participating longevity-linked life annuities (PLLA). It uses generalized age-period-cohort stochastic mortality models, a longevity option decomposition approach with risk-neutral simulation, and Taiwanese data from 1980-2019. The study investigates the impact of asset allocation strategies on valuation and provides insights for retirement income and longevity protection.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:58:40.088502
5d8fab955f1e2b4c,Pricing with finite dimensional dependence,"We consider derivative pricing in factor models, where the factor is Markov with Finite Dimensional Dependence (FDD). The FDD condition allows for explicit formulas for derivative prices and their term structure and in this respect is a serious competitor of models with affine dynamic factors. The approach is illustrated by a comparison of the prices of realized and integrated volatility swaps. We show that the usual practice of replacing a payoff written on the realized volatility by the payoff written on the integrated volatility can imply pricing errors which are not negligible when the volatility of the volatility is large.",,2015,10.1016/j.jeconom.2015.02.027,,proquest,"This paper explores derivative pricing within factor models where the factor exhibits Finite Dimensional Dependence (FDD). The FDD condition enables the derivation of explicit formulas for derivative prices and their term structure, presenting a viable alternative to models with affine dynamic factors. The study illustrates this approach by comparing prices of realized and integrated volatility swaps, demonstrating that substituting integrated volatility for realized volatility can lead to significant pricing errors, especially when the volatility of volatility is high.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:58:42.864838
b5f453c341dd0e18,Probabilistic forecasts of volatility and its risk premia,"The object of this paper is to produce distributional forecasts of asset price volatility and its associated risk premia using a non-linear state space approach. Option and spot market information on the latent variance process is captured by using dual 'model-free' variance measures to define a bivariate observation equation in the state space model. The premium for variance diffusive risk is defined as linear in the latent variance (in the usual fashion) whilst the premium for variance jump risk is specified as a conditionally deterministic dynamic process, driven by a function of past measurements. The inferential approach adopted is Bayesian, implemented via a Markov chain Monte Carlo algorithm that caters for the multiple sources of non-linearity in the model and for the bivariate measure. The method is applied to spot and option price data on the S&P500 index from 1999 to 2008, with conclusions drawn about investors' required compensation for variance risk during the recent financial turmoil. The accuracy of the probabilistic forecasts of the observable variance measures is demonstrated, and compared with that of forecasts yielded by alternative methods. To illustrate the benefits of the approach, it is used to produce forecasts of prices of derivatives on volatility itself. In addition, the posterior distribution is augmented by information on daily returns to produce value at risk predictions. Linking the variance risk premia to the risk aversion parameter in a representative agent model, probabilistic forecasts of (approximate) relative risk aversion are also produced. All rights reserved, Elsevier",,2012,10.1016/j.jeconom.2012.06.006,,proquest,"This paper uses a non-linear state space approach to generate distributional forecasts of asset price volatility and its risk premia. It incorporates option and spot market information through a bivariate observation equation and models variance risk premia. The methodology is Bayesian, implemented with Markov chain Monte Carlo, and applied to S&P500 data from 1999-2008 to analyze variance risk compensation during financial turmoil. The study demonstrates the accuracy of its probabilistic forecasts, compares them to alternative methods, and uses the approach to forecast prices of volatility derivatives and produce value at risk predictions. It also forecasts relative risk aversion by linking variance risk premia to a representative agent model.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:59:20.995975
1f915ca874b0d1e8,Quadratic stochastic intensity and prospective mortality tables,"We consider a quadratic stochastic intensity model with a Gaussian autoregressive factor, derive explicit formulas for predictive mortality tables and recursive updating formulas are also provided. We also explain how to use appropriately the Kalman filter to estimate the parameters of the model and to approximate the values of the underlying factor. This methodology is applied to French human mortality tables. (C) 2008 Elsevier B.V. Ail rights reserved.","Gourieroux, C.; Monfort, A.",2008,10.1016/j.insmatheco.2008.05.010,,wos,"This paper presents a quadratic stochastic intensity model with a Gaussian autoregressive factor, offering explicit formulas for predictive mortality tables and recursive updating. It also details the use of the Kalman filter for parameter estimation and factor approximation, with an application to French human mortality data.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:59:22.653213
e80e3012da1e8609,QuantFactor REINFORCE: Mining Steady Formulaic Alpha Factors With Variance-Bounded REINFORCE,"Alpha factor mining aims to discover investment signals from the historical financial market data, which can be used to predict asset returns and gain excess profits. Powerful deep learning methods for alpha factor mining lack interpretability, making them unacceptable in the risk-sensitive real markets. Formulaic alpha factors are preferred for their interpretability, while the search space is complex and powerful explorative methods are urged. Recently, a promising framework is proposed for generating formulaic alpha factors using deep reinforcement learning, and quickly gained research focuses from both academia and industries. This paper first argues that the originally employed policy training method, i.e., Proximal Policy Optimization (PPO), faces several important issues in the context of alpha factors mining. Herein, a novel reinforcement learning algorithm based on the well-known REINFORCE algorithm is proposed. REINFORCE employs Monte Carlo sampling to estimate the policy gradient—yielding unbiased but high variance estimates. The minimal environmental variability inherent in the underlying state transition function, which adheres to the Dirac distribution, can help alleviate this high variance issue, making REINFORCE algorithm more appropriate than PPO. A new dedicated baseline is designed to theoretically reduce the commonly suffered high variance of REINFORCE. Moreover, the information ratio is introduced as a reward shaping mechanism to encourage the generation of steady alpha factors that can better adapt to changes in market volatility. Evaluations on real assets data indicate the proposed algorithm boosts correlation with returns by 3.83%, and a stronger ability to obtain excess returns compared to the latest alpha factors mining methods, which meets the theoretical results well.",J. Zhao; C. Zhang; M. Qin; P. Yang,2025,10.1109/tsp.2025.3576781,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11024173,ieeexplore,"This paper proposes QuantFactor REINFORCE, a novel reinforcement learning algorithm for mining interpretable formulaic alpha factors. It addresses limitations of PPO in alpha factor mining by using REINFORCE with a new baseline and information ratio reward shaping to reduce variance and encourage steady factors. Evaluations show improved correlation with returns and excess return generation compared to existing methods.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:59:48.647188
d59e983e8c52c31d,RETRACTED: Wheat Futures Prices Prediction in China: A Hybrid Approach (Retracted Article),"Stocks markets play their financial roles of price shocks and hedging just when they are proficient. The imperative highlights of productive market are that one cannot make extraordinary profit from the stocks markets. This research investigates whether China wheat futures price can be predicted by employing artificial intelligence neural network. This would add to our knowledge whether wheat futures market is resourceful and would enable traders, sellers, and investors to improve cost-effective trading strategy. We utilize the traditional financial model to forecast the wheat futures price and acquire out of sample point estimates. We additionally assess the robustness of our outcomes by applying several alternative forecasting techniques such as artificial intelligence with one hidden layer and autoregressive integrated moving average (ARIMA) model. Furthermore, the statistical significance of our point estimation was further tested through the Mariano and Diebold test. Considering random walk forecast as the bench mark, we used a number of economic indicators, trader's expectation towards futures prices, and lagged value of futures price of wheat in order to forecast the evaluation of wheat futures price. The computable significance of out of sample estimations recommends that our ANN with one hidden layer has the best anticipating presentation among all the models considered in this exploration and has the estimating power in foreseeing wheat futures returns. Furthermore, this investigation discovers that the futures price of wheat can be predicted, and the wheat futures market of China is not productive.","Sun, Yunpeng; Guo, Jin; Shan, Shan; Khan, Yousaf Ali",2021,10.1155/2021/5545802,,wos,"This retracted article investigates the predictability of China's wheat futures prices using artificial intelligence (ANN) and ARIMA models. The study found that the ANN model with one hidden layer outperformed other methods, suggesting that wheat futures prices can be predicted and the market is not efficient. The research aimed to inform trading strategies.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:59:53.342938
002bc07ecf503c9e,Radial Basis Functions With Adaptive Input and Composite Trend Representation for Portfolio Selection,"We propose a set of novel radial basis functions with adaptive input and composite trend representation (AICTR) for portfolio selection (PS). Trend representation of asset price is one of the main information to be exploited in PS. However, most state-of-the-art trend representation-based systems exploit only one kind of trend information and lack effective mechanisms to construct a composite trend representation. The proposed system exploits a set of RBFs with multiple trend representations, which improves the effectiveness and robustness in price prediction. Moreover, the input of the RBFs automatically switches to the best trend representation according to the recent investing performance of different price predictions. We also propose a novel objective to combine these RBFs and select the portfolio. Extensive experiments on six benchmark data sets (including a new challenging data set that we propose) from different real-world stock markets indicate that the proposed RBFs effectively combine different trend representations and AICTR achieves state-of-the-art investing performance and risk control. Besides, AICTR withstands the reasonable transaction costs and runs fast; hence, it is applicable to real-world financial environments.",Z. -R. Lai; D. -Q. Dai; C. -X. Ren; K. -K. Huang,2018,10.1109/tnnls.2018.2827952,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8356708,ieeexplore,"This paper introduces novel adaptive input and composite trend representation (AICTR) radial basis functions (RBFs) for portfolio selection. The system improves price prediction by combining multiple trend representations and adaptively switching inputs based on performance. Experiments show AICTR achieves state-of-the-art performance and risk control, is robust to transaction costs, and is computationally efficient for real-world applications.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T10:59:57.655596
8b07c85a311ecc17,Rare Disasters and Risk Attitudes: International Differences and Implications for Integrated Assessment Modeling,"Evaluation of public policies with uncertain economic outcomes should consider society's preferences regarding risk. However, the preference models used in most integrated assessment models, including those commonly used to inform climate policy, do not adequately characterize the risk attitudes revealed by typical investment decisions. Here, we adopt an empirical approach to risk preference description using international historical data on investment returns and the occurrence of rare economic disasters. We improve on earlier analyses by employing a hierarchical Bayesian inference procedure that allows for nation-specific estimates of both disaster probabilities and preference parameters. This provides a stronger test of the underlying investment model than provided by previous calibrations and generates some compelling hypotheses for further study. Specifically, results suggest that society is substantially more averse to risk than typically assumed in integrated assessment models. In addition, there appear to be systematic differences in risk preferences among nations. These results are likely to have important implications for policy recommendations: higher aversion to risk increases the precautionary value of taking action to avoid low-probability, high-impact outcomes. However, geographically variable attitudes toward risk indicate that this precautionary value could vary widely across nations, thereby potentially complicating the negotiation of transboundary agreements focused on risk reduction. © 2012 Society for Risk Analysis. © 2013 Elsevier B.V., All rights reserved.; MEDLINE® is the source for the MeSH terms of this document.","Ding, P.; Gerst, M.D.; Bernstein, A.; Howarth, R.B.; Borsuk, M.E.",2012,10.1111/j.1539-6924.2012.01872.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869495004&doi=10.1111%2Fj.1539-6924.2012.01872.x&partnerID=40&md5=3d9232bba36bfd98b20035479434effc,scopus,"This study empirically investigates societal risk attitudes using international historical data on investment returns and rare economic disasters. It employs a hierarchical Bayesian inference procedure to estimate nation-specific disaster probabilities and preference parameters, suggesting higher risk aversion than commonly assumed in integrated assessment models. The findings highlight systematic differences in risk preferences among nations, which could impact policy recommendations and the negotiation of transboundary agreements for risk reduction.",True,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:00:12.693482
d600bf6f8813eb90,Rating Crop Insurance Contracts with Nonparametric Bayesian Model Averaging,Crop insurance is plagued by relatively little historical information but significant spatial information. We investigate the efficacy of using nonparametric Bayesian model averaging (BMA) to incorporate extraneous information into the estimated premium rates. Nonparametric BMA is particularly suited to this application because it does not make any assumptions about parametric form or the extent to which yields are similar. We evaluate the proposed estimator under small-to-medium sample sizes and various geographical restrictions on the distance of spatial smoothing for policy relevance. The nonparametric BMA consistently decreases error and enables statistically significant and economically important rents to be captured.,,2020,10.22004/ag.econ.302453,,proquest,"This paper proposes a nonparametric Bayesian model averaging (BMA) approach to improve crop insurance premium rate estimation by incorporating spatial information, especially when historical data is limited. The method is evaluated for its effectiveness in reducing errors and capturing economic rents under various conditions.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:00:14.583858
cc99eea515945c08,Recent empirical evidence on the impact of the primary budget deficit on nominal longer term treasury note interest rate yields,"This study empirically investigates the impact of the federal budget deficit on the nominal interest rate yields on seven and ten year treasury notes over the 1992-2003 period. To measure the budget deficit, the primary budget deficit, which excludes net interest payments by the treasury, is adopted. In a loanable funds model that includes the monetary base, expected inflation, an ex ante real short term interest rate yield and an ex ante real intermediate term interest rate yield, the percentage growth rate of real GDP, and the percentage growth rate of the S&P 500 stock index, instrumental variables estimations using quarterly data reveal that the primary deficit has raised the nominal interest rate yields on both seven year and ten year treasury notes over the study period, raising serious concerns about the currently surging national debt.",,2005,10.1504/gber.2005.006919,,proquest,"This empirical study examines the effect of the primary budget deficit on nominal interest rates of 7- and 10-year Treasury notes from 1992-2003. Using a loanable funds model and instrumental variables estimation, the research finds that the primary deficit increased these interest rates, raising concerns about the national debt.",False,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:00:21.588507
d709c1992299932e,Reconciling interest rates evidence with theory: Rejecting unit roots when the HD(1) is a competing alternative,"The paper introduces the HD(1), a Markovian process of order one with reversion rates that are faster the farther the process is from equilibrium. The aHD(1) approximation is introduced to allow for an estimation -calibration procedure based on available ARMA routines. Critical values of unit root tests with aHD(1) alternative are tabulated for the signed likelihood -ratio statistic. Revisiting the non-stationarity of interest rates stylized fact, the aHD(1) is found to be preferred to ARMA, SETAR and RCA and the resulting tests to reject the unit root hypothesis for all rates and yields considered.","Palandri, Alessandro",2024,10.1016/j.jbankfin.2024.107113,,wos,"This paper proposes the HD(1) model, a Markovian process with faster reversion rates when further from equilibrium, and its approximation aHD(1) for estimation. It introduces critical values for unit root tests against the aHD(1) alternative and applies them to interest rates. The findings suggest the aHD(1) is superior to other models (ARMA, SETAR, RCA) and leads to the rejection of the unit root hypothesis for all examined interest rates and yields.",False,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:00:22.811619
c47d7837392e4b40,Regime-specific exchange rate predictability,"We study temporary phases of exchange rate predictability in a two-regime threshold predictive regression framework allowing for persistent predictors. Regime switches are triggered by an observable transition variable which relates to media news, expectations, uncertainty and global financial conditions. As predictors for G7 currencies and effective US-Dollar exchange rates, we study various interest rate spreads, yield curve factors, uncertainty measures and deviations from fundamental exchange rate parities. Besides established uncertainty measures, we use a wide range of measures for media coverage and construct uncertainty measures from survey data as transition variables for the activation of the predictability regime. Our results emphasize that short recurring phases of significant predictability are characterized by nonlinear patterns. Phases of predictability are triggered by increased media coverage and high uncertainty with interest rate dynamics emerging as the most important predictor. We find broadly similar results for a contemporaneous threshold analysis where our regressors are allowed to affect the exchange rate in the same period. From a theoretical point of view, we argue that our empirical results are useful for the empirical identification of scapegoat effects and that media coverage and uncertainty affect the exchange rate via the heterogeneity of private signals and the precision of public signals. © 2025 Elsevier B.V., All rights reserved.","Beckmann, J.; Kerkemeier, M.; Kruse, R.",2025,10.1016/j.jedc.2025.105095,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003202194&doi=10.1016%2Fj.jedc.2025.105095&partnerID=40&md5=1f794f64bc449a52a369b9e0c25980fb,scopus,"This study investigates temporary periods of exchange rate predictability using a two-regime threshold predictive regression model. Regime switches are influenced by observable variables like media news, expectations, uncertainty, and global financial conditions. The research examines various interest rate spreads, yield curve factors, uncertainty measures, and deviations from fundamental exchange rate parities as predictors for G7 currencies and effective US-Dollar exchange rates. Findings indicate that short, recurring phases of significant predictability are linked to nonlinear patterns, triggered by increased media coverage and high uncertainty, with interest rate dynamics being the most crucial predictor. The results are consistent with a contemporaneous threshold analysis and offer theoretical insights into scapegoat effects, media coverage, and uncertainty influencing exchange rates through private and public signal heterogeneity and precision.",True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:01:03.809528
3aba891800eef614,Relationship between expected treasury bill and Eurodollar interest rates: A fractional cointegration analysis,"In this paper, we extend Booth and Tse's (BT) 1995 analysis of fractional cointegration between the expected Eurodollar and Treasury bill interest rates implied by their respective futures contracts. The definition of fractional cointegration suggested by Cheung and Lai (1993) and used by BT is refined so that it requires the cointegrating relationship to be stationary as well as mean-reverting. In addition to the Geweke and Porter-Hudak method used by BT, a more efficient Maximum Likelihood (ML) method is used to estimate the cointegrating relationship. The LM (Engle (1982)) test indicates the possible existence of a heteroscedastic cointegrating relationship. Therefore, we use heteroscedastic models (GARCH and Exponential GARCH) to represent the cointegrating regression instead of the simple homoscedastic model used by BT. The empirical evidence cannot reject the null hypothesis of a stationary fractional cointegration relationship between the Eurodollar and Treasury bill interest rates. © 2001 Kluwer Academic Publishers. © 2018 Elsevier B.V., All rights reserved.","Shrestha, K.; Welch, R.L.",2001,10.1023/a:1008340408261,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846540251&doi=10.1023%2FA%3A1008340408261&partnerID=40&md5=899be94c4ad1bef4c2b113c4e3aaee7f,scopus,"This paper investigates the fractional cointegration between expected Eurodollar and Treasury bill interest rates using refined definitions and advanced estimation methods (Maximum Likelihood, GARCH, Exponential GARCH). The findings suggest a stationary fractional cointegration relationship between these interest rates, contradicting previous homoscedastic models.",True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:01:06.102674
8239e0ec15a172c7,Research on Early Warning of Banking Crises from the Perspective of Credit Structures,"The relationship between credit expansion and banking crises is complex and cannot be fully explained by total credit alone. A systematic analysis of the relationship between the amount and structure of total credit and banking crises is important for an objective prediction of the influence of potential financial risks. This paper, drawing on data from 15 selected countries, delves into the power of credit indicators in the early warning of banking crises from the perspectives of industrial structure, sector structure, and term structure of credit. Various machine learning methods were used, including Logistic Regression, Random Forest, Decision Tree, Support Vector Machine (SVM), Bagging, and Boosting models. The empirical findings indicate that credit expansion plays a crucial role in triggering banking crises. However, total credit is better suited for the early warning of short-term banking crises, whereas credit structure is more useful for the early warning of long-term banking crises. Moreover, in an early warning system, identifying key early warning indicators is more meaningful than merely increasing the number of indicators. Machine learning can somewhat enhance the early warning power, but it may not always be robust. Therefore, more attention should be paid to potential systemic banking crises resulting from an imbalance in credit structure while regulating the total credit threshold. © 2024 Elsevier B.V., All rights reserved.","Yuqin, Z.; Zixuan, L.; Wu, W.",2024,10.19873/j.cnki.2096-0212.2024.03.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210133695&doi=10.19873%2Fj.cnki.2096-0212.2024.03.003&partnerID=40&md5=0337e2a748df7f5e03df0a89befbb474,scopus,"This paper investigates the role of credit structures in the early warning of banking crises, using data from 15 countries and applying various machine learning methods. It finds that while credit expansion is a trigger, total credit is better for short-term warnings and credit structure for long-term warnings. The study emphasizes the importance of identifying key indicators and suggests focusing on credit structure imbalances for systemic crisis prevention.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:01:08.232643
2d91d5fdaf6e3910,Research on e-commerce platform credit supervision resilience based on stochastic catastrophe theory; 基于随机突变的电商平台信用监管弹性研究,"With the development of the Internet economy, e-commerce platforms such as Taobao, DiDi, and Uber have received widespread attention. These enterprises have changed people′s lifestyles and have played an important role in promoting consumption and employment. However, the strong growth of the platform market comes with issues and scandals, including false or misleading information, poor quality, fake products, and counterfeits. The increasing number of such disputes has harmed the interests of consumers and has had a significant adverse social impact. Thus, how to achieve effective e-commerce platform supervision has become the focus of the government, practitioners, and researchers. E-commerce platform credit supervision requires collaboration and coordination between the government and e-commerce platforms. However, the collaborative regulation system is vulnerable and subject to various internal and external factors, such as interaction attributes, utility perception, and incentives. Resilience research explores whether changes in these factors lead to sudden changes in participants′ strategy selection, where resilience refers to the ability of the regulation system to maintain the current equilibrium state. Probing the mechanism of credit supervision resilience evolution can help capture the nonlinearity and randomness in the platform-government relationship from a quantitative perspective and provide decision support for developing credit supervision strategies. However, existing research on e-commerce platform credit supervision focuses on finding equilibrium strategies and their existence conditions, and “resilience” is rarely mentioned. In addition, existing resilience research is rarely based on a catastrophe perspective, and sudden changes in credit supervision are difficult to describe. Therefore, this study integrates resilience theory and stochastic catastrophe theory to propose a catastrophe-based resilience measure for the collaborative regulation system, investigate how system resilience is affected by how the system is regulated, and recommend preventive measures to effectively avoid unexpected radical changes. In the first part, the general framework and research context of the study are presented. In the second part, the catastrophe of supervision behavior is described using an evolutionary game and stochastic catastrophe theory. First, an evolutionary game model between the government and e-commerce platforms is proposed, and a payoff matrix is established. On this foundation, the Gaussian White noise is used to show the random disturbance in the game, and Itô stochastic differential equations are introduced to develop a stochastic dynamic system. Subsequently, a probability density function is introduced to build the stochastic cusp catastrophe model, and the catastrophe set is found to explain the catastrophe of the regulation system. In the third section, with the areas of catastrophes identified, the concept of credit regulation resilience is presented based on Holling′s definition of resilience, and a catastrophe-based resilience measure for the collaborative regulation system is proposed. In the fourth part, simulation experiments are conducted to explore the influence of excess return, punishment, the degree of collaboration between the government and e-commerce platforms, and the degree of public and media participation, and the effectiveness of the model is verified with practical cases. Finally, the advice on e-commerce platform credit supervision is presented in terms of results. Some important conclusions and managerial insights are derived. First, catastrophe occurs suddenly whenthe parameters cross the borderline of the catastrophe set. Therefore, controlling the relevant parameters away from the catastrophe set can maintain the effectiveness of the regulation system. Second, the resilience evolution process is consistent with the catastrophe process: the occurrence of catastrophe leads to rapid changes in resilience. Therefore, it is possible to predict the state of the regulation system based on resilience and intervene in time to control the occurrence of catastrophes when the value of resilience is found to change rapidly or be close to zero. Third, different parameters have different effects on resilience. Punishment has a significant impact on resilience, and the two share a “U”-shaped relationship. In addition, under the constraint of catastrophe threshold, resilience decreases with increasing excess returns and increases with increasing collaboration degree and public and media participation. Therefore, it would be more effective to combine resilience and catastrophe conditions to optimize the regulatory strategy. In general, this research integrates resilience theory and stochastic catastrophe theory to study e-commerce platform credit supervision issues, which provides new ideas for e-commerce platform credit supervision research. The study analyzes the impact of system parameter changes on catastrophe degree from a quantitative perspective through resilience measurement, and the conclusion can provide a basis for e-commerce platform supervision warnings and policy optimization. © 2024 Elsevier B.V., All rights reserved.","Xiaochao, W.; She, S.; Guihua, N.",2024,10.13587/j.cnki.jieem.2024.01.020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185965736&doi=10.13587%2Fj.cnki.jieem.2024.01.020&partnerID=40&md5=a3635447abe5d98ab6aa9b0b3e59b0d9,scopus,"This study integrates resilience theory and stochastic catastrophe theory to analyze the credit supervision resilience of e-commerce platforms. It proposes a catastrophe-based resilience measure for collaborative regulation systems involving government and platforms. The research uses an evolutionary game model with Gaussian White noise and Itô stochastic differential equations to develop a stochastic cusp catastrophe model. Simulation experiments explore the influence of factors like excess return, punishment, collaboration degree, and public/media participation on system resilience. The findings suggest controlling parameters away from the catastrophe set, monitoring resilience for early warnings of catastrophic changes, and optimizing regulatory strategies by considering both resilience and catastrophe conditions. The study aims to provide quantitative insights and decision support for e-commerce platform credit supervision.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:01:10.423055
4ee8fbad2db12a73,Research on the Volatility of the Cotton Market under Different Term Structures: Perspective from Investor Attention,"This study performed comprehensive investigations of the complex interconnections between investor attention and cotton futures price volatility under different term structures. In this paper, in-sample analysis, out-of-sample forecast, influencing mechanisms, as well as nonlinear connections are fully explored using several linear model specifications. The results can be summarized as follows: first, investor attention is the Granger causality of the cotton futures volatility and shows significant linear impacts on cotton volatility; second, models incorporated with investor attention significantly improve the prediction accuracy of cotton volatility in the long term compared with the commonly used AR benchmark model; third, the influence of investor attention on cotton volatility may occur through open interest; and fourth, investor attention presents nonlinear impacts on cotton volatility as well. Overall, the results of this article can provide strong supporting evidence for the important roles of investor attention in asset pricing applications. © 2023 Elsevier B.V., All rights reserved.","Zhou, Q.; Zhu, P.; Wu, Y.; Zhang, Y.",2022,10.3390/su142114389,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146714935&doi=10.3390%2Fsu142114389&partnerID=40&md5=44ec11308eaba61a09e9858a300f5c37,scopus,"This study investigates the relationship between investor attention and cotton futures price volatility across different term structures. It employs linear models to analyze Granger causality, forecast accuracy improvements, the mediating role of open interest, and nonlinear impacts of investor attention on volatility. The findings suggest investor attention plays a significant role in asset pricing.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:01:13.963703
ceb068473580003b,Retail Demand Forecasting Using Spatial-Temporal Gradient Boosting Methods,"With the significant growth of the e-commerce business, the retail industry is experiencing rapid developments, leading to the explosion of the number of stock-keeping units (SKUs). Therefore, it calls for forecasting algorithms to forecast a large number of product-level demands over a short forecasting horizon. We developed a novel machine learning algorithm—the spatial-temporal gradient boosting tree (ST-GBT)—for demand forecasting for the retail industry. By incorporating the cross-section and time-series information in the existing gradient-boosting decision tree algorithm, our new algorithm can accurately forecast tremendous SKUs in one process. Furthermore, we show potential factors related to the retail industry, while new factors, such as higher-order statistics and risk-free interest, are also proposed for demand forecasting tasks. The numerical experiment results based on a large e-commerce company’s historical transaction records support the comparative merits of the new algorithm with superior accuracy and automation ability. © 2024 Elsevier B.V., All rights reserved.","Wang, J.; Chong, W.K.; Lin, J.; Hedenstierna, C.P.T.",2024,10.1080/08874417.2023.2240753,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166741941&doi=10.1080%2F08874417.2023.2240753&partnerID=40&md5=23943f325a6dac965bd1f19dd163e2b1,scopus,"This paper introduces a novel spatial-temporal gradient boosting tree (ST-GBT) algorithm for retail demand forecasting, designed to handle a large number of SKUs and short forecasting horizons. The algorithm integrates cross-sectional and time-series data, proposing new factors like higher-order statistics and risk-free interest. Experiments on e-commerce data demonstrate its superior accuracy and automation compared to existing methods.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:01:15.290468
7dd667d475419652,Retrospectives: Irving fisher's Appreciation and Interest (1896) and the fisher relation,"Irving Fisher's monograph Appreciation and Interest (1896) proposed his famous equation showing expected inflation as the difference between nominal interest and real interest rates. In addition, he drew attention to insightful remarks and numerical examples scattered through the earlier literature, and he derived results ranging from the uncovered interest arbitrage parity condition between currencies to the expectations theory of the term structure of interest rates. As J. Bradford DeLong wrote in this journal (Winter 2000), ""The story of 20th century macroeconomics begins with Irving Fisher"" and specifically with Appreciation and Interest because ""the transformation of the quantity theory of money into a tool for making quantitative analyses and predictions of the price level, inflation, and interest rates was the creation of Irving Fisher."" I discuss the message of Appreciation and Interest, and assess how original he was. © 2013 Elsevier B.V., All rights reserved.","Dimand, R.W.; Gomez Betancourt, R.G.",2012,10.1257/jep.26.4.185,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873335341&doi=10.1257%2Fjep.26.4.185&partnerID=40&md5=d258eb0252514708325ae1e7caafec1c,scopus,"This article discusses Irving Fisher's 1896 monograph ""Appreciation and Interest,"" focusing on his famous equation relating nominal interest, real interest, and expected inflation. It also examines Fisher's contributions to other areas like interest arbitrage parity and the expectations theory of the term structure, highlighting his foundational role in 20th-century macroeconomics and his transformation of the quantity theory of money into a predictive tool.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:01:16.768731
a70a5d5938d86e6e,Revealing the correlation between economic indicators and gold prices for forecasting: Medium term forecast framework with data patch,"Predicting gold prices through the analysis of key economic indicators such as inflation rates, Government Bond Yields, and the U.S. Dollar Index, alongside historical Gold Prices, is crucial for enabling investors to better understand market dynamics and make vital decisions to maximize returns. However, previous studies have faced challenges in extracting hidden factors related to gold price prediction from diverse economic indicators, and the comprehensive exploration of gold price data is yet to be fully achieved. To address this, the present study introduces a mid to long-term gold price prediction model named DPformer. This model utilizes a patching strategy to investigate the relationships between different economic indicators and Gold Prices. It also employs a decomposition approach to discover the mid to long-term trend characteristics and yearly seasonal patterns of Gold Prices. The core of the model integrates a Transformer module, which is solely based on an Encoder structure, and enhances it with multiple attention mechanisms and convolutions. This enhancement allows the improved Transformer model to more effectively capture the long-term dependencies of Gold Prices. The empirical results demonstrate that DPformer consistently outperforms a suite of advanced models widely adopted in terms of mid to long-term forecasting accuracy, including LSTM, GRU, Transformer, DLinear, and PatchTST. Notably, for the 30-step gold price prediction task, DPformer achieves a 21.78 % reduction in Mean Squared Error compared to PatchTST. Moreover, by quantitatively analyzing how various economic indicators influence gold price forecasts, this study provides substantial support for investors in making informed decisions at critical moments. © 2025 Elsevier B.V., All rights reserved.","Bao, G.; Niu, Y.; Cui, B.; Ji, W.",2026,10.1016/j.eswa.2025.129594,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105015297138&doi=10.1016%2Fj.eswa.2025.129594&partnerID=40&md5=b39221e28ac6635c40a32273e8371cef,scopus,"This study introduces DPformer, a Transformer-based model for mid to long-term gold price forecasting. It incorporates a patching strategy to analyze relationships between economic indicators (inflation, bond yields, USD index) and gold prices, and uses a decomposition approach to capture trends and seasonal patterns. DPformer demonstrates superior forecasting accuracy compared to several advanced models, including LSTM, GRU, Transformer, DLinear, and PatchTST, with significant improvements in Mean Squared Error.",True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:01:29.324022
91b7a341ab735f04,Revisiting the Dynamic Linkages of Treasury Bond Yields for the BRICS: A Forecasting Analysis,"We examined the dynamic linkages among money market interest rates in the so-called “BRICS” countries (Brazil, Russia, India, China, and South Africa) by using weekly data of the overnight, one-, three-, and six- months, as well as of one year, Treasury bills rates covering the period from January 2005 to August 2019. A long-run relationship among interest rates was established by employing the Vector Error Correction modeling (VECM), which revealed the validation of the Expectation Hypothesis Theory (EH) of the term structure of interest rates, taking into account long-run deviations from equilibrium and inherent nonlinearities. We unveiled short-run dynamic adjustments for the term structure of the BRICS, subject to regime switches. We then used Markov Switching Vector Error Correction models (MS-VECM) to forecast them dynamically during an out-of-sample period of May 2016 through August 2019. The MSIH-VECM forecasts were found to be superior to the VECM approaches. The novelty of our paper is mainly due to the exploration of the possibility of parameter instability as a crucial factor, which might explain the rejection of the restricted version of the cointegration space, and on the dynamic out-of-sample forecasts of the term structure over a more recent time span in order to assess further the usefulness of our nonlinear MS-VECM characterization of the term structure, capturing the effects of the global and domestic financial crisis. © 2022 Elsevier B.V., All rights reserved.","Bekiros, S.; Avdoulas, C.",2020,10.3390/forecast2020006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124554003&doi=10.3390%2Fforecast2020006&partnerID=40&md5=143c1143e1b1541385f42e9f6876da88,scopus,"This study investigates the dynamic relationships between Treasury bill rates in BRICS countries from 2005 to 2019. Using Vector Error Correction Models (VECM) and Markov Switching VECM (MS-VECM), the research validates the Expectation Hypothesis Theory and reveals regime switches affecting short-run adjustments. The MS-VECM model demonstrated superior forecasting accuracy compared to standard VECM, particularly in capturing effects of financial crises. The paper highlights parameter instability and provides dynamic out-of-sample forecasts.",True,True,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T11:01:49.409335
1283bdc00da780ff,Re‐Investigating the UIP Hypothesis: Recent Evidence From BRICS Economies,"The study re‐investigates the existence of the Uncovered interest parity (UIP) hypothesis and substantially adds to the literature by offering the most recent evidence during the period from 2000 to 2022 from developing and emerging economies. The study further augments the literature by extending the standard UIP hypothesis to account for the monetary policy stance and risk premium. The estimates of nonlinear autoregressive distributed lag (NARDL) and component generalised autoregressive conditional heteroscedasticity (C‐GARCH) show that the UIP hypothesis does not exist in any of the BRICS economies. Nevertheless, after accounting for the risk premium and monetary policy stance using inflation levels, the interest rate differential significantly and positively influences the expected changes in the spot exchange rates. This indicates three important aspects: first, the necessity of risk premium to make up for the higher risk that comes with holding the foreign bond for the benefit of domestic investors. Second that the UIP puzzle does not hold, such that higher interest differential depreciates the domestic currency. Third, the analysis underscores the substantial and direct impact of US inflation level, particularly for Brazil, Russia and India, in determining the changes in the spot exchange rate. These insights hold crucial implications for policymakers and regulators.",,2025,10.1111/ecno.70002,,proquest,"This study examines the Uncovered Interest Parity (UIP) hypothesis in BRICS economies from 2000-2022, incorporating risk premium and monetary policy. Using NARDL and C-GARCH models, it finds that the standard UIP hypothesis does not hold. However, when accounting for risk premium and inflation, interest rate differentials significantly predict exchange rate changes, suggesting the importance of risk premium and the impact of US inflation on exchange rates in these economies.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:03:55.480621
c717e8521a127885,Risk-based optimal bidding patterns in the deregulated power market using extended Markowitz model,"Deregulation of power industry has entailed important changes in the energy market. With the power industry being restructured, a generation company (GenCo) sells energy through auctions in a daily market, and submission of the appropriate amount of electricity with the right bidding price is important for a GenCo to maximize their profits and minimize the acceptance risk. The objective of this paper is to propose a novel approach for determination of the optimal biding patterns among GenCos in the deregulated power market using a hybrid of Markowitz Model and Genetic Algorithm (GA). While Markowitz Model as an optimization model considers the risk premium for biding patterns and GA as a search engine, considering the acceptance risk in deregulated market. A case study is used to examine the findings of the proposed approach. Also, to compare the proposed model, neural network by back propagation learning algorithm and real proposed pattern were considered. The numerical results indicate that the proposed model is statistically efficient and offers effective curves and biding patterns by lesser risk and equal profitability in day-ahead market as it is able to achieve better results compared to the neural network. © 2020 Elsevier B.V., All rights reserved.","Ostadi, B.; Motamedi Sedeh, O.; Husseinzadeh Kashan, A.",2020,10.1016/j.energy.2019.116516,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075892372&doi=10.1016%2Fj.energy.2019.116516&partnerID=40&md5=38a0c00b1e470d79a0ad16e40e62044f,scopus,"This paper proposes a novel approach using a hybrid of the Markowitz Model and Genetic Algorithm (GA) to determine optimal bidding patterns for generation companies (GenCos) in deregulated power markets. The approach aims to maximize profits while minimizing acceptance risk by considering risk premiums and using GA as a search engine. A case study compares the proposed model with a neural network, showing the proposed model is statistically efficient, offers effective bidding patterns with lesser risk and equal profitability.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:03:57.700679
98742387c3e762c5,Risk-neutral valuation of participating life insurance contracts in a stochastic interest rate environment,"Over the last years, the valuation of life insurance contracts using concepts from financial mathematics has become a popular research area for actuaries as well as financial economists. In particular, several methods have been proposed of how to model and price participating policies, which are characterized by an annual interest rate guarantee and some bonus distribution rules. However, despite the long terms of life insurance products, most valuation models allowing for sophisticated bonus distribution rules and the inclusion of frequently offered options assume a simple Black-Scholes setup and, more specifically, deterministic or even constant interest rates.We present a framework in which participating life insurance contracts including predominant kinds of guarantees and options can be valuated and analyzed in a stochastic interest rate environment. In particular, the different option elements can be priced and analyzed separately. We use Monte Carlo and discretization methods to derive the respective values.The sensitivity of the contract and guarantee values with respect to multiple parameters is studied using the bonus distribution schemes as introduced in [Bauer, D., Kiesel, R., Kling, A., Russ, J., 2006. Risk-neutral valuation of participating life insurance contracts. Insurance: Math. Econom. 39, 171-183]. Surprisingly, even though the value of the contract as a whole is only moderately affected by the stochasticity of the short rate of interest, the value of the different embedded options is altered considerably in comparison to the value under constant interest rates. Furthermore, using a simplified asset portfolio and empirical parameter estimations, we show that the proportion of stock within the insurer's asset portfolio substantially affects the value of the contract. Published by Elsevier B. V.","Zaglauer, Katharina; Bauer, Daniel",2008,10.1016/j.insmatheco.2007.09.003,,wos,"This paper presents a framework for valuing participating life insurance contracts in a stochastic interest rate environment, using Monte Carlo and discretization methods. It analyzes the sensitivity of contract and guarantee values to various parameters, finding that stochastic interest rates significantly impact embedded options, and that the proportion of stocks in the insurer's portfolio affects contract value. The study contrasts these findings with models assuming constant interest rates.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:04:00.490354
e0a69d7554298af5,Risk-return relationship in equity markets: Using a robust GMM estimator for GARCH-M models,"While most asset pricing models postulate a positive relationship between excess returns and risk, there is no consensus on the nature of the relationship due to conflicting empirical evidence. The relationship is particularly ambiguous within a GARCH-M framework. This paper demonstrates that such a conflict can be attributed primarily to the downward bias of standard estimators that neglect additive outliers (AO) commonly observed in financial returns, and proposes a feasible estimation method (RGMME) for the GARCH-M model based upon a robust variant of the GMM. Monte Carlo experiments demonstrate that AOs cause more serious bias in the ML and GMM estimates of the relationship coefficient than previously expected. Therefore, in the presence of AOs, the RGMME appears superior to other standard estimators in terms of the root mean square error criterion. There is strong evidence favouring the RGMME over standard estimators based on its empirical application. In particular, it is substantially evident from the results of the RGMME that there is support for a positive relationship between excess returns and conditional volatility for all three major equity markets. © 2009 Elsevier B.V., All rights reserved.","Park, B.-J.",2009,10.1080/14697680801898584,https://www.scopus.com/inward/record.uri?eid=2-s2.0-61449224631&doi=10.1080%2F14697680801898584&partnerID=40&md5=299f690c41cd8d9b3024ed2a38d344d4,scopus,"This paper addresses the conflicting empirical evidence on the risk-return relationship in equity markets within a GARCH-M framework. It proposes a robust GMM estimator (RGMME) to account for additive outliers, which are shown to bias standard estimators. Monte Carlo experiments and empirical applications suggest that RGMME is superior and provides strong evidence for a positive relationship between excess returns and conditional volatility in major equity markets.",True,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:04:03.482706
d380412620ad2d3a,Robust term structure estimation in developed and emerging markets,"Despite powerful advances in interest rate curve modeling for data-rich countries in the last 30 years, comparatively little attention has been paid to the key practical problem of estimation of the term structure of interest rates for emerging markets. This may be partly due to limited data availability. However, emerging bond markets are becoming increasingly important and liquid. It is, therefore, important to be understand whether conclusions drawn from developed countries carry over to emerging markets. We estimate model parameters of fully flexible Nelson–Siegel–Svensson term structures model which has become one of the most popular term structure model among academics, practitioners, and central bankers. We investigate four sets of bond data: U.S. Treasuries, and three major emerging market government bond data-sets (Brazil, Mexico and Turkey). By including both the very dense U.S. data and the comparatively sparse emerging market data, we ensure that are results are not specific to a particular data-set. We find that gradient and direct search methods perform poorly in estimating term structures of interest rates, while global optimization methods, particularly the hybrid particle swarm optimization introduced in this paper, do well. Our results are consistent across four countries, both in- and out-of-sample, and for perturbations in prices and starting values. For academics and practitioners interested in optimization methods, this study provides clear evidence of the practical importance of choice of optimization method and validates a method that works well for the NSS model.",,2018,10.1007/s10479-016-2282-5,,proquest,"This paper estimates the Nelson–Siegel–Svensson term structure model for developed (U.S.) and emerging markets (Brazil, Mexico, Turkey), comparing the performance of different optimization methods. It finds that global optimization methods, specifically a hybrid particle swarm optimization, outperform gradient and direct search methods for term structure estimation, with results consistent across countries and data conditions. The study highlights the practical importance of choosing the right optimization method for the NSS model.",True,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:04:20.524150
95cf4c32af8b022e,SCORING: FAILURE RISK MANAGEMENT TOOL FOR SMES IN ALGERIA,"Algerian public banks need to implement credit risk management techniques tailored to the specific characteristics of SMEs to prevent the deterioration of the banks' solvency due to the degradation of the quality of their SME portfolios. In this regard, our main objective is to highlight the interest that credit risk management will have within the People's Credit of Algeria by developing a Credit Scoring model based on the logistic regression technique, using a sample of 226 SMEs. The study results demonstrate the importance of the logistic regression model in classifying companies and its predictive ability for default, with a good classification rate of 91.2%.",,2024,10.2478/bsaft-2024-0018,,proquest,"This study develops a credit scoring model using logistic regression for Algerian SMEs to manage credit risk in public banks. The model achieved a 91.2% classification rate, demonstrating its effectiveness in predicting default.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:04:25.617551
10c0455867adbe4c,SF-Transformer: A Mutual Information-Enhanced Transformer Model with Spot-Forward Parity for Forecasting Long-Term Chinese Stock Index Futures Prices,"The complexity in stock index futures markets, influenced by the intricate interplay of human behavior, is characterized as nonlinearity and dynamism, contributing to significant uncertainty in long-term price forecasting. While machine learning models have demonstrated their efficacy in stock price forecasting, they rely solely on historical price data, which, given the inherent volatility and dynamic nature of financial markets, are insufficient to address the complexity and uncertainty in long-term forecasting due to the limited connection between historical and forecasting prices. This paper introduces a pioneering approach that integrates financial theory with advanced deep learning methods to enhance predictive accuracy and risk management in China’s stock index futures market. The SF-Transformer model, combining spot-forward parity and the Transformer model, is proposed to improve forecasting accuracy across short and long-term horizons. Formulated upon the arbitrage-free futures pricing model, the spot-forward parity model offers variables such as stock index price, risk-free rate, and stock index dividend yield for forecasting. Our insight is that the mutual information generated by these variables has the potential to significantly reduce uncertainty in long-term forecasting. A case study on predicting major stock index futures prices in China demonstrates the superiority of the SF-Transformer model over models based on LSTM, MLP, and the stock index futures arbitrage-free pricing model, covering both short and long-term forecasting up to 28 days. Unlike existing machine learning models, the Transformer processes entire time series concurrently, leveraging its attention mechanism to discern intricate dependencies and capture long-range relationships, thereby offering a holistic understanding of time series data. An enhancement of mutual information is observed after introducing spot-forward parity in the forecasting. The variation of mutual information and ablation study results highlights the significant contributions of spot-forward parity, particularly to the long-term forecasting. Overall, these findings highlight the SF-Transformer model’s efficacy in leveraging spot-forward parity for reducing uncertainty and advancing robust and comprehensive approaches in long-term stock index futures price forecasting.",,2024,10.3390/e26060478,,proquest,"This paper proposes the SF-Transformer model, which integrates spot-forward parity with the Transformer architecture, to improve the accuracy of long-term Chinese stock index futures price forecasting. The model leverages mutual information from variables like stock index price, risk-free rate, and dividend yield, demonstrating superior performance over LSTM, MLP, and arbitrage-free pricing models in predicting prices up to 28 days ahead. The study highlights the significant contribution of spot-forward parity, especially for long-term forecasting.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:04:31.308944
a1733dd5c20d41d0,SPCM: A Machine Learning Approach for Sentiment-Based Stock Recommendation System,"Recommendation systems play a pivotal role in delivering user preference information. However, they often face the challenge of information cocoons due to repeated content delivery, particularly prevalent in stock recommendations that are susceptible to investor sentiment. In response to the information cocoons, we propose the Sentiment and Price Combined Model (SPCM), which leverages sentiment features and price factors to predict stock price movements. This novel framework combines collective sentiment analysis with state-of-the-art BERT transformer models and advanced machine learning techniques. Over a three-year period, we collected 40 million stock comments from the Guba platform, extracting investor sentiment conveyed in text information and investigating the impact of metrics such as homophily on stock recommendations. Experimental results indicate that both the volume of posts and the agreement index affect the effectiveness of investor sentiment, while homophily reduces the accuracy of participants’ stock price judgments. The recognition accuracy of the BERT-based sentiment analysis model reaches an impressive 84.12%, and the portfolio constructed by SPCM yields a cumulative return four times that of the industry benchmark. Furthermore, homogeneous quantitative metrics also enhance diversification in stock selection.",J. Wang; Z. Chen,2024,10.1109/access.2024.3357114,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10411881,ieeexplore,"This paper introduces the Sentiment and Price Combined Model (SPCM), a machine learning approach using sentiment analysis (BERT) and price factors to predict stock price movements and improve stock recommendations. The model was trained on 40 million stock comments and demonstrated a significant improvement in portfolio returns compared to the benchmark.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:04:34.251046
8338ecaf338c20d8,Semiparametric Estimates of Monetary Policy Effects: String Theory Revisited,"We develop flexible semiparametric time series methods for the estimation of the causal effect of monetary policy on macroeconomic aggregates. Our estimator captures the average causal response to discrete policy interventions in a macrodynamic setting, without the need for assumptions about the process generating macroeconomic outcomes. The proposed estimation strategy, based on propensity score weighting, easily accommodates asymmetric and nonlinear responses. Using this estimator, we show that monetary tightening has clear effects on the yield curve and on economic activity. Monetary accommodation, however, appears to generate less pronounced responses from both. Estimates for recent financial crisis years display a similarly dampened response to monetary accommodation. © 2018 Elsevier B.V., All rights reserved.","Angrist, J.D.; Jordà, Ò.; Kuersteiner, G.M.",2018,10.1080/07350015.2016.1204919,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019125628&doi=10.1080%2F07350015.2016.1204919&partnerID=40&md5=c8ba3071db31226cdbb135e277f4b0e5,scopus,"This paper introduces a semiparametric time series method to estimate the causal effect of monetary policy on macroeconomic aggregates. The method uses propensity score weighting to capture average causal responses to discrete policy interventions, allowing for asymmetric and nonlinear responses without assuming the process generating macroeconomic outcomes. The study finds that monetary tightening affects the yield curve and economic activity, while monetary accommodation shows less pronounced responses, especially during recent financial crisis years.",True,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:04:43.789219
d4806ca198cba917,Sentiment lost: the effect of projecting the pricing kernel onto a smaller filtration set,"This paper provides a theoretical analysis on the impacts of using a suboptimal information set for the estimation of the pricing kernel and, more in general, for the validity of the fundamental theorems of asset pricing. While inferring the risk-neutral measure from options data provides a naturally forward-looking estimate, extracting the real world measure from historical returns is only partially informative, thus suboptimal with respect to investors’ future beliefs. As a consequence of this disalignment, the two measures no longer share the same nullset, thus distorting the investors’ risk premium and the validity of the pricing measure. From a probabilistic viewpoint, the missing beliefs are totally unaccessible stopping times on the coarser filtration set, so that an absolutely continuous strict local martingale, once projected on it, becomes continuous with jumps. Some empirical examples complete the paper.",,2020,10.1080/07362994.2019.1711119,,proquest,"This paper theoretically analyzes how using a suboptimal information set (a smaller filtration set) to estimate the pricing kernel affects the fundamental theorems of asset pricing. It explains that this misalignment causes the real-world and risk-neutral measures to have different nullsets, distorting risk premiums and pricing validity. Probabilistically, missing beliefs manifest as inaccessible stopping times, causing continuous processes to exhibit jumps when projected onto the coarser filtration. Empirical examples are included.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:05:38.593461
996f7b5a24667f4b,"Sentiment, Attention, and Earnings Pricing","We find that investor sentiment restrains the predictability of earnings news on announcement returns but the constraining effect of sentiment on the predictive power of earnings news diminishes as sentiment falls. We document that investor attention works as an important channel in the relation between investor sentiment and announcement returns. Investor attention enhances the immediate price reaction to earnings news by curbing the impact of sentiment on the predictive power of earnings news. Our findings reflect the joint effect of attention and sentiment on the source of excess returns documented in the prior earnings-based market anomaly literature. © 2024 Elsevier B.V., All rights reserved.","Cai, Q.; Yung, K.",2024,10.1080/15427560.2022.2100381,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134382035&doi=10.1080%2F15427560.2022.2100381&partnerID=40&md5=380f51696ca6525d0a7916caf01ca30e,scopus,"This study investigates how investor sentiment and attention influence the pricing of earnings news and announcement returns. It finds that sentiment initially restrains predictability but this effect lessens with falling sentiment. Investor attention acts as a channel, enhancing immediate price reactions by mitigating sentiment's impact. The research highlights the combined effect of attention and sentiment on excess returns, contributing to the understanding of earnings-based market anomalies.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:05:52.056430
b426a797302dce5e,Short rate nonlinearities and regime switches,"Using non-parametric estimation methods, various authors have shown distinct non-linearities in the drift and volatility function of the US short rate, which are inconsistent with standard affine term structure models. We document how a regime-switching model with state-dependent transition probabilities between regimes can replicate the patterns found by the non-parametric studies. To do so, we use data from the UK and Germany in addition to US data and include term spreads in some of our models. We also examine the drift and volatility function of the term spread. (C) 2002 Elsevier Science B.V. All rights reserved.","Ang, A; Bekaert, G",2002,10.1016/s0165-1889(01)00042-2,,wos,"This paper uses a regime-switching model with state-dependent transition probabilities to replicate non-linearities in the US short rate's drift and volatility, as identified by non-parametric methods. The study extends its analysis to UK and German data, incorporating term spreads and examining their drift and volatility functions.",True,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:06:02.676548
967c7f9c55b36c26,Shrinkage drift parameter estimation for multi-factor ornstein-uhlenbeck processes,"We consider some inference problems concerning the drift parameters of multi-factors Vasicek model (or multivariate Ornstein-Uhlebeck process). For example, in modeling for interest rates, the Vasicek model asserts that the term structure of interest rate is not just a single process, but rather a superposition of several analogous processes. This motivates us to develop an improved estimation theory for the drift parameters when homogeneity of several parameters may hold. However, the information regarding the equality of these parameters may be imprecise. In this context, we consider Stein-rule (or shrinkage) estimators that allow us to improve on the performance of the classical maximum likelihood estimator (MLE). Under an asymptotic distributional quadratic risk criterion, their relative dominance is explored and assessed. We illustrate the suggested methods by analyzing interbank interest rates of three European countries. Further, a simulation study illustrates the behavior of the suggested method for observation periods of small and moderate lengths of time. Our analytical and simulation results demonstrate that shrinkage estimators (SEs) provide excellent estimation accuracy and outperform the MLE uniformly. An over-ridding theme of this paper is that the SEs provide powerful extensions of their classical counterparts. Copyright © 2009 John Wiley & Sons, Ltd. © 2010 Elsevier B.V., All rights reserved.","Nkurunziza, S.; Ahmed, S.E.",2010,10.1002/asmb.775,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950822342&doi=10.1002%2Fasmb.775&partnerID=40&md5=87b9e60a35435f4613bac68331ed6461,scopus,"This paper proposes shrinkage estimators for the drift parameters of multi-factor Ornstein-Uhlenbeck processes, motivated by interest rate modeling. The study explores the performance of these estimators against the classical maximum likelihood estimator under quadratic risk, using interbank interest rates from three European countries for illustration. Simulation studies assess the estimators' behavior with short and moderate observation periods, concluding that shrinkage estimators offer improved accuracy and outperform the MLE.",False,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:06:04.981238
f566ead501f33867,Shrinking return forecasts,"We develop a new approach that shrinks a given model forecast to the benchmark model forecast in order to improve forecasting performance. Simulation results show the superior performance of our approach, relative to popular methods such as forecast combination and the robustness to model misspecification. We apply our method to forecasting the returns on the S&P 500 index and find significant predictability when shrinking the principal component (PC) regression forecasts based on statistical and economic evaluation criteria. The forecast improvement from our shrinkage approach can be explained by the ability of its hyperparameters to be better predict real economic changes.","Liu, Li; Pan, Zhiyuan; Wang, Yudong",2022,10.1111/fire.12297,,wos,"This paper introduces a novel method for improving forecast accuracy by shrinking a given model's forecast towards a benchmark model's forecast. The approach demonstrates superior performance in simulations compared to existing methods and is applied to forecasting S&P 500 returns, showing significant predictability. The improvement is attributed to hyperparameters better predicting economic changes.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:06:07.094750
60af6446b18c4e40,Sign realized jump risk and the cross-section of stock returns: Evidence from China's stock market,"Using 5-minute high frequency data from the Chinese stock market, we employ a non-parametric method to estimate Fama-French portfolio realized jumps and investigate whether the estimated positive, negative and sign realized jumps could forecast or explain the cross-sectional stock returns. The Fama-MacBeth regression results show that not only have the realized jump components and the continuous volatility been compensated with risk premium, but also that the negative jump risk, the positive jump risk and the sign jump risk, to some extent, could explain the return of the stock portfolios. Therefore, we should pay high attention to the downside tail risk and the upside tail risk.Using 5-minute high frequency data from the Chinese stock market, we employ a non-parametric method to estimate Fama-French portfolio realized jumps and investigate whether the estimated positive, negative and sign realized jumps could forecast or explain the cross-sectional stock returns. The Fama-MacBeth regression results show that not only have the realized jump components and the continuous volatility been compensated with risk premium, but also that the negative jump risk, the positive jump risk and the sign jump risk, to some extent, could explain the return of the stock portfolios. Therefore, we should pay high attention to the downside tail risk and the upside tail risk.",,2017,10.1371/journal.pone.0181990,,proquest,"This study uses high-frequency data from China's stock market to estimate realized jumps and their components (positive, negative, and sign jumps). It then investigates whether these jump risks, along with continuous volatility, can explain or forecast cross-sectional stock returns using Fama-MacBeth regressions. The findings suggest that both jump components and continuous volatility are compensated with risk premiums, and that negative, positive, and sign jump risks contribute to explaining stock portfolio returns, highlighting the importance of tail risks.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:06:09.148668
fcf25faf491d1950,Smoothing spline nonlinear nonparametric regression models,"Almost all of the current nonparametric regression methods, such as smoothing splines, generalized additive models, and varying-coefficients models, assume a linear relationship when nonparametric functions are regarded as parameters. In this article we propose a general class of smoothing spline nonlinear nonparametric models that allow nonparametric functions to act nonlinearly. They arise in many fields as either theoretical or empirical models. Our new estimation methods are based on an extension of the Gauss-Newton method to infinite-dimensional spaces and the backfilling procedure. We extend the generalized cross-validation and generalized maximum likeli-hood methods to estimate smoothing parameters. We establish connections between some nonlinear nonparametric models and nonlinear mixed-effects models. We derive approximate Bayesian confidence intervals for inference. We illustrate the methods with an application to term structure of interest rates and conduct simulations to evaluate the finite-sample performance of our methods. © 2012 Elsevier B.V., All rights reserved.","Ke, C.; Wang, Y.",2004,10.1198/016214504000000755,https://www.scopus.com/inward/record.uri?eid=2-s2.0-10844220603&doi=10.1198%2F016214504000000755&partnerID=40&md5=279118ed9878d6bf94922de6cfe6fa44,scopus,"This paper introduces a new class of smoothing spline nonlinear nonparametric models that allow nonparametric functions to act nonlinearly, extending current methods that assume linearity. The proposed estimation methods utilize an extension of the Gauss-Newton method and a backfilling procedure. The authors also extend generalized cross-validation and generalized maximum likelihood for smoothing parameter estimation, establish connections to nonlinear mixed-effects models, and derive approximate Bayesian confidence intervals. The methods are demonstrated with an application to the term structure of interest rates and evaluated through simulations.",True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:06:49.373361
1045c81233fea906,Some benefits and costs of genetic improvement in new zealand’s sheep and beef cattle industry:ii discounted costs and returns on a farm basis following selection,"Estimated physical returns and their calculated value net of food costs have been used as the basis for an analysis of discounted costs and returns from a selection programme for a 200-ewe flock and a 100-cow beef herd. Recording costs (computing, labour, tags, and scales) of $1–2 per ewe and $2–3 per cow are assumed, with a discount rate of 10% as used by the New Zealand Treasury. It is also assumed that no stock for breeding were purchased or sold. Results are expressed in 1979 dollars, for 1 year or round of selection applied. The net present value of 10 years of selection is $4950 or $3600 in sheep ($1 or $2 per ewe in costs) and $3670–4870 in beef cattle, depending on recording costs and the assumed level of food costs incurred by the herd in achieving higher weight-for-age. With sheep, net production is much more important than fleece weight in its contribution to higher profits. A very important contribution in the beef herd under consideration is the return from a higher net calf crop; even small percentage biological gains are of great economic importance. Net returns for a breeder also selling stock for breeding would probably be much higher. © 1980 Taylor & Fracis Group, LLC. © 2016 Elsevier B.V., All rights reserved.","Morris, C.A.",1980,10.1080/03015521.1980.10426284,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34447574295&doi=10.1080%2F03015521.1980.10426284&partnerID=40&md5=52a22c886f9f1a8745ae7df917205a97,scopus,"This study analyzes the discounted costs and returns of genetic improvement programs for sheep and beef cattle in New Zealand. It estimates physical returns, net of food costs, and applies a 10% discount rate. The analysis considers recording costs and assumes no stock purchases or sales. Results, expressed in 1979 dollars, indicate positive net present values for 10 years of selection in both sheep and beef cattle, with net production being crucial for sheep profits and higher net calf crop for beef herds.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:06:51.150403
40b1a6f001f4f67c,Sovereign bond yield spreads: A time-varying coefficient approach,"We study the determinants of sovereign bond yield spreads across 10 EMU countries between Q1/1999 and Q1/2010. We apply a semiparametric time-varying coefficient model to identify, to what extent an observed change in the yield spread is due to a shift in macroeconomic fundamentals or due to altering risk pricing. We find that at the beginning of EMU, the government debt level and the general investors' risk aversion had a significant impact on interest differentials. In the subsequent years, however, financial markets paid less attention to the fiscal position of a country and the safe haven status of Germany diminished in importance. By the end of 2006, two years before the fall of Lehman Brothers, financial markets began to grant Germany safe haven status again. One year later, when financial turmoil began, the market reaction to fiscal loosening increased considerably. The altering in risk pricing over time period confirms the need of time-varying coefficient models in this context. © 2011 Elsevier Ltd. © 2012 Elsevier B.V., All rights reserved.","Bernoth, K.; Erdogan, B.",2012,10.1016/j.jimonfin.2011.10.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857451801&doi=10.1016%2Fj.jimonfin.2011.10.006&partnerID=40&md5=b27a4c46ffb5293d5ad3fdea6037a760,scopus,"This study investigates the determinants of sovereign bond yield spreads in 10 EMU countries from 1999 to 2010 using a semiparametric time-varying coefficient model. It distinguishes between changes in macroeconomic fundamentals and shifts in risk pricing. The findings indicate that initially, government debt and investor risk aversion were significant, but their importance waned over time. Germany's safe haven status also fluctuated. The study highlights the increased market reaction to fiscal loosening during financial turmoil and emphasizes the necessity of time-varying coefficient models.",True,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:07:01.389484
f727ea2b065264c6,Sovereign debt spreads in EMU: The time-varying role of fundamentals and market distrust,"This paper provides further analysis on the determinants of sovereign debt spreads for peripheral Eurozone countries since the start of EMU, paying special attention to episodes that characterized the global financial crisis aftermath starting in 2007. More specifically, the purpose of our research is to disentangle the role of fundamental variables and market perception about variations on risk in order to explain the evolution of sovereign spreads in EMU during the recent crisis. Our results, in line with previous literature, show the importance of three groups of observable variables, namely, changes in risk-aversion of creditors, fiscal indebtedness and liquidity variables. In addition, our model includes unobserved components that are estimated through the Kalman filter as time-varying deviation from fixed-mean parameters of spread determinants. This shows the importance of expectations (market sentiments), amplifying (or reducing) the relative importance of the spread determinants over time through the time-varying behavior of the parameters around their steady-state estimates. © 2017 Elsevier B.V., All rights reserved.","Paniagua, J.; Sapena, J.; Tamarit, C.",2017,10.1016/j.jfs.2016.06.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003968366&doi=10.1016%2Fj.jfs.2016.06.004&partnerID=40&md5=30cea9ab8a37d258da1d42b867fa74c8,scopus,"This paper analyzes the determinants of sovereign debt spreads in peripheral Eurozone countries, focusing on the period since the start of EMU and the aftermath of the global financial crisis. It disentangles the roles of fundamental variables (risk aversion, fiscal indebtedness, liquidity) and market perception (expectations, market sentiments) in explaining spread variations. The study uses a model with unobserved components estimated via the Kalman filter to capture time-varying parameters, highlighting how market sentiments amplify or reduce the importance of spread determinants.",False,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:07:03.629347
6d7c5e9a1bb1743e,Sovereign default risk premia: Evidence from the default swap market,"This study explores the risk premia embedded in sovereign default swaps using a term structure model. The risk premia remunerate investors for unexpected changes in the default intensity. A number of interesting results emerge from the analysis. First, the risk premia contribution to spreads decreases over the sample, 2003-07, and rebounds at the start of the 'credit crunch.' Second, daily risk premia co-move with US macro variables and corporate default risk. Third, global factors explain most of Latin American countries' premia, and local factors best explain European and Asian premia. The importance of global factors grows over time. Finally, conditioning on lagged local and global variables at a weekly frequency, sovereign risk premia are highly predictable. (c) 2012 Elsevier B.V. All rights reserved.","Zinna, Gabriele",2013,10.1016/j.jempfin.2012.12.006,,wos,"This study analyzes sovereign default swap data from 2003-07 to understand risk premia. It finds that risk premia, which compensate investors for unexpected changes in default intensity, decreased over the sample and then increased during the credit crunch. Daily risk premia correlate with US macro variables and corporate default risk. Global factors significantly influence Latin American countries' premia, while local factors are more important for European and Asian countries, with the influence of global factors increasing over time. The study also demonstrates that sovereign risk premia are predictable using lagged local and global variables at a weekly frequency.",False,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:07:05.979260
9efd0961480d44fa,Stock Market Over-reaction: The South African Evidence,"It has been suggested that stock markets over-react and that investors pay too much attention to recent “dramatic” news. If over-reaction does occur and prices overshoot then there should be a subsequent revision in the opposite direction. This paper outlines empirical research into the over-reaction hypothesis on the Johannesburg Stock Exchange using data over the period July 1974 to June 1989 for two hundred and four relatively well traded securities. The results are consistent with the over-reaction hypothesis and indicate substantial weak form inefficiencies in the South African stock market in the long-term. The performance of portfolios of shares formed on the basis of prior return data can be predicted and, on average, portfolios of prior ‘losers’ outperformed prior ‘winners’ by about twenty percent over the three years after portfolio formation. Finally, comparison between the empirical results and a similar study for the New York Stock Exchange calls into some question the hypothesis that exceptionally large returns in January in the USA are due to investor tax loss selling. There is evidence of both a January effect and an asymmetric excess returns effect for the South African market but it is less pronounced than for the American market. © 1992, Taylor & Francis Group, LLC. All rights reserved. © 2015 Elsevier B.V., All rights reserved.","Page, M.J.; Way, C.V.",1992,10.1080/10293523.1992.11082314,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649803751&doi=10.1080%2F10293523.1992.11082314&partnerID=40&md5=9cd756ed9454bc64c6b0446e9333f57b,scopus,"This paper investigates the stock market over-reaction hypothesis on the Johannesburg Stock Exchange using data from July 1974 to June 1989. The findings suggest that the market does over-react, leading to inefficiencies where portfolios of 'losers' outperform 'winners' by approximately 20% over three years. The study also notes a less pronounced January effect and asymmetric excess returns in South Africa compared to the US market.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:07:54.505499
f529c768b7421166,Stock Ranking with Multi-Task Learning,"Stock prediction, aiming at predicting the future trends of stocks, plays a key role in stock investment. Towards the investment target, the primary task is selecting the stocks with potentials to obtain the highest excess returns, always regarded as stock ranking. List-wise stock ranking is able to consider the relative comparisons of multiple stocks, approaching the essence of stock ranking most. However, most existing methods fail in list-wise stock ranking, because the information complexity and small number of samples bring in training difficulties. To address these limitations, a novel Deep Multi-Task Learning (DMTL) solution is proposed, called Multi-Task Stock Ranking (MTSR). It utilizes the joint learning framework of DMTL to learn the list-wise stock ranking with the enhancements of auxiliary tasks. With DMTL, the easily-trained tasks act as learning guider, providing extra gradient backpropagation, to help learn the hardly-trained list-wise ranking task. Additionally, Task Relation Attention is utilized to capture the dynamic task relations to achieve better knowledge transfer between tasks. The experiments conducted on real-world stock datasets demonstrate the superiority of MTSR over several state-of-the-art methods. © 2022 Elsevier B.V., All rights reserved.","Ma, T.; Tan, Y.",2022,10.1016/j.eswa.2022.116886,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127127011&doi=10.1016%2Fj.eswa.2022.116886&partnerID=40&md5=8ec2a8a3cf191d9de530898998c026d3,scopus,"This paper proposes a novel Deep Multi-Task Learning (DMTL) solution called Multi-Task Stock Ranking (MTSR) to address the challenges in list-wise stock ranking. MTSR utilizes a joint learning framework with auxiliary tasks to enhance the learning of the primary stock ranking task, incorporating Task Relation Attention for dynamic knowledge transfer. Experiments on real-world stock data show MTSR outperforms existing methods.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:07:56.073304
4d2e4e1efd54f90f,Stock market volatility predictability in a data-rich world: A new insight,"This study develops a shrinkage method, LASSO with a Markov regime-switching model (MRS-LASSO), to predict US stock market volatility. A set of 17 well-known macroeconomic and financial factors are used. The out-of-sample results reveal that the MRS-LASSO model yields statistically and economically significant volatility predictions. We further investigate the predictability of MRS-LASSO with respect to different market conditions, business cycles, and variable selection. Three factors (equity market returns, a short-term reversal factor, and a consumer sentiment index) are the most frequent predictors. To investigate the practical implications, we construct the expected variance risk premium (VRP) by using volatility forecasts generated from the LASSO and MRS-LASSO models to forecast future stock returns and find that those models are also powerful. © 2023 Elsevier B.V., All rights reserved.","Ma, F.; Wang, J.; Wahab, M.I.M.; Ma, Y.",2023,10.1016/j.ijforecast.2022.08.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140291431&doi=10.1016%2Fj.ijforecast.2022.08.010&partnerID=40&md5=2f88412fe379d55e2ce23fed80d4a33d,scopus,"This study introduces a new shrinkage method, MRS-LASSO, to predict US stock market volatility using macroeconomic and financial factors. The model demonstrates significant out-of-sample predictive power and identifies key predictors like equity market returns, a short-term reversal factor, and a consumer sentiment index. The forecasts also prove useful in predicting future stock returns.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:07:57.955128
47139e88004ecf9f,Stock return prediction: Stacking a variety of models,"We employ an ensemble learning approach, “stacking”, to refine and combine a variety of linear and nonlinear individual stock return prediction models. In an application of forecasting U.S. market excess return, stacking with a simple structure can outperform the traditional historical mean benchmark, Mallows model averaging, simple combination forecast, complete subset regression, combination elastic net forecast, and several other models in terms of both in- and out-of-sample performance measures on a consistent basis. More importantly, we find that the out-of-sample gains of stacking are especially evident during extreme downside market movements. Overall, stacking can generate substantive improvements in market excess return predictability. © 2022 Elsevier B.V., All rights reserved.","Zhao, A.B.; Cheng, T.",2022,10.1016/j.jempfin.2022.04.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129550064&doi=10.1016%2Fj.jempfin.2022.04.001&partnerID=40&md5=d484a59706ba72cf605ca0e12ac292de,scopus,"This study utilizes an ensemble learning technique called stacking to improve stock return prediction by combining various linear and nonlinear models. The approach demonstrates superior performance compared to traditional benchmarks and other combination methods, particularly in predicting excess market returns and during periods of significant market downturns.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:07:59.320359
9968a1106eb5e0f0,Stripping the Swiss discount curve using kernel ridge regression,"We analyze and implement the kernel ridge regression (KR) method developed in Filipovic et al. (Stripping the discount curve—a robust machine learning approach. Swiss Finance Institute Research Paper No. 22–24. SSRN. https://ssrn.com/abstract=4058150, 2022) to estimate the risk-free discount curve for the Swiss government bond market. We show that the insurance industry standard Smith–Wilson method is a special case of the KR framework. We recapitulate the curve estimation methods of the Swiss Solvency Test (SST) and the Swiss National Bank (SNB). In an extensive empirical study covering the years 2010–2022 we compare the KR curves with the SST and SNB curves. The KR method proves to be robust, flexible, transparent, reproducible and easy to implement, and outperforms the benchmarks in- and out-of-sample. We show the limitations of all methods for extrapolating the yield curve and propose possible solutions for the extrapolation problem. We conclude that the KR method is the preferred method for estimating the discount curve. © 2024 Elsevier B.V., All rights reserved.","Camenzind, N.; Filipovic, D.",2024,10.1007/s13385-024-00386-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195370618&doi=10.1007%2Fs13385-024-00386-4&partnerID=40&md5=f8b724d5222c2422b156f081fb57b69f,scopus,"This paper implements and analyzes the kernel ridge regression (KR) method to estimate the risk-free discount curve for the Swiss government bond market. It demonstrates that the Smith-Wilson method is a special case of KR and compares KR with the Swiss Solvency Test (SST) and Swiss National Bank (SNB) methods. The study finds KR to be robust, flexible, transparent, reproducible, and superior to benchmarks in empirical tests from 2010-2022. Limitations in yield curve extrapolation are discussed, and KR is recommended as the preferred method.",True,True,False,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T11:10:31.252853
76f158d70acb9ec7,Structural models of corporate bond pricing with maximum likelihood estimation,"This paper empirically examines the proxy, volatility-restriction (VR) and maximum likelihood (ML) approaches to implementing structural corporate bond pricing models, and documents that ML estimation is the best among the three implementation methods. Empirical studies using either the proxy approach or the VR method conclude that barrier-independent models significantly underestimate corporate bond yields. Although barrier-dependent models tend to overestimate the yield on average, they generate a sizable degree of underestimation. The present paper shows that the proxy approach is an upwardly biased estimator of the corporate assets and makes the empirical framework work systematically against structural models of corporate bond pricing. The VR approach may generate inconsistent corporate bond prices or may fail to give a positive corporate bond price for some structural models. When the Merton, LS, BD and LT models are implemented with ML estimation, we find substantial improvement in their performances. Our empirical analysis shows that the LT model is very accurate for predicting short-term bond yields, whereas the LS and BD models are good predictors for medium-term and long-term bonds. The Merton model however significantly overestimates short-term bond yields and underestimates long-term bond yields. Unlike empirical studies in the past, the Merton model implemented with ML estimation does not consistently underestimate corporate bond yields. All rights reserved, Elsevier",,2008,10.1016/j.jempfin.2008.01.001,,proquest,"This paper compares three methods for implementing structural corporate bond pricing models: proxy, volatility-restriction (VR), and maximum likelihood (ML). It finds that ML estimation performs best. The study demonstrates that the proxy approach is a biased estimator and the VR approach can lead to inconsistent prices. When implemented with ML, the Merton, LS, BD, and LT models show improved performance. The LT model is accurate for short-term yields, LS and BD for medium- to long-term yields, and the Merton model's performance improves significantly compared to previous studies.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:10:33.585334
b583a5af78b01dc0,Surrogate-assisted optimal re-dispatch control for risk-aware regulation of dynamic total transfer capability,"To enable reliable power delivery through transmission tie-lines, total transfer capability (TTC) must be calculated and regulated to accommodate the transferred amount. However, the traditional optimal power flow (OPF)-based total transfer capability calculation is computationally expensive for efficient total transfer capability control due to the inclusion of a large set of differential-algebraic equations (DAEs) to verify transient stability constraints. In order to enable practicable total transfer capability regulation, a novel risk-aware deep learning-assisted paradigm is proposed here. First, a deep belief network (DBN) is employed to establish the total transfer capability predictor and surrogate the computation-intensive differential-algebraic equations in original optimal power flow formulas, simplifying the high-dimensional and intractable constraints deep belief networks without loss of nonlinearity. Particularly, in order to be aware of control risk from the predictive error of the deep belief networks, prediction intervals (PIs) are produced improved by using ensemble learning and used to disclose the probability of insufficient actions, further guaranteeing the sufficient and cost-effective control by compromising the tradeoff between cost and risk. Symbiotic organisms search (SOS) is then applied to solve the proposed risk-aware deep belief network-assisted total transfer capability control problem globally. The numerical studies testify that the proposed method enables economical, reliable, and full nonlinearity-retained dynamic total transfer capability regulation control within a risk-free surrogate-assisted and tractable physical model-driven hybrid framework. © 2021 Elsevier B.V., All rights reserved.","Qiu, G.; Liu, Y.; Liu, J.; Wang, L.; Liu, T.; Gao, H.; Jawad, S.",2021,10.1049/gtd2.12147,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101158129&doi=10.1049%2Fgtd2.12147&partnerID=40&md5=73bcf2eadd06cf306adf48ea591817a5,scopus,"This paper proposes a novel risk-aware deep learning-assisted paradigm for total transfer capability (TTC) regulation in power systems. It uses a deep belief network (DBN) as a surrogate for computationally expensive differential-algebraic equations (DAEs) to predict TTC. Ensemble learning is used to generate prediction intervals (PIs) to quantify control risk, and Symbiotic Organisms Search (SOS) is applied to solve the risk-aware control problem. The method aims for economical, reliable, and nonlinearity-retained dynamic TTC regulation within a tractable framework.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:10:35.025991
5b3f2c4540a8a7c5,THE REGIME-DEPENDENT EVOLUTION of CREDIBILITY: A FRESH LOOK at Hong Kong'S LINKED EXCHANGE RATE SYSTEM,"An estimated Markov-switching DSGE modeling framework that allows for parameter shifts across regimes is employed to test the hypothesis of regime-dependent credibility of Hong Kong's linked exchange rate system. The baseline model distinguishes two regimes with respect to the time-series properties of the risk premium. Regime-dependent impulse responses to macroeconomic shocks reveal substantial differences in spreads. To test the sensitivity of the results, a number of robustness checks are performed. The findings contribute to efforts at modeling exchange rate regime credibility as a nonlinear process with two distinct regimes. © 2019 Elsevier B.V., All rights reserved.","Blagov, B.; Funke, M.",2019,10.1017/s136510051700075x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045762185&doi=10.1017%2FS136510051700075X&partnerID=40&md5=e7efb5e1e34ec2ce7ac00bf000c579d0,scopus,"This study uses a Markov-switching DSGE model to analyze the credibility of Hong Kong's linked exchange rate system, considering parameter shifts across different economic regimes. The model identifies two distinct regimes based on risk premium time-series properties and shows regime-dependent responses to macroeconomic shocks, impacting spreads. Robustness checks confirm these findings, suggesting that exchange rate regime credibility can be modeled as a nonlinear process with two regimes.",True,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:10:37.536812
b1aa1d6583414fbd,THE VALUE OF MORTGAGE PREPAYMENT AND DEFAULT OPTIONS,"We use an implicit alternating direction numerical procedure to estimate the value of a fixed-rate mortgage (FRM) with embedded default and prepayment options. The value of FRMs depends on interest rates, the house value, and mortgage maturity. Our numerical results suggest that the joint option value of prepayment and default is considerably high, even at loan origination. We extend the model to include prepayment penalties in FRM valuation. (C) 2009 Wiley Periodicals, Inc. Jrl Fut Mark 29:840-861, 2009","Chen, Yong; Connolly, Michael; Tang, Wenjin; Su, Tie",2009,10.1002/fut.20388,,wos,"This paper uses a numerical procedure to estimate the value of fixed-rate mortgages with embedded default and prepayment options, considering interest rates, house value, and mortgage maturity. The study finds that the combined option value of prepayment and default is significant even at loan origination and explores the impact of prepayment penalties.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:10:38.789238
c3de53dd92c70087,Targeting Long Rates in a Model with Segmented Markets,"This paper develops a model of segmented financial markets in which the net worth of financial institutions limits the degree of arbitrage across the term structure. The model is embedded into the canonical Dynamic New Keynesian (DNK) framework. We estimate the model using data on the term premium. Our principal results include the following. First, the estimated segmentation coefficient implies a nontrivial effect of central bank asset purchases on yields and real activity. Second, there are welfare gains to having the central bank respond to the term. premium, e.g., including the term premium in the Taylor Rule. Third, a policy that directly targets the term premium sterilizes the real economy from shocks originating in the financial sector. A term-premium peg can have sigmficant welfare effects. (ILL E12, E23, E31, E43, E52, E58)","Carlstrom, Charles T.; Fuerst, Timothy S.; Paustian, Matthias",2017,10.1257/mac.20150179,,wos,"This paper presents a model of segmented financial markets where institutional net worth restricts arbitrage across the term structure. Embedded within the Dynamic New Keynesian (DNK) framework, the model is estimated using term premium data. Key findings suggest that central bank asset purchases significantly impact yields and economic activity, and that central bank policy should consider the term premium for welfare gains and to insulate the real economy from financial shocks.",False,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:10:40.454077
8b3b7d7aa07bfbd6,Techno-Economic Investment Risk Modeling of Battery Energy Storage System Participating in Day-Ahead Frequency Regulation Market,"Owing to its high capital cost, Battery Energy Storage System (BESS) investment risk has received considerable attention in recent years. Currently, day-ahead frequency regulation service is one major revenue source for BESS, and the revenue is exposed to a compound of stochastic market risk and technical risk. On the market risk side, due to a lack of long-term contracts, investors are exposed to “price risk” and “volume (revenue hours) risk” over the entire investment horizon of 5–10 years. On the technical side, performance issues such as equipment degradation and inherent defects can result in reduced or even negative revenue for BESS under the pay-for-performance remuneration structure. Quantifying these risks is important for investors and banks to assess a project’s investability and bankability. However, existing BESS techno-economic literature has mostly focused on developing optimal control strategies to maximize revenue or optimal battery sizing to reduce capital expenditure. To our knowledge, none of the literature to date has addressed the long-term risk perspective of BESS investment. This study aims to fill this gap by developing a long-term probabilistic revenue estimate that considers these risk factors using Monte Carlo simulations. A case study using Taiwan’s newly launched day-ahead market, which has similar grid dynamics and revenue risk factors as in most markets, is also presented in this paper. Simulation result shows that, for the conservative P90 (90% exceedance probability) scenario, expected return of a hypothetical 10MW (half-hour battery) BESS investment is 8.65% and its debt service coverage ratio is 1.189.",P. -H. Hsi; J. C. P. Shieh,2024,10.1109/access.2024.3390439,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10504254,ieeexplore,"This study addresses the long-term investment risk of Battery Energy Storage Systems (BESS) participating in the day-ahead frequency regulation market. It develops a probabilistic revenue estimation model using Monte Carlo simulations to account for market risks (price and volume) and technical risks (equipment degradation). A case study in Taiwan demonstrates the model's application, showing an expected return of 8.65% under a P90 scenario for a hypothetical BESS investment.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:10:41.768662
88ec6de29272b73e,Term structure estimation with liquidity-adjusted Affine Nelson Siegel model: A nonlinear state space approach applied to the Indian bond market,"Efficient term structure estimation in emerging markets is difficult not only because of overall lack of liquidity, but also because of the concentration of liquidity in a few securities. Using the arbitrage-free Affine Nelson-Siegel model, we explicitly incorporate this phenomenon using a proxy for liquidity based on observable data in the bond pricing function and estimate the term structure for Indian Government bond markets in a nonlinear state space setting using the Unscented Kalman Filter. We find strong empirical evidence in support of the extended model with both i) a better in-sample fit to bond prices, and ii) the likelihood ratio test rejecting the restrictions assumed in the standard AFNS specification. In an alternative specification, we also model liquidity as a latent risk factor within the AFNS framework. The estimated latent liquidity factor is found to be strongly correlated with the standard market benchmarks of overall liquidity and the India VIX index.",,2022,10.1080/00036846.2021.1967866,,proquest,This study estimates the term structure of the Indian bond market using a liquidity-adjusted Affine Nelson-Siegel model within a nonlinear state space framework. The model explicitly accounts for liquidity issues prevalent in emerging markets. The findings suggest the extended model provides a better fit to bond prices and that a latent liquidity factor is strongly correlated with market liquidity benchmarks.,True,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:11:01.162738
045ccef9485f38e1,Testing the empirical performance of stochastic volatility models of the short-term interest rate,"I introduce two-factor discrete time stochastic volatility models of the short-term interest rate to compare the relative performance of existing and alternative empirical specifications. I develop a nonlinear asymmetric framework that allows for comparisons of non-nested models featuring conditional heteroskedasticity and sensitivity of the volatility process to interest rate levels. A new class of stochastic volatility models with asymmetric drift and nonlinear asymmetric diffusion process is introduced in discrete time and tested against the popular continuous time and symmetric and asymmetric GARCH models. The existing models are rejected in favor of the newly proposed models because of the asymmetric drift of the short rate, and the presence of nonlinearity, asymmetry, GARCH, and level effects in its volatility. I test the predictive power of nested and non-nested models in capturing the stochastic behavior of the risk-free rate. Empirical evidence on three-, six-, and 12-month U.S. Treasury bills indicates that two-factor stochastic volatility models are better than diffusion and GARCH models in forecasting the future level and volatility of interest rate changes. © 2018 Elsevier B.V., All rights reserved.","Bali, T.G.",2000,10.2307/2676190,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034386111&doi=10.2307%2F2676190&partnerID=40&md5=9adf8aee423204a0e105653f3f21f549,scopus,"This paper introduces and empirically tests two-factor discrete time stochastic volatility models for the short-term interest rate. It compares these models against existing specifications, including GARCH models, and finds that the proposed models, which account for asymmetric drift and nonlinear asymmetric diffusion, perform better in forecasting interest rate levels and volatility. The study uses U.S. Treasury bill data.",True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:11:35.063800
02493d3e75cc8c16,"The ""forward premium puzzle"" and the sovereign default risk","Carry-trade strategies which consist of buying forward high-yield currencies tend to yield positive excess returns when global financial markets are booming, whereas they generate losses during crises. Firstly, we show that the sovereign default risk, which is taken on by investing in high-yield currencies, may increase the magnitude of the gains during the boom periods and the losses during crises. We empirically test for this hypothesis on a sample of 18 emerging currencies over the period from June 2005 to September 2010, the default risk being proxied by the sovereign credit default swap spread. Relying on smooth transition regression (STR) models, we show that default risk contributes to the carry-trade gains during booms, and worsens the losses during busts. Secondly, we turn to the ""Fama regression"" linking the exchange-rate depreciation to the interest-rate differential. We propose a nonlinear estimation of this equation, explaining the puzzling evolution of its coefficient by the change in the market volatility along the financial cycle. Then, we introduce the default risk into this equation and show that the ""forward bias"", usually evidenced by a coefficient smaller than unity in this regression, is somewhat alleviated, as the default risk is significant to explain the exchange-rate change. © 2012 Elsevier Ltd. © 2017 Elsevier B.V., All rights reserved.","Coudert, V.; Mignon, V.",2013,10.1016/j.jimonfin.2012.05.025,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874775246&doi=10.1016%2Fj.jimonfin.2012.05.025&partnerID=40&md5=b7fab6ec75aa72f841ac0789d409a42e,scopus,"This study investigates the ""forward premium puzzle"" and sovereign default risk in relation to carry-trade strategies. It demonstrates that sovereign default risk amplifies both gains during economic booms and losses during crises for carry trades involving high-yield currencies. The research uses a sample of 18 emerging currencies from 2005-2010, with sovereign credit default swap spreads as a proxy for default risk. Smooth transition regression (STR) models are employed to show the impact of default risk on carry-trade performance. Additionally, the study examines the Fama regression, linking exchange-rate depreciation to interest-rate differentials, and proposes a nonlinear estimation to account for market volatility changes. The findings suggest that default risk helps explain the ""forward bias"" observed in this regression.",True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:11:53.806976
1144b886f92f558b,The Case for Causal Factor Investing,"Researchers use factor models to obtain unbiased estimates of the premia harvested by assets exposed to certain risk characteristics. These estimates are unbiased only if the factor models are correctly specified. Choosing the correct model specification requires knowledge of the causal graph that characterizes the underlying data-generating process. Following the current econometric canon, however, factor researchers choose their model specifications using associational (noncausal) arguments, such as the model's explanatory power, instead of applying causal inference procedures, such as do-calculus. As a result, factor investing models are likely misspecified, and the estimates of risk premia are biased. This article explains the dire consequences of factor investing's specification errors and calls for the need to rebuild the discipline under the more scientific foundations of causal factor investing. © 2024 Elsevier B.V., All rights reserved.","de Prado, M.; Lipton, A.; Zoonekynd, V.",2024,10.3905/jpm.2024.51.1.146,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209686202&doi=10.3905%2Fjpm.2024.51.1.146&partnerID=40&md5=b31ceb8daae39541bf7abf6b6d68e29d,scopus,"This article argues that current factor investing models are likely misspecified due to the use of associational rather than causal inference methods, leading to biased estimates of risk premia. It calls for a shift towards causal factor investing for more scientifically sound foundations.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:13:27.093762
1aec91d9d8da5a64,The Dynamic Impact of Macro Factors on the Performance of Blended Real Estate Equity Strategies,"This article uses a small number of macro factors to model the performance of private US core real estate and associated blended strategies that incorporate listed and private non-core components. The macro factors selected are economic growth, real rate, expected inflation, the term structure, and credit spreads. Private real estate performance was de-smoothed using a nonlinear modeling approach that accounted for differing smoothing effects during identifiable regimes through market cycles. The estimated linear factor loadings are aligned with economic intuition and expectations, including real estate’s inflation hedging characteristics. Using threshold regression modeling to capture nonlinearities in the relationships, a smaller number of the factors were found to be of greater statistical significance. The impact of these factors is found to evolve over time, particularly during phases of market disruption. Although linear factor modeling remains the common approach to estimate risk–return exposures for asset allocation and portfolio risk management processes, the results suggest that these linear models should be adapted to consider these shifting relationships and resulting implications. © 2025 Elsevier B.V., All rights reserved.","Farrelly, K.; Moss, A.",2025,10.3905/jpm.2025.51.11.142,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017711928&doi=10.3905%2Fjpm.2025.51.11.142&partnerID=40&md5=ef0473a31b5e01f9ac2af83908260360,scopus,"This study models the performance of blended real estate equity strategies using key macro factors like economic growth, real rates, inflation, term structure, and credit spreads. It employs a nonlinear approach to de-smooth private real estate performance, accounting for market cycle variations. The findings indicate that while linear factor models are common, nonlinearities and evolving factor impacts, especially during market disruptions, necessitate model adaptation.",True,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:13:30.274848
1206b2b863e2d10a,The Effect of Risk Factor Disclosures on the Pricing of Credit Default Swaps,"This study examines the relation between narrative risk disclosures in mandatory reports and the pricing of credit risk. In particular, we investigate whether and how the Securities and Exchange Commission (SEC) mandate of risk factor disclosures (RFDs) affects credit default swap (CDS) spreads. Based on the theory of Duffie and Lando (2001), we predict and find that CDS spreads decrease significantly after RFDs are made available in 10‐K/10‐Q filings. These results suggest that RFDs improve information transparency about the firm's underlying risk, thereby reducing the information risk premium in CDS spreads. The content analysis further reveals that disclosures pertinent to financial and idiosyncratic risk are especially relevant to credit investors. In cross‐sectional analyses, we document that RFDs are more useful for evaluating the business prospects and default risk of firms with greater information uncertainty/asymmetry. Overall, our findings imply that the SEC requirement for adding a risk factor section to periodic reports enhances the transparency of firm risk and facilitates credit investors in evaluating the credit quality of the firm.",,2018,10.1111/1911-3846.12362,,proquest,"This study investigates how mandatory risk factor disclosures (RFDs) in SEC filings impact the pricing of credit default swaps (CDS). The findings indicate that RFDs lead to a significant decrease in CDS spreads, suggesting improved information transparency about firm risk and a reduced information risk premium. Disclosures related to financial and idiosyncratic risks are particularly important for credit investors, and RFDs are more beneficial for firms with higher information uncertainty.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:13:32.292382
7267afe860c9d705,The Informational Content of the Term Spread in Forecasting the US Inflation Rate: A Nonlinear Approach,"The difficulty in modelling inflation and the significance in discovering the underlying data-generating process of inflation is expressed in an extensive literature regarding inflation forecasting. In this paper we evaluate nonlinear machine learning and econometric methodologies in forecasting US inflation based on autoregressive and structural models of the term structure. We employ two nonlinear methodologies: the econometric least absolute shrinkage and selection operator (LASSO) and the machine-learning support vector regression (SVR) method. The SVR has never been used before in inflation forecasting considering the term spread as a regressor. In doing so, we use a long monthly dataset spanning the period 1871:1-2015:3 that covers the entire history of inflation in the US economy. For comparison purposes we also use ordinary least squares regression models as a benchmark. In order to evaluate the contribution of the term spread in inflation forecasting in different time periods, we measure the out-of-sample forecasting performance of all models using rolling window regressions. Considering various forecasting horizons, the empirical evidence suggests that the structural models do not outperform the autoregressive ones, regardless of the model's method. Thus we conclude that the term spread models are not more accurate than autoregressive models in inflation forecasting. Copyright © 2016 John Wiley & Sons, Ltd.",,2017,10.1002/for.2417,,proquest,"This paper evaluates nonlinear machine learning (LASSO, SVR) and econometric methodologies for forecasting US inflation using the term spread. The study utilizes a long monthly dataset from 1871 to 2015 and compares the performance of these models against ordinary least squares regression using rolling window regressions. The findings indicate that structural models do not outperform autoregressive models, and term spread models are not more accurate than autoregressive models for inflation forecasting.",True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:13:41.186958
53693a3db3c1fcab,The Nonlinear Nature of Country Risk and its Implications for DSGE Models,"Country risk premia can substantially affect macroeconomic dynamics. We concentrate on one of their most important determinants- A country's net foreign asset (NFA) position and-in contrast to the existing research-investigate its nonlinear link to risk premia. The importance of this particular nonlinearity is two-fold. First, it allows to identify the NFA level above which the elasticity becomes much (possibly dangerously) higher. Second, such a nonlinear relationship is a standard ingredient of dynamic stochastic general equilibrium (DSGE) models, but its proper calibration/estimation is missing. Our estimation shows that indeed the link is highly nonlinear and helps to identify the NFA position where the nonlinearity kicks in at approximately-70% to-75% of GDP. We also provide a proper calibration of the risk premium-NFA relationship which can be used in DSGE models and demonstrate that its slope matters significantly for economic dynamics in such a model. © 2020 Elsevier B.V., All rights reserved.","Brzoza-Brzezina, M.; Kotłowski, J.",2020,10.1017/s136510051800038x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052611952&doi=10.1017%2FS136510051800038X&partnerID=40&md5=37354b76d72bae70019c8812aed9cfcb,scopus,"This paper investigates the nonlinear relationship between a country's net foreign asset (NFA) position and its risk premium, identifying a critical NFA level (approximately -70% to -75% of GDP) where the elasticity significantly increases. The authors calibrate this nonlinear relationship for use in Dynamic Stochastic General Equilibrium (DSGE) models, demonstrating its substantial impact on economic dynamics.",True,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:13:43.505290
e160e6b80df70894,The October 2014 United States Treasury bond flash crash and the contributory effect of mini flash crashes,"We investigate the causal uncertainty surrounding the flash crash in the U.S. Treasury bond market on October 15, 2014, and the unresolved concern that no clear link has been identified between the start of the flash crash at 9:33 and the opening of the U.S. equity market at 9:30. We consider the contributory effect of mini flash crashes in equity markets, and find that the number of equity mini flash crashes in the three-minute window between market open and the Treasury Flash Crash was 2.6 times larger than the number experienced in any other three-minute window in the prior ten weekdays. We argue that (a) this statistically significant finding suggests that mini flash crashes in equity markets both predicted and contributed to the October 2014 U.S. Treasury Bond Flash Crash, and (b) mini-flash crashes are important phenomena with negative externalities that deserve much greater scholarly attention.",,2017,10.1371/journal.pone.0186688,,proquest,"This paper examines the October 2014 U.S. Treasury bond flash crash, investigating a potential link between mini flash crashes in equity markets and the Treasury market event. The study finds a statistically significant increase in equity mini flash crashes preceding the Treasury flash crash, suggesting they contributed to it and highlighting the need for further research into these phenomena.",False,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:13:46.219398
bb9b735a815837e5,The Predictability of the Exchange Rate When Combining Machine Learning and Fundamental Models,"In 1983, Meese and Rogoff showed that traditional economic models developed since the 1970s do not perform better than the random walk in predicting out-of-sample exchange rates when using data obtained after the beginning of the floating rate system. Subsequently, whether traditional economical models can ever outperform the random walk in forecasting out-of-sample exchange rates has received scholarly attention. Recently, a combination of fundamental models with machine learning methodologies was found to outcompete the predictability of random walk (Amat et al. 2018). This paper focuses on combining modern machine learning methodologies with traditional economic models and examines whether such combinations can outperform the prediction performance of random walk without drift. More specifically, this paper applies the random forest, support vector machine, and neural network models to four fundamental theories (uncovered interest rate parity, purchase power parity, the monetary model, and the Taylor rule models). We performed a thorough robustness check using six government bonds with different maturities and four price indexes, which demonstrated the superior performance of fundamental models combined with modern machine learning in predicting future exchange rates in comparison with the results of random walk. These results were examined using a root mean squared error (RMSE) and a Diebold–Mariano (DM) test. The main findings are as follows. First, when comparing the performance of fundamental models combined with machine learning with the performance of random walk, the RMSE results show that the fundamental models with machine learning outperform the random walk. In the DM test, the results are mixed as most of the results show significantly different predictive accuracies compared with the random walk. Second, when comparing the performance of fundamental models combined with machine learning, the models using the producer price index (PPI) consistently show good predictability. Meanwhile, the consumer price index (CPI) appears to be comparatively poor in predicting exchange rate, based on its poor results in the RMSE test and the DM test. © 2023 Elsevier B.V., All rights reserved.","Zhang, Y.; Hamori, S.",2020,10.3390/jrfm13030048,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091877651&doi=10.3390%2Fjrfm13030048&partnerID=40&md5=352d6320ec6190df592c437a799b8a70,scopus,"This paper investigates the predictability of exchange rates by combining traditional economic models with machine learning techniques (random forest, support vector machine, neural network). The study applies these combinations to four fundamental theories (uncovered interest rate parity, purchase power parity, monetary model, Taylor rule) and evaluates their performance against a random walk benchmark using RMSE and Diebold-Mariano tests. Results indicate that fundamental models augmented with machine learning generally outperform the random walk, with the producer price index showing better predictability than the consumer price index.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:14:05.954437
f4423359be40fb95,The Price of Predictability: Estimating Inconsistency Premiums in Social Interactions,"For financial decision-making, people trade off the expected value (return) and the variance (risk) of an option, preferring higher returns to lower ones and lower risks to higher ones. To make decision-makers indifferent between a risky and risk-free option, the expected value of the risky option must exceed the value of the risk-free option by a certain amount-the risk premium. Previous psychological research suggests that similar to risk aversion, people dislike inconsistency in an interaction partner's behavior. In eight experiments (total N = 2,412) we pitted this inconsistency aversion against the expected returns from interacting with an inconsistent partner. We identified the additional expected return of interacting with an inconsistent partner that must be granted to make decision-makers prefer a more profitable, but inconsistent partner to a consistent, but less profitable one. We locate this inconsistency premium at around 31% of the expected value of the risk-free option.","Gerten, Judith; Zuern, Michael K.; Topolinski, Sascha",2022,10.1177/0146167221998533,,wos,"This study investigates inconsistency aversion in social interactions, analogous to risk aversion in financial decision-making. Through eight experiments with 2,412 participants, the researchers quantified the 'inconsistency premium' – the additional expected return needed to make individuals indifferent between interacting with a profitable but inconsistent partner and a consistent but less profitable one. The findings suggest this premium is approximately 31% of the expected value of the risk-free option.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:14:09.165162
dd8b9653412c131a,The Pricing of Time-Varying Exchange Rate Risk in the Stock Market: A Nonparametric Approach,"This paper reexamines the pricing of exchange rate risk in the U.S. stock market. We first construct stock portfolios based on the Foreign Exchange Income (FEI), a measure of currency exposure of firms, reported in their annual reports. We then develop two-factor and multi-factor nonparametric models that allow time variation in risk exposure and risk premium, and nonlinearity in the return generating process. When we assume that risk exposure can be time-varying but risk premium is constant, the estimated premium for exchange rate risk is significant only for the most positive FEI-ranked portfolio and marginally significant for the most negative FEI-ranked portfolio. When we further assume that both risk exposure and risk premium can be time-varying, results suggest that exchange rate risk is significantly priced for all the FEI-ranked portfolios except the one with little exposure.","Chung, Y. Peter; Zhou, Zhong-guo",2012,10.1515/1558-3708.1634,,wos,"This paper investigates the pricing of exchange rate risk in the U.S. stock market using a nonparametric approach. It constructs stock portfolios based on Foreign Exchange Income (FEI) and develops time-varying multi-factor models to account for nonlinearities and time variations in risk exposure and premium. The study finds that exchange rate risk is priced, particularly for portfolios with significant currency exposure.",True,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:14:11.834232
5311c7fe6ee66e24,"The UK Companies Act of 2006, the Sarbanes-Oxley Act of 2002, and important reviews of 2009. Implications for the certainty equivalent coefficient net present value criterion","Purpose - The purpose of this paper is to draw attention to the fact that the certainty equivalent coefficient net present value criterion, CEC(NPV), in disregarding a fundamental requirement for the calculation of cash flows for purposes of discounted cash flow analysis, invalidates this capital budgeting criterion from the perspective of sound research methodology. The paper also investigates the impact of the UK Companies Act of 2006, the Sarbanes-Oxley Act of 2002, and important reviews such as the Turner Review of 2009, the Walker Review of 2009, and the Review of the Combined Code of 2009 on this operationally invalid capital budgeting criterion, as well as its impact on the process of financial managerial decision making. Design/methodology/approach - The CEC(NPV) as a discounted cash flow capital budgeting criterion was examined from the perspective of the axioms of cash flow estimation as well as from the definition of the cost of capital in order to ascertain the contribution of this criterion to financial management. The relevant sections of the UK Companies Act of 2006, the Sarbanes-Oxley Act of 2002, the Turner Review of 2009, the Walker Review of 2009, and the Review of the Combined Code of 2009 were studied in order to establish whether the CEC(NPV) was able to satisfy the requirements of this legislation and these important reviews. Findings - The CEC(NPV) is construct invalid and does not measure what it purports to measure: it over-states financial viability. As a consequence, it does not meet the requirements of sound research methodology and therefore is at odds with the UK Companies Act of 2006, the Sarbanes-Oxley Act of 2002, and falls foul of the Turner Review of 2009, the Walker Review of 2009, the 2009 Review of the Combined Code issued by the Financial Reporting Council. As such it cannot be endorsed by the Financial Services Authority. Originality/value - The paper usefully shows that the CEC(NPV) denies financial managers application of Fisherian analysis for resolving conflicts in the rankings of mutually exclusive projects, and, the comparison of project cost of capital with their respective internal rates of return. Comparisons of the internal rate of return, not with the risk-free rate (that is assumed to be a constant and which exhibits minimal variability in comparison with the cost of capital), but with the cost of capital cost of capital, are a sine qua non for managerial decision making, especially capital budgeting.",,2010,10.1108/17542431011093162,,proquest,"This paper critiques the certainty equivalent coefficient net present value (CEC(NPV)) criterion for capital budgeting, arguing it is methodologically flawed due to incorrect cash flow calculations. It examines the implications of the UK Companies Act 2006, Sarbanes-Oxley Act 2002, and several 2009 reviews (Turner, Walker, Combined Code) on this criterion, concluding that CEC(NPV) is construct invalid, overstates financial viability, and conflicts with these legislative and review requirements. The paper highlights that CEC(NPV) hinders financial managers from applying Fisherian analysis for project ranking and comparing internal rates of return with the cost of capital.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:14:14.353364
6b9cd4fca22e9660,The UK and the Eurozone,"The article reviews the case for the UK to join the Eurozone by way of presenting a review of HM Treasury's widely well-regarded ""Euro Report"" (2003). The review provides an opportunity to rehearse and update the elements of optimum currency area (OCA) theory. In particular, the study draws attention to fresh estimates of the trade effect of the UK's adhesion to the Eurozone, the small size of which sharply contrasts with earlier estimates. They substantially remove a challenge to the Report's negative conclusion. The study sets the review in the perspective of public opinion surveys and HM Government's decisions. © 2006 Oxford University Press. © 2008 Elsevier B.V., All rights reserved.","Artis, M.",2006,10.1093/cesifo/ifj002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750451943&doi=10.1093%2Fcesifo%2Fifj002&partnerID=40&md5=bd9251b7434142022294a2c53adc8d2b,scopus,"This article reviews the UK's potential accession to the Eurozone, analyzing the HM Treasury's 2003 ""Euro Report"" and updating elements of optimum currency area theory. It highlights new estimates of the trade effect of UK adhesion, which are smaller than previously thought and support the report's negative conclusion. The study also considers public opinion and government decisions.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:14:17.024378
e6571fb417c7b72e,The Yield Curve as a Recession Leading Indicator. An Application for Gradient Boosting and Random Forest,"Most representative decision-tree ensemble methods have been used to examine the variable importance of Treasury term spreads to predict US economic recessions with a balance of generating rules for US economic recession detection. A strategy is proposed for training the classifiers with Treasury term spreads data and the results are compared in order to select the best model for interpretability. We also discuss the use of SHapley Additive exPlanations (SHAP) framework to understand US recession forecasts by analyzing feature importance. Consistently with the existing literature we find the most relevant Treasury term spreads for predicting US economic recession and a methodology for detecting relevant rules for economic recession detection. In this case, the most relevant term spread found is 3-month–6-month, which is proposed to be monitored by economic authorities. Finally, the methodology detected rules with high lift on predicting economic recession that can be used by these entities for this propose. This latter result stands in contrast to a growing body of literature demonstrating that machine learning methods are useful for interpretation comparing many alternative algorithms and we discuss the interpretation for our result and propose further research lines aligned with this work. © 2022 Elsevier B.V., All rights reserved.","Delgado, P.C.; Congregado, E.; Golpe, A.A.; Vides, J.C.",2022,10.9781/ijimai.2022.02.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127570531&doi=10.9781%2Fijimai.2022.02.006&partnerID=40&md5=9f07996466ae0db75c8618f7c33d0667,scopus,This study applies gradient boosting and random forest models to predict US economic recessions using Treasury term spreads. It identifies the 3-month–6-month term spread as the most relevant predictor and proposes a methodology for economic authorities to use these findings for recession detection. The study also discusses the interpretability of the machine learning models using SHAP values.,True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:14:23.219496
56291c6ae5cabc13,The causality link between political risk and stock prices,"PurposePrior studies have paid close attention to the impact of political risk on financial markets. Following this strand of literature, this paper aims to focus on the causality link between political shocks and their impacts on emerging stock markets.Design/methodology/approachThis paper highlights an innovative counterfactual model for political risk assessment. Based on a natural experiment, i.e. the Taiwan Strait Crisis in 1995-1996, this study utilizes one data-driven approach, e.g. the synthetic control methods (SCMs), to estimate causal impact of this political shock on Taiwan’s stock market.FindingsMajor findings in this study are consistent with existing literature on the price of political risk, e.g. political uncertainty commands a risk premium. The SCM estimations suggest that Taiwan’s stock prices dramatically underperformed its newly industrialized peers and other developed markets during the crisis. The SCM results are statistically significant and robust to various cross-validation tests.Research limitations/implicationsFindings in this study indicate that political risks could generate enormous impacts on emerging financial markets. In particular, political uncertainty following new geopolitical dynamics requires proper identification and assessment.Originality/valueTo the author’s knowledge, this paper is the first rigorous counterfactual study to the causality relationship between political uncertainty and stock prices in emerging markets. This paper is distinct from previous studies in applying a data-driven approach to combine the features of learning from others (cross-sectional) and learning from the past (time series).",,2019,10.1108/jfep-07-2018-0106,,proquest,"This study uses the synthetic control method (SCM), a data-driven approach, to investigate the causal relationship between political shocks and emerging stock markets, specifically examining the Taiwan Strait Crisis of 1995-1996. The findings indicate that political uncertainty leads to a risk premium, causing Taiwan's stock market to underperform significantly during the crisis.",True,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:14:38.834773
e1060b1fe4f87af2,The composition of CMBS risk,"This paper identifies the put-option, liquidity availability proportion, and shadow liquidity risk premia embedded within commercial mortgage backed securities (CMBS) using reduced form and structural generalization models. These risk values are then interpreted as trading signals which are tested with automated trading strategies that buy undervalued and sell overvalued CMBS from November 2007 through June 2015. All three signals generate substantial positive trading profits in testing for the reduced form model but not for the structural generalization. The risk signals constructed independently of market pricing provide more profitable automated trading insights than those constructed from interactions between modeled risk measures and market spreads. In my tests of the information content of the risk signals with respect to future macroeconomic indicators, I find statistically significant evidence in keeping with recent studies. While I cannot reject CMBS efficiency, this paper's disclosure of new risk measures, the profitability of automated strategies based on those risk measures, and the statistical significance of their forward guidance capabilities, together contributes to our understanding of CMBS risk and the credit spread puzzle debate. (C) 2016 Elsevier B.V. All rights reserved.","Christopoulos, Andreas D.",2017,10.1016/j.jbankfin.2016.12.005,,wos,"This paper develops and tests three risk measures for Commercial Mortgage-Backed Securities (CMBS): put-option, liquidity availability proportion, and shadow liquidity risk premia. These measures are used as trading signals in automated strategies. The reduced form model's signals were profitable, while the structural generalization model's were not. The risk signals, independent of market pricing, outperformed those derived from market interactions. The signals also showed predictive power for future macroeconomic indicators, contributing to the understanding of CMBS risk and the credit spread puzzle.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:14:40.746639
d9e7ca11936aaf69,The decoupling between public debt fundamentals and bond spreads after the European sovereign debt crisis,"We contribute to the literature that documents empirically that the relationship between public debt fundamentals and sovereign bond spreads in Spain, France, and Italy (versus Germany) weakened after the 2010–2012 episode of sovereign debt markets’ significant distress. To construct our measure of public debt fundamentals, we build on the literature that combines the Value at Risk approach with the estimation of the correlation pattern of public debt dynamics’ macroeconomic determinants via Vector Auto Regressions (VARs) to estimate the probability distribution of alternative debt trajectories. Since we incorporate in the VAR new information in a sequential manner, we are able to retrieve time-varying probabilities that characterize the expected behaviour of debt at a given point in time in the future. We then empirically confront such probabilistic indicators with market-derived sovereign bond spreads.",,2023,10.1080/00036846.2022.2120959,,proquest,"This study investigates the weakened relationship between public debt fundamentals and sovereign bond spreads in Spain, France, and Italy compared to Germany, following the 2010-2012 European sovereign debt crisis. The authors develop a measure of public debt fundamentals by combining the Value at Risk approach with Vector Auto Regressions (VARs) to estimate time-varying probabilities of future debt trajectories. These probabilistic indicators are then empirically compared with market-derived sovereign bond spreads.",True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:14:50.490297
58601be935f1ee81,"The demand for M1 in the U.S.A., 1960-1988","Estimated U.S. M1 demand functions appear unstable, regularly “breaking down,” over 1960-1988 (e.g. missing money, great velocity decline, M1-explosion). We propose a money demand function whose arguments include inflation, real income, long-term bond yield and risk, T-bill interest rates, and learning curve weighted yields on newly introduced instruments in Ml and non-transactions M2. The model is estimated in dynamic error-correction form; it is constant and, with an equation standard error of 0.4%, variance-dominates most previous models. Estimating alternative specifications explains earlier “breakdowns,” showing the model’s distinctive features to be important in accounting for the data. © 1992 The Review of Economic Studies Limited. © 2016 Elsevier B.V., All rights reserved.","Baba, Y.; Hendry, D.F.; Starr, R.M.",1992,10.2307/2297924,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963027196&doi=10.2307%2F2297924&partnerID=40&md5=8fb6ec0e52860e6e8e381822049eedc4,scopus,"This paper estimates U.S. M1 demand functions from 1960-1988, addressing apparent instability by proposing a new model that includes inflation, real income, bond yields, interest rates, and yields on new instruments. The dynamic error-correction model is shown to be stable and more accurate than previous models, explaining earlier ""breakdowns"" by highlighting the importance of its distinctive features.",True,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:14:52.129260
3a2fe3ca49a5826d,The determinants of main stock exchange index changes in emerging countries: evidence from Turkey in COVID-19 pandemic age,"With the emergence and spreading of COVID-19 pandemic all over the world, the uncertainty has been increasing for countries. Depending on this condition, especially emerging countries have been affected negatively by foreign portfolio investment outflows from stock exchanges, and main stock exchange indices have been collapsed. The study examines the causes of the main stock exchange index changes in Turkey in the COVID-19 period. In this context, 14 variables (3 global, 6 country-level, 5 market-level) are analyzed by employing random forest and support vector machine algorithms and using daily data between 01.02.2020 and 05.15.2020, which includes the pre-pandemic and the pandemic periods. The findings prove that (i) the most important variables are the retention amount of foreign investors in the equity market, credit default swap spreads, government bonds interest rates, Morgan Stanley Capital International (MSCI) emerging markets index, and volatility index in the pre-pandemic period; (ii) the importance of variables changes as MSCI emerging markets index, the volatility index, retention amount of foreign investors in the equity market, amount of securities held by the Central Bank of Republic of Turkey (CBRT), equity market traded value in the pandemic period; (iii) support vector machine has superior estimation accuracy concerning random forest algorithms in both pre-pandemic and pandemic period.","Kartal, Mustafa Tevfik; Depren, Ozer; Depren, Serpil Kilic",2020,10.3934/qfe.2020025,,wos,"This study investigates the determinants of changes in Turkey's main stock exchange index during the COVID-19 pandemic using daily data from February to May 2020. It employs Random Forest and Support Vector Machine algorithms to analyze 14 variables, including global, country-level, and market-level factors. The findings highlight the varying importance of factors like foreign investor retention, credit default swap spreads, government bond interest rates, and market indices in pre-pandemic versus pandemic periods, with SVM showing superior accuracy.",True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:15:18.570536
056f1ead8d9317b9,The dynamic interaction of speculation and diversification,"A discrete time model of a financial market is developed, in which heterogeneous interacting groups of agents allocate their wealth between two risky assets and a riskless asset. In each period each group formulates its demand for the risky assets and the risk-free asset according to myopic mean-variance maximizazion. The market consists of two types of agents: fundamentalists, who hold an estimate of the fundamental values of the risky assets and whose demand for each asset is a function of the deviation of the current price from the fundamental, and chartists, a group basing their trading decisions on an analysis of past returns. The time evolution of the prices is modelled by assuming the existence of a market maker, who sets excess demand of each asset to zero at the end of each trading period by taking an offsetting long or short position, and who announces the next period prices as functions of the excess demand for each asset and with a view to long-run market stability. The model is reduced to a seven-dimensional nonlinear discrete-time dynamical system, that describes the time evolution of prices and agents' beliefs about expected returns, variances and correlation. The unique steady state of the model is determined and the local asymptotic stability of the equilibrium is analysed, as a function of the key parameters that characterize agents' behaviour. In particular it is shown that when chartists update their expectations sufficiently fast, then the stability of the equilibrium is lost through a supercritical Neimark-Hopf bifurcation, and self-sustained price fluctuations along an attracting limit cycle appear in one or both markets. Global analysis is also performed, by using numerical techniques, in order to understand the role played by the chartists' behaviour in the transition to a regime characterized by irregular oscillatory motion and coexistence of attractors. It is also shown how changes occurring in one market may affect the price dynamics of the alternative risky asset, as a consequence of the dynamic updating of agents' portfolios. © 2005 Taylor & Francis Group Ltd. © 2005 Elsevier B.V., All rights reserved.","Chiarella, C.; Dieci, R.; Gardini, L.",2005,10.1080/1350486042000260072,https://www.scopus.com/inward/record.uri?eid=2-s2.0-16644376238&doi=10.1080%2F1350486042000260072&partnerID=40&md5=eb95c9185d3b76bd89c850f8dda27d54,scopus,"This paper presents a discrete-time model of a financial market with heterogeneous agents who trade between risky and riskless assets. Agents are either fundamentalists (trading based on fundamental values) or chartists (trading based on past returns). The model uses a market maker to set prices and analyzes the stability of the unique steady state. It shows that rapid chartist behavior can lead to price fluctuations and limit cycles, and explores the transition to irregular oscillatory motion and the coexistence of attractors. The study also examines how changes in one market can affect another due to portfolio updates.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:15:20.596466
1254037e95e027a2,The dynamic relationship between the prices of ADRs and their underlying stocks: evidence from the threshold vector error correction model,"This paper sets out to estimate the dynamic relationship that exists between the prices of ADRs and their underlying stocks, in both the short run and the long run, using a number of recent developments of the threshold cointegration framework. The empirical results support the notion of nonlinear mean reversion of the prices of ADRs and their underlying stocks.","Chung, HM; Ho, TW; Wei, LJ",2005,10.1080/00036840500218729,,wos,"This paper investigates the dynamic relationship between ADR prices and their underlying stocks using a threshold vector error correction model, finding evidence of nonlinear mean reversion.",True,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:15:22.253698
3f2dc0f0809e9321,The efficacy of neural networks in predicting returns on stock and bond indices,"This paper uses two recently developed tests to identify neglected nonlinearity in the relationship between excess returns on four asset classes and several economic and financial variables. Having found some evidence of possible nonlinearity, it was then investigated whether the predictive power of these variables could be enhanced by using neural network models instead of linear regression or GARCH models. Some evidence of nonlinearity in the relationships between the explanatory variables and large stocks and corporate bonds was found. It was also found that the GARCH models are conditionally efficient with respect to neural network models, but the neural network models outperform GARCH models if financial performance measures are used. In resonance with the results reported for the tests for neglected nonlinearity, it was found that the neural network forecasts are conditionally efficient with respect to linear regression models for large stocks and corporate bonds, whereas the evidence is not statistically significant for small stocks and intermediate-term government bonds. This difference persists even when financial performance measures for individual asset classes are used for comparison. © 2018 Elsevier B.V., All rights reserved.","Desai, V.S.; Bharati, R.",1998,10.1111/j.1540-5915.1998.tb01582.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032392191&doi=10.1111%2Fj.1540-5915.1998.tb01582.x&partnerID=40&md5=a3740dd5ed940d8a119fb152e4c41aab,scopus,"This study investigates the efficacy of neural networks in predicting returns on stock and bond indices, comparing them against linear regression and GARCH models. Evidence of nonlinearity was found in the relationships between explanatory variables and large stocks and corporate bonds. Neural networks outperformed GARCH models when financial performance measures were used, and showed conditional efficiency against linear regression for large stocks and corporate bonds, though not for smaller stocks or intermediate-term government bonds.",True,True,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T11:15:24.848498
19ee415ed7fb651b,"The impact of contagion effects of media reports, investors' sentiment and attention on the stock market based on HAR-RV model","In this paper, the Shanghai Securities Composite Index and 18 A-share listed companies are used to justify the impact of contagion effects of media reports, investors' sentiment and attention on stock market. Five indicators are built: The news media optimistic tendency, investors' attention, investors' sentiment, investors' sentiment disagreement and media sentiment disagreement. Furthermore, theoretical models are constructed based on HAR-RV model to analyze the contagion structure between media sentiment and investors' sentiment and its impact on the performance of stock market. Additionally, the reverse silence spiral theory is proposed to analyze the regulatory role of sentiment disagreement in the contagion effects according to the information communication theory. The empirical results demonstrate the following conclusions. (1) The optimism degree of media reports positively affects investors' subjective sentiment and increases their transaction volume. (2) Strengthening investors' attention to corporate-related information is the main path by which media sentiment interferes with investors' sentiment. (3) Media sentiment will indirectly affect the excess return and volatility of stocks through investors' sentiment and their transactions. (4) Media sentiment disagreement has weakened the influence of media sentiment on investors' attention and sentiment. Investors' sentiment disagreement has alleviated its impact on the excess returns and volatility of stocks.","Lei, Bolin; Song, Yuping",2023,10.1142/s242478632350010x,,wos,"This study investigates the influence of media reports, investor sentiment, and attention on the stock market using the HAR-RV model. It constructs indicators for media optimism, investor attention, sentiment, and disagreement. The findings suggest that media optimism boosts investor sentiment and trading volume, with investor attention acting as a key mediator. Media sentiment indirectly impacts stock returns and volatility via investor sentiment and trading. Sentiment disagreement, both in media and among investors, mitigates these contagion effects.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:15:26.804758
7fa232dd8733f7ab,The implied volatility smirk of commodity options,"This paper studies the implied volatility (IV) smirks in four commodity markets by adopting Zhang and Xiang's methodology. First, we document the term structure and dynamics of IV smirks. Overall, the commodity IV curves are negatively skewed with a positive curvature. Then we analyze the commodity and S&P 500 returns' predictability based on in-sample and out-of-sample tests and find that the information embedded in IV smirks can significantly predict monthly commodity and S&P 500 returns. For example, the risk-neutral fourth cumulant (FC) from the crude oil market outperforms all of the standard predictors in predicting the S&P 500 returns.","Jia, Xiaolan; Ruan, Xinfeng; Zhang, Jin E.",2021,10.1002/fut.22161,,wos,"This paper analyzes implied volatility (IV) smirks in four commodity markets, finding negatively skewed IV curves with positive curvature. It demonstrates that information from IV smirks can predict monthly returns in commodity and S&P 500 markets, with the risk-neutral fourth cumulant from crude oil showing superior predictive power for S&P 500 returns.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:15:28.329404
2c0b739e8ae2211a,The information content of option-implied volatility for credit default swap valuation,"Credit default swaps (CDS) are similar to out-of-the-money put options in that both offer a low cost and effective protection against downside risk. This study investigates whether put option-implied volatility is an important determinant of CDS spreads. Using a large sample of firms with both CDS and options data, we find that individual firms put option-implied volatility dominates historical volatility in explaining the time-series variation in CDS spreads. To understand this result, we show that implied volatility is a more efficient forecast for future realized volatility than historical volatility. More importantly, the volatility risk premium embedded in option prices covaries with the CDS spread. These findings complement existing empirical evidence based on market-level data. (C) 2010 Elsevier B.V. All rights reserved.","Cao, Charles; Yu, Fan; Zhong, Zhaodong",2010,10.1016/j.finmar.2010.01.002,,wos,"This study examines the relationship between option-implied volatility and Credit Default Swap (CDS) spreads. It finds that implied volatility is a significant predictor of CDS spreads, outperforming historical volatility. The research suggests that the volatility risk premium in option prices is linked to CDS spreads, providing empirical evidence for this connection.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:15:29.685658
73a39603932c7788,The informational content of the embedded deflation option in TIPS,"We estimate the value of the embedded option in U.S. Treasury Inflation-Protected Securities (TIPS). The embedded option value exhibits time variation that is correlated with periods of deflationary expectations. We construct embedded option explanatory variables that are statistically and economically significant for explaining future inflation, even in the presence of traditional inflation variables such as lagged inflation, the gold return, the crude oil return, the VIX return, liquidity, surveys, and the yield spread between nominal Treasuries and TIPS. After conducting robustness tests, we conclude that the TIPS embedded option contains useful information for future inflation. (C) 2016 Elsevier B.V. All rights reserved.","Grishchenko, Olesya V.; Vanden, Joel M.; Zhang, Jianing",2016,10.1016/j.jbankfin.2015.12.004,,wos,"This study estimates the value of the embedded option in U.S. Treasury Inflation-Protected Securities (TIPS), finding that its value varies with deflationary expectations. The authors demonstrate that this option provides significant explanatory power for future inflation, even when controlling for traditional inflation predictors. Robustness tests confirm the informational content of the TIPS embedded option for predicting future inflation.",False,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:15:34.141446
af84a9d1919ec35b,The performance of variance ratio unit root tests under nonlinear stationary TAR and STAR processes: Evidence from Monte Carlo simulations and applications,"This paper investigates the performance of variance ratio unit root tests under nonlinear stationary three-regime threshold autoregressive (TAR) and smooth transition autoregressive (STAR) processes that are significant for some economic theories and variables. Variance ratio unit root tests are effective tools in empirical analysis because they can theoretically consider broad classes of nonlinear stationary processes under the null or alternative hypothesis. Nevertheless, our Monte Carlo simulations demonstrate that these tests perform poorly (with severe size distortions or low power) under stationary TAR and STAR processes. To verify our Monte Carlo results, we apply these tests to yield spreads such as the TAR and STAR processes.","Maki, Daiki",2008,10.1007/s10614-007-9107-1,,wos,"This paper evaluates the effectiveness of variance ratio unit root tests when applied to nonlinear stationary TAR and STAR processes, which are relevant in economic contexts. Through Monte Carlo simulations, the study finds that these tests exhibit poor performance, characterized by significant size distortions or low power, when applied to stationary TAR and STAR processes. The findings are further validated by applying the tests to yield spreads, which are modeled as TAR and STAR processes.",True,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:15:38.842966
01e6238a15a50b30,The predictive distributions of thinning-based count processes,"This paper shows that the term structure of conditional (i.e. predictive) distributions allows for closed form expression in a large family of (possibly higher order or infinite order) thinning-based count processes such as INAR(p), INARCH(p), NBAR(p), and INGARCH(1,1). Such predictive distributions are currently often deemed intractable by the literature and existing approximation methods are usually time consuming and induce approximation errors. In this paper, we propose a Taylor's expansion algorithm for these predictive distributions, which is both exact and fast. Through extensive simulation exercises, we demonstrate its advantages with respect to existing methods in terms of the computational gain and/or precision. © 2021 Elsevier B.V., All rights reserved.","Lu, Y.",2021,10.1111/sjos.12438,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077400529&doi=10.1111%2Fsjos.12438&partnerID=40&md5=51d29189308bafffdf7533af066f2877,scopus,"This paper presents an exact and fast Taylor's expansion algorithm for the predictive distributions of thinning-based count processes, including INAR(p), INARCH(p), NBAR(p), and INGARCH(1,1). The proposed method overcomes the intractability and computational issues of existing approximation methods, demonstrating superior performance in simulations.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:15:40.171109
edcd8b865beeefe6,The price of power: The valuation of power and weather derivatives,"Pricing contingent claims on power presents numerous challenges due to (1) the unique behavior of power prices, and (2) time-dependent variations in prices. We propose and implement a model in which the spot price of power is a function of two state variables: demand (load) and fuel price. in this model, any power derivative price must satisfy a PDE with boundary conditions that reflect capacity limits and the non-linear relation between load and the spot price of power. Moreover, since power is non-storable and demand is not a traded asset, the power derivative price embeds a market price of risk. Using inverse problem techniques and power forward prices from the PJM market, we solve for this market price of risk function. During 1999-2001, the upward bias in the forward price was as large as $50/MWh for some days in July. By 2005, the largest estimated upward bias had fallen to $19/MWh. These large biases are plausibly due to the extreme right skewness of power prices: this induces left skewness in the payoff to short forward positions, and a large risk premium is required to induce traders to sell power forwards. This risk premium suggests that the power market is not fully integrated with the broader financial markets. (C) 2008 Published by Elsevier B.V.","Pirrong, Craig; Jermakyan, Martin",2008,10.1016/j.jbankfin.2008.04.007,,wos,"This paper proposes a model for pricing power derivatives, considering the unique behavior of power prices and time-dependent variations. It incorporates demand (load) and fuel price as state variables, leading to a PDE with boundary conditions reflecting capacity limits and non-linear relationships. The model also accounts for the market price of risk due to power's non-storability and demand not being a traded asset. Using PJM market data, the study estimates this risk premium, finding significant upward biases in forward prices between 1999-2001, which decreased by 2005. These biases are attributed to the right skewness of power prices, suggesting the power market is not fully integrated with broader financial markets.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:15:41.850685
6b731db9ed4214e8,The relation between the equity risk premium and the bond maturity premium in the UK: 1900-2006,"Using a rich data set for the UK for over a century, we find that the relation between the equity risk premium and the government bond maturity premium is nonlinear and subject to stochastic regime switching. We identify a regime in which both premia are jointly characterized by low volatility and another regime in which both premia are characterized by high volatility. The occurrence of the high volatility regime chronologically coincides with major changes in the pound exchange rate. The low volatility regime has a higher probability of turning up over two consecutive years than the high volatility regime, but it is not perceived by investors to be an absorbing regime. The lagged maturity premium is a strong predictor of the equity risk premium only in the regime of low volatility. In addition, the lagged equity premium is a predictor of the maturity premium also in the low volatility regime. This result on regime-dependent bidirectional predictability is robust to alternative definitions of the equity premium, and to the inclusion of real interest rate and real growth effects. © 2008 Springer Science+Business Media, LLC. © 2009 Elsevier B.V., All rights reserved.","Kanas, A.",2009,10.1007/s12197-008-9038-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-65249147174&doi=10.1007%2Fs12197-008-9038-2&partnerID=40&md5=8f24282c2b2092e99ed1ca80871e306d,scopus,"This study investigates the relationship between the equity risk premium and the government bond maturity premium in the UK from 1900-2006. It identifies two distinct regimes: one with low volatility and another with high volatility, the latter coinciding with significant changes in the pound's exchange rate. The research finds that the lagged maturity premium predicts the equity risk premium, and vice versa, specifically within the low volatility regime. This bidirectional predictability is robust to various definitions and the inclusion of other economic factors.",True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:16:31.989548
ec4e0a59fc7c80b4,The role of supervised learning in the decision process to fair trade US municipal debt,"Determining a fair price and an appropriate timescale to trade municipal debt is a complex decision. This research uses data informatics to explore transaction characteristics and trading activity of investment grade US municipal bonds. Using the relatively recent data stream distributed by the Municipal Securities Rulemaking Board, we provide an institutional summary of market participants and their trading behavior. Subsequently, we focus on a sample of AAA bonds to derive a new methodology to estimate a trade-weighted benchmark municipal yield curve. The methodology integrates the study of ridge regression, artificial neural networks, and support vector regression. We find an enhanced radial basis function artificial neural network outperforms alternate methods used to estimate municipal term structure. This result forms the foundation for establishing a decision theory on optimal municipal bond trading. Using multivariate modeling of a liquidity domain measured across three dependent variables, we investigate the proposed decision theory by estimating weekly production-theoretic bond liquidity returns to scale. Across the three liquidity measures and for almost all weeks investigated, bond trading liquidity is elastic with respect to the modeled factors. This finding leads us to conclude that an optimal trading policy for municipal debt can be implemented on a weekly timescale using the elasticity estimates of bond price, trade size, risk, days-to-maturity, and the macroeconomic influences of labor in the workforce and building activity.",,2018,10.1007/s40070-018-0079-2,,proquest,"This research explores transaction characteristics and trading activity of investment grade US municipal bonds using data informatics. It proposes a new methodology to estimate a trade-weighted benchmark municipal yield curve by integrating ridge regression, artificial neural networks, and support vector regression. An enhanced radial basis function artificial neural network was found to outperform other methods. The study also investigates a decision theory on optimal municipal bond trading by modeling bond liquidity returns to scale, concluding that an optimal trading policy can be implemented weekly using elasticity estimates.",True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:16:44.927309
d28b32f410fc35cb,The role of term spread and pattern changes in predicting stock returns and volatility of the United Kingdom: Evidence from a nonparametric causality-in-quantiles test using over 250 years of data,"Given the existence of nonlinear relationship between equity premium and term spread, as well as pattern changes and the interaction of pattern changes with the term-spread and changes in the shape of the yield curve, we use a nonparametric k-th order causality-in-quantiles test to predict the movement in excess returns and volatility based on changes in the shape of the yield curve. With the test applied to over 250 years of monthly data for the UK covering the period 1753:08 to 2017:02, we find that pattern changes and the interaction of pattern changes with the term-spread, besides the term spread itself, tends to also play an important role in predicting volatility at the upper end of its conditional distribution. In addition, the effect on excess returns from term spread, pattern changes and the interaction is found to have improved markedly over time, barring at the conditional median of the equity premium. Finally, comparisons are made with historical data of the US and South Africa, and implications of our results are discussed. © 2019 Elsevier B.V., All rights reserved.","Gupta, R.; Risse, M.; Volkman, D.A.; Wohar, M.E.",2019,10.1016/j.najef.2018.05.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047273301&doi=10.1016%2Fj.najef.2018.05.006&partnerID=40&md5=0b1ce5fd52c3e2ea5edef836d785eb8c,scopus,"This study employs a nonparametric causality-in-quantiles test to investigate the predictive power of term spread and pattern changes on UK stock returns and volatility over 250 years. The findings indicate that term spread, pattern changes, and their interaction are significant predictors of volatility, particularly at higher levels. The predictive impact on excess returns has also increased over time, except at the median. Comparisons with US and South African data are also presented.",True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:16:51.529113
e3dd71ea367283cb,The term structure of Eurozone peripheral bond yields: an asymmetric regime-switching equilibrium correction approach,"Several studies have established the predictive power of the yield curve i.e. the difference between long and short-term bond rates and the role of asymmetries in the term structure of bond yields with respect to real economic activity. Using an extensive dataset, comprising 3-month, 1-year, 5-year and 10-year constant maturity Treasury bonds for the Eurozone southern periphery countries - the so-called PIIGS - from January 1999 to April 2019, we investigate the links between bond yields of different maturities for the Eurozone southern peripheral countries and we find they co-evolve in line with the predictions of the Expectations Hypothesis theory. We demonstrate the presence of nonlinearities in the term structure, and utilize a multivariate asymmetric two-regime Markov-switching VAR methodology to model them properly. Moreover, we address the economic reasoning behind the introduction of an equilibrium-correction regime-switching approach, hence providing potentially important insights on the behaviour of the entire yield curve. We reveal that the regime shifts are related to the state of the business cycle, particularly in economies in which monetary policy decisions are implemented via changes in short-term rates as a response to deviations of output from equilibrium levels. Our results may have important statistical and economic implications on the behaviour of the term structure of bond yields.","Avdoulas, Christos; Bekiros, Stelios; Lucey, Brian",2020,10.1515/snde-2018-0105,,wos,"This study investigates the term structure of Eurozone peripheral bond yields using a multivariate asymmetric two-regime Markov-switching VAR methodology. It finds that bond yields co-evolve according to the Expectations Hypothesis and demonstrates nonlinearities and regime shifts related to the business cycle. The approach provides insights into the behavior of the entire yield curve, particularly in economies where monetary policy responds to output deviations.",True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:16:54.258524
ebe956d6dd174735,The term structure of policy rules,"A formula is derived that links the coefficients of the monetary policy rule for the short-term interest rate to the coefficients of the implied affine equations for long-term interest rates. The formula predicts that an increase in the coefficients in the monetary policy rule will lead to an increase in the coefficients in the affine equations. Empirical evidence for such a prediction is provided. The curve of the response coefficients by maturity is also predicted by the formula. The formula's predictive accuracy and its closed form make it a useful tool for studying the policy implications of embedding no-arbitrage affine theories into macro models. All rights reserved, Elsevier",,2009,10.1016/j.jmoneco.2009.09.004,,proquest,"This paper derives a formula connecting coefficients of monetary policy rules for short-term interest rates to implied affine equations for long-term interest rates. It predicts that increased policy rule coefficients lead to increased affine equation coefficients, supported by empirical evidence. The formula also predicts the response coefficients curve by maturity and is useful for embedding no-arbitrage affine theories into macro models.",False,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:16:55.705451
8247c28423e466eb,The “probability of recession”: Evaluating probabilistic and non-probabilistic forecasts from probit models of U.S. recessions,"This letter evaluates forecasts from probit models that use the slope of the yield curve to forecast recessions. These models give reliable non-probabilistic warnings of recessions, but the estimated probabilities do not match the conditional frequency of recession months.",,2013,10.1016/j.econlet.2013.09.002,,proquest,"This study evaluates probit models that forecast U.S. recessions using the yield curve slope. While the models provide reliable non-probabilistic recession warnings, the estimated probabilities do not accurately reflect the actual frequency of recession months.",True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:17:01.219530
c0e7db18be904853,"Time-varying market, interest rate, and exchange rate risk premia in the US commercial bank stock returns","This paper examines the role of market, interest rate, and exchange rate risks in pricing a sample of the US Commercial Bank stocks by developing and estimating a multi-factor model under both unconditional and conditional frameworks. Three different econometric methodologies are used to conduct the estimations and testing. Estimations based on nonlinear seemingly unrelated regression (NLSUR) via GMM approach indicate that interest rate risk is the only priced factor in the unconditional three-factor model. However, based on 'pricing kernel' approach by Dumas and Solnik [(1995). J. Finance 50, 445-479], strong evidence of exchange rate risk is found in both large bank and regional bank stocks in the conditional three-factor model with time-varying risk prices. Finally, estimations based on the multivariate GARCH in mean (MGARCH-M) approach where both conditional first and second moments of bank portfolio returns and risk factors are estimated simultaneously show strong evidence of time-varying interest rate and exchange rate risk premia and weak evidence of time-varying world market risk premium for all three bank portfolios, namely those of Money Center bank, Large bank, and Regional bank. © 2000 Elsevier Science B.V. All rights reserved. JEL classification: C32; G12; G21. © 2020 Elsevier B.V., All rights reserved.","Tai, C.S.",2000,10.1016/s1042-444x(00)00031-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034416407&doi=10.1016%2Fs1042-444x%2800%2900031-1&partnerID=40&md5=5c47f8f299b31f1053bf00598a4e8e5c,scopus,"This paper investigates the impact of market, interest rate, and exchange rate risks on US commercial bank stock returns using a multi-factor model. Different econometric methods (NLSUR via GMM, pricing kernel approach, and MGARCH-M) reveal that interest rate risk is priced unconditionally. Conditionally, exchange rate risk is significant, and the MGARCH-M approach shows time-varying interest rate and exchange rate risk premia, with weak evidence for market risk premium.",True,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:17:05.441644
430a63506326c22c,Time-varying risk of nominal bonds: How important are macroeconomic shocks?,"I study the sufficiency of macroeconomic information to explain the time-variation in second moments of stock and bond returns, with a particular attention to stock-bond correlations. I propose an external habit model supplemented with realistic non-Gaussian fundamentals estimated solely from macroeconomic data. Intertemporal smoothing and precautionary savings effects – driven by consumption shocks – combine with a time-varying covariance between consumption and inflation to generate large positive and negative stock-bond return correlations. Macroeconomic shocks are most important in explaining second moments of stock and bond returns from the late 1970’s to mid-1990’s and during the Great Recession. © 2022 Elsevier B.V., All rights reserved.","Ermolov, A.",2022,10.1016/j.jfineco.2022.04.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129401050&doi=10.1016%2Fj.jfineco.2022.04.003&partnerID=40&md5=1d94bf5d3b46c594a19799c91da57432,scopus,"This study investigates whether macroeconomic information can explain the time-varying second moments of stock and bond returns, focusing on stock-bond correlations. An external habit model with non-Gaussian fundamentals, estimated from macroeconomic data, is proposed. Consumption shocks and a time-varying covariance between consumption and inflation generate significant stock-bond return correlations. Macroeconomic shocks are found to be particularly influential during the late 1970s to mid-1990s and the Great Recession.",False,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:17:20.569371
3f9abe9daf6dd16e,Towards ubiquitous information supply for individual investors: A decision support system design,"This paper introduces an IT artifact called MoFiN DSS that comprises hard- and software components that provide the basis for a prototype of a financial decision support system (DSS) to support individual investors reacting to unforeseen market events. We have derived our motivation for building such a system design from behavioral finance research. Analyses of the behavior of individual investors provide evidence that this segment does react more significantly to any public news published compared to institutional investors. On the other hand, the analyses show that they react significantly slower than their institutional counterparts. Since empirical intraday event study analyses show that capital markets react promptly to new information and that excess returns decrease over a specific period of time, individual investors miss significant trading opportunities due to their current strategies of information research. We address the problem that this market segment is not able to continuously observe diverse information channels and to assess all the new information available. Our prototype decision support system continuously observes company announcements and forecasts their potential impact on the corresponding stock price. After identifying those events for which significant market reactions can be expected, wireless push-based message services provide the technical basis for prompt and location-independent information supply. Based on a novel simulation-based evaluation methodology we have developed, we demonstrate and quantify the advantages that the developed system provides to the individual investors. © 2009 Elsevier B.V. All rights reserved. © 2009 Elsevier B.V., All rights reserved.","Muntermann, J.",2009,10.1016/j.dss.2009.01.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-64949122389&doi=10.1016%2Fj.dss.2009.01.003&partnerID=40&md5=63342a2f3bab4265f4a2068b33fff64b,scopus,"This paper presents MoFiN DSS, a financial decision support system designed to help individual investors react to market events. It addresses the tendency of individual investors to react slower to news than institutional investors, leading to missed trading opportunities. The system continuously monitors company announcements, predicts their impact on stock prices, and uses push notifications for timely, location-independent information delivery. A novel simulation-based methodology is used to evaluate the system's benefits.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:17:22.645465
e901b823bc8df6c4,Trading Macro-Cycles of Foreign Exchange Markets Using Hybrid Models,"Most existing studies on forecasting exchange rates focus on predicting next-period returns. In contrast, this study takes the novel approach of forecasting and trading the longer-term trends (macro-cycles) of exchange rates. It proposes a unique hybrid forecast model consisting of linear regression, multilayer neural network, and combination models embedded with technical trading rules and economic fundamentals to predict the macro-cycles of the selected currencies and investigate the predicative power and market timing ability of the model. The results confirm that the combination model has a significant predictive power and market timing ability, and outperforms the benchmark models in terms of returns. The finding that the government bond yield differentials and CPI differentials are the important factors in exchange rate forecasts further implies that interest rate parity and PPP have strong influence on foreign exchange market participants.",,2021,10.3390/su13179820,,proquest,"This study proposes a hybrid model combining linear regression, multilayer neural networks, and technical/fundamental rules to forecast and trade longer-term exchange rate macro-cycles. The model demonstrates significant predictive power and market timing ability, outperforming benchmarks. Government bond yield differentials and CPI differentials are identified as key factors influencing exchange rates, suggesting the importance of interest rate parity and PPP.",True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:17:37.961511
7bf6f432caa24633,Turning point vs trend,"At the beginning of the paper some shortcomings of the existing forecasting systems are demonstrated on examples of products of the EU forecasting service and of the Macroeconomic Prospect Team of the Treasury of the UK. And apart of it the smoothed lines of Nobel Prize winner of 2010 Professor Pissarides are considered in comparison with clear forecasts of turning points received by the author on the same time series. Then a description of forecasting of the recession of the early 1990s in the UK is given, as a part of forecasting of innovative growth. It is underlined that statistics must show explicitly ‘the height of technological leap’ and provide separate parameters of old and new technologies. And that the current focusing of attention on the most advanced technologies only should be broadened to all technologies which actually are being implementing in the economy. Comparison with the Cambridge Multisectoral Dynamic Model of the British Economy shows how peculiarities of reflection of new technologies could affect ability of seeing turning points. At the end some remarks are contributed to the current discussion between the competing schools. Positive aspects of the “Great Recession” of 2008 – 2010 are highlighted along with their similarity with previous crises. At that an attempt to restore the “shattered intellectual structure” of Alan Greenspan is made. © 2021 Elsevier B.V., All rights reserved.","Ryaboshlyk, V.",2011,10.14254/2071-8330.2011/4-1/6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969842369&doi=10.14254%2F2071-8330.2011%2F4-1%2F6&partnerID=40&md5=9e9f89c66430456f736ae5a0762ab880,scopus,"This paper critiques existing forecasting systems, using examples from the EU and UK Treasury. It compares smoothed trend lines with identified turning points, discusses forecasting the UK's early 1990s recession as part of innovative growth, and emphasizes the need for statistics to differentiate between old and new technologies. The author contrasts their approach with the Cambridge Multisectoral Dynamic Model, highlighting how technology reflection impacts turning point identification. The paper also touches on the 2008-2010 ""Great Recession"" and attempts to reconstruct Alan Greenspan's economic framework.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:17:40.777770
fa8f409ad6d08349,Unconventional Monetary Policy in the Euro Zone,"The European Central Bank adopted a policy of quantitative easing early in 2015, long after the US and UK, and after implementing a succession of measures to increase liquidity in the Euro zone financial markets, none of which proved sufficient eventually. The paper draws out lessons for the Euro zone from US and UK experience. Numerous event studies have been undertaken to uncover the effects of QE on yields on and prices of financial assets. Estimated effects on long-term government bond yields are then converted into the size of the cut in the policy rate that would normally have been needed to produce them. From these implicit cuts in policy rates, estimates of the effect on GDP and inflation are generated. Euro zone QE appears to have had a much smaller effect on bond yields for the core members states than did QE in the US or UK. Therefore its effects on output and inflation are likely to be proportionately smaller. Its effects on long-term government bond yields in periphery members are greater. QE is compressing interest differential among Euro zone member states. The dangers of QE to which various commentators draw attention, that it creates a danger of inflation in the future, that it creates asset price bubbles, that it allows zombie firms and banks to survive, slowing down the process of adjustment, seem remote. Meanwhile it makes a useful contribution to cutting the costs of debt service and allowing member states more fiscal room for maneouvre.",,2016,10.1007/s11079-016-9393-0,,proquest,"This paper analyzes the effectiveness of the European Central Bank's quantitative easing (QE) policy, implemented in 2015, by drawing lessons from US and UK experiences. It uses event studies to estimate QE's impact on bond yields, GDP, and inflation, finding a smaller effect in the Eurozone core compared to the US/UK, but a greater effect on periphery member states, leading to compressed interest rate differentials. The paper dismisses common concerns about QE, such as future inflation or asset bubbles, and highlights its benefits in reducing debt servicing costs and increasing fiscal flexibility for member states.",False,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:17:42.484935
1198155656cc7f11,Unconventional monetary policy in a nonlinear quadratic model,"After the financial market meltdown and the Great Recession of the years 2007–9, the financial market-macro link has become an important issue in monetary policy modeling. We develop a dynamic model that contains a nonlinear Phillips curve, a dynamic output equation, and a nonlinear credit flow equation – capturing the importance of credit cycles, risk premia, and credit spreads. Our Nonlinear Quadratic Model (NLQ) model has three dynamic state equations and a quadratic objective function. It can be used to evaluate the response of central banks to the Great Recession in moving from conventional to unconventional monetary policy. We solve the model with a new numerical procedure using estimated parameters for the euro area. We conduct simulations to explore the (de)stabilizing effects of the nonlinearities in the model. We demonstrate that credit flows, risk premia, and credit spreads play an important role as an amplification mechanism and in affecting the transmission of monetary policy. We thereby highlight the importance of the natural rate of interest as an anchor for a central bank target and the weight it places on the credit flows for the effectiveness of unconventional monetary policy. Our model is similar in structure compared to larger scale macro-econometric models which many central banks employ.",,2020,10.1515/snde-2019-0099,,proquest,"This paper introduces a Nonlinear Quadratic (NLQ) dynamic model to analyze the impact of unconventional monetary policy, particularly in response to the 2007-9 Great Recession. The model incorporates a nonlinear Phillips curve, an output equation, and a credit flow equation, emphasizing credit cycles, risk premia, and credit spreads. Using estimated parameters for the euro area and a novel numerical procedure, the study simulates the model to assess the stabilizing effects of nonlinearities and the role of credit flows in monetary policy transmission. The findings underscore the significance of the natural rate of interest and credit flows for effective unconventional monetary policy, drawing parallels to larger-scale models used by central banks.",True,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:17:45.132276
1a56df038c2e8350,Uncovering nonlinear dependencies in the Treasury-funds rate spread: Quantile-based explanation,"This study examines the structural dynamics of the spread between the 10-year Treasury yield and the federal funds rate, a key indicator of U.S. financial conditions. Cross-quantilogram analysis reveals a nonlinear dependency across adjacent periods, with stronger connectedness observed in the tail distribution than in the middle. Additionally, the functional quantile autoregression model confirms the spread's nonlinear and asymmetric nature from a distributional perspective. Specifically, higher quantiles of the previous spread exert a stronger influence on the current spread, indicating a positive persistence mechanism. Conversely, lower quantiles of the previous spread negatively affect the higher quantiles of the current spread. These findings suggest that bullish market conditions tend to sustain themselves, whereas bearish conditions hinder upward momentum, underscoring the need for quantile-specific policy interventions.","Meng, Fanyu",2025,10.1016/j.frl.2025.107216,,wos,"This study uses cross-quantilogram and functional quantile autoregression to analyze the nonlinear and asymmetric relationship between the 10-year Treasury yield and the federal funds rate spread. It finds stronger dependencies in the tails of the distribution and suggests that market conditions tend to persist, with higher quantiles influencing current spreads more strongly. The authors advocate for quantile-specific policy interventions.",True,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:17:50.030977
51c6a633d70ff311,Understanding the determinants of bond excess returns using explainable AI,"Recent empirical evidence indicates that bond excess returns can be predicted using machine learning models. However, although the predictive power of machine learning models is intriguing, they typically lack transparency. This paper introduces the state-of-the-art explainable artificial intelligence technique SHapley Additive exPlanations (SHAP) to open the black box of these models. Our analysis identifies the key determinants that drive the predictions of bond excess returns produced by machine learning models and recognizes how these determinants relate to bond excess returns. This approach facilitates an economic interpretation of the predictions of bond excess returns made by machine learning models and contributes to a thorough understanding of the determinants of bond excess returns, which is critical for the decisions of market participants and the evaluation of economic theories. © 2023 Elsevier B.V., All rights reserved.","Beckmann, L.; Debener, J.; Kriebel, J.",2023,10.1007/s11573-023-01149-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160660732&doi=10.1007%2Fs11573-023-01149-5&partnerID=40&md5=e58f5c87140bb001abb38c4aebeb2e78,scopus,"This paper applies the SHapley Additive exPlanations (SHAP) technique to machine learning models predicting bond excess returns. It aims to increase transparency by identifying key determinants driving these predictions and facilitating economic interpretation, thereby enhancing understanding for market participants and economic theory evaluation.",True,True,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T11:17:57.717590
398476d4b3717e7e,Unemployment insurance and mortgages,"We consider mortgages including the possibility of an unemployment insurance. The insurance company pays the cash flows of the credit as soon as the borrower becomes unemployed, for a maximal number of payments fixed in the contract. We develop a probabilistic model for describing the cash flows paid by the insurance company. We jointly take into account unemployment, job search and prepayment phenomena. With such a model it is possible to study the probabilistic properties of the cash flow pattern as a function of the age of the credit. Finally, we discuss the estimation of the parameters of such a model and its use for pricing the insurance contract. (C) 1997 Elsevier Science B.V.","Gourieroux, C; Scaillet, O",1997,10.1016/s0167-6687(97)00003-6,,wos,"This paper models mortgages with unemployment insurance, where the insurer covers payments upon borrower unemployment for a fixed period. It incorporates unemployment, job search, and prepayment dynamics to analyze cash flow patterns and price insurance contracts.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:18:06.537782
2648f39614791bbd,Unit-root tests and asymmetric adjustment with an example using the term structure of interest rates,"This article develops critical values to test the null hypothesis of a unit root against the alternative of stationarity with asymmetric adjustment. Specific attention is paid to threshold and momentum threshold autoregressive processes. The standard Dickey-Fuller tests emerge as a special case. Within a reasonable range of adjustment parameters, the power of the new tests is shown to be greater than that of the corresponding Dickey-Fuller test. The use of the tests is illustrated using the term structure of interest rates. It is shown that the movements toward the long-run equilibrium relationship are best estimated as an asymmetric process.","Enders, W; Granger, CWJ",1998,10.2307/1392506,,wos,"This paper introduces new unit-root tests that account for asymmetric adjustment, outperforming standard Dickey-Fuller tests in power. The methodology is applied to the term structure of interest rates, revealing asymmetric adjustments towards long-run equilibrium.",False,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:18:07.979405
3c76378dff70bc76,Univariate and multivariate forecasting of the electricity futures curve using Dynamic Recurrent Neural Networks,"In recent years international power markets have witnessed high uncertainty and extraordinary volatility which, given the inherent complexity of the market, has made the Electricity Price Forecasting (EPF) process increasingly difficult. Therefore the development of a proper forecasting framework suitable for both stable and volatile periods has assumed an increasing importance for market players and policymakers in both strategic planning and risk management. At present, the majority of the studies on electricity price forecasting focused on the analysis of spot markets, neglecting the importance of derivative price modeling to mitigate the risks induced by market downturns and turmoil. Our study nests within this research stream and analyzes the potential of a set of state-of-the-art Machine Learning (ML) models for the prediction of the term structure of electricity futures prices. The objective is to define an ML-based framework capable of ensuring high predictive performance of the term structure during both stable and extremely turbulent conditions. In this regard we examined the predictive capabilities of a variety of Dynamic Recurrent Neural Networks (DRNNs) including: Nonlinear Autoregressive Neural Networks (NAR-NNs), NAR with Exogenous Inputs (NARX-NNs), Long Short-Term Memory (LSTM-NNs), Stacked Long Short-Term Memory (ST-LSTM-NNs), Bidirectional Long Short-Term Memory (BI-LSTM-NNs) and Encoder–Decoder Long Short-Term Memory Neural Networks (ED-LSTM-NNs). The models were applied to both low fluctuating and volatile sets of daily futures prices of the European Energy Exchange (EEX) for univariate as well as multivariate forecasting. Additionally, we compared this set of networks to baseline models commonly used in the EPF literature, including classical statistical and ML methods. Empirical results highlighted that DRNN models predictions are consistent with futures prices trends observed under different market regimes and outperform the competitors’ performance. Overall, main outcomes of the study may be summarized as follows: LSTM-based models seem to have the highest predictive power, with robust performance under various conditions. In detail the Multivariate BI-LSTM-NN performs better under quiet market conditions ensuring an accuracy level of 98.11 %, while the Univariate ED-LSTM-NN ensures superior predictive performance in presence of turmoil, achieving a 95.33 % accuracy. © 2025 Elsevier B.V., All rights reserved.","Castello, O.; Resta, M.",2025,10.1016/j.apenergy.2025.126082,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006717802&doi=10.1016%2Fj.apenergy.2025.126082&partnerID=40&md5=e71b13e9cb03bed028f944d93dbc5d7e,scopus,"This study investigates the use of Dynamic Recurrent Neural Networks (DRNNs), including various LSTM architectures, for forecasting electricity futures prices. The models were applied to both stable and volatile market conditions, comparing their performance against baseline methods. Results indicate that LSTM-based models, particularly Multivariate BI-LSTM-NN and Univariate ED-LSTM-NN, demonstrate strong predictive power under different market regimes.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:18:15.524972
164d3e1462675b58,Unstable volatility: the break-preserving local linear estimator,"The objective of this paper is to introduce the break-preserving local linear (BPLL) estimator for the estimation of unstable volatility functions for independent and asymptotically independent processes. Breaks in the structure of the conditional mean and/or the volatility functions are common in Finance. Nonparametric estimators are well suited for these events due to the flexibility of their functional form and their good asymptotic properties. However, the local polynomial kernel estimators are not consistent at points where the volatility function has a break. The estimator presented in this paper generalises the classical local linear (LL). The BPLL estimator maintains the desirable properties of the LL estimator with regard to the bias and the boundary estimation while it estimates the breaks consistently. An extensive Monte Carlo study is shown as well as detailed proofs of the estimator asymptotic behaviour.","Casas, Isabel; Gijbels, Irene",2012,10.1080/10485252.2012.720981,,wos,"This paper introduces the break-preserving local linear (BPLL) estimator for unstable volatility functions in independent and asymptotically independent processes, addressing common breaks in financial conditional mean and volatility functions. The BPLL estimator generalizes the classical local linear estimator, maintaining desirable properties while consistently estimating breaks, unlike standard local polynomial kernel estimators.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:18:24.239606
061c3ab3803363ee,Using Neural Networks to Forecast Volatility for an Asset Allocation Strategy Based on the Target Volatility,"The objective of this study is to use artificial neural networks for volatility forecasting to enhance the ability of an asset allocation strategy based on the target volatility. The target volatility level is achieved by dynamically allocating between a risky asset and a risk-free cash position. However, a challenge to data-driven approaches is the limited availability of data since periods of high volatility, such as during financial crises, are relatively rare. To resolve this issue, we apply a stability-oriented approach to compare data for the current period to a past set of data for a period of low volatility, providing a much more abundant source of data for comparison. In order to explore the impact of the proposed model, the results of this approach will be compared to different volatility forecast methodologies, such as the volatility index, the historical volatility, the exponentially weighted moving average (EWMA), and the generalized autoregressive conditional heteroskedasticity (GARCH) model. Trading measures are used to evaluate the performance of the models for forecasting volatility. An empirical study of the proposed model is conducted using the Korea Composite Stock Price Index 200 (KOSPI 200) and certificate of deposit interest rates from January, 2006 to February, 2016. (C) 2016 The Authors. Published by Elsevier B.V.","Kim, Youngmin; Enke, David",2016,10.1016/j.procs.2016.09.335,,wos,"This study uses neural networks to forecast volatility for an asset allocation strategy based on target volatility. It addresses data limitations by comparing current data to past low-volatility periods and compares its model to other forecasting methods (VIX, historical volatility, EWMA, GARCH) using trading measures for evaluation. The empirical study uses KOSPI 200 and CD rates from 2006-2016.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:18:26.091703
4e973d99311508de,Using the yield curve to forecast economic growth,"This paper finds the yield curve to have a well-performing ability to forecast the real gross domestic product growth in the USA, compared to professional forecasters and time series models. Past studies have different arguments concerning growth lags, structural breaks, and ultimately the ability of the yield curve to forecast economic growth. This paper finds such results to be dependent on the estimation and forecasting techniques employed. By allowing various interest rates to act as explanatory variables and various window sizes for the out-of-sample forecasts, significant forecasts from many window sizes can be found. These seemingly good forecasts may face issues, including persistent forecasting errors. However, by using statistical learning algorithms, such issues can be cured to some extent. The overall result suggests, by scientifically deciding the window sizes, interest rate data, and learning algorithms, many outperforming forecasts be produced for all lags from one quarter to 3 years, although some may be worse than the others due to the irreducible noise of the data.","Yang, Parley Ruogu",2020,10.1002/for.2676,,wos,"This paper investigates the predictive power of the yield curve for US real GDP growth, comparing its performance against professional forecasters and time series models. It highlights that forecast accuracy depends on estimation and forecasting techniques, including the choice of interest rates and window sizes. The study demonstrates that statistical learning algorithms can mitigate issues like persistent forecasting errors, enabling the production of outperforming forecasts across various lags by optimizing window sizes, interest rate data, and learning algorithms.",True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:18:27.830923
b369ae93e8a8645c,VALUATION OF EMBEDDED OPTIONS IN NON-MARKETABLE CALLABLE BONDS: A NEW NUMERICAL APPROACH,"The issue of how to price options embedded in callable bonds has attracted a lot of interest over the years. The usual bond valuation methods rely on yield curves, risk premium, and other parameters to estimate interest rates used in discounted cash flow calculations. The option to retire the bond is, however, neglected in the standard pricing models, causing a systematic overvaluation of callable bonds. In the event of a decline in interest rates, investors are exposed to the risk of a lower return on investment than indicated by the yield to maturity. We propose a novel approach to valuing the risk that the issuer will use the right to buy back the bond at a specific call price. While prior models are focused on valuing marketable callable bonds, we deliver a unique approach to valuing bonds with an embedded European option (or a multiple option) that are traded solely through private transactions. These can typically be characterized by the lack of historical records on transaction prices. The modular character of calculation we propose allows us to take into account additional information, such as probable behaviour of the issuer, available opportunities for achieving alternative earnings or different estimates in terms of interest rate development.","Skalicky, Roman; Zinecker, Marek; Balcerzak, Adam P.; Pietrzak, Michal Bernard; Rogalska, Elzbieta",2022,10.3846/tede.2022.17060,,wos,"This paper proposes a new numerical approach to value embedded options in non-marketable callable bonds, addressing the systematic overvaluation issue in standard pricing models that neglect the issuer's right to retire the bond. The method accounts for factors like issuer behavior and interest rate development, offering a unique approach for bonds traded privately without historical price records.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:18:28.979347
eaf9d6fda873b291,Value-at-Risk via mixture distributions reconsidered,"Value-at-Risk (VaR) has evolved as one of the most prominent measures of downside risk in financial markets. Zhang and Cheng [M.-H. Zhang, Q.-S. Cheng, An Approach to VaR for capital markets with Gaussian mixture, Applied Mathematics and Computation 168 (2005) 1079-1085] proposed an approach to VaR for daily returns based on Gaussian mixtures, which have become rather popular in empirical economics and finance since the seminal paper of Hamilton [J.D. Hamilton, A new approach to the economic analysis of nonstationary time series and the business cycle, Econometrica 57 (2) (1989) 357-384]. However, they do not conduct tests to assess the accuracy of the mixture-implied VaR measures. Recently, Guidolin and Timmermann [M. Guidolin, A. Timmermann, Term structure of risk under alternative econometric specifications, Journal of Econometrics, 131 (2006) 285-308] showed that Markov mixture models do well in measuring VaR at a monthly frequency, but the results may not hold for daily returns due to their more pronounced non-Gaussian features. This paper provides an extensive application of various Markov mixture models to VaR for daily returns of major European stock markets, including out-of-sample backtesting. To accommodate the properties of daily returns, we consider both Gaussian and Student's t mixtures, and we compare the performance of both uni- and multivariate models under different parameter updating schemes. We find that a univariate mixture of two Student's t distributions performs best overall. However, by the example of the recent turmoil in financial markets, we also highlight a weak point of the approach. © 2009 Elsevier Inc. All rights reserved. © 2009 Elsevier B.V., All rights reserved.","Haas, M.",2009,10.1016/j.amc.2009.08.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349981419&doi=10.1016%2Fj.amc.2009.08.005&partnerID=40&md5=37f9b7ead5757561295b093089af7ee0,scopus,"This paper evaluates various Markov mixture models for calculating Value-at-Risk (VaR) for daily stock market returns, comparing Gaussian and Student's t mixtures. A univariate mixture of two Student's t distributions performed best overall, though a weakness was identified during recent market turmoil.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:18:30.222415
5a945e5c94a9bfc5,Valuing the Treasury's Capital Assistance Program,"T he Capital Assistance Program (CAP) was created by the U. S. government in February 2009 to provide backup capital to large financial institutions unable to raise sufficient capital from private investors. Under the terms of the CAP, a participating bank receives contingent capital by issuing preferred shares to the Treasury combined with embedded options for both parties: The bank gets the option to redeem the shares or convert them to common equity, with conversion mandatory after seven years; the Treasury earns dividends on the preferred shares and gets warrants on the bank's common equity. We develop a contingent claims framework in which to estimate market values of these CAP securities. The interaction between the competing options held by the buyer and issuer of these securities creates a game between the two parties, and our approach captures this strategic element of the joint valuation problem and clarifies the incentives it creates. We apply our method to the 18 publicly held bank holding companies that participated in the Supervisory Capital Assessment Program (the stress test) launched together with the CAP. On average, we estimate that compared to a market transaction, the CAP securities carry a net value of approximately 30% of the capital invested for a bank participating to the maximum extent allowed under the terms of the program. We also find that the net value varies widely across banks. We compare our estimates with abnormal stock price returns for the stress test banks at the time the terms of the CAP were announced; we find correlations between 0.78 and 0.85, depending on the precise choice of period and set of banks included. These results suggest that our valuation aligns with shareholder perception of the value of the program, prompting questions about industry reactions and the overall impact of the program.","Glasserman, Paul; Wang, Zhenyu",2011,10.1287/mnsc.1110.1351,,wos,"This paper values the U.S. Treasury's Capital Assistance Program (CAP) using a contingent claims framework that accounts for the strategic interaction between the bank and the Treasury regarding embedded options. The authors estimate that CAP securities provided a net value of approximately 30% of the invested capital to participating banks, with significant variation across institutions. The valuation aligns with abnormal stock returns observed around the CAP announcement, suggesting market recognition of the program's value.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:18:31.363249
2a226e9e781cf124,Variable Selection and Oversampling in the Use of Smooth Support Vector Machines for Predicting the Default Risk of Companies,"In the era of Basel II a powerful tool for bankruptcy prognosis is vital for banks. The tool must be precise but also easily adaptable to the bank's objectives regarding the relation of false acceptances (Type I error) and false rejections (Type II error). We explore the suitability of smooth support vector machines (SSVM), and investigate how important factors such as the selection of appropriate accounting ratios (predictors), length of training period and structure of the training sample influence the precision of prediction. Moreover, we show that oversampling can be employed to control the trade-off between error types, and we compare SSVM with both logistic and discriminant analysis. Finally, we illustrate graphically how different models can be used jointly to support the decision-making process of loan officers. Copyright (C) 2008 John Wiley & Sons, Ltd.","Haerdle, Wolfgang; Lee, Yuh-Jye; Schaefer, Dorothea; Yeh, Yi-Ren",2009,10.1002/for.1109,,wos,"This study investigates the use of smooth support vector machines (SSVM) for predicting company default risk, comparing it with logistic and discriminant analysis. It explores the impact of variable selection, training period length, and sample structure on prediction accuracy. The research also demonstrates how oversampling can be used to manage the trade-off between Type I and Type II errors, offering a tool for loan officers to support decision-making.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:18:34.777011
f0282a2efcf8a584,Volatility forecasts embedded in the prices of crude-oil options,"This paper evaluates the ability of alternative option-implied volatility measures to forecast crude-oil return volatility. We find that a corridor implied volatility measure that aggregates information from a narrow range of option contracts consistently outperforms forecasts obtained by the popular Black-Scholes and model-free volatility expectations, as well as those generated by a realized volatility model. This measure ranks favorably in regression-based tests, delivers the lowest forecast errors under different loss functions, and generates economically significant gains in volatility timing exercises. Our results also show that the Chicago Board Options Exchange's oil-VIX index performs poorly, as it routinely produces the least accurate forecasts.","Gilder, Dudley; Tsiaras, Leonidas",2020,10.1002/fut.22114,,wos,"This paper assesses how well different option-implied volatility measures can predict crude-oil return volatility. A novel corridor implied volatility measure, which uses data from a specific range of option contracts, proves superior to common methods like Black-Scholes, model-free expectations, and realized volatility models. This measure also performs well in regression tests, shows lower forecast errors across various loss functions, and offers practical benefits in volatility timing. The study also highlights the poor performance of the Chicago Board Options Exchange's oil-VIX index in forecasting.",False,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:18:36.808562
40ca5d0204bc68fe,War discourse and global equity returns,"This study investigates the asset pricing implications of war risks in global stock markets. We employ a novel war discourse index developed by Hirshleifer et al. (2023a), which captures market attention to war through news. Extending this approach to both developed and emerging markets, we uncover a significantly positive relation between war risks and global stock market excess returns, which is robust to a range of sensitivity checks. Our findings indicate that investor attention to war risks significantly influences equity premium in global markets. © 2024 Elsevier B.V., All rights reserved.","Wang, J.; Fang, Y.; Hu, X.; Zhong, A.",2024,10.1016/j.frl.2024.106068,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203652926&doi=10.1016%2Fj.frl.2024.106068&partnerID=40&md5=5190fb5a9dcd24cdbbd57eb075a3d053,scopus,"This study examines the relationship between war risks, measured by a war discourse index based on news attention, and global stock market returns. The research finds a significant positive correlation between war risks and excess returns in both developed and emerging markets, suggesting that investor attention to war influences the equity premium.",False,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:18:49.349412
68a8425f93fcd105,Wasserstein barycenter regression for estimating the joint dynamics of renewable and fossil fuel energy indices,"In order to characterize non-linear system dynamics and to generate term structures of joint distributions, we propose a flexible and multidimensional approach, which exploits Wasserstein barycentric coordinates for histograms. We apply this methodology to study the relationships between the performance in the European market of the renewable energy sector and that of the fossil fuel energy one. Our methodology allows us to estimate the term structure of conditional joint distributions. This optimal barycentric interpolation can be interpreted as a posterior version of the joint distribution with respect to the prior contained in the past histograms history. Once the underlying dynamics mechanism among the set of variables are obtained as optimal Wasserstein barycentric coordinates, the learned dynamic rules can be used to generate term structures of joint distributions.",,2023,10.1007/s10287-023-00436-4,,proquest,"This paper introduces a novel approach using Wasserstein barycentric coordinates for histograms to analyze the joint dynamics of renewable and fossil fuel energy indices in the European market. The method estimates the term structure of conditional joint distributions, offering a flexible way to characterize non-linear system dynamics and generate future distribution terms based on historical data.",True,False,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:18:52.783511
df6c8aefb227da2b,Wavelet Neural Network Model for Yield Spread Forecasting,"In this study, a hybrid method based on coupling discrete wavelet transforms (DWTs) and artificial neural network (ANN) for yield spread forecasting is proposed. The discrete wavelet transform (DWT) using five different wavelet families is applied to decompose the five different yield spreads constructed at shorter end, longer end, and policy relevant area of the yield curve to eliminate noise from them. The wavelet coefficients are then used as inputs into Levenberg-Marquardt (LM) ANN models to forecast the predictive power of each of these spreads for output growth. We find that the yield spreads constructed at the shorter end and policy relevant areas of the yield curve have a better predictive power to forecast the output growth, whereas the yield spreads, which are constructed at the longer end of the yield curve do not seem to have predictive information for output growth. These results provide the robustness to the earlier results.",,2017,10.3390/math5040072,,proquest,"This study proposes a hybrid model combining Discrete Wavelet Transforms (DWT) and Artificial Neural Networks (ANN) to forecast yield spreads. The DWT decomposes yield spreads into wavelet coefficients, which are then used as inputs for an ANN to predict output growth. The findings indicate that shorter-term and policy-relevant yield spreads are better predictors of output growth than longer-term spreads.",True,True,True,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:18:58.079692
190ed5fb32b9d0d7,Yield Curve Modeling: Applicability of the Traditional Factor Models for Sri Lanka Government Bonds,"The main aim of this study is to evaluate the effectiveness of two widely discussed yield curve models, the Nelson-Siegel model and the Nelson-Siegel-Svensson model, in estimating Sri Lanka Government Bond yields. The parameters for both models were estimated using the YieldCurve package in R-Studio. The average R-squared values were 96.25% for the Nelson-Siegel model and 98.75% for the Nelson-Siegel-Svensson model. However, neither model consistently achieved high R-squared values across the entire sample period. The Nelson-Siegel-Svensson model demonstrated greater consistency in R-squared values compared to the Nelson-Siegel model throughout the period. But the R-squared value declined in 2022 compared with the previous period for both models as the yield curve accompanied more twists and turns with volatile economic conditions. These results suggest that there is significant potential for developing more representative yield curve models or enhancing existing models by incorporating additional influential factors. The monetary authorities and Investment banks of the country would pay more attention to data-driven decision-making in the future to set up economic and monetary targets as well as to achieve hurdle rates for the client’s portfolios. Having an accurate yield curve model would be a play major role in this regard. © 2025 Elsevier B.V., All rights reserved.","Dayarathne, K.P.N.S.; Thayasivam, U.",2025,10.1007/s42979-025-04075-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008883628&doi=10.1007%2Fs42979-025-04075-1&partnerID=40&md5=bfd2bb858b8bff203220d422f64d79a5,scopus,"This study evaluates the Nelson-Siegel and Nelson-Siegel-Svensson yield curve models for Sri Lanka Government Bonds. While both models show high average R-squared values, their performance varied, particularly during volatile economic conditions in 2022. The Nelson-Siegel-Svensson model was more consistent. The findings suggest a need for more representative models or the incorporation of additional factors, highlighting the importance of accurate yield curve modeling for monetary authorities and investment banks.",False,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:19:00.202606
7aed82193826f66e,Yield curve estimation of the nelson-siegel class model by using hybrid method with L-BFGS-B iterations approach,"This paper discussed about model extension in determines the yield curve. Determine of yield curve using Nelson-Siegel class model. This class model consisting of: 3-factor model, 4-factor model, the 5-factor model, and 6-factor model. 6-factor model is a model extended from 5-factor models. The extension aims to increase the level of accuracy in determine the yield curve. Nelson-Siegel class model is model that more difficult to estimate because it has two shape the parameters, i.e. the linear and nonlinear parameters. Extension of this model is done by adding the fourth hump into 5-factor model. In addition, we obtain new model, this model have local minimum multiple so that it is more difficult to be estimated. To estimate this model, we propose estimation using a hybrid method. Hybrid method is combines method of estimation the nonlinear least squares with constrained optimization, and then continued with L-BFGS-B iteration approach. Estimation of the class model was done by full estimation, i.e. estimating the linear parameters and nonlinear parameters simultaneously. Then, we calculated MSE, AIC, and BIC. The purpose of calculating this component is to determine the best of model. The best model obtainable if the models have component value which is smaller than the other models. This paper uses data from Indonesian government bonds. Based on data processing, we obtained the best model i.e. 6- factors model. © 2015 Elsevier B.V., All rights reserved.","Muslim; Rosadi, D.; Gunardi, G.; Abdurakhman, n.",2015,10.12988/ams.2015.43209,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929934326&doi=10.12988%2Fams.2015.43209&partnerID=40&md5=68f1e179c410a82458ca47a5b40c3e7f,scopus,"This paper proposes a hybrid estimation method combining nonlinear least squares with constrained optimization and L-BFGS-B iterations to estimate extended Nelson-Siegel class models (3- to 6-factor) for yield curve determination. The 6-factor model, an extension of the 5-factor model with an added hump, was found to be the best model based on MSE, AIC, and BIC using Indonesian government bond data.",True,True,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:19:02.647370
ab0c1b13706fa110,Yield curve extrapolation with machine learning,"Yield curve extrapolation to unobservable tenors is a key technique for the market-consistent valuation of actuarial liabilities required by Solvency II and forthcoming similar regulations. Since the regulatory method, the Smith-Wilson method, is inconsistent with observable yield curve dynamics, parsimonious parametric models, the Nelson-Siegel model and its extensions, are often used for yield curve extrapolation in risk management. However, it is difficult for the parsimonious parametric models to extrapolate yield curves without excessive volatility because of their limited ability to represent observed yield curves with a limited number of parameters. To extend the representational capabilities, we propose a novel yield curve extrapolation method using machine learning. Using the long short-term memory architecture, we achieve purely data-driven yield curve extrapolation with better generalization performance, stability, and consistency with observed yield curve dynamics than the previous parsimonious parametric models on US and Japanese yield curve data. In addition, our method has model interpretability using the backpropagation algorithm. The findings of this study prove that neural networks, which have recently received considerable attention in mortality forecasting, are useful for yield curve extrapolation, where they have not been used before. © 2025 Elsevier B.V., All rights reserved.","Akiyama, S.; Matsuyama, N.",2025,10.1017/asb.2024.27,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210168150&doi=10.1017%2Fasb.2024.27&partnerID=40&md5=5439066d1de45cd8165ba925b1874d67,scopus,"This study proposes a novel yield curve extrapolation method using Long Short-Term Memory (LSTM) neural networks, a machine learning technique. The method aims to improve upon traditional models like Smith-Wilson and Nelson-Siegel by offering better generalization, stability, and consistency with observed yield curve dynamics. The research demonstrates the effectiveness of this data-driven approach on US and Japanese yield curve data, highlighting its potential for market-consistent valuation of actuarial liabilities under regulations like Solvency II. The authors also note the interpretability of their model through backpropagation and suggest that neural networks, successful in mortality forecasting, are also valuable for yield curve extrapolation.",True,True,True,gemini-2.5-flash-lite,Ulrik,Y,,2025-10-24T11:23:25.891264
40d5957f898788bd,ZONE-TARGETING MONETARY POLICY PREFERENCES AND FINANCIAL MARKET CONDITIONS: A FLEXIBLE NON-LINEAR POLICY REACTION FUNCTION OF THE SARB MONETARY POLICY,"We estimate a flexible model of the monetary policy reaction function of the South African Reserve Bank based on a representation of the policymaker's preferences that capture asymmetries and zone-targeting behaviours. We augment the analysis to allow for responses to financial market conditions over and above inflation and output stabilisation to address the current debate on the importance of financial asset prices in monetary policy decision making. The empirical results show that the monetary authorities' response to inflation is zone symmetric. Secondly, the monetary authorities' response to output is asymmetric with increased reaction during business cycle downturns relative to upturns. Thirdly, the monetary authorities pay close attention to the financial conditions index by placing an equal weight on financial market booms and recessions.","Naraidoo, Ruthira; Raputsoane, Leroi",2010,10.1111/j.1813-6982.2010.01256.x,,wos,"This study estimates a flexible, non-linear monetary policy reaction function for the South African Reserve Bank, incorporating zone-targeting preferences and responses to financial market conditions. The findings indicate symmetric responses to inflation, asymmetric responses to output (stronger during downturns), and significant attention to financial conditions, with equal weight given to booms and recessions.",True,False,False,gemini-2.5-flash-lite,Ulrik,N,,2025-10-24T11:23:31.349836
