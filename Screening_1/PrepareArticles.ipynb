{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prepare Articles (Clean, Normalize, Enhance)\n",
        "\n",
        "This notebook builds a unified dataset from raw sources and precomputes Gemini 2.5 Flash summaries + I1â€“I5 hints for each paper. It saves the enhanced dataset to `articles_with_gpt_response.csv` for use by the review notebook.\n",
        "\n",
        "Run top-to-bottom whenever raw inputs change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- imports ---\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import asyncio\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List, Optional\n",
        "\n",
        "import pandas as pd\n",
        "from pydantic import BaseModel\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "# Paths (relative to this notebook directory)\n",
        "OUT_DIR = Path(\".\")\n",
        "RAW_DIR_IN = OUT_DIR / \"raw_data\"\n",
        "\n",
        "# LLM\n",
        "GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
        "if GOOGLE_API_KEY:\n",
        "    os.environ[\"GEMINI_API_KEY\"] = GOOGLE_API_KEY  # map to SDK expected env var\n",
        "\n",
        "\n",
        "# --- config / paths ---\n",
        "OUT_DIR = Path(\".\")\n",
        "ENHANCED_CSV = OUT_DIR / \"articles_with_gpt_response.csv\"\n",
        "DEBUG_DIR = OUT_DIR / \"debug_out\"\n",
        "DEBUG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "DEBUG_JSONL = DEBUG_DIR / \"batch_debug.jsonl\"\n",
        "\n",
        "GOOGLE_LLM_MODEL = os.getenv(\"GOOGLE_LLM_MODEL\", \"gemini-2.5-flash-lite\")\n",
        "API_KEY = os.getenv(\"GEMINI_API_KEY\") or os.getenv(\"GOOGLE_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“„ ieeexplore: 43 articles\n",
            "ðŸ“„ proquest: 489 articles\n",
            "ðŸ“„ scopus: 792 articles\n",
            "ðŸ“„ wos: 825 articles\n",
            "\n",
            "ðŸ“Š Total combined: 2149 articles\n",
            "âœ… After filtering: 2149 articles\n",
            "\n",
            "ðŸ” Deduplication process:\n",
            "   - Articles with DOI: 2149\n",
            "   - Articles without DOI: 0\n",
            "   - Removed 944 duplicate DOIs\n",
            "   - After DOI dedup: 1205 articles\n",
            "   - Removed 2 duplicate titles\n",
            "   - After title dedup: 1203 articles\n",
            "\n",
            "âœ¨ Final unified dataset: 1203 unique articles\n"
          ]
        }
      ],
      "source": [
        "# Load and normalize raw sources into a unified dataframe\n",
        "\n",
        "SOURCES = {\n",
        "    \"ieeexplore.csv\": \"ieeexplore\",\n",
        "    \"proquest.csv\": \"proquest\",\n",
        "    \"scopus.csv\": \"scopus\",\n",
        "    \"wos.csv\": \"wos\",\n",
        "}\n",
        "\n",
        "common_cols = [\"title\", \"abstract\", \"authors\", \"year\", \"doi\", \"link\", \"database\"]\n",
        "\n",
        "alt_names = {\n",
        "    \"title\": [\"title\", \"document title\", \"ti\", \"Title\", \"Article title\", \"article title\"],\n",
        "    \"abstract\": [\"abstract\", \"ab\", \"Abstract\"],\n",
        "    \"authors\": [\"authors\", \"au\", \"Author(s)\", \"Author\"],\n",
        "    \"year\": [\"year\", \"py\", \"Publication Year\", \"Year\", \"PubDate\"],\n",
        "    \"doi\": [\"doi\", \"di\", \"DOI\"],\n",
        "    \"link\": [\"link\", \"pdf link\", \"url\", \"Link\"],\n",
        "}\n",
        "\n",
        "def pick(row, keys):\n",
        "    for k in keys:\n",
        "        if k in row and pd.notna(row[k]):\n",
        "            return row[k]\n",
        "    return None\n",
        "\n",
        "frames = []\n",
        "source_counts = {}\n",
        "for fname, src in SOURCES.items():\n",
        "    p = RAW_DIR_IN / fname\n",
        "    if not p.exists():\n",
        "        print(f\"âš ï¸  {src}: file not found, skipping\")\n",
        "        continue\n",
        "    # WoS uses semicolon as delimiter, others use comma\n",
        "    sep = ';' if fname == 'wos.csv' else ','\n",
        "    temp = pd.read_csv(p, sep=sep)\n",
        "    lower_cols = {c: c.strip().lower() for c in temp.columns}\n",
        "    temp = temp.rename(columns=lower_cols)\n",
        "    out = pd.DataFrame()\n",
        "    out[\"title\"] = temp.apply(lambda r: pick(r, [c.lower() for c in alt_names[\"title\"]]), axis=1)\n",
        "    out[\"abstract\"] = temp.apply(lambda r: pick(r, [c.lower() for c in alt_names[\"abstract\"]]), axis=1)\n",
        "    out[\"authors\"] = temp.apply(lambda r: pick(r, [c.lower() for c in alt_names[\"authors\"]]), axis=1)\n",
        "    out[\"year\"] = temp.apply(lambda r: pick(r, [c.lower() for c in alt_names[\"year\"]]), axis=1)\n",
        "    out[\"doi\"] = temp.apply(lambda r: pick(r, [c.lower() for c in alt_names[\"doi\"]]), axis=1)\n",
        "    out[\"link\"] = temp.apply(lambda r: pick(r, [c.lower() for c in alt_names[\"link\"]]), axis=1)\n",
        "    out[\"database\"] = src\n",
        "    source_counts[src] = len(out)\n",
        "    print(f\"ðŸ“„ {src}: {len(out)} articles\")\n",
        "    frames.append(out)\n",
        "\n",
        "if frames:\n",
        "    merged = pd.concat(frames, ignore_index=True)\n",
        "else:\n",
        "    merged = pd.DataFrame(columns=common_cols)\n",
        "\n",
        "print(f\"\\nðŸ“Š Total combined: {len(merged)} articles\")\n",
        "\n",
        "# Normalize types and deduplicate\n",
        "merged[\"title\"] = merged[\"title\"].astype(str).str.strip()\n",
        "merged[\"abstract\"] = merged[\"abstract\"].astype(str)\n",
        "merged[\"authors\"] = merged[\"authors\"].astype(str)\n",
        "\n",
        "# Handle year extraction - for ProQuest articles, extract from PubDate format \"2016-03-01\"\n",
        "def extract_year_with_pubdate_fallback(row):\n",
        "    year = row.get('year')\n",
        "    if pd.notna(year) and str(year).strip() != '':\n",
        "        year_str = str(year).strip()\n",
        "        \n",
        "        # For ProQuest articles, try to extract from PubDate format first\n",
        "        if row.get('database') == 'proquest' and '-' in year_str and len(year_str.split('-')[0]) == 4:\n",
        "            try:\n",
        "                return int(year_str.split('-')[0])\n",
        "            except (ValueError, IndexError):\n",
        "                pass\n",
        "        \n",
        "        # Try to convert to numeric for other cases\n",
        "        try:\n",
        "            return pd.to_numeric(year, errors=\"coerce\")\n",
        "        except:\n",
        "            return year\n",
        "    \n",
        "    return year\n",
        "\n",
        "merged[\"year\"] = merged.apply(extract_year_with_pubdate_fallback, axis=1)\n",
        "merged[\"year\"] = pd.to_numeric(merged[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
        "merged[\"doi\"] = merged[\"doi\"].astype(str).str.strip().str.lower()\n",
        "merged[\"link\"] = merged[\"link\"].astype(str).str.strip()\n",
        "\n",
        "# Drop rows without title\n",
        "before_title_filter = len(merged)\n",
        "merged = merged[merged[\"title\"].str.len() > 0].copy()\n",
        "removed_no_title = before_title_filter - len(merged)\n",
        "if removed_no_title > 0:\n",
        "    print(f\"ðŸ—‘ï¸  Removed {removed_no_title} articles without title\")\n",
        "print(f\"âœ… After filtering: {len(merged)} articles\")\n",
        "\n",
        "# Deduplicate strategy (matching literature-review-ProbAI approach):\n",
        "# 1. First deduplicate by DOI (normalized)\n",
        "# 2. Then deduplicate by Title (all papers)\n",
        "\n",
        "print(f\"\\nðŸ” Deduplication process:\")\n",
        "\n",
        "# Separate rows with NaN in 'doi'\n",
        "nan_doi_rows = merged[merged['doi'].isna() | (merged['doi'] == 'nan') | (merged['doi'].str.len() == 0)]\n",
        "non_nan_doi_rows = merged[merged['doi'].notna() & (merged['doi'] != 'nan') & (merged['doi'].str.len() > 0)].copy()\n",
        "\n",
        "print(f\"   - Articles with DOI: {len(non_nan_doi_rows)}\")\n",
        "print(f\"   - Articles without DOI: {len(nan_doi_rows)}\")\n",
        "\n",
        "# Normalize DOI for non-NaN rows\n",
        "non_nan_doi_rows[\"doi\"] = non_nan_doi_rows[\"doi\"].str.lower()\n",
        "non_nan_doi_rows[\"doi\"] = non_nan_doi_rows[\"doi\"].str.replace(\"https://doi.org/\", \"\", regex=False)\n",
        "\n",
        "# Drop duplicates based on DOI\n",
        "before_doi_dedup = len(non_nan_doi_rows)\n",
        "non_nan_doi_rows = non_nan_doi_rows.drop_duplicates(subset=[\"doi\"], keep='first')\n",
        "removed_doi_dupes = before_doi_dedup - len(non_nan_doi_rows)\n",
        "print(f\"   - Removed {removed_doi_dupes} duplicate DOIs\")\n",
        "\n",
        "# Concatenate back\n",
        "merged = pd.concat([non_nan_doi_rows, nan_doi_rows], ignore_index=True)\n",
        "print(f\"   - After DOI dedup: {len(merged)} articles\")\n",
        "\n",
        "# Drop duplicates based on title (for ALL papers, not just those without DOI)\n",
        "before_title_dedup = len(merged)\n",
        "merged = merged.groupby(['title'], as_index=False).first()\n",
        "merged = merged.copy()\n",
        "removed_title_dupes = before_title_dedup - len(merged)\n",
        "print(f\"   - Removed {removed_title_dupes} duplicate titles\")\n",
        "print(f\"   - After title dedup: {len(merged)} articles\")\n",
        "\n",
        "# Compute paper_id\n",
        "import hashlib as _hash\n",
        "\n",
        "def derive_id_row(r):\n",
        "    parts = [str(r.get(\"title\", \"\")), str(r.get(\"year\", \"\")), str(r.get(\"doi\", \"\"))]\n",
        "    key = \"|||\".join(parts).strip().lower()\n",
        "    return _hash.sha256(key.encode(\"utf-8\")).hexdigest()[:16]\n",
        "\n",
        "merged[\"paper_id\"] = merged.apply(derive_id_row, axis=1)\n",
        "\n",
        "# Reorder\n",
        "cols = [\"paper_id\", \"title\", \"abstract\", \"authors\", \"year\", \"doi\", \"link\", \"database\"]\n",
        "merged = merged[cols]\n",
        "\n",
        "print(f\"\\nâœ¨ Final unified dataset: {len(merged)} unique articles\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "ðŸ” DUPLICATE TITLE INVESTIGATION\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š Summary:\n",
            "   - Total unique titles with duplicates: 2\n",
            "   - Total papers with duplicate titles: 4\n",
            "   - Papers that will be removed: 2\n",
            "\n",
            "ðŸ” Top 10 most duplicated titles:\n",
            "\n",
            "   1. Title (appears 2 times):\n",
            "      \"On filtering in Markovian term structure models: An approximation approach\"\n",
            "      - scopus     | Year: 2001 | DOI: 10.1239/aap/1011994030         | Authors: Chiarella, C.; Pasquali, S.; Runggaldier, W.J. â­ KEPT\n",
            "      - wos        | Year: 2001 | DOI: 10.1017/s0001867800011204      | Authors: Chiarella, C; Pasquali, S; Runggaldier, WJ âŒ REMOVED\n",
            "\n",
            "   2. Title (appears 2 times):\n",
            "      \"Nonlinear predictability of stock returns using financial and economic variables\"\n",
            "      - scopus     | Year: 1999 | DOI: 10.1080/07350015.1999.10524830 | Authors: Qi, M. â­ KEPT\n",
            "      - wos        | Year: 1999 | DOI: 10.2307/1392399                | Authors: Qi, M âŒ REMOVED\n",
            "\n",
            "ðŸ’¾ Full duplicate report saved to: duplicate_titles_report.csv\n",
            "   Total rows in report: 4\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Investigate duplicate titles that were removed\n",
        "print(\"=\" * 80)\n",
        "print(\"ðŸ” DUPLICATE TITLE INVESTIGATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Need to recreate the state before title deduplication\n",
        "# Re-load and process to get to the point before title dedup\n",
        "frames_inv = []\n",
        "for fname, src in SOURCES.items():\n",
        "    p = RAW_DIR_IN / fname\n",
        "    if not p.exists():\n",
        "        continue\n",
        "    sep = ';' if fname == 'wos.csv' else ','\n",
        "    temp = pd.read_csv(p, sep=sep)\n",
        "    lower_cols = {c: c.strip().lower() for c in temp.columns}\n",
        "    temp = temp.rename(columns=lower_cols)\n",
        "    out = pd.DataFrame()\n",
        "    out[\"title\"] = temp.apply(lambda r: pick(r, [c.lower() for c in alt_names[\"title\"]]), axis=1)\n",
        "    out[\"abstract\"] = temp.apply(lambda r: pick(r, [c.lower() for c in alt_names[\"abstract\"]]), axis=1)\n",
        "    out[\"authors\"] = temp.apply(lambda r: pick(r, [c.lower() for c in alt_names[\"authors\"]]), axis=1)\n",
        "    out[\"year\"] = temp.apply(lambda r: pick(r, [c.lower() for c in alt_names[\"year\"]]), axis=1)\n",
        "    out[\"doi\"] = temp.apply(lambda r: pick(r, [c.lower() for c in alt_names[\"doi\"]]), axis=1)\n",
        "    out[\"link\"] = temp.apply(lambda r: pick(r, [c.lower() for c in alt_names[\"link\"]]), axis=1)\n",
        "    out[\"database\"] = src\n",
        "    frames_inv.append(out)\n",
        "\n",
        "merged_inv = pd.concat(frames_inv, ignore_index=True)\n",
        "merged_inv[\"title\"] = merged_inv[\"title\"].astype(str).str.strip()\n",
        "merged_inv[\"doi\"] = merged_inv[\"doi\"].astype(str).str.strip().str.lower()\n",
        "merged_inv[\"year\"] = pd.to_numeric(merged_inv[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
        "merged_inv = merged_inv[merged_inv[\"title\"].str.len() > 0].copy()\n",
        "\n",
        "# Do DOI dedup\n",
        "nan_doi_rows_inv = merged_inv[merged_inv['doi'].isna() | (merged_inv['doi'] == 'nan') | (merged_inv['doi'].str.len() == 0)]\n",
        "non_nan_doi_rows_inv = merged_inv[merged_inv['doi'].notna() & (merged_inv['doi'] != 'nan') & (merged_inv['doi'].str.len() > 0)].copy()\n",
        "non_nan_doi_rows_inv[\"doi\"] = non_nan_doi_rows_inv[\"doi\"].str.lower()\n",
        "non_nan_doi_rows_inv[\"doi\"] = non_nan_doi_rows_inv[\"doi\"].str.replace(\"https://doi.org/\", \"\", regex=False)\n",
        "non_nan_doi_rows_inv = non_nan_doi_rows_inv.drop_duplicates(subset=[\"doi\"], keep='first')\n",
        "before_title_dedup_inv = pd.concat([non_nan_doi_rows_inv, nan_doi_rows_inv], ignore_index=True)\n",
        "\n",
        "# Find duplicated titles\n",
        "title_counts = before_title_dedup_inv['title'].value_counts()\n",
        "duplicate_titles = title_counts[title_counts > 1]\n",
        "\n",
        "print(f\"\\nðŸ“Š Summary:\")\n",
        "print(f\"   - Total unique titles with duplicates: {len(duplicate_titles)}\")\n",
        "print(f\"   - Total papers with duplicate titles: {duplicate_titles.sum()}\")\n",
        "print(f\"   - Papers that will be removed: {duplicate_titles.sum() - len(duplicate_titles)}\")\n",
        "\n",
        "if len(duplicate_titles) > 0:\n",
        "    print(f\"\\nðŸ” Top 10 most duplicated titles:\")\n",
        "    for i, (title, count) in enumerate(duplicate_titles.head(10).items(), 1):\n",
        "        print(f\"\\n   {i}. Title (appears {count} times):\")\n",
        "        print(f\"      \\\"{title[:100]}{'...' if len(title) > 100 else ''}\\\"\")\n",
        "        \n",
        "        # Show all instances of this title\n",
        "        dupes = before_title_dedup_inv[before_title_dedup_inv['title'] == title][['database', 'year', 'doi', 'authors']]\n",
        "        for idx, row in dupes.iterrows():\n",
        "            kept = \" â­ KEPT\" if idx == dupes.index[0] else \" âŒ REMOVED\"\n",
        "            author_preview = str(row['authors'])[:50] + '...' if len(str(row['authors'])) > 50 else str(row['authors'])\n",
        "            print(f\"      - {row['database']:10} | Year: {row['year']} | DOI: {row['doi'][:30] if pd.notna(row['doi']) else 'N/A':30} | Authors: {author_preview}{kept}\")\n",
        "\n",
        "# Save detailed duplicate report\n",
        "duplicate_report = []\n",
        "for title in duplicate_titles.index:\n",
        "    dupes = before_title_dedup_inv[before_title_dedup_inv['title'] == title].copy()\n",
        "    dupes['is_kept'] = False\n",
        "    dupes.iloc[0, dupes.columns.get_loc('is_kept')] = True\n",
        "    duplicate_report.append(dupes)\n",
        "\n",
        "if duplicate_report:\n",
        "    duplicate_df = pd.concat(duplicate_report, ignore_index=True)\n",
        "    duplicate_df = duplicate_df.sort_values(['title', 'is_kept'], ascending=[True, False])\n",
        "    report_path = OUT_DIR / \"duplicate_titles_report.csv\"\n",
        "    duplicate_df.to_csv(report_path, index=False)\n",
        "    print(f\"\\nðŸ’¾ Full duplicate report saved to: {report_path}\")\n",
        "    print(f\"   Total rows in report: {len(duplicate_df)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ’¾ Saved merged dataset to: merged_articles.csv\n",
            "   1203 articles saved\n"
          ]
        }
      ],
      "source": [
        "# Save merged dataset to CSV\n",
        "merged_csv = OUT_DIR / \"merged_articles.csv\"\n",
        "merged.to_csv(merged_csv, index=False)\n",
        "print(f\"ðŸ’¾ Saved merged dataset to: {merged_csv}\")\n",
        "print(f\"   {len(merged)} articles saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'enhanced_csv': 'articles_with_gpt_response.csv', 'model': 'gemini-2.5-flash-lite'}\n",
            "Unified rows: 1203\n",
            "Batch to_process: 316\n",
            "Batch returned rows: 316\n",
            "Sample results (first 3): [{'paper_id': '16b768df753f1bf5', 'llm_summary': 'This paper investigates the presence of low-dimensional chaos in the dry bulk shipping market using the Baltic Dry Index. While evidence suggests a nonlinear structure, chaotic and deterministic behavior could not be confirmed. The study found that GARCH and EGARCH models explain a significant portion of the nonlinearity.', 'i1_hint': True, 'i2_hint': False, 'i3_hint': True, 'model': 'gemini-2.5-flash-lite'}, {'paper_id': '32873e86106b0a11', 'llm_summary': 'This article presents a method for exact likelihood inference in a class of non-Gaussian state space models, applicable to models with stochastic intensity, volatility, or duration. It also details procedures for filtering and smoothing to estimate latent variables, suitable for both Bayesian and frequentist estimation.', 'i1_hint': False, 'i2_hint': False, 'i3_hint': False, 'model': 'gemini-2.5-flash-lite'}, {'paper_id': 'aaa155be6197dbf8', 'llm_summary': \"This paper proposes a Cox process model with a piecewise-constant and decreasing intensity process to capture gradually disappearing events, such as reduced accidents due to advanced driver assistance systems or increased longevity due to medical advances. The model's distributional properties, moments, and the probability of no further events are derived. Potential applications in insurance and other fields are discussed.\", 'i1_hint': False, 'i2_hint': False, 'i3_hint': True, 'model': 'gemini-2.5-flash-lite'}]\n",
            "Saved enhanced dataset: articles_with_gpt_response.csv rows: 1203 | newly processed: 316\n",
            "Debug logs (per-row snapshots) at: debug_out/batch_debug.jsonl\n"
          ]
        }
      ],
      "source": [
        "# tuneables\n",
        "_MAX_RETRIES_429 = 2\n",
        "MAX_OUTPUT_TOKENS = 1024\n",
        "CONCURRENCY = 400\n",
        "\n",
        "# --- screening prompt & schema ---\n",
        "CRITERIA_TEXT = {\n",
        "    \"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\",\n",
        "    \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\",\n",
        "    \"I3\": \"The article is about forecasting or prediction\",\n",
        "    #\"I3\": \"Fundamental model with prediction horizon > 1 month\",\n",
        "}\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You assist literature screening. Given title and abstract, return a concise summary and \"\n",
        "    \"boolean hints for criteria I1..I3. Respond with STRICT JSON only, no prose. \"\n",
        "    \"Output must be JSON with keys: summary (str), criteria_hints (object with I1..I3 booleans), notes (str). Do not return and empty booleans. Always give a TRUE or FALSE answer.\"\n",
        ")\n",
        "\n",
        "# --- pydantic schemas (works with parsed=...) ---\n",
        "class CriteriaHints(BaseModel):\n",
        "    I1: bool | None = None\n",
        "    I2: bool | None = None\n",
        "    I3: bool | None = None\n",
        "    #I3: bool | None = None\n",
        "\n",
        "class ScreenResult(BaseModel):\n",
        "    summary: str\n",
        "    criteria_hints: CriteriaHints\n",
        "    notes: str | None = \"\"\n",
        "\n",
        "# --- utils ---\n",
        "def _short(s: Any, n=900) -> str:\n",
        "    s = \"\" if s is None else str(s)\n",
        "    return s if len(s) <= n else s[:n] + \" â€¦ [truncated]\"\n",
        "\n",
        "def _flatten(x: Any) -> str:\n",
        "    \"\"\"Ensure we pass clean strings to the API (no tuples/lists/Series).\"\"\"\n",
        "    if isinstance(x, (list, tuple)):\n",
        "        return \" \".join(_flatten(t) for t in x)\n",
        "    return \"\" if x is None else str(x)\n",
        "\n",
        "def _to_dict(parsed):\n",
        "    if parsed is None: return None\n",
        "    if hasattr(parsed, \"model_dump\"): return parsed.model_dump()      # pydantic v2\n",
        "    if hasattr(parsed, \"dict\"): return parsed.dict()                  # pydantic v1\n",
        "    return parsed\n",
        "\n",
        "def _extract_first_json(s: str) -> Dict[str, Any] | None:\n",
        "    depth = 0; start = -1\n",
        "    for i, ch in enumerate(s or \"\"):\n",
        "        if ch == \"{\":\n",
        "            if depth == 0: start = i\n",
        "            depth += 1\n",
        "        elif ch == \"}\":\n",
        "            if depth: depth -= 1\n",
        "            if depth == 0 and start >= 0:\n",
        "                try: return json.loads(s[start:i+1])\n",
        "                except Exception: pass\n",
        "    return None\n",
        "\n",
        "def _resp_tokens(resp) -> Dict[str, Any]:\n",
        "    um = getattr(resp, \"usage_metadata\", None)\n",
        "    return {\n",
        "        \"prompt\": getattr(um, \"prompt_token_count\", None) if um else None,\n",
        "        \"candidates\": getattr(um, \"candidates_token_count\", None) if um else None,\n",
        "        \"total\": getattr(um, \"total_token_count\", None) if um else None,\n",
        "    }\n",
        "\n",
        "def _write_debug(record: Dict[str, Any]) -> None:\n",
        "    with open(DEBUG_JSONL, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "def init_gemini_client():\n",
        "    if not API_KEY:\n",
        "        raise RuntimeError(\"No API key. Set GEMINI_API_KEY or GOOGLE_API_KEY.\")\n",
        "    return genai.Client(api_key=API_KEY)\n",
        "\n",
        "_client_sync: Optional[genai.Client] = None\n",
        "\n",
        "def _mk_prompt(title: str, abstract: str) -> str:\n",
        "    # Light clipping to avoid pathological inputs\n",
        "    if len(abstract) > 20000:\n",
        "        abstract = abstract[:20000]\n",
        "    return f\"Title: {title}\\nAbstract: {abstract}\\n\\nCriteria: {json.dumps(CRITERIA_TEXT, ensure_ascii=False)}\"\n",
        "\n",
        "# --- single sync call with 3-stage fallback (structured -> unstructured -> plain) ---\n",
        "def _call_sync_all(prompt: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Returns dict with keys:\n",
        "      ok (bool), data (dict|None), logs (dict)\n",
        "    \"\"\"\n",
        "    global _client_sync\n",
        "    if _client_sync is None:\n",
        "        _client_sync = init_gemini_client()\n",
        "\n",
        "    logs: Dict[str, Any] = {\"prompt_preview\": _short(prompt, 1200)}\n",
        "\n",
        "    # Attempt 1: structured JSON (schema + application/json)\n",
        "    resp1 = None\n",
        "    for attempt in range(_MAX_RETRIES_429 + 1):\n",
        "        try:\n",
        "            resp1 = _client_sync.models.generate_content(\n",
        "                model=GOOGLE_LLM_MODEL,\n",
        "                contents=prompt,\n",
        "                config=types.GenerateContentConfig(\n",
        "                    temperature=0,\n",
        "                    max_output_tokens=MAX_OUTPUT_TOKENS,\n",
        "                    response_mime_type=\"application/json\",\n",
        "                    response_schema=ScreenResult,\n",
        "                    system_instruction=SYSTEM_PROMPT,\n",
        "                ),\n",
        "            )\n",
        "            break\n",
        "        except Exception as e:\n",
        "            msg = str(e)\n",
        "            logs[f\"attempt1_{attempt}_error\"] = f\"{type(e).__name__}: {msg}\"\n",
        "            if (\"429\" in msg or \"RESOURCE_EXHAUSTED\" in msg) and attempt < _MAX_RETRIES_429:\n",
        "                time.sleep(5)\n",
        "                continue\n",
        "            logs[\"attempt1_final_error\"] = f\"{type(e).__name__}: {msg}\"\n",
        "            resp1 = None\n",
        "            break\n",
        "\n",
        "    if resp1 is not None:\n",
        "        logs[\"attempt1_tokens\"] = _resp_tokens(resp1)\n",
        "        parsed1 = _to_dict(getattr(resp1, \"parsed\", None))\n",
        "        logs[\"attempt1_parsed\"] = parsed1\n",
        "        if isinstance(parsed1, dict) and parsed1.get(\"summary\"):\n",
        "            return {\"ok\": True, \"data\": parsed1, \"logs\": logs}\n",
        "\n",
        "        # try recover from raw text/parts\n",
        "        raw1 = getattr(resp1, \"text\", \"\") or \"\"\n",
        "        if not raw1 and getattr(resp1, \"candidates\", None):\n",
        "            cand = resp1.candidates[0]\n",
        "            content = getattr(cand, \"content\", None)\n",
        "            parts = (getattr(content, \"parts\", None) or []) if content else []\n",
        "            raw1 = \"\".join(p.get(\"text\", \"\") if isinstance(p, dict) else getattr(p, \"text\", \"\") for p in parts)\n",
        "        logs[\"attempt1_raw\"] = _short(raw1, 1500)\n",
        "        j1 = _extract_first_json(raw1) if raw1 else None\n",
        "        logs[\"attempt1_raw_parsed\"] = j1\n",
        "        if isinstance(j1, dict) and j1.get(\"summary\"):\n",
        "            return {\"ok\": True, \"data\": j1, \"logs\": logs}\n",
        "\n",
        "    return {\"ok\": False, \"data\": None, \"logs\": logs}\n",
        "\n",
        "# --- async batch via threadpool (no aiohttp session issues) ---\n",
        "async def _async_screen_many(rows: List[Dict[str, Any]], model: str, concurrency: int = CONCURRENCY):\n",
        "    sem = asyncio.Semaphore(concurrency)\n",
        "\n",
        "    async def one(r):\n",
        "        paper_id = r.get(\"paper_id\")\n",
        "        title = _flatten(r.get(\"title\", \"\")).strip()\n",
        "        abstract = _flatten(r.get(\"abstract\", \"\")).strip()\n",
        "        prompt = _mk_prompt(title, abstract)\n",
        "\n",
        "        async with sem:\n",
        "            result = await asyncio.to_thread(_call_sync_all, prompt)\n",
        "            logs = result[\"logs\"]\n",
        "            logs[\"mode\"] = \"threadpool\"\n",
        "            logs[\"paper_id\"] = paper_id\n",
        "            logs[\"model\"] = model\n",
        "            _write_debug(logs)\n",
        "\n",
        "            if not result[\"ok\"] or not isinstance(result[\"data\"], dict):\n",
        "                return {\n",
        "                    \"paper_id\": paper_id,\n",
        "                    \"llm_summary\": \"LLM parse/error\",\n",
        "                    \"i1_hint\": None, \"i2_hint\": None, \"i3_hint\": None,\n",
        "                    \"model\": GOOGLE_LLM_MODEL if API_KEY else \"none\",\n",
        "                }\n",
        "\n",
        "            data = result[\"data\"]\n",
        "            hints = (data.get(\"criteria_hints\", {}) or {})\n",
        "            return {\n",
        "                \"paper_id\": paper_id,\n",
        "                \"llm_summary\": data.get(\"summary\", \"\"),\n",
        "                \"i1_hint\": hints.get(\"I1\"),\n",
        "                \"i2_hint\": hints.get(\"I2\"),\n",
        "                \"i3_hint\": hints.get(\"I3\"),\n",
        "                \"model\": GOOGLE_LLM_MODEL if API_KEY else \"none\",\n",
        "            }\n",
        "\n",
        "    return await asyncio.gather(*[one(r) for r in rows])\n",
        "\n",
        "# --- runner helpers ---\n",
        "def run_async(coro):\n",
        "    try:\n",
        "        return asyncio.run(coro)\n",
        "    except RuntimeError:\n",
        "        try:\n",
        "            import nest_asyncio\n",
        "            nest_asyncio.apply()\n",
        "        except Exception:\n",
        "            pass\n",
        "        loop = asyncio.get_event_loop()\n",
        "        return loop.run_until_complete(coro)\n",
        "\n",
        "\n",
        "if \"merged\" not in globals():\n",
        "    raise RuntimeError(\"Expected a DataFrame named `merged` with columns: paper_id, title, abstract\")\n",
        "\n",
        "if not API_KEY:\n",
        "    raise RuntimeError(\"No API key. Set GEMINI_API_KEY or GOOGLE_API_KEY.\")\n",
        "\n",
        "# Sanitize input columns just in case\n",
        "merged = merged.copy()\n",
        "merged[\"title\"] = merged[\"title\"].map(_flatten)\n",
        "merged[\"abstract\"] = merged[\"abstract\"].map(_flatten)\n",
        "\n",
        "print({\"enhanced_csv\": str(ENHANCED_CSV), \"model\": GOOGLE_LLM_MODEL})\n",
        "print(\"Unified rows:\", len(merged))\n",
        "\n",
        "# Load previous hints (optional cache)\n",
        "prev_df = None\n",
        "if ENHANCED_CSV.exists():\n",
        "    try:\n",
        "        prev_df = pd.read_csv(ENHANCED_CSV)\n",
        "    except Exception:\n",
        "        prev_df = None\n",
        "\n",
        "hint_cols = [\"llm_summary\", \"i1_hint\", \"i2_hint\", \"i3_hint\", \"model\"]\n",
        "if prev_df is not None and \"paper_id\" in prev_df.columns:\n",
        "    prev_hints = prev_df[[\"paper_id\"] + [c for c in hint_cols if c in prev_df.columns]].copy()\n",
        "else:\n",
        "    prev_hints = pd.DataFrame(columns=[\"paper_id\"] + hint_cols)\n",
        "\n",
        "merged_with_hints = merged.merge(prev_hints, on=\"paper_id\", how=\"left\")\n",
        "\n",
        "needs_hints_mask = merged_with_hints[\"llm_summary\"].isna() | (merged_with_hints[\"llm_summary\"].astype(str).str.len() == 0)\n",
        "to_process = [\n",
        "    {\"paper_id\": r[\"paper_id\"], \"title\": str(r.get(\"title\",\"\")), \"abstract\": str(r.get(\"abstract\",\"\"))}\n",
        "    for _, r in merged_with_hints[needs_hints_mask].iterrows()\n",
        "]\n",
        "\n",
        "print(\"Batch to_process:\", len(to_process))\n",
        "if not to_process:\n",
        "    print(\"No rows need processing. Exiting without writing.\")\n",
        "    raise SystemExit(0)\n",
        "\n",
        "rows = run_async(_async_screen_many(to_process, model=GOOGLE_LLM_MODEL, concurrency=CONCURRENCY))\n",
        "print(\"Batch returned rows:\", len(rows))\n",
        "print(\"Sample results (first 3):\", rows[:3])\n",
        "\n",
        "# Validate results strictly; DO NOT write if any are empty/invalid\n",
        "REQUIRED_HINT_KEYS = (\"I1\", \"I2\", \"I3\")\n",
        "\n",
        "def _is_invalid(rec: Dict[str, Any]) -> tuple[bool, str]:\n",
        "    # summary must be non-empty\n",
        "    if rec.get(\"llm_summary\") in (None, \"\", \"LLM parse/error\"):\n",
        "        return True, \"summary_missing_or_error\"\n",
        "    # each hint must exist and be a boolean (True/False), not None\n",
        "    for k in REQUIRED_HINT_KEYS:\n",
        "        v = rec.get(f\"{k.lower()}_hint\")  # fields are i1_hint, ..., I3_hint\n",
        "        if not isinstance(v, bool):\n",
        "            return True, f\"{k}_not_bool_or_missing\"\n",
        "    return False, \"\"\n",
        "\n",
        "invalid_detail = [(_r, _is_invalid(_r)) for _r in rows]\n",
        "invalid = [r for r, (bad, _) in invalid_detail if bad]\n",
        "\n",
        "# If everything looks good, merge and write\n",
        "new_hints_df = pd.DataFrame(rows)\n",
        "final_df = merged_with_hints.drop(columns=[c for c in hint_cols if c in merged_with_hints.columns], errors=\"ignore\")\n",
        "final_df = final_df.merge(\n",
        "    pd.concat([prev_hints, new_hints_df], ignore_index=True).drop_duplicates(subset=[\"paper_id\"], keep=\"last\"),\n",
        "    on=\"paper_id\",\n",
        "    how=\"left\",\n",
        ")\n",
        "\n",
        "final_df.to_csv(ENHANCED_CSV, index=False)\n",
        "print(\"Saved enhanced dataset:\", ENHANCED_CSV, \"rows:\", len(final_df), \"| newly processed:\", len(new_hints_df))\n",
        "print(f\"Debug logs (per-row snapshots) at: {DEBUG_JSONL}\")\n",
        "\n",
        "# Clean shutdown of sync client (tidy logs; not strictly required)\n",
        "try:\n",
        "    if _client_sync is not None:\n",
        "        _client_sync.close()\n",
        "except Exception:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ðŸ“Š LLM ENHANCEMENT QUALITY ANALYSIS\n",
            "============================================================\n",
            "\n",
            "ðŸ“ˆ Dataset Overview:\n",
            "   Total articles: 1203\n",
            "\n",
            "ðŸ“ Summary Quality:\n",
            "   âœ… Valid summaries: 1203\n",
            "   âŒ Empty summaries: 0\n",
            "   âš ï¸  Error summaries: 0\n",
            "\n",
            "ðŸ” Inclusion Criteria Coverage:\n",
            "\n",
            "   I1 (The article includes an empirical Machine Learning...):\n",
            "      âœ“ True: 838 (69.7%)\n",
            "      âœ— False: 365 (30.3%)\n",
            "      ? None/Missing: 0 (0.0%)\n",
            "\n",
            "   I2 (The article is about government/sovereign bonds an...):\n",
            "      âœ“ True: 360 (29.9%)\n",
            "      âœ— False: 843 (70.1%)\n",
            "      ? None/Missing: 0 (0.0%)\n",
            "\n",
            "   I3 (The article is about forecasting or prediction...):\n",
            "      âœ“ True: 875 (72.7%)\n",
            "      âœ— False: 328 (27.3%)\n",
            "      ? None/Missing: 0 (0.0%)\n",
            "\n",
            "ðŸŽ¯ Overall Quality Score:\n",
            "   Complete records (all fields valid): 1203/1203 (100.0%)\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Analysis: LLM Enhancement Quality Check\n",
        "\n",
        "# Define criteria text if not already available\n",
        "if 'CRITERIA_TEXT' not in globals():\n",
        "    CRITERIA_TEXT = {\n",
        "        \"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\",\n",
        "        \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\",\n",
        "        \"I3\": \"The article is about forecasting or prediction\",\n",
        "    }\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ðŸ“Š LLM ENHANCEMENT QUALITY ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check if enhanced CSV exists\n",
        "if 'ENHANCED_CSV' not in globals():\n",
        "    ENHANCED_CSV = Path(\".\") / \"articles_with_gpt_response.csv\"\n",
        "\n",
        "if not ENHANCED_CSV.exists():\n",
        "    print(f\"âŒ Enhanced dataset not found at {ENHANCED_CSV}\")\n",
        "    print(\"   Please run the LLM enhancement cell first!\")\n",
        "else:\n",
        "    # Load the enhanced dataset\n",
        "    enhanced_df = pd.read_csv(ENHANCED_CSV)\n",
        "    \n",
        "    print(f\"\\nðŸ“ˆ Dataset Overview:\")\n",
        "    print(f\"   Total articles: {len(enhanced_df)}\")\n",
        "    \n",
        "    # Check summaries\n",
        "    print(f\"\\nðŸ“ Summary Quality:\")\n",
        "    empty_summaries = enhanced_df[enhanced_df['llm_summary'].isna() | (enhanced_df['llm_summary'].astype(str).str.len() == 0)]\n",
        "    error_summaries = enhanced_df[enhanced_df['llm_summary'].astype(str).str.contains('LLM parse/error', na=False)]\n",
        "    print(f\"   âœ… Valid summaries: {len(enhanced_df) - len(empty_summaries) - len(error_summaries)}\")\n",
        "    print(f\"   âŒ Empty summaries: {len(empty_summaries)}\")\n",
        "    print(f\"   âš ï¸  Error summaries: {len(error_summaries)}\")\n",
        "    \n",
        "    # Check inclusion criteria\n",
        "    print(f\"\\nðŸ” Inclusion Criteria Coverage:\")\n",
        "    for i in range(1, 4):\n",
        "        col = f'i{i}_hint'\n",
        "        if col in enhanced_df.columns:\n",
        "            total = len(enhanced_df)\n",
        "            none_count = enhanced_df[col].isna().sum()\n",
        "            true_count = (enhanced_df[col] == True).sum()\n",
        "            false_count = (enhanced_df[col] == False).sum()\n",
        "            \n",
        "            print(f\"\\n   I{i} ({CRITERIA_TEXT[f'I{i}'][:50]}...):\")\n",
        "            print(f\"      âœ“ True: {true_count} ({true_count/total*100:.1f}%)\")\n",
        "            print(f\"      âœ— False: {false_count} ({false_count/total*100:.1f}%)\")\n",
        "            print(f\"      ? None/Missing: {none_count} ({none_count/total*100:.1f}%)\")\n",
        "    \n",
        "    # Overall quality score\n",
        "    print(f\"\\nðŸŽ¯ Overall Quality Score:\")\n",
        "    complete_rows = enhanced_df[\n",
        "        (enhanced_df['llm_summary'].notna()) & \n",
        "        (enhanced_df['llm_summary'].astype(str).str.len() > 0) &\n",
        "        (~enhanced_df['llm_summary'].astype(str).str.contains('LLM parse/error', na=False)) &\n",
        "        (enhanced_df['i1_hint'].notna()) &\n",
        "        (enhanced_df['i2_hint'].notna()) &\n",
        "        (enhanced_df['i3_hint'].notna())\n",
        "    ]\n",
        "    quality_score = len(complete_rows) / len(enhanced_df) * 100\n",
        "    print(f\"   Complete records (all fields valid): {len(complete_rows)}/{len(enhanced_df)} ({quality_score:.1f}%)\")\n",
        "    \n",
        "    if quality_score < 100:\n",
        "        print(f\"\\nâš ï¸  Warning: {len(enhanced_df) - len(complete_rows)} articles have incomplete data\")\n",
        "        incomplete = enhanced_df[~enhanced_df.index.isin(complete_rows.index)]\n",
        "        print(f\"   Sample incomplete records:\")\n",
        "        for idx, row in incomplete.head(3).iterrows():\n",
        "            issues = []\n",
        "            if pd.isna(row['llm_summary']) or len(str(row['llm_summary'])) == 0:\n",
        "                issues.append(\"no summary\")\n",
        "            if 'LLM parse/error' in str(row['llm_summary']):\n",
        "                issues.append(\"error summary\")\n",
        "            if pd.isna(row['i1_hint']):\n",
        "                issues.append(\"I1 missing\")\n",
        "            if pd.isna(row['i2_hint']):\n",
        "                issues.append(\"I2 missing\")\n",
        "            if pd.isna(row['i3_hint']):\n",
        "                issues.append(\"I3 missing\")\n",
        "            print(f\"      - {row['paper_id']}: {', '.join(issues)}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assignments saved: outputs/assignments.csv rows: 1203\n"
          ]
        }
      ],
      "source": [
        "# Reviewer assignment (reproducible, saved for screening)\n",
        "import numpy as np\n",
        "\n",
        "REVIEWERS = [\"Olav\", \"Ulrik\", \"Trine\"]\n",
        "SEED = int(os.environ.get(\"SCREENING_SEED\", 42))\n",
        "ASSIGNMENTS_CSV = OUT_DIR / \"outputs/assignments.csv\"\n",
        "ASSIGNMENTS_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "assignments = final_df[[\"paper_id\", \"title\", \"year\", \"doi\"]].copy()\n",
        "reviewers_cycle = (REVIEWERS * ((len(assignments) + len(REVIEWERS) - 1) // len(REVIEWERS)))[:len(assignments)]\n",
        "np.random.seed(SEED)\n",
        "np.random.shuffle(reviewers_cycle)\n",
        "assignments[\"reviewer\"] = reviewers_cycle\n",
        "assignments.to_csv(ASSIGNMENTS_CSV, index=False)\n",
        "print(\"Assignments saved:\", ASSIGNMENTS_CSV, \"rows:\", len(assignments))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We duplicate the generated assignments.csv document and randomize the assignment to another reviewer to facilitate for dual initial screening of each paper:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "ASSIGNMENTS_IN = r\"..\\Screening_1\\outputs\\assignments.csv\"\n",
        "ASSIGNMENTS_OUT = r\"..\\Screening_1\\outputs\\initial_screening_2\\assignments2.csv\"\n",
        "assignments = pd.read_csv(ASSIGNMENTS_IN)\n",
        "\n",
        "REVIEWERS = [\"Olav\", \"Ulrik\", \"Trine\"]\n",
        "SEED = int(os.environ.get(\"SCREENING_SEED\", 42))\n",
        "np.random.seed(SEED)\n",
        "\n",
        "n = len(assignments)\n",
        "base = n // len(REVIEWERS)\n",
        "remainder = n % len(REVIEWERS)\n",
        "counts = [base + (1 if i < remainder else 0) for i in range(len(REVIEWERS))]\n",
        "\n",
        "# build balanced pool and try to shuffle into a derangement (no fixed points)\n",
        "candidates = []\n",
        "for reviewer, cnt in zip(REVIEWERS, counts):\n",
        "    candidates += [reviewer] * cnt\n",
        "\n",
        "max_tries = 2000\n",
        "for _ in range(max_tries):\n",
        "    np.random.shuffle(candidates)\n",
        "    if all(c != r for c, r in zip(candidates, assignments[\"reviewer\"].astype(str))):\n",
        "        break\n",
        "else:\n",
        "    # greedy fallback swaps to resolve remaining conflicts\n",
        "    for i, (orig, cand) in enumerate(zip(assignments[\"reviewer\"].astype(str), candidates)):\n",
        "        if orig == cand:\n",
        "            for j in range(n):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                if candidates[j] != orig and candidates[i] != assignments.loc[j, \"reviewer\"]:\n",
        "                    candidates[i], candidates[j] = candidates[j], candidates[i]\n",
        "                    break\n",
        "\n",
        "# final safety: force any remaining conflicts to a different reviewer\n",
        "for i, orig in enumerate(assignments[\"reviewer\"].astype(str)):\n",
        "    if candidates[i] == orig:\n",
        "        for r in REVIEWERS:\n",
        "            if r != orig:\n",
        "                candidates[i] = r\n",
        "                break\n",
        "\n",
        "# overwrite the assignments column\n",
        "assignments[\"reviewer\"] = candidates\n",
        "assignments[[\"paper_id\", \"title\", \"year\", \"doi\", \"reviewer\"]].to_csv(ASSIGNMENTS_OUT, index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
