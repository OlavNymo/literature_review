{"prompt_preview": "Title: A Markov regime-switching model for the semiconductor industry cycles\nAbstract: Because of the huge fluctuation in the semiconductor business, it has been a challenging work for the industry researchers to predict the turning points of the semiconductor industry cycles. To catch the cyclical behavior of the semiconductor business, we propose a Markov Regime-Switching model with two regimes representing expansion and contraction. The simple nonlinear, two states, regime-switching model shows a successful in-sample prediction on the contraction of semiconductor industry sales during the period of 1990:01-2003:08. (c) 2006 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 281, "candidates": 125, "total": 406}, "attempt1_parsed": {"summary": "This paper proposes a Markov regime-switching model with two states (expansion and contraction) to predict turning points in the semiconductor industry cycles. The model successfully predicted the contraction in sales from 1990 to 2003.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The model is a regime-switching model, which is a type of non-linear model, and it is applied to predict industry cycles."}, "mode": "threadpool", "paper_id": "99d0e8a05fcc941c", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: A New Linear Estimator for Gaussian Dynamic Term Structure Models\nAbstract: This article proposes a novel regression-based approach to the estimation of Gaussian dynamic term structure models. This new estimator is an asymptotic least-square estimator defined by the no-arbitrage conditions upon which these models are built. Further, we note that our estimator remains easy-to-compute and asymptotically efficient in a variety of situations in which other recently proposed approaches might lose their tractability. We provide an empirical application in the context of the Canadian bond market.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 251, "candidates": 110, "total": 361}, "attempt1_parsed": {"summary": "This paper introduces a new regression-based estimator for Gaussian dynamic term structure models, which is asymptotically efficient and easy to compute. It is applied to the Canadian bond market.", "criteria_hints": {"I1": false, "I2": true, "I3": false}, "notes": "The article uses a novel estimator but does not explicitly mention machine learning or models beyond linear ones. It focuses on estimation rather than forecasting."}, "mode": "threadpool", "paper_id": "d476fa9fc94bed8b", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: A framework to measure integrated risk\nAbstract: A framework underlying various models that measure the credit risk of a portfolio is extended in this paper to allow the integration of credit risk with a range of market risks using Monte Carlo simulation. A structural model is proposed that allows interest rates to be stochastic and provides closed-form expressions for the market value of a firm's equity and its probability of default. This model is embedded within the integrated framework and the general approach illustrated by measuring the risk of a foreign exchange forward when there is a significant probability of default by the counterparty. For this example moving from a market risk calculation to an integrated risk calculation reduces the expected future value of the instrument by an amount that Could not be calculated using the common pre-settlement exposure technique for estimating the credit risk of a derivative.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecas … [truncated]", "attempt1_tokens": {"prompt": 308, "candidates": 158, "total": 466}, "attempt1_parsed": {"summary": "This paper extends a framework for measuring credit risk to integrate it with market risks using Monte Carlo simulation. It proposes a structural model with stochastic interest rates, providing closed-form expressions for equity value and default probability. The approach is illustrated by measuring the risk of a foreign exchange forward with counterparty default risk, showing that integrated risk calculation significantly reduces the expected future value compared to standard pre-settlement exposure techniques.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The abstract does not mention machine learning, government bonds, or forecasting. It focuses on a simulation-based framework for integrated risk measurement."}, "mode": "threadpool", "paper_id": "58faaf67a8d7f201", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: A Chaos Analysis of the Dry Bulk Shipping Market\nAbstract: Finding low-dimensional chaos is a relevant issue as it could allow short-term reliable forecasting. However, the existence of chaos in shipping freight rates remains an open and outstanding matter as previous research used methodology that can produce misleading results. Using daily data, this paper aims to unveil the nonlinear dynamics of the Baltic Dry Index that has been proposed as a measure of the shipping rates for certain raw materials. We tested for the existence of nonlinearity and low-dimensional chaos. We have also examined the chaotic dynamics throughout three sub-sampling periods, which have been determined by the volatility pattern of the series. For this purpose, from a comprehensive view we apply several metric and topological techniques, including the most suitable methods for noisy time series analysis. The proposed methodology considers the characteristics of chaotic time series, such as nonlinearity, determinism, sensitivity to initial conditions, fractal dimension and recurrence. Although there is strong evidence of a nonlinear structure, a chaotic and, therefore, deterministic behavior cannot b … [truncated]", "attempt1_tokens": {"prompt": 407, "candidates": 137, "total": 544}, "attempt1_parsed": {"summary": "This paper investigates the presence of low-dimensional chaos in the dry bulk shipping market using the Baltic Dry Index. While evidence suggests a nonlinear structure, chaotic and deterministic behavior could not be confirmed. The study found that GARCH and EGARCH models explain a significant portion of the nonlinearity.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses statistical models (GARCH, EGARCH) which can be considered beyond simple linear models, and it explicitly tests for forecasting potential through chaos analysis."}, "mode": "threadpool", "paper_id": "16b768df753f1bf5", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: A daily view of the term structure dynamics: Some international evidence\nAbstract: The paper employs daily interest rates from the short-end of the Eurocurrency market in order to test the validity of the Expectations Hypothesis (EH). In particular, exploiting the stochastic trends embedded in the time series the EH implications are tested in a multivariate cointegration framework. The empirical findings indicate that once daily rates are used the estimated coefficients are very close to their theoretical values as predicted by the EH. Furthermore, we cannot reject the hypothesis that the EH is an adequate description of the US yield curve. Similarly, for the German and UK yield curves the number of common stochastic trends present in their yield curves is consistent with the EH. However, the restrictions imposed by the theory on parameters of the cointegration space are rejected.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 306, "candidates": 145, "total": 451}, "attempt1_parsed": {"summary": "This paper tests the Expectations Hypothesis (EH) using daily Eurocurrency interest rates and a multivariate cointegration framework. The findings suggest that the EH is a valid description for the US, German, and UK yield curves when daily data is used, although theoretical restrictions on cointegration parameters are rejected.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The study uses daily interest rates but does not explicitly mention government/sovereign bonds. It focuses on testing the Expectations Hypothesis rather than forecasting. No machine learning applications are described."}, "mode": "threadpool", "paper_id": "d3acea4f9be74018", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: A Cox model for gradually disappearing events\nAbstract: Innovations in medicine provide us longer and healthier life, leading lower mortality. Sooner rather than later, much greater longevity would be possible for us due to artificial intelligence advances in health care. Similarly, Advanced Driver Assistance Systems (ADAS) in highly automated vehicles may reduce or even eventually eliminate accidents by perceiving dangerous situations, which would minimize the number of accidents and lead to fewer loss claims for insurance companies. To model the survivor function capturing greater longevity as well as the number of claims reflecting less accidents in the long run, in this paper, we study a Cox process whose intensity process is piecewise-constant and decreasing. We derive its ultimate distributional properties, such as the Laplace transform of intensity integral process, the probability generating function of point process, their associated moments and cumulants, and the probability of no more claims for a given time point. In general, this simple model may be applicable in many other areas for modeling the evolution of gradually disappearing events, such as corporate defa … [truncated]", "attempt1_tokens": {"prompt": 390, "candidates": 164, "total": 554}, "attempt1_parsed": {"summary": "This paper proposes a Cox process model with a piecewise-constant and decreasing intensity process to capture gradually disappearing events, such as reduced accidents due to advanced driver assistance systems or increased longevity due to medical advances. The model's distributional properties, moments, and the probability of no further events are derived. Potential applications in insurance and other fields are discussed.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The paper discusses prediction of gradually disappearing events, which aligns with I3. However, it does not appear to involve empirical machine learning applications beyond simple models (I1) or focus on government bonds/yield curves (I2)."}, "mode": "threadpool", "paper_id": "aaa155be6197dbf8", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: A Simulation and Empirical Study of the Maximum Likelihood Estimator for Stochastic Volatility Jump-Diffusion Models\nAbstract: We investigate the behaviour of the maximum likelihood estimator (MLE) for stochastic volatility jump-diffusion models commonly used in financial risk management. A simulation study shows the practical conditions under which the MLE behaves according to theory. In an extensive empirical study based on nine indices and more than 6000 individual stocks, we nonetheless find that the MLE is unable to replicate key higher moments. We then introduce a moment-targeted MLE - robust to model misspecification - and revisit both simulation and empirical studies. We find it performs better than the MLE, improving the management of financial risk.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 285, "candidates": 157, "total": 442}, "attempt1_parsed": {"summary": "This study examines the Maximum Likelihood Estimator (MLE) for stochastic volatility jump-diffusion models. A simulation and empirical study using financial data show that the standard MLE has limitations in capturing higher moments. The authors propose a moment-targeted MLE, which demonstrates improved performance in both simulations and empirical analyses, leading to better financial risk management.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses simulation and empirical data, and introduces a new estimator, suggesting it goes beyond simple models. It is about financial risk management and improving predictions for risk, thus fitting I1 and I3."}, "mode": "threadpool", "paper_id": "e5bc9c76cee36aba", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: A Machine Learning Approach to Forecast Economic Recessions-An Italian Case Study\nAbstract: In economic activity, recessions represent a period of failure in Gross Domestic Product (GDP) and usually are presented as episodic and non-linear. For this reason, they are difficult to predict and appear as one of the main problems in macroeconomics forecasts. A classic example turns out to be the great recession that occurred between 2008 and 2009 that was not predicted. In this paper, the goal is to give a different, although complementary, approach concerning the classical econometric techniques, and to show how Machine Learning (ML) techniques may improve short-term forecasting accuracy. As a case study, we use Italian data on GDP and a few related variables. In particular, we evaluate the goodness of fit of the forecasting proposed model in a case study of the Italian GDP. The algorithm is trained on Italian macroeconomic variables over the period 1995:Q1-2019:Q2. We also compare the results using the same dataset through Classic Linear Regression Model. As a result, both statistical and ML approaches are able to predict economic downturns but higher accuracy is obtained using … [truncated]", "attempt1_tokens": {"prompt": 401, "candidates": 170, "total": 571}, "attempt1_parsed": {"summary": "This paper presents a Machine Learning (ML) approach, specifically a Nonlinear Autoregressive with exogenous variables (NARX) model, to forecast economic recessions using Italian GDP data. The ML model is trained on macroeconomic variables from 1995:Q1-2019:Q2 and compared against a Classic Linear Regression Model. The study concludes that while both methods can predict downturns, the NARX model achieves higher accuracy.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses ML beyond simple linear models and focuses on forecasting economic recessions. It does not cover government/sovereign bonds or the yield curve."}, "mode": "threadpool", "paper_id": "fd4f0e95b6a42d4c", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: A MODEL-SELECTION APPROACH TO ASSESSING THE INFORMATION IN THE TERM STRUCTURE USING LINEAR-MODELS AND ARTIFICIAL NEURAL NETWORKS\nAbstract: We take a model-selection approach to the question of whether forward-interest rates are useful in predicting future spot rates, using a variety of out-of-sample forecast-based model-selection criteria-forecast mean squared error, forecast direction accuracy, and forecast-based trading-system profitability. We also examine the usefulness of a class of novel prediction models called artificial neural networks and investigate the issue of appropriate window sizes for rolling-window-based prediction methods. Results indicate that the premium of the forward rate over the spot rate helps to predict the sign of future changes in the interest rate. Furthermore, model selection based on an in-sample Schwarz information criterion (SIC) does not appear to be a reliable guide to out-of-sample performance in the case of short-term interest rates. Thus, the in-sample SIC apparently fails to offer a convenient shortcut to true out-of-sample performance measures.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure … [truncated]", "attempt1_tokens": {"prompt": 362, "candidates": 186, "total": 548}, "attempt1_parsed": {"summary": "This study uses a model-selection approach to evaluate the predictive power of forward interest rates for future spot rates, employing criteria such as forecast mean squared error, direction accuracy, and trading system profitability. It also explores the utility of artificial neural networks and optimal window sizes for prediction models. The findings suggest that the forward rate premium can predict the direction of future interest rate changes, but in-sample model selection criteria like the Schwarz Information Criterion (SIC) do not reliably predict out-of-sample performance for short-term rates.", "criteria_hints": {"I1": true, "I2": true, "I3": true}, "notes": "The study uses artificial neural networks, which go beyond simple linear models, and applies them to interest rate forecasting, which relates to government bonds and the yield curve."}, "mode": "threadpool", "paper_id": "f1779cf7208a0e34", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: A Class of Non-Gaussian State Space Models With Exact Likelihood Inference\nAbstract: The likelihood function of a general nonlinear, non-Gaussian state space model is a high-dimensional integral with no closed-form solution. In this article, I show how to calculate the likelihood function exactly for a large class of non-Gaussian state space models that include stochastic intensity, stochastic volatility, and stochastic duration models among others. The state variables in this class follow a nonnegative stochastic process that is popular in econometrics for modeling volatility and intensities. In addition to calculating the likelihood, I also show how to perform filtering and smoothing to estimate the latent variables in the model. The procedures in this article can be used for either Bayesian or frequentist estimation of the model's unknown parameters as well as the latent state variables. Supplementary materials for this article are available online.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\":  … [truncated]", "attempt1_tokens": {"prompt": 316, "candidates": 145, "total": 461}, "attempt1_parsed": {"summary": "This article presents a method for exact likelihood inference in a class of non-Gaussian state space models, applicable to models with stochastic intensity, volatility, or duration. It also details procedures for filtering and smoothing to estimate latent variables, suitable for both Bayesian and frequentist estimation.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The article focuses on theoretical advancements in state space models and likelihood inference, rather than empirical applications or specific financial instruments like government bonds. While it mentions forecasting implicitly through state estimation, it is not the primary focus."}, "mode": "threadpool", "paper_id": "32873e86106b0a11", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: A comprehensive investigation on the predictive power of economic policy uncertainty from non-US countries for US stock market returns\nAbstract: This study uses economic policy uncertainty (EPU) indices for ten developed countries, three diffusion models, and five combination methods to forecast excess returns in the U.S. stock market. It shows empirically that, over the period January 1997 to January 2022, non-U.S. EPU indices have better predictive power for U.S. equity market excess returns than the U.S. EPU index itself. This illustrates how economic information from interna-tional markets can affect the U.S. stock market. This finding challenges the extensively recognized view that the U.S. is where important market signals are initially transmitted to other markets, suggesting that this belief is incomplete. Our outcomes are robust to a battery of tests covering model selection, model specification, forecast horizons, and the pandemic period, and their economic values are assessed. The findings are essential for the financial field to confront future fierce situations and crises.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pur … [truncated]", "attempt1_tokens": {"prompt": 361, "candidates": 219, "total": 580}, "attempt1_parsed": {"summary": "This study investigates the predictive power of economic policy uncertainty (EPU) from ten developed non-US countries for US stock market returns. Using three diffusion models and five combination methods from January 1997 to January 2022, the research finds that non-US EPU indices are better predictors of US equity market excess returns than the US EPU index. This suggests that international economic information impacts the US stock market, challenging the traditional view of US market primacy. The findings are robust across various tests and have implications for the financial field.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses diffusion models and combination methods, which go beyond simple linear models, thus satisfying I1. The focus is on stock market returns, not government bonds or the yield curve, making I2 false. The core of the study is forecasting US stock market returns, fulfilling I3."}, "mode": "threadpool", "paper_id": "7736932495ba615c", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: A hybrid convolutional neural network with long short-term memory for statistical arbitrage\nAbstract: We propose a CNN-LSTM deep learning model, which has been trained to classify profitable from unprofitable spread sequences of cointegrated stocks, for a large scale market backtest ranging from January 1991 to December 2017. We show that the proposed model can achieve high levels of accuracy and successfully derives features from the market data. We formalize and implement a trading strategy based on the model output which generates significant risk-adjusted excess returns that are orthogonal to market risks. The generated out-of-sample Sharpe ratio and alpha coefficient significantly outperform the reference model, which is based on a standard deviation rule, even after accounting for transaction costs.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 296, "candidates": 177, "total": 473}, "attempt1_parsed": {"summary": "This paper introduces a hybrid CNN-LSTM deep learning model for statistical arbitrage in financial markets. The model classifies profitable spread sequences of cointegrated stocks and is backtested on data from 1991 to 2017. The strategy based on the model achieves significant risk-adjusted excess returns, outperforming a reference model in terms of Sharpe ratio and alpha, even after transaction costs.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study applies a deep learning model (CNN-LSTM) to financial data for prediction and trading, fulfilling I1 and I3. However, it focuses on stock market arbitrage rather than government bonds or yield curves, making I2 false."}, "mode": "threadpool", "paper_id": "06dce1e4e6c88e26", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: A measure of Turkey's sovereign and banking sector credit risk: Asset swap spreads\nAbstract: The existence of the credit derivatives written on the eurobonds such as credit default swaps or asset swaps allows policymakers and investors to monitor the evolvement of credit risk. However, these instruments are mostly available in advanced economies, whereas the market for credit derivatives in emerging market countries, including Turkey, is limited in terms of liquidity and maturity. In this regard, this study aims to construct a proxy for the credit risk of the Turkish Treasury and banking sector in international markets by calculating asset swap spread for US dollar-denominated fixed coupon eurobonds, which requires a robust estimation of the relevant yield curves. The study firstly presents the estimation of the sovereign and banking sector yield curves and then constructs a synthetic asset swap structure to obtain embedded credit risk premia in the eurobond curves. Our findings show that the proposed credit risk indicator is vastly correlated with credit default swap premium. In addition to this, estimated eurobond curves are also useful for monitoring borrowing cost dynami … [truncated]", "attempt1_tokens": {"prompt": 394, "candidates": 146, "total": 540}, "attempt1_parsed": {"summary": "This study constructs a proxy for the credit risk of the Turkish Treasury and banking sector by calculating asset swap spreads for US dollar-denominated eurobonds. It estimates sovereign and banking sector yield curves and uses a synthetic asset swap structure to derive credit risk premia. The findings indicate a strong correlation with credit default swap premiums and suggest the yield curves can monitor borrowing costs.", "criteria_hints": {"I1": false, "I2": true, "I3": false}, "notes": "The study focuses on constructing a credit risk indicator and monitoring borrowing costs, not on forecasting or prediction using machine learning."}, "mode": "threadpool", "paper_id": "7e93c19fba02abbf", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: A quasi-maximum likelihood method for estimating the parameters of multivariate diffusions\nAbstract: A quasi-maximum likelihood procedure for estimating the parameters of multi-dimensional diffusions is developed in which the transitional density is a multivariate Gaussian density with first and second moments approximating the true moments of the unknown density. For affine drift and diffusion functions, the moments are exactly those of the true transitional density and for nonlinear drift and diffusion functions the approximation is extremely good and is as effective as alternative methods based on likelihood approximations. The estimation procedure generalises to models with latent factors. A conditioning procedure is developed that allows parameter estimation in the absence of proxies. (C) 2012 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 289, "candidates": 141, "total": 430}, "attempt1_parsed": {"summary": "This paper proposes a quasi-maximum likelihood method for estimating parameters of multivariate diffusions, approximating the transitional density with a multivariate Gaussian. This method is exact for affine drift/diffusion functions and provides a good approximation for nonlinear cases, generalizing to models with latent factors and incorporating a conditioning procedure for parameter estimation without proxies.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The abstract does not mention any empirical machine learning applications, government bonds, or forecasting/prediction. The focus is on statistical estimation methods for diffusion models."}, "mode": "threadpool", "paper_id": "9cd458c37bf8d94f", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: A new portmanteau test for predictive regression models with possible embedded endogeneity\nAbstract: In the widely used predictive regression model, any possible serial correlation in innovations leads to estimation bias and statistical inference distortions. Hence, it is important to pretest the existence of such serial correlation. Nevertheless, in the presence of embedded endogeneity, which is a common problem in the predictive regression setting, traditional serial correlation tests such as Box-Pierce (BP) and Ljung-Box (LB) tests are found to perform poorly. Motivated by this, we develop a new portmanteau test in this article as a pretest for serial correlation in predictive regression under possible embedded endogeneity. This test is based on the sample splitting idea and the jackknife empirical likelihood method. The asymptotic distribution of the proposed test has been derived, and the Monte Carlo simulations confirm good finite sample performances. As an illustration, we apply our proposed test in pretesting the serial correlation in predictive regression, where financial variables are used to predict the excess return of S&P 500.\n\nCriteria: {\"I1\": \"The article incl … [truncated]", "attempt1_tokens": {"prompt": 361, "candidates": 134, "total": 495}, "attempt1_parsed": {"summary": "This paper proposes a new portmanteau test to detect serial correlation in predictive regression models, particularly when endogeneity is present. The test utilizes sample splitting and jackknife empirical likelihood methods. Simulation results show good performance, and the test is illustrated with an application predicting S&P 500 excess returns using financial variables.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The application is a prediction task, but it does not involve machine learning beyond simple linear models."}, "mode": "threadpool", "paper_id": "6094397f5a71118e", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: A nonlinear Bayesian filtering approach to estimating adaptive market efficiency\nAbstract: The adaptive market hypothesis (AMII) supplies a convincing motivation for why market efficiency should not be regarded as a stable property in time. This paper explores a Bayesian methodology for estimating weak-form market efficiency under the AMII using a test of evolving efficiency (TEE). More precisely, a generalized TEE (GTEE) approach is proposed in which the conditional first moment of a time series is assumed to be a nonlinear function of its conditional second moment, i.e., a nonlinear feedback term is present in the conditional mean equation. We then discuss a maximum likelihood estimation procedure for the resulting nonlinear model using the state-space approach and extended Kalman filtering. This methodology is used to estimate time-varying, weak-form market efficiency in four, specifically chosen, markets over a time period that includes the global financial crisis of 2007/2008.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds a … [truncated]", "attempt1_tokens": {"prompt": 333, "candidates": 183, "total": 516}, "attempt1_parsed": {"summary": "This paper proposes a Bayesian methodology using a generalized test of evolving efficiency (GTEE) to estimate weak-form market efficiency under the adaptive market hypothesis. The GTEE assumes a nonlinear relationship between the conditional first and second moments of a time series. The authors use a state-space approach and extended Kalman filtering for estimation and apply it to four markets, including the period of the 2007/2008 global financial crisis.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The paper uses a nonlinear Bayesian filtering approach and extended Kalman filtering, which can be considered advanced statistical methods beyond simple linear models. The application to market efficiency over time, especially during a crisis, implies a forecasting or prediction aspect."}, "mode": "threadpool", "paper_id": "f1226e3a19defec0", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: A novel, rule-based technical pattern identification mechanism: Identifying and evaluating saucers and resistant levels in the US stock market\nAbstract: This paper has two main purposes. The first one is the development of a rigorous rule-based mechanism for identifying the rounding bottoms (also known as saucers) pattern and resistant levels. The design of this model is based solely on principles of technical analysis, and thus making it a proper system for evaluating the efficacy of the aforementioned technical trading patterns. The second aim of this paper is measuring the predictive power of buy-signals generated by these technical patterns. Empirical results obtained from seven US tech stocks indicate that simple resistant levels outperform saucers patterns. Furthermore, positive statistical significant excess returns are being generated only in first sub-periods of examination. These returns decline or even vanish as the experiment proceeds to recent years. Our findings are aligned with the results reported by various former studies. The proposed identification mechanism can be used as a component of an expert system to assist academic community in evaluating trading s … [truncated]", "attempt1_tokens": {"prompt": 370, "candidates": 182, "total": 552}, "attempt1_parsed": {"summary": "This paper develops a rule-based mechanism to identify and evaluate 'saucers' and 'resistant levels' in the US stock market, based on technical analysis principles. It then measures the predictive power of buy-signals from these patterns. Empirical results on seven US tech stocks show that resistant levels outperform saucers, and significant excess returns are observed only in earlier periods, diminishing over time. The authors suggest their mechanism can aid in evaluating trading strategies involving technical patterns.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The study uses a rule-based mechanism, not machine learning. It focuses on stock market technical patterns, not bonds or yield curves. It does involve forecasting/prediction of stock performance based on technical patterns."}, "mode": "threadpool", "paper_id": "08245f9f6de44032", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Algorithmic estimation of risk factors in financial markets with stochastic drift\nAbstract: We assume a financial market governed by a diffusion process reverting to a stochastic mean which is itself governed by an unobservable ergodic diffusion, similar to those observed in electricity and other energy markets. We develop a moment method algorithm for the estimation of the parameters of both the observable process and the unobservable stochastic mean. Our approach is contrasted with other methods for parameter estimation of partially observed diffusions, and applications to the modelling of interest rates and commodity prices are discussed. (C) 2010 Elsevier Ltd. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 268, "candidates": 141, "total": 409}, "attempt1_parsed": {"summary": "This paper proposes a moment method algorithm to estimate parameters in a financial market model with a stochastic drift, applicable to interest rates and commodity prices. The model assumes a diffusion process reverting to a stochastic mean, which is itself governed by an unobservable ergodic diffusion, drawing parallels to energy markets.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The abstract mentions 'estimation of parameters' and 'modelling', but not explicitly forecasting or prediction. The methods described appear to be statistical estimation techniques rather than machine learning applications."}, "mode": "threadpool", "paper_id": "b812228bd22517dc", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: ADAPTIVE LASSO-TYPE ESTIMATION FOR MULTIVARIATE DIFFUSION PROCESSES\nAbstract: The least absolute shrinkage and selection operator (LASSO) is a widely used statistical methodology for simultaneous estimation and variable selection. It is a shrinkage estimation method that allows one to select parsimonious models. In other words, this method estimates the redundant parameters as zero in the large samples and reduces variance of estimates. In recent years, many authors analyzed this technique from a theoretical and applied point of view. We introduce and study the adaptive LASSO problem for discretely observed multivariate diffusion processes. We prove oracle properties and also derive the asymptotic distribution of the LASSO estimator. This is a nontrivial extension of previous results by Wang and Leng (2007, Journal of the American Statistical Association, 102(479), 1039-1048) on LASSO estimation because of different rates of convergence of the estimators in the drift and diffusion coefficients. We perform simulations and real data analysis to provide some evidence on the applicability of this method.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning appli … [truncated]", "attempt1_tokens": {"prompt": 364, "candidates": 137, "total": 501}, "attempt1_parsed": {"summary": "This paper introduces and analyzes an adaptive LASSO estimation method for discretely observed multivariate diffusion processes. It extends previous LASSO work by addressing different convergence rates for drift and diffusion coefficients and demonstrates the method's applicability through simulations and real data analysis.", "criteria_hints": {"I1": true, "I2": false, "I3": false}, "notes": "The paper mentions simulations and real data analysis, indicating an empirical application beyond pure theory. It also discusses variable selection and estimation, which can be considered a form of model building beyond simple linear models."}, "mode": "threadpool", "paper_id": "09e7ecec6845d7fd", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: A two-regime threshold model with conditional skewed Student t distributions for stock returns\nAbstract: This paper proposes a two-regime threshold model for the conditional distribution of stock returns in which returns follow a distinct skewed Student t distribution within each regime: the model allows capturing time variation in the conditional distribution of returns, as well as higher order moments. An application of the model to daily U.S. stock returns illustrates the advantages of the proposed model in comparison to alternative specifications: the model performs well in terms of in-sample fit; it more accurately estimates the conditional volatility; and it produces useful risk assessment as measured by the term structure of value at risk. (C) 2014 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 296, "candidates": 159, "total": 455}, "attempt1_parsed": {"summary": "This paper introduces a two-regime threshold model using conditional skewed Student t distributions to analyze stock returns. The model captures time-varying conditional distributions and higher-order moments. An application to U.S. stock returns demonstrates its superior in-sample fit, more accurate conditional volatility estimation, and improved risk assessment compared to other models.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The paper uses a statistical model with an empirical application to stock returns, which can be considered a form of machine learning beyond simple linear models. It also focuses on risk assessment and volatility estimation, implying a predictive element."}, "mode": "threadpool", "paper_id": "7fe86973d0f86928", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: AN ARBITRAGE-FREE ESTIMATE OF PREPAYMENT OPTION PRICES IN FIXED-RATE GNMA MORTGAGE-BACKED SECURITIES\nAbstract: In an efficient market, the no-arbitrage condition implies that the price difference between any two assets must be the market value of ail differences in their cash flows. We use this logic to deduce the price of the prepayment option embedded in fixed-rate Government National Mortgage Association (GNMA) mortgage-backed securities. The option price equals the difference between an observed GNMA price and the cost of a synthetic, nonprepayable GNMA constructed from the least expensive portfolio of Treasury securities that exactly replicates the promised GNMA cash flow stream, assuming prepayment is precluded. We regress the option prices on variables found significant in previous prepayment studies, finding that five key regressors explain more than 90% of the prepayment option value in pooled time-series cross-sectional analysis. We also show that the time value of the prepayment option calculated by our method displays a pattern similar to that produced by the Black-Scholes (1973) option pricing model. An additional empirical result is the existence of negative op … [truncated]", "attempt1_tokens": {"prompt": 450, "candidates": 209, "total": 659}, "attempt1_parsed": {"summary": "This paper develops an arbitrage-free method to estimate the price of the prepayment option in fixed-rate GNMA mortgage-backed securities. It constructs a synthetic non-prepayable GNMA from Treasury securities and calculates the option price as the difference between the observed GNMA price and the synthetic price. The study finds that five key variables explain over 90% of the prepayment option value and observes negative option prices and time values, attributing them to homeowners exercising options out-of-the-money and refinancing costs. The method is independent of interest rate process assumptions and homeowner behavior, serving as a benchmark for prepayment models.", "criteria_hints": {"I1": true, "I2": false, "I3": false}, "notes": "The study uses regression analysis, which can be considered a machine learning application beyond simple linear models. However, it is not about government/sovereign bonds or forecasting/prediction."}, "mode": "threadpool", "paper_id": "412fc4122a3c36ac", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: An Efficient Deep Learning Based Model to Predict Interest Rate Using Twitter Sentiment\nAbstract: In macroeconomics, decision making is highly sensitive and significantly influences the financial and business world, where the interest rate is a crucial factor. In addition, the interest rate is used by the governments to manage the monetary policy. There is a need to design an efficient algorithm for interest rate prediction. The analysis of the social media sentiment impact on financial decision making is also an open research area. In this study, we deploy a deep learning model for the accurate forecasting of the interest rate for the UK, Turkey, China, Hong Kong, and Mexico. For this purpose, daily data of the interest rate and exchange rate covering the period from Jan 2010 to Oct 2019 is used for all the mentioned countries. We also incorporate the input of the twitter sentiments of six mega-events, namely the US election 2012, Mexican election 2012, Gaza under attack 2014, Hong Kong protest 2014, Refugee Welcome 2015, and Brexit 2016. Our results provide evidence that the error of the deep learning model significantly decreases when event sentiment is incorporated. A no … [truncated]", "attempt1_tokens": {"prompt": 439, "candidates": 148, "total": 587}, "attempt1_parsed": {"summary": "This study proposes a deep learning model to predict interest rates for the UK, Turkey, China, Hong Kong, and Mexico, incorporating daily interest rate, exchange rate, and Twitter sentiment data from six major global events between 2010 and 2019. The model demonstrated improved accuracy, particularly for Hong Kong, with a significant reduction in prediction error when event sentiment was included.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study focuses on interest rates, not specifically government/sovereign bonds or the yield curve."}, "mode": "threadpool", "paper_id": "dde2fd0b63571dc0", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Analysis of the relevance of sentiment data for the prediction of excess returns in a multiasset framework\nAbstract: In this study, we look at the relevance of sentiment data for the prediction of excess returns in a multiasset analysis. We start by initial exploratory data analysis in order to assess the pertinence of the sentiment data. We then compare the performance of rule-based algorithms with and without the sentiment data. The data considered are provided by RavenPack. Finally, we explore the economic relevance of the forecast model in a long-only and long-short context. Inclusion of sentiment data leads to encouraging results.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 267, "candidates": 149, "total": 416}, "attempt1_parsed": {"summary": "This study investigates the predictive power of sentiment data for excess returns in a multi-asset framework. It compares rule-based algorithms with and without sentiment data, using data from RavenPack, and explores the economic relevance of the forecasting model in different investment contexts. The inclusion of sentiment data yielded positive results.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses rule-based algorithms and sentiment data for prediction, which implies an empirical ML application beyond simple linear models. It does not focus on government/sovereign bonds or the yield curve."}, "mode": "threadpool", "paper_id": "f9ea0fbfd77a9f06", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Alternative models for stock price dynamics\nAbstract: This paper evaluates the role of various volatility specifications, such as multiple stochastic volatility (SV) factors and jump components, in appropriate modeling of equity return distributions. We use estimation technology that facilitates nonnested model comparisons and use a long data set which provides rich information about the conditional and unconditional distribution of returns. We consider two broad families of models: (I) the multifactor loglinear family, and (2) the affine-jump family. Both classes of models have attracted much attention in the derivatives and econometrics literatures. There are various tradeoffs in considering such diverse specifications. If pure diffusion SV models are chosen over jump diffusions, it has important implications for hedging strategies. If logarithmic models are chosen over affine ones, it may seriously complicate option pricing. Comparing many different specifications of pure diffusion multifactor models and jump diffusion models, we find that (I) log linear models have to be extended to two factors with feedback in the mean reverting factor, (2) affine models have to have a  … [truncated]", "attempt1_tokens": {"prompt": 481, "candidates": 149, "total": 630}, "attempt1_parsed": {"summary": "This paper compares different models for stock price dynamics, including multifactor stochastic volatility and affine-jump models. It finds that models need to account for significant volatility movements, potentially with jumps. The choice between models depends on factors like option pricing and hedging, with affine models with jumps in volatility being potentially preferred for closed-form derivative prices.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The paper discusses model comparisons and empirical fit, suggesting a focus on forecasting and prediction. It also mentions machine learning aspects by evaluating different model specifications beyond simple linear models."}, "mode": "threadpool", "paper_id": "bcfa69807c6a868e", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: An Application of Damped Diffusion for Modeling Volatility Dynamics\nAbstract: This paper proposes a damped constant elasticity variance (CEV) stochastic volatility (DCEV) model, which remedies the possible explosive behavior of the CEV model and also accommodates the mean-reverting dynamics more appropriately than the nonlinear drift (NLD) stochastic volatility model. As the DCEV model maintains the linear drift, an analytic formula is available to efficiently infer latent variances from VIX levels, after which both its physical and risk-neutral parameters can be simultaneously estimated with the maximum-likelihood approach given S&P 500 returns and inferred variances. The DCEV model outperforms the CEV and NLD models in in-sample fitting performance and in out-of-sample variance forecasting under the physical measure. It also exhibits superior ability in out-of-sample option pricing over the CEV and models under the risk-neutral measure. This satisfactory performance demonstrates the suitability of describing volatility dynamics with the DCEV model and the potential of applying this to study other issues.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning … [truncated]", "attempt1_tokens": {"prompt": 357, "candidates": 226, "total": 583}, "attempt1_parsed": {"summary": "This paper introduces a Damped Constant Elasticity Variance (DCEV) stochastic volatility model to address limitations in existing models for volatility dynamics. The DCEV model, which allows for mean-reversion and avoids explosive behavior, enables efficient inference of latent variances from VIX levels. The model's parameters are estimated using maximum likelihood with S&P 500 returns and inferred variances. Empirical results show the DCEV model outperforms CEV and Nonlinear Drift (NLD) models in fitting and forecasting volatility under the physical measure, and in option pricing under the risk-neutral measure.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study applies a novel stochastic volatility model (DCEV) to S&P 500 returns and VIX levels, involving empirical analysis and forecasting, thus meeting criteria I1 and I3. It does not concern government bonds or the yield curve, so I2 is false."}, "mode": "threadpool", "paper_id": "e040435996e03d39", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Arbitrage bounds in markets with noisy prices and the puzzle of negative option prices implicit in bonds\nAbstract: The term structure of interest rates has occupied economists for many years. This is testimony to the importance of the term structure (or, equivalently, valuation operator) and to the difficulty of obtaining reliable estimates of it. Until recently, it was not possible to reconcile the theoretical existence of a multiplicity of valuation operators in an incomplete bond market and the empirical nonexistence of even a single one. MacKay and Prisman [Estimating valuation operators in incomplete market with noises: Can noise complete the market. Working paper, 1996] tackle this problem. In this paper, an amendment to and a generalization of their methodology is presented. The amendment preserves the linearity of valuation operators and allows use of efficient linear programming techniques. Further, it facilitates an admissible consideration of the puzzle of negative option prices embedded in bonds. The empirical results presented in this paper were obtained using data on extendable Government of Canada bonds. The results indicate that although the gap between the l … [truncated]", "attempt1_tokens": {"prompt": 404, "candidates": 162, "total": 566}, "attempt1_parsed": {"summary": "This paper presents an amendment and generalization to MacKay and Prisman's methodology for estimating valuation operators in incomplete bond markets with noisy prices. It preserves linearity, allows for efficient linear programming, and addresses the puzzle of negative option prices implicit in bonds. Empirical results using Canadian government bond data show that while valuation operator price gaps are significant, they do not resolve the puzzle.", "criteria_hints": {"I1": false, "I2": true, "I3": false}, "notes": "The paper mentions \"efficient linear programming techniques\" but does not describe an empirical Machine Learning application beyond simple linear models. It focuses on valuation operators and the puzzle of negative option prices rather than forecasting."}, "mode": "threadpool", "paper_id": "30d5228daba342c6", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: ASYMMETRICAL EFFECTS OF REAL EFFECTIVE EXCHANGE RATE OF KUNA ON FOREIGN TRADE BALANCE IN REPUBLIC OF CROATIA\nAbstract: This paper strives on giving an answer to whether assymetrical effects of real effective ex-change rate of kuna, J-curve and Marshall-Lerner condition exist in the Republic of Croatia. Price rigidity, capacity constraints, and adjustment costs are just some reasons why depreciation and appreciation of the real effective exchange rate do not necessarily have equally strong effect of the opposite direction on net exports. Econometric estimation by nonlinear autoregressive distributed lags method (NARDL) enables separation of positive (appreciation) from negative (depreciation) changes in the real effective exchange rate, showing that its changes do not affect net exports sym-metrically and linearly. Existence of J-curve and positive impact of the real effective exchange rate depreciation on trade balance (Marshall-Lerner condition) are confirmed, while appreciation has no significant effect. Results obtained in this paper point out that Croatia `s foreign trade bal-ance can be improved by real effective exchange rate depreciation. However, this should be taken … [truncated]", "attempt1_tokens": {"prompt": 589, "candidates": 265, "total": 854}, "attempt1_parsed": {"summary": "This paper investigates the asymmetrical effects of the real effective exchange rate (REER) of the Croatian Kuna on the foreign trade balance. Using the nonlinear autoregressive distributed lags (NARDL) method, the study confirms the existence of the J-curve and the Marshall-Lerner condition, indicating that depreciation of the REER positively impacts the trade balance, while appreciation has no significant effect. The research highlights potential benefits of REER depreciation for improving the trade balance but cautions about its negative consequences on external debt, inflation, and economic activity. Limitations include low REER variability and challenges in selecting appropriate exchange rate measures and proxy variables. The study extends previous research by confirming asymmetric effects on the balance of goods and services and discussing potential causes of nonlinearity.", "criteria_hints": {"I1": true, "I2": false, "I3": false}, "notes": "The study uses NARDL, which is an econometric method that can be considered a form of machine learning application as it goes beyond simple linear models. The study is about foreign trade balance and exchange rates, not government bonds or yield curves. While it discusses effects on the trade balance, it is not primarily a forecasting or prediction study."}, "mode": "threadpool", "paper_id": "efd37e8cd45dfd0b", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Anchoring Bias in Consensus Forecasts and Its Effect on Market Prices\nAbstract: Previous empirical studies on the rationality of economic. and financial forecasts generally test for generic properties such as bias or autocorrelated errors but provide only limited insight into the behavior behind inefficient forecasts. This paper tests for a specific form of forecast bias. In particular, we examine whether expert consensus forecasts of monthly economic releases are systematically biased toward the value of previous months' releases. Such a bias would be consistent with the anchoring and adjustment heuristic described by Tversky and Kahneman (1974) or could arise from professional forecasters' strategic incentives. We find broad-based and significant evidence for this form of bias, which in some cases results in sizable predictable forecast errors. To investigate whether market participants' expectations are influenced by this bias, we examine interest rate reactions to economic news. We find that bond yields react only to the residual, or unpredictable, component of the forecast error and not to the component induced by anchoring, suggesting that expectations of market partic … [truncated]", "attempt1_tokens": {"prompt": 360, "candidates": 168, "total": 528}, "attempt1_parsed": {"summary": "This study investigates anchoring bias in consensus forecasts of economic releases, finding significant evidence that forecasts are influenced by previous months' values. The research also examines the impact of this bias on market prices by analyzing interest rate reactions to economic news. Results indicate that bond yields react only to the unpredictable component of forecast errors, suggesting market participants anticipate the anchoring bias in expert forecasts.", "criteria_hints": {"I1": false, "I2": true, "I3": true}, "notes": "The study examines bond yields and interest rates, fitting I2. It also focuses on forecasting and prediction, fitting I3. However, it does not appear to use machine learning applications beyond simple linear models, thus not fitting I1."}, "mode": "threadpool", "paper_id": "ef29280b50c0dfb2", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Asset Pricing When 'This Time Is Different'\nAbstract: Recent evidence suggests that younger people update beliefs in response to aggregate shocks more than older people. We embed this generational learning bias in an equilibrium model in which agents have recursive preferences and are uncertain about exogenous aggregate dynamics. The departure from rational expectations is statistically modest, but generates high average risk premiums varying at generational frequencies, a positive relation between past returns and agents' future return forecasts, and substantial and persistent over-and undervaluation. Consistent with the model, the price-dividend ratio is empirically more sensitive to macroeconomic shocks when the fraction of young in the population is higher.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 271, "candidates": 184, "total": 455}, "attempt1_parsed": {"summary": "This paper develops an equilibrium asset pricing model incorporating a generational learning bias, where younger individuals update beliefs more than older ones in response to aggregate shocks. This bias leads to high average risk premiums, a positive relationship between past returns and future forecasts, and significant over/undervaluation. Empirically, the price-dividend ratio is more sensitive to macroeconomic shocks when the young population fraction is higher.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The paper discusses forecasting of returns and over/undervaluation, which aligns with I3. However, it does not appear to involve empirical machine learning applications beyond simple linear models (I1) or focus on government/sovereign bonds and the yield curve (I2)."}, "mode": "threadpool", "paper_id": "1aa5adffb2e84661", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Artificial Intelligence in Economics Research: What Have We Learned? What Do We Need to Learn?\nAbstract: Motivated by the recent boom in artificial intelligence (AI) playing a significant role in the economics of individuals, firms, and government bodies, we investigate the role of AI in economics by reviewing the literature (2231 articles) during the last 34 years (1990 to November 2024). We identify five research streams: (1) AI and economic modeling, (2) AI and macroeconomics (eight sub-streams), (3) AI and equity and debt market, (4) AI and prediction modeling (three sub-streams), and (5) AI and economics of innovation. Further, we offer suggestions for future research (20 questions). Additionally, we outline a framework to consider changes in economics before and after AI adoption. Further, the critical AI-based methods are identified and discussed.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 337, "candidates": 172, "total": 509}, "attempt1_parsed": {"summary": "This review examines the role of AI in economics over the past 34 years, identifying five research streams: AI and economic modeling, macroeconomics, equity and debt markets, prediction modeling, and innovation. It suggests future research directions and outlines a framework for understanding economic changes due to AI adoption, while also discussing critical AI methods.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The article discusses prediction modeling as a research stream and mentions AI and prediction modeling, indicating a focus on forecasting. However, it does not explicitly state the use of empirical machine learning applications beyond simple linear models, nor does it focus on government/sovereign bonds or the yield curve."}, "mode": "threadpool", "paper_id": "c7505f1ac2274c1a", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Assessing the relevance of sell-side analyst recommendations\nAbstract: This paper evaluates the informational value and alpha-generating potential of sell-side analyst recommendations. We explore this by employing a monthly portfolio-sorted long-short strategy based on consensus analyst recommendations. Our findings indicate that the long-short equal-weighted and value-weighted portfolios yield significant excess returns. However, the value-weighted excess returns are primarily driven by the predictive power of the lowest decile (sell recommendations). The long-short strategy for the value-weighted portfolios yields a monthly excess return ranging from 1.36% to 1.57%, above the risk-free rate. Our analysis further examines variations in recommendation effectiveness across economic cycles, industries, investment banks/brokers, and firm sizes, providing further insights into the value of analyst recommendations.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or predic … [truncated]", "attempt1_tokens": {"prompt": 312, "candidates": 150, "total": 462}, "attempt1_parsed": {"summary": "This study assesses the informational value and alpha-generating potential of sell-side analyst recommendations using a monthly portfolio-sorted long-short strategy. Significant excess returns were found, particularly for value-weighted portfolios driven by sell recommendations. The effectiveness of recommendations was further analyzed across various factors like economic cycles and firm sizes.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The study uses a portfolio-sorted strategy which is an empirical method, but it does not explicitly mention machine learning applications beyond simple linear models. It focuses on prediction of returns based on analyst recommendations."}, "mode": "threadpool", "paper_id": "cecdcfa68c964887", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Are embedded calls valuable? Evidence from agency bonds\nAbstract: This paper examines the call option values embedded in callable agency bonds. For FHLB, FNMA, and SLMA bonds, call value estimates range from 1.23% of par to 1.47% on average, which are between those for the treasury and corporate debt securities. FHLMC bonds, on the other hand, have an average call value estimate of 2.85%. Call values are significantly larger for bonds with a longer remaining maturity and greater default risk. Most interestingly, call values in the call protection period are significantly larger than those in the callable period except for the SLMA bonds, whereas previous studies on corporate debt find no significant difference in call values between these two periods.In general, call value exhibits a downward trend over time as the callable bond approaches maturity. Also, call value is inversely related to the level of interest rates. Interest rate drops are usually accompanied by an increase in Call values. An analysis of the determinants of call values suggests the following conclusions. First, call values are negatively related to short-term interest rates and the slope of the yield curve … [truncated]", "attempt1_tokens": {"prompt": 449, "candidates": 202, "total": 651}, "attempt1_parsed": {"summary": "This paper analyzes the value of embedded call options in agency bonds (FHLB, FNMA, SLMA, FHLMC). It finds that call values vary across bond types and are influenced by maturity, default risk, and interest rate movements. Notably, call values are higher during the call protection period for most agency bonds, unlike findings for corporate debt. The study also identifies factors affecting call values, including interest rates, yield curve slope, coupon rate, maturity, and the presence of call protection.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The study focuses on agency bonds and their embedded call options, not government/sovereign bonds or the yield curve. It uses statistical analysis rather than machine learning. While it analyzes trends and relationships, it does not appear to be focused on forecasting or prediction."}, "mode": "threadpool", "paper_id": "302ced4755753438", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Asymmetric impacts of individual investor sentiment on the time-varying risk-return relation in stock market\nAbstract: This study investigates the impacts of investor sentiments, including individual sentiment and market-wide sentiments, on time-varying risk-return tradeoffs in the U.S. stock market using quantile regressions. Empirical results show that the individual sentiment has a significant negative effect on the time-varying risk-return tradeoff across all quantiles, indicating the heterogeneity of the individual sentiment effect. Specifically, the positive individual sentiment weakens the time-varying risk-return tradeoff while the negative individual sentiment enhances it. Besides, there are asymmetric effects of the individual sentiment at quantiles (0.25, 0.75), that is, a negative individual sentiment associated with bad news has a stronger impact than a positive individual sentiment associated with good news. These findings are robust for alternative estimate methods and individual sentiments. However, the study finds that the time-varying riskreturn tradeoff is less sensitive to the market-wide sentiment than to the individual sentiment, indicating that the ind … [truncated]", "attempt1_tokens": {"prompt": 372, "candidates": 170, "total": 542}, "attempt1_parsed": {"summary": "This study examines how individual and market-wide investor sentiments affect the time-varying risk-return relationship in the U.S. stock market using quantile regressions. It finds that individual sentiment significantly impacts this relationship across all quantiles, with positive sentiment weakening it and negative sentiment strengthening it. The impact of negative sentiment is stronger than positive sentiment, especially during bad news periods. The study concludes that individual sentiment is more influential than market-wide sentiment in stock price determination.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The study uses quantile regressions, which are statistical methods, not machine learning. The focus is on forecasting/prediction of risk-return tradeoffs."}, "mode": "threadpool", "paper_id": "4b22c1332c433aaa", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Asymmetry in the link between the yield spread and industrial production: Threshold effects and forecasting\nAbstract: We analyse the nonlinear behaviour of the information content in the spread for future real economic activity. The spread linearly predicts one-year-ahead real growth in nine industrial production sectors of the USA and four of the UK over the last 40 years. However, recent investigations on the spread-real activity relation have questioned both its linear nature and its time-invariant framework. Our in-sample empirical evidence suggests that the spread-real activity relationship exhibits asymmetries that allow for different predictive power of the spread when past spread values were above or below some threshold value. We then measure the out-of-sample forecast performance of the nonlinear model using predictive accuracy tests. The results show that significant improvement in forecasting accuracy, at least for one-step-ahead forecasts, can be obtained over the linear model. Copyright (C) 2004 John Wiley Sons, Ltd.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\":  … [truncated]", "attempt1_tokens": {"prompt": 340, "candidates": 143, "total": 483}, "attempt1_parsed": {"summary": "This study investigates the nonlinear relationship between the yield spread and industrial production, finding that the spread's predictive power for future economic activity is asymmetric and depends on threshold values. A nonlinear model demonstrates improved forecasting accuracy over a linear model for one-year-ahead real growth in US and UK industrial sectors.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study focuses on the yield spread and industrial production, not specifically government bonds or the yield curve itself. However, the yield spread is a component of the yield curve."}, "mode": "threadpool", "paper_id": "cc2d45a53ccb3e61", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Bank portfolio exposure to emerging markets and its effects on bank market value\nAbstract: This study estimates a model of banking company equity returns taking into consideration book value and market value measures of their exposure to emerging markets debt. In this estimation, general systematic market factors, such as the rate of return on the S&P500 stock index and yields on a constant maturity 5-year Treasury note, are held constant such that the exposure variables are accounting for effects due to banks' exposure to emerging market debt. The results, although not uniform among banking companies, support the hypothesis that the extent of exposure to emerging market debt are factored into the valuation of banking company equity contemporaneously. The inclusion of a market value indicator adds to the explanation of equity returns of some banks. It is also clear that knowing the extent of the exposure on a book value basis is important information alone that may allow investors to take account of or evaluate the effects of changes in banking company equity valuation from LDC debt exposures. We also perform an event study for three major debt crises to determine whether th … [truncated]", "attempt1_tokens": {"prompt": 452, "candidates": 180, "total": 632}, "attempt1_parsed": {"summary": "This study investigates the impact of bank portfolio exposure to emerging markets debt on bank market value. It uses a model to estimate equity returns, considering both book and market value measures of exposure, while controlling for general market factors. The findings suggest that emerging market debt exposure is contemporaneously factored into banking equity valuation. An event study on debt crises indicated limited direct impact on equity returns, possibly due to embedded information in exposure changes and prior market valuation.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The study uses regression models and an event study, not machine learning. It focuses on emerging market debt, not government bonds or the yield curve. While it discusses valuation, it is not primarily about forecasting or prediction."}, "mode": "threadpool", "paper_id": "dc9164700f08b9ff", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Bayesian inference and state number determination for hidden Markov models: an application to the information content of the yield curve about inflation\nAbstract: This paper is concerned with Bayesian inference in hidden Markov models. Focusing oil switching regression models, we propose a new methodology that delivers a joint estimation of the parameters and the number of regimes that have actually appeared in the studied sample. The only prior information that is required on the latter quantity is an upper bound. We implement a particle filter algorithm to compute the corresponding estimates. Applying this methodology to the information content of the yield curve regarding future inflation in four OECD countries, we show that the predictive content for given country and combination of maturities is subject to regime switching. (C) 2003 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 300, "candidates": 173, "total": 473}, "attempt1_parsed": {"summary": "This paper proposes a Bayesian inference methodology for hidden Markov models, specifically for switching regression models. It jointly estimates model parameters and the number of regimes, requiring only an upper bound for the number of regimes. A particle filter algorithm is used for computation. The methodology is applied to analyze the information content of the yield curve regarding future inflation in four OECD countries, revealing regime switching in predictive content.", "criteria_hints": {"I1": true, "I2": true, "I3": true}, "notes": "The application to the yield curve's information content about inflation directly addresses I2 and I3. The use of hidden Markov models and switching regression, along with the particle filter algorithm, goes beyond simple linear models, satisfying I1."}, "mode": "threadpool", "paper_id": "aaff782e22358a7e", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Bias in the estimation of the mean reversion parameter in continuous time models\nAbstract: It is well known that for continuous time models with a linear drift standard estimation methods yield biased estimators for the mean reversion parameter both in finite discrete samples and in large in-fill samples. In this paper, we obtain two expressions to approximate the bias of the least squares/maximum likelihood estimator of the mean reversion parameter in the Ornstein-Uhlenbeck process with a known long run mean when discretely sampled data are available. The first expression mimics the bias formula of Marriott and Pope (1954) for the discrete time model. Simulations show that this expression does not work satisfactorily when the speed of mean reversion is slow. Slow mean reversion corresponds to the near unit root situation and is empirically realistic for financial time series. An improvement is made in the second expression where a nonlinear correction term is included into the bias formula. It is shown that the nonlinear term is important in the near unit root situation. Simulations indicate that the second expression captures the magnitude, the curvature and the non-monoto … [truncated]", "attempt1_tokens": {"prompt": 388, "candidates": 148, "total": 536}, "attempt1_parsed": {"summary": "This paper derives two expressions to approximate the bias of the least squares/maximum likelihood estimator for the mean reversion parameter in the Ornstein-Uhlenbeck process using discrete time data. The second expression, which includes a nonlinear correction term, is shown to be more accurate, especially in situations of slow mean reversion (near unit root), which are common in financial time series.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The paper focuses on theoretical bias estimation in continuous time models, not empirical applications of machine learning, government bonds, or forecasting."}, "mode": "threadpool", "paper_id": "f94a815f95694f43", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Asset-pricing implications of biologically based non-expected utility\nAbstract: Results in population ecology suggest that evolutionary successful species should have an adaptive (reference-based) S-shaped utility function that is intrinsically more sensitive to aggregate than uninsured idiosyncratic shocks the former cannot be diversified demographically. To test the asset-pricing relevance of these ideas, I embed the non-expected utility specification implied by evolutionary theory into an economy with partial risk sharing due to limited commitment. For the benchmark specification (CARA = 6 over gains), Monte Carlo simulations of a Markov growth economy produce the following results: (i) matching the degree of consumption-smoothing in the cross section, the Sharpe ratio for a Lucas tree is 0.33, an increase of 44 percent relative to expected utility; (ii) the risk-free rate is low, stable and counter-cyclical, hence equity returns, unlike in the expected utility case, have the correct pattern of predictability; (iii) in the cross section, excess returns across equity classes exhibit both a value premium and a size discount with risk adjusted returns that are at least two t … [truncated]", "attempt1_tokens": {"prompt": 394, "candidates": 219, "total": 613}, "attempt1_parsed": {"summary": "This paper explores the asset-pricing implications of a biologically inspired non-expected utility function, derived from evolutionary theory. The study embeds this utility specification into an economy with partial risk sharing due to limited commitment. Monte Carlo simulations in a Markov growth economy show that this non-expected utility model can better match cross-sectional consumption smoothing, leading to a higher Sharpe ratio for a Lucas tree compared to the expected utility model. It also generates a low, stable, and counter-cyclical risk-free rate, and provides a more accurate pattern of equity return predictability, including value premiums and size discounts.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The study uses Monte Carlo simulations, which can be considered a computational method, but it does not appear to be an empirical Machine Learning application as defined by the criteria. The focus is on asset pricing and equity returns, not government bonds or the yield curve."}, "mode": "threadpool", "paper_id": "0c787f997c47e0be", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Beyond Stochastic Volatility and Jumps in Returns and Volatility\nAbstract: While a great deal of attention has been focused on stochastic volatility in stock returns, there is strong evidence suggesting that return distributions have time-varying skewness and kurtosis as well. Under the risk-neutral measure, for example, this can be observed from variation across time in the shape of Black Scholes implied volatility smiles. This article investigates model characteristics that are consistent with variation in the shape of return distributions using a stochastic volatility model with a regime-switching feature to allow for random changes in the parameters governing volatility of volatility, leverage effect, and jump intensity. The analysis consists of two steps. First, the models are estimated using only information from observed returns and option-implied volatility. Standard model assessment tools indicate a strong preference in favor of the proposed models. Since the information from option-implied skewness and kurtosis is not used in fitting the models, it is available for diagnostic purposes. In the second step of the analysis, regressions of option-implied skewness and k … [truncated]", "attempt1_tokens": {"prompt": 383, "candidates": 151, "total": 534}, "attempt1_parsed": {"summary": "This article proposes a stochastic volatility model with a regime-switching feature to capture time-varying skewness and kurtosis in stock returns, beyond just stochastic volatility and jumps. The model is estimated using observed returns and option-implied volatility, and its explanatory power for option-implied skewness and kurtosis is assessed.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The article uses a stochastic volatility model with regime-switching, which can be considered an empirical application beyond simple linear models. It also focuses on explaining and potentially forecasting characteristics of return distributions."}, "mode": "threadpool", "paper_id": "fe830c5db5e38b8f", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Bias in estimating multivariate and univariate diffusions\nAbstract: Multivariate continuous time models are now widely used in economics and finance. Empirical applications typically rely on some process of discretization so that the system may be estimated with discrete data. This paper introduces a framework for discretizing linear multivariate continuous time systems that includes the commonly used Euler and trapezoidal approximations as special cases and leads to a general class of estimators for the mean reversion matrix. Asymptotic distributions and bias formulae are obtained for estimates of the mean reversion parameter. Explicit expressions are given for the discretization bias and its relationship to estimation bias in both multivariate and in univariate settings. in the univariate context, we compare the performance of the two approximation methods relative to exact maximum likelihood (ML) in terms of bias and variance for the Vasicek process. The bias and the variance of the Euler method are found to be smaller than the trapezoidal method, which are in turn smaller than those of exact ML. Simulations suggest that when the mean reversion is slow, the approximation  … [truncated]", "attempt1_tokens": {"prompt": 444, "candidates": 234, "total": 678}, "attempt1_parsed": {"summary": "This paper presents a framework for discretizing linear multivariate continuous time systems, including Euler and trapezoidal approximations, to estimate the mean reversion matrix. It derives asymptotic distributions and bias formulas for the mean reversion parameter, detailing discretization bias and its relation to estimation bias in both multivariate and univariate settings. The study compares the Euler and trapezoidal methods against exact maximum likelihood (ML) for the Vasicek process, finding that approximation methods generally have smaller bias and variance than ML, especially for scalar models with slow mean reversion. For the square root process, the Euler method is shown to outperform other methods in terms of bias and variance.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The paper discusses estimation methods for continuous time models and their discretization, but does not appear to involve empirical machine learning applications beyond simple linear models, nor does it focus on government bonds or yield curves. While it discusses estimation and comparison of methods, the primary focus is not forecasting or prediction in the typical sense."}, "mode": "threadpool", "paper_id": "4353d80b6dc1ec37", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Binomial Markov-Switching Multifractal model with Skewed t innovations and applications to Chinese SSEC Index\nAbstract: This paper presents the Binomial Markov-switching Multifractal (BMSM) model of asset returns with Skewed t innovations (BMSM-Skewed t for short), which considers the fat tails, skewness and multifractality in asset returns simultaneously. The parameters of BMSM-Skewed t model can be estimated by Maximum Likelihood (ML) methods, and volatility forecasting can be accomplished via Bayesian updating. In order to evaluate the performance of BMSM-Skewed t model, BMSM model with Normal innovations (BMSM-N), BMSM model with Student-t innovations (BMSM-t) and GARCH(1,1) models (GARCH-N, GARCH-t and GARCH-Skewed t) are chosen for comparison. Through empirical studies on Shanghai Stock Exchange Composite Index (SSEC), we find that for sample estimation, BMSM models outperform the GARCH(1,1) models through BIC and AIC rules, and BMSM-Skewed t performs the best among all the models due to its fat tails, skewness and multifractality. In addition, BMSM-Skewed t model dominates other models at most forecasting horizons for out-of-sample volatility forecasts in terms of MSE … [truncated]", "attempt1_tokens": {"prompt": 453, "candidates": 205, "total": 658}, "attempt1_parsed": {"summary": "This paper introduces the Binomial Markov-switching Multifractal (BMSM) model with Skewed t innovations to simultaneously capture fat tails, skewness, and multifractality in asset returns. The model is estimated using Maximum Likelihood methods and volatility forecasting is done via Bayesian updating. Empirical studies on the Shanghai Stock Exchange Composite Index (SSEC) show that the BMSM models outperform GARCH(1,1) models, with the BMSM-Skewed t model performing best. The BMSM-Skewed t model also demonstrates superior out-of-sample volatility forecasting performance.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses a complex statistical model (BMSM-Skewed t) which can be considered beyond simple linear models, and it is applied empirically to financial data for forecasting."}, "mode": "threadpool", "paper_id": "3cf97d4770aaca9e", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Biased Bayesian learning with an application to the risk-free rate puzzle\nAbstract: Based on the axiomatic framework of Choquet decision theory, we develop a closed-form model of Bayesian learning with ambiguous beliefs about the mean of a normal distribution. In contrast to rational models of Bayesian learning the resulting Choquet Bayesian estimator results in a long-run bias that reflects the agent's ambiguity attitudes. By calibrating the standard equilibrium conditions of the consumption based asset pricing model we illustrate that our approach contributes towards a resolution of the risk-free rate puzzle. For a plausible parameterization we obtain a risk-free rate in the range of 3.5-5%. This is 1-2.5% closer to the empirical risk-free rate than according calibrations of the rational expectations model. (C) 2013 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 322, "candidates": 185, "total": 507}, "attempt1_parsed": {"summary": "This paper presents a model of Bayesian learning with ambiguous beliefs about the mean of a normal distribution, based on Choquet decision theory. The model results in a long-run bias that reflects the agent's ambiguity attitudes. When applied to the consumption-based asset pricing model, this approach helps resolve the risk-free rate puzzle, yielding a risk-free rate closer to empirical observations than rational expectations models.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The paper discusses a theoretical model and its application to asset pricing, which includes forecasting the risk-free rate. However, it does not appear to involve empirical machine learning applications beyond simple linear models, nor does it focus on government/sovereign bonds or the yield curve."}, "mode": "threadpool", "paper_id": "d7cc9979d5792ab1", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Bond risk premiums at the zero lower bound\nAbstract: We document that the spread between long- and short-term government bond yields is a stronger predictor of excess bond returns when the U.S. economy is at the zero lower bound (ZLB) than away from this bound. The Gaussian shadow rate model with a linear or quadratic shadow rate is unable to explain this change in return predictability. The same holds for the quadratic term structure model and the autoregressive gamma-zero model that also enforce the ZLB. In contrast, the linear-rational square-root model explains our new empirical finding because the model allows for unspanned stochastic volatility as seen in bond yields.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 281, "candidates": 151, "total": 432}, "attempt1_parsed": {"summary": "This paper investigates bond risk premiums at the zero lower bound (ZLB). It finds that the spread between long- and short-term government bond yields is a better predictor of excess bond returns at the ZLB. Existing models like the Gaussian shadow rate model and the quadratic term structure model fail to explain this, but a linear-rational square-root model succeeds by incorporating unspanned stochastic volatility.", "criteria_hints": {"I1": false, "I2": true, "I3": true}, "notes": "The paper discusses empirical findings and model performance but does not appear to use machine learning applications beyond simple linear models."}, "mode": "threadpool", "paper_id": "72088005c119975b", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Bilateral multiple gamma returns: Their risks and rewards\nAbstract: The bilateral gamma model for returns is naturally derived from the lognormal model. Maximizing entropy in a random time change delivers the symmetric variance gamma model. The asymmetric variance gamma follows on incorporating skewness. Differential speeds for the upward and downward motions lead to the bilateral gamma. A further generalizations results in the bilateral double gamma model when the speed parameter of the bilateral gamma model is itself taken to be gamma distributed on entropy maximization. A rich five to seven parameter specification of preferences renders possible the extraction of physical densities from option prices. The quality of such extraction is measured by examining the uniformity of the estimated distribution functions evaluated at realized forward returns. The economic value of risky returns is seen to embed three/five risk premia for the bilateral gamma/bilateral double gamma. For the bilateral gamma they are up and down side volatilities compensated in up and down side drifts, and the down side drift compensated in the up side drift. For the bilateral double gamma one adds in a … [truncated]", "attempt1_tokens": {"prompt": 379, "candidates": 205, "total": 584}, "attempt1_parsed": {"summary": "This paper introduces the bilateral gamma model for returns, derived from the lognormal model and incorporating skewness and differential speeds for upward and downward movements. It generalizes to the bilateral double gamma model. The model allows for the extraction of physical densities from option prices using a five to seven parameter specification of preferences. The economic value of risky returns is analyzed through risk premia, identifying three/five such premia for the bilateral gamma/bilateral double gamma models, respectively. The study observes a decrease in downside risk premiums since the crisis and an increase in the recent period.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The paper discusses a model for returns and its application in extracting physical densities from option prices, which can be considered an empirical application beyond simple linear models. It also involves forecasting implied by risk premia and density extraction."}, "mode": "threadpool", "paper_id": "60a879db8106ce03", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Bond risk premia in a small open economy with volatile capital flows: The case of Korea\nAbstract: This paper investigates bond risk premia embedded in Korean government bonds. Unlike the U.S., Korea is a small open economy characterized by highly volatile capital flows and non-reserve currency country. My empirical findings show that among alternative predictive variables (including the macro and global liquidity factors) for one-year-ahead excess bond returns, the global liquidity factors, extracted from the panel data set of various global liquidity variables, are the only predictors that perform well across both in- and out-of-sample forecast analysis. In a similar vein, the regression analysis for the determinants of the estimated bond risk premia (with both monthly and quarterly frequencies) reveals that similar to the case of U.S. bond market, the risk premia in Korean government bonds are affected by domestic expected inflation, but more importantly, that they are affected heavily by the global liquidity variables, such as VIX, bank capital flows and the leverage of global banks. (C) 2019 Elsevier Ltd. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an em … [truncated]", "attempt1_tokens": {"prompt": 370, "candidates": 176, "total": 546}, "attempt1_parsed": {"summary": "This paper examines bond risk premia in Korean government bonds, highlighting Korea's status as a small open economy with volatile capital flows. Empirical findings indicate that global liquidity factors are significant predictors of excess bond returns, outperforming domestic macroeconomic variables. The study also reveals that while domestic expected inflation influences Korean bond risk premia, global liquidity variables like VIX, bank capital flows, and global bank leverage play a more substantial role.", "criteria_hints": {"I1": false, "I2": true, "I3": true}, "notes": "The abstract mentions \"predictive variables\" and \"forecast analysis\", suggesting forecasting is a key component. However, it does not explicitly mention machine learning or models beyond simple linear ones, hence I1 is false."}, "mode": "threadpool", "paper_id": "46572bea2afbeb26", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: CONDITIONAL HETEROSKEDASTICITY IN ASSET RETURNS - A NEW APPROACH\nAbstract: GARCH models have been applied in modelling the relation between conditional variance and asset risk premia. These models, however, have at least three major drawbacks in asset pricing applications: (i) Researchers beginning with Black (1976) have found a negative correlation between current returns and future returns volatility. GARCH models rule this out by assumption. (ii) GARCH models impose parameter restrictions that are often violated by estimated coefficients and that may unduly restrict the dynamics of the conditional variance process. (iii) Interpreting whether shocks to conditional variance persist or not is difficult in GARCH models, because the usual norms measuring persistence often do not agree. A new form of ARCH is proposed that meets these objections. The method is used to estimate a model of the risk premium on the CRSP Value-Weighted Market Index from 1962 to 1987.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\",  … [truncated]", "attempt1_tokens": {"prompt": 342, "candidates": 150, "total": 492}, "attempt1_parsed": {"summary": "This paper proposes a new approach to modeling conditional heteroskedasticity in asset returns, addressing limitations of existing GARCH models such as the inability to capture the negative correlation between current returns and future volatility, parameter restriction issues, and difficulties in interpreting shock persistence. The new method is applied to estimate the risk premium on the CRSP Value-Weighted Market Index.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The paper proposes a new model and applies it empirically, suggesting a machine learning application beyond simple linear models. It also involves forecasting the risk premium."}, "mode": "threadpool", "paper_id": "21724fc90a15e265", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Can we measure inflation expectations using Twitter?\nAbstract: Drawing on Italian tweets, we employ textual data and machine learning techniques to build new real-time measures of consumers' inflation expectations. First, we select keywords to identify tweets related to prices and expectations thereof. Second, we build a set of daily measures of inflation expectations around the selected tweets, combining the Latent Dirichlet Allocation (LDA) with a dictionary-based approach, using manually labeled bi-grams and tri-grams. Finally, we show that Twitter-based indicators are highly correlated with both monthly survey-based and daily market-based inflation expectations. Our new indicators anticipate consumers' expectations, proving to be a good real-time proxy, and provide additional information beyond market based expectations, professional forecasts, and realized inflation. The results suggest that Twitter can be a new timely source for eliciting beliefs. (C) 2022 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/so … [truncated]", "attempt1_tokens": {"prompt": 335, "candidates": 135, "total": 470}, "attempt1_parsed": {"summary": "This study explores the use of Twitter data and machine learning (LDA and dictionary-based approach) to create real-time measures of consumer inflation expectations in Italy. The developed indicators show a strong correlation with survey-based and market-based expectations, suggesting Twitter as a timely source for eliciting beliefs.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses machine learning techniques (LDA) and a dictionary-based approach to analyze textual data from Twitter for forecasting inflation expectations."}, "mode": "threadpool", "paper_id": "56598c57394f26e9", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Changes in predictive ability with mixed frequency data\nAbstract: When assessing the predictive power of financial variables for economic activity, researchers usually aggregate higher-frequency data before estimating a forecasting model that assumes the relationship between the financial variable and the dependent variable to be linear. This paper proposes a model called smooth transition mixed data sampling (STMIDAS) regression, which relaxes both of these assumptions. Simulation exercises indicate that the improvements in forecasting accuracy from the use of mixed data sampling are larger in nonlinear than in linear specifications. When forecasting output growth with financial variables in real time, statistically significant improvements over a linear regression are more likely to arise from forecasting with STMIDAS than with MIDAS regressions. (C) 2012 International Institute of Forecasters. Published by Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The arti … [truncated]", "attempt1_tokens": {"prompt": 307, "candidates": 136, "total": 443}, "attempt1_parsed": {"summary": "This paper introduces the smooth transition mixed data sampling (STMIDAS) regression model, which improves forecasting accuracy by relaxing assumptions of linear relationships and data aggregation. Simulation exercises and real-time forecasting of output growth with financial variables show that STMIDAS outperforms traditional MIDAS regressions, especially in nonlinear specifications.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The paper proposes a new model (STMIDAS) and demonstrates its empirical application in forecasting output growth, going beyond simple linear models."}, "mode": "threadpool", "paper_id": "925a15bd08c4a54b", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Climate Change and ESG: Focused on Green Bond Design\nAbstract: We investigate about climate change, which is now emerging as a hot potato, and realize its seriousness. To respond to climate change, carbon reduction strategies are vitally important. We examine the effect of carbon reduction on ESG score improvement and show the importance of ESG management by using the concept of green spread which is a financial environmental cleanliness measure. An aim of this study is to predict numerically the impact of carbon reduction on national cleanliness and economic benefits using the methodology of CO2 emission-backed securities. In order to get attention from many people to the reduction of GHG gas, we compute risk premiums of the securities and did securitization of CO2 emissions. Then, issuing the securities will stimulate investors about the national CO2 reduction activities. Also, we expect that this study gives countries an incentive to reduce their CO2 emissions and prepare for climate change.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sove … [truncated]", "attempt1_tokens": {"prompt": 326, "candidates": 151, "total": 477}, "attempt1_parsed": {"summary": "This study investigates the impact of carbon reduction strategies on ESG score improvement and national economic benefits, using the concept of green spread and CO2 emission-backed securities. It aims to numerically predict the effects of carbon reduction and incentivize countries to reduce CO2 emissions and prepare for climate change.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The study mentions predicting impacts numerically, which aligns with I3. However, it does not explicitly state the use of machine learning beyond simple linear models (I1) or focus on government/sovereign bonds (I2)."}, "mode": "threadpool", "paper_id": "1b1a8cfbbcbec1d6", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Capturing the Regime-Switching and Memory Properties of Interest Rates\nAbstract: We propose a mean-reverting interest rate model whose mean-reverting level, speed of mean-reversion and volatility are all modulated by a weak Markov chain (WMC). This model features a simple way to capture the regime-switching evolution of the parameters as well as the memory property of the data. Concentrating on the second-order WMC framework, we derive the filters of the WMC and other auxiliary processes through a change of reference probability measure. Optimal estimates of model parameters are provided by employing the EM algorithm. The h-step ahead forecasts under our proposed set-up are examined and compared with those under the usual Markovian regime-switching framework. We obtain better goodness-of-fit performance based on our numerical results generated from the implementation of WMC-based filters to a 10-year dataset of weekly short-term-maturity Canadian yield rates. Some statistical inference issues of the proposed modelling approach are also discussed.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear  … [truncated]", "attempt1_tokens": {"prompt": 353, "candidates": 200, "total": 553}, "attempt1_parsed": {"summary": "This paper introduces a new interest rate model that uses a weak Markov chain (WMC) to capture regime-switching and memory properties. The model allows for changes in mean-reversion level, speed, and volatility based on the WMC. The authors derive filters for the WMC and other processes, use the EM algorithm for parameter estimation, and provide forecasts. They demonstrate better performance compared to standard Markovian regime-switching models using Canadian yield rate data and discuss statistical inference.", "criteria_hints": {"I1": true, "I2": true, "I3": true}, "notes": "The application of the EM algorithm and the WMC framework for parameter estimation and forecasting can be considered an empirical Machine Learning application beyond simple linear models. The use of Canadian yield rates directly relates to government bonds and the yield curve. The paper explicitly discusses h-step ahead forecasts."}, "mode": "threadpool", "paper_id": "46aaeaa8f110a015", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Closed-Form Expansion, Conditional Expectation, and Option Valuation\nAbstract: Enlightened by the theory of Watanabe [Watanabe S (1987) Analysis of Wiener functionals (Malliavin calculus) and its applications to heat kernels. Ann. Probab. 15:1-39] for analyzing generalized random variables and its further development in Yoshida [Yoshida N (1992a) Asymptotic expansions for statistics related to small diffusions. J. Japan Statist. Soc. 22: 139-159], Takahashi [Takahashi A (1995) Essays on the valuation problems of contingent claims. Ph.D. thesis, Haas School of Business, University of California, Berkeley, Takahashi A (1999) An asymptotic expansion approach to pricing contingent claims. Asia-Pacific Financial Markets 6:115-151] as well as Kunitomo and Takahashi [Kunitomo N, Takahashi A (2001) The asymptotic expansion approach to the valuation of interest rate contingent claims. Math. Finance 11(1):117-151, Kunitomo N, Takahashi A (2003) On validity of the asymptotic expansion approach in contingent claim analysis. Ann. Appl. Probab. 13(3):914-952] etc., we focus on a wide range of multivariate diffusion models and propose a general probabilistic method of small-time asymptotic … [truncated]", "attempt1_tokens": {"prompt": 609, "candidates": 177, "total": 786}, "attempt1_parsed": {"summary": "This paper proposes a general probabilistic method for approximating option prices using small-time asymptotic expansions, applicable to multivariate diffusion models. It introduces an algorithm and closed-form formulas for conditional expectations of iterated stochastic integrals, demonstrating its use in various models, including options on VIX under GARCH diffusion and double lognormal stochastic volatility models. The method's robustness is theoretically supported by uniform convergence.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The paper discusses option pricing and forecasting, which aligns with I3. However, it does not appear to involve empirical machine learning applications beyond simple linear models (I1) or focus on government/sovereign bonds and the yield curve (I2)."}, "mode": "threadpool", "paper_id": "9cb31eeb7dae5c0a", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Common Risk Factors in Cryptocurrency\nAbstract: We find that three factors-cryptocurrency market, size, and momentum-capture the cross-sectional expected cryptocurrency returns. We consider a comprehensive list of price- and market-related return predictors in the stock market and construct their cryptocurrency counterparts. Ten cryptocurrency characteristics form successful long-short strategies that generate sizable and statistically significant excess returns, and we show that all of these strategies are accounted for by the cryptocurrency three-factor model. Lastly, we examine potential underlying mechanisms of the cryptocurrency size and momentum effects.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 254, "candidates": 124, "total": 378}, "attempt1_parsed": {"summary": "This study identifies three key factors (cryptocurrency market, size, and momentum) that explain expected returns in the cryptocurrency market. It tests ten cryptocurrency characteristics for their predictive power, finding that all are captured by the three-factor model, and explores the mechanisms behind the size and momentum effects.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The study uses factor models which are not considered advanced ML applications beyond simple linear models."}, "mode": "threadpool", "paper_id": "8ac5d4f4177d3e3a", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Closed-form likelihood expansions for multivariate time-inhomogeneous diffusions\nAbstract: The aim of this paper is to find approximate log-transition density functions for multivariate time-nhomogeneous diffusions in closed form. There are many empirical evidences supporting that the data generating process governing dynamics of many economics variables might vary over time because of economic climate changes or time effects. One possible way to explain the time-dependent dynamics of state variables is to model the drift or volatility terms as functions of time t as well as state variables. A way to find closed-form likelihood expansion for a multivariate time-homogeneous diffusion has been developed by Ait-Sahalia (2008). This research is built on his work and extends his results to time-inhomogeneous cases. We conduct Monte Carlo simulation studies to examine performance of the approximate transition density function when it is used to obtain ML estimates. The results reveal that our method yields a very accurate approximate likelihood function, which can be a good candidate when the true likelihood function is unavailable as is often the case. (C) 2013 Elsevier B.V. All  … [truncated]", "attempt1_tokens": {"prompt": 375, "candidates": 174, "total": 549}, "attempt1_parsed": {"summary": "This paper develops closed-form approximate log-transition density functions for multivariate time-inhomogeneous diffusions, extending previous work on time-homogeneous diffusions. The authors use Monte Carlo simulations to demonstrate the accuracy of their method for Maximum Likelihood estimation, suggesting it's a viable alternative when the true likelihood function is unknown.", "criteria_hints": {"I1": true, "I2": false, "I3": false}, "notes": "The paper uses Monte Carlo simulations to examine the performance of the approximate transition density function when used for ML estimates, which can be considered an empirical application beyond pure theory. However, it does not focus on government bonds or yield curves, nor does it explicitly mention forecasting or prediction as its primary goal."}, "mode": "threadpool", "paper_id": "ae63f674b58debe4", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Comparison of non-linear optimization algorithms for yield curve estimation\nAbstract: The yield curve is a very important financial tool used in investment and policy decisions. Its estimation from market data is essentially a non-linear optimization problem. In this paper, we compare a diversity of non-linear optimization algorithms for estimating yield curves based on actual bond market data and conclude that certain classes of algorithms are more effective due to the nature of the problem. (C) 2007 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 249, "candidates": 127, "total": 376}, "attempt1_parsed": {"summary": "This paper compares various non-linear optimization algorithms for estimating yield curves using real bond market data, finding certain algorithms to be more effective for this financial application.", "criteria_hints": {"I1": true, "I2": true, "I3": false}, "notes": "I1 is true because non-linear optimization is used on empirical data, going beyond simple linear models. I2 is true as the yield curve is explicitly mentioned. I3 is false as the paper focuses on estimation, not forecasting."}, "mode": "threadpool", "paper_id": "280680c0ee200d86", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Comparative analysis of responses of risky and safe haven assets to stock market risk before and after the yield curve inversions in the US\nAbstract: This study examines the response of safe -haven assets (gold, US dollar) and Bitcoin to market risk before and after yield curve inversions in the U.S. market. Using the VIX volatility index and static and dynamic cross-quantilogram approach, the analysis reveals that gold and the U.S. dollar act as safe havens, showing positive responses to increased VIX values, while Bitcoin behaves as a risky asset, negatively responding to higher VIX values during market turbulence. Changes in the VIX index have an immediate impact on asset price returns, but the effect diminishes over time, suggesting the need for timely updates to investment strategies. Yield curve inversions have altered the VIX-US dollar relationship: pre -inversion changes were influential in calmer markets, but post -inversion, they played a bigger role during turbulent phases, suggesting potential changes in investor behavior and market dynamics. The findings offer practical insights for investors seeking stability and protection during uncertain market conditions.\n\n … [truncated]", "attempt1_tokens": {"prompt": 371, "candidates": 153, "total": 524}, "attempt1_parsed": {"summary": "This study analyzes how gold, the US dollar, and Bitcoin respond to stock market risk (measured by VIX) before and after US yield curve inversions. Gold and the US dollar act as safe havens, while Bitcoin acts as a risky asset. The yield curve inversion appears to have altered the relationship between VIX and the US dollar, particularly during turbulent market conditions.", "criteria_hints": {"I1": false, "I2": true, "I3": false}, "notes": "The study uses a cross-quantilogram approach, which is an econometric method, not typically considered a machine learning application beyond simple linear models."}, "mode": "threadpool", "paper_id": "8a2720754be4ab08", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Combined DEMATEL technique with a novel MCDM model for exploring portfolio selection based on CAPM\nAbstract: This research proposes a novel MCDM model, including DEMATEL, ANP, and VIKOR for exploring portfolio selection based on CAPM. We probe into the influential factors and relative weights of risk-free rate, expected market return, and beta of the security. The purpose of this research is to establish an investment decision model and provides investors with a reference of portfolio selection most suitable for investing effects to achieve the greatest returns. Taking full consideration of the interrelation effects among criteria/variables of the decision model, this paper examined leading semiconductor companies spanning the hottest sectors of integrated circuit (IC) design, wafer foundry, and IC packaging by experts. Empirical findings revealed that risk-free rate was affected by budget deficit, discount rate, and exchange rate; expected market return was affected by country risk, industrial structure, and macroeconomic factors; and beta of the security was affected by firm-specific risk and financial risk. Also, the factors of the CAPM possessed a self-effect relationshi … [truncated]", "attempt1_tokens": {"prompt": 451, "candidates": 174, "total": 625}, "attempt1_parsed": {"summary": "This research introduces a new MCDM model combining DEMATEL, ANP, and VIKOR for portfolio selection based on CAPM. It analyzes the influence of risk-free rate, expected market return, and security beta, considering interrelations among criteria. The study uses expert evaluations of semiconductor companies and finds that macroeconomic factors, exchange rates, and firm-specific risk are key. The wafer foundry industry emerged as the optimal portfolio choice.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The study uses MCDM techniques which are not considered machine learning in the context of the criteria. The study is about portfolio selection and forecasting investment decisions, not government bonds or yield curves."}, "mode": "threadpool", "paper_id": "c5534bf69e592d8c", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Conditioning information and variance bounds on pricing kernels\nAbstract: Gallant, Hansen, and Tauchen (1990) show how to use conditioning information optimally to construct a sharper unconditional variance bound (the GHT bound) on pricing kernels. The literature predominantly resorts to a simple but suboptimal procedure that scales returns with predictive instruments and computes standard bounds using the original and scaled returns. This article provides a formal bridge between the two approaches. We propose an optimally scaled bound that coincides with the GHT bound when the first and second conditional moments are known. When these moments are misspecified, our optimally scaled bound yields a valid lower bound for the standard deviation of pricing kernels, whereas the GHT bound does not. We illustrate the behavior of the bounds using a number of linear and nonlinear models for consumption growth and bond and stock returns. We also illustrate how the optimally scaled bound can be used as a diagnostic for the specification of the first two conditional moments of asset returns.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theor … [truncated]", "attempt1_tokens": {"prompt": 340, "candidates": 187, "total": 527}, "attempt1_parsed": {"summary": "This article bridges the gap between the optimal conditioning information approach (GHT bound) and a common suboptimal procedure for bounding pricing kernels. It proposes an \"optimally scaled bound\" that matches the GHT bound when conditional moments are known and provides a valid lower bound for the standard deviation of pricing kernels even when these moments are misspecified. The authors demonstrate the bounds' behavior with linear and nonlinear models for consumption growth and asset returns, and suggest the optimally scaled bound as a diagnostic tool for moment specification.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The article uses linear and nonlinear models, which could be considered beyond simple linear models. It also explicitly mentions forecasting/prediction in the context of predictive instruments and conditional moments of asset returns."}, "mode": "threadpool", "paper_id": "980a360dd8f36999", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Constrained Factor Models for High-Dimensional Matrix-Variate Time Series\nAbstract: High-dimensional matrix-variate time series data are becoming widely available in many scientific fields, such as economics, biology, and meteorology. To achieve significant dimension reduction while preserving the intrinsic matrix structure and temporal dynamics in such data, Wang, Liu, and Chen proposed a matrix factor model, that is, shown to be able to provide effective analysis. In this article, we establish a general framework for incorporating domain and prior knowledge in the matrix factor model through linear constraints. The proposed framework is shown to be useful in achieving parsimonious parameterization, facilitating interpretation of the latent matrix factor, and identifying specific factors of interest. Fully utilizing the prior-knowledge-induced constraints results in more efficient and accurate modeling, inference, dimension reduction as well as a clear and better interpretation of the results. Constrained, multi-term, and partially constrained factor models for matrix-variate time series are developed, with efficient estimation procedures and their asymptotic properties. We … [truncated]", "attempt1_tokens": {"prompt": 453, "candidates": 149, "total": 602}, "attempt1_parsed": {"summary": "This article proposes a constrained matrix factor model for high-dimensional matrix-variate time series data, incorporating domain knowledge through linear constraints to improve parameterization, factor interpretation, and overall modeling accuracy. The constrained models demonstrate faster convergence rates and better out-of-sample performance compared to unconstrained models in empirical applications.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The article discusses applications and demonstrates performance through simulation studies and real-world examples, indicating an empirical ML application. It also explicitly mentions forecasting/prediction as a benefit of the proposed model."}, "mode": "threadpool", "paper_id": "f4767033c2a4eee5", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Conditional risk-return relationship in a time-varying beta model\nAbstract: We investigate the asymmetric risk-return relationship in a time-varying beta CAPM. A state space model is established and estimated by the Adaptive Least Squares with Kalman foundations proposed by McCulloch. Using SP 500 daily data from 1987:11-2003:12, we find a positive risk-return relationship in the up market (positive market excess returns) and a negative relationship in the down market (negative market excess returns). This supports the argument of Pettengill et al., who use a constant beta model. However, our model outperforms theirs by eliminating the unexplained returns and improving the accuracy of the estimated risk price.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 295, "candidates": 185, "total": 480}, "attempt1_parsed": {"summary": "This study examines the asymmetric risk-return relationship within a time-varying beta Capital Asset Pricing Model (CAPM). Employing a state space model estimated via Adaptive Least Squares with Kalman filters, and using S&P 500 daily data from 1987 to 2003, the research reveals a positive risk-return link during market upturns and a negative link during downturns. This aligns with prior findings using constant beta models but offers improved accuracy and explanation of returns.", "criteria_hints": {"I1": true, "I2": false, "I3": false}, "notes": "The study uses a state space model with Kalman filters, which can be considered an advanced statistical/econometric technique beyond simple linear models, fitting the spirit of I1."}, "mode": "threadpool", "paper_id": "465418f6b1f7d557", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Corporate Probability of Default: A Single-Index Hazard Model Approach\nAbstract: Corporate probability of default (PD) prediction is vitally important for risk management and asset pricing. In search of accurate PD prediction, we propose a flexible yet easy-to-interpret default-prediction single-index hazard model (DSI). By applying it to a comprehensive U.S. corporate bankruptcy database we constructed, we discover an interesting V-shaped relationship, indicating a violation of the common linear hazard specification. Most importantly, the single-index hazard model passes the Hosmer-Lemeshow goodness-of-fit calibration test while neither does a state-of-the-art linear hazard model in finance nor a parametric class of Box-Cox transformation survival models. In an economic value analysis, we find that this may translate to as much as three times of profit compared to the linear hazard model. In model estimation, we adopt a penalized-spline approximation for the unknown function and propose an efficient algorithm. With a diverging number of spline knots, we establish consistency and asymptotic theories for the penalized-spline likelihood estimators. Furthermore, we reexamine th … [truncated]", "attempt1_tokens": {"prompt": 422, "candidates": 182, "total": 604}, "attempt1_parsed": {"summary": "This paper introduces a single-index hazard model (DSI) for predicting corporate probability of default (PD), which is crucial for risk management and asset pricing. The model demonstrates a V-shaped relationship, outperforming traditional linear hazard models and Box-Cox transformation models in calibration tests. The study also finds that the DSI model weakens or eliminates the distress risk anomaly. The methodology involves penalized-spline approximation and establishes theoretical consistency for estimators.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses a novel hazard model and applies it to a bankruptcy database, going beyond simple linear models. It is focused on corporate default prediction, not government bonds. The core of the paper is about forecasting/prediction."}, "mode": "threadpool", "paper_id": "438204aa4a86c9c1", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Currency Value\nAbstract: We assess the properties of currency value strategies based on real exchange rates. We find that real exchange rates have predictive power for the cross-section of currency excess returns. However, adjusting real exchange rates for key country-specific fundamentals (productivity, the quality of export goods, net foreign assets, and output gaps) better isolates information related to the currency risk premium. In turn, the resultant measure of currency value displays considerably stronger predictive power for currency excess returns. Finally, the predictive information content in our currency value measure is distinct from that embedded in popular currency strategies, such as carry and momentum.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 269, "candidates": 141, "total": 410}, "attempt1_parsed": {"summary": "This study investigates currency value strategies using real exchange rates and country-specific fundamentals. It finds that adjusting real exchange rates for factors like productivity and net foreign assets enhances their predictive power for currency excess returns. The proposed currency value measure is distinct from existing strategies like carry and momentum.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The study focuses on currency excess returns and does not involve government bonds or the yield curve. While it uses predictive models, it does not explicitly mention machine learning applications beyond simple linear models."}, "mode": "threadpool", "paper_id": "17468938c29ef9d9", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Crude oil price forecasting: a biogeography-based optimization approach\nAbstract: The importance of crude oil in the world economy has made it imperative for efficient models to be designed for predicting future prices. This paper proposes an alternative approach based on a time series and biogeography-based optimization (BMMR-BBO) for the estimation of the West Texas Intermediate (WTI) crude oil price. To evaluate the forecasting ability of the presented model, we compared its performance with those of time series functions. The results of the experiment showed that BMMR-BBO performed better than the other methods and is a fairly good option for crude oil price prediction. The proposed model can be useful in the formulation of policies related to international crude oil price estimations, development plans, and industrial production.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 304, "candidates": 132, "total": 436}, "attempt1_parsed": {"summary": "This paper proposes a new model, BMMR-BBO, which combines time series analysis with biogeography-based optimization to forecast West Texas Intermediate (WTI) crude oil prices. The model outperformed traditional time series methods in experiments and is suggested for policy formulation related to oil prices.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses a machine learning approach (biogeography-based optimization) for forecasting, which goes beyond simple linear models."}, "mode": "threadpool", "paper_id": "a4d80f5bda510793", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Cross-market volatility forecasting with attention-based spatial-temporal graph convolutional networks\nAbstract: We propose a cross-market volatility forecasting framework by applying attention-based spatial-temporal graph convolutional network model (ASTGCN) to forecast future volatility of stock indices in 18 financial markets. In our work, we construct cross-market volatility networks to integrate interrelations among financial markets and the corresponding features of each market. ASTGCN combines the spatial-temporal attention mechanisms with the spatial-temporal convolutions to simultaneously capture the dynamic spatial-temporal characteristics of global volatility data. Compared with competitive models, ASTGCN exhibits superiority in multivariate predictive accuracies under multiple forecasting horizons. Our proposed framework demonstrates outstanding stability through several robustness checks. We also inspect the training process of ASTGCN by extracting spatial attention matrices and find that interrelations among global financial markets perform differently in tranquil and turmoil periods. Our study levitates empirical findings in financial networks to practical app … [truncated]", "attempt1_tokens": {"prompt": 345, "candidates": 146, "total": 491}, "attempt1_parsed": {"summary": "This paper introduces an attention-based spatial-temporal graph convolutional network (ASTGCN) for forecasting stock index volatility across 18 financial markets. The model integrates inter-market relationships and individual market features to capture dynamic spatial-temporal characteristics of global volatility data. ASTGCN outperforms competitive models in predictive accuracy and demonstrates stability. Analysis of attention matrices reveals differing inter-market relationships during tranquil and turmoil periods.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study focuses on stock market volatility, not government bonds or yield curves."}, "mode": "threadpool", "paper_id": "893197e230ff363c", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Contemporaneous threshold autoregressive models: Estimation, testing and forecasting\nAbstract: This paper proposes a contemporaneous smooth transition threshold autoregressive model (C-STAR) as a modification of the smooth transition threshold autoregressive model surveyed in Terasvirta [1998. Modelling economic relationships with smooth transition regressions. In: Ullah, A., Giles, D.E.A. (Eds.), Handbook of Applied Economic Statistics. Marcel Dekker, New York, pp. 507-552.], in which the regime weights depend on the ex ante probability that a latent regime-specific variable will exceed a threshold value. We argue that the contemporaneous model is well suited to rational expectations applications (and pricing exercises), in that it does not require the initial regimes to be predetermined. We investigate the properties of the model and evaluate its finite-sample maximum likelihood performance. We also propose a method to determine the number of regimes based on a modified Hansen [1992. The likelihood ratio test under nonstandard conditions: testing the Markov switching model of GNP. Journal of Applied Econometrics 7, S61-S82.] procedure. Furthermore, we construct multiple-st … [truncated]", "attempt1_tokens": {"prompt": 429, "candidates": 199, "total": 628}, "attempt1_parsed": {"summary": "This paper introduces the Contemporaneous Smooth Transition Threshold Autoregressive (C-STAR) model, an extension of existing STAR models. The C-STAR model's regime weights are influenced by the ex ante probability of a latent regime-specific variable exceeding a threshold. This model is particularly suitable for rational expectations and pricing applications as it doesn't necessitate predetermined initial regimes. The paper examines the model's properties, evaluates its maximum likelihood performance in finite samples, and proposes a method for determining the number of regimes using a modified Hansen procedure. It also covers multi-step ahead forecasting and presents an empirical application to short-term interest rate yields.", "criteria_hints": {"I1": true, "I2": true, "I3": true}, "notes": "The empirical application is to short-term interest rate yields, which relates to bonds and the yield curve."}, "mode": "threadpool", "paper_id": "eed3af2446ec0a3a", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Credit Spreads, Leverage and Volatility: A Cointegration Approach\nAbstract: This work documents the existence of a cointegration relationship between credit spreads, leverage and equity volatility for a large set of US companies. It is shown that accounting for the long-run equilibrium dynamic between these variables is essential to correctly explain credit spread changes. Using a novel structural model in which equity is modeled as a compound option on the firm's assets, a new methodology for estimating the unobservable market value of the firm's assets and volatility is developed. The proposed model allows to significantly reduce the pricing errors in predicting credit spreads when compared with several structural models. In terms of correlation analysis, it is shown that not accounting for the long-run equilibrium equation embedded in an error correction mechanism (ECM) results into a misspecification problem when regressing a set of explanatory variables onto the spread changes. Once credit spreads, leverage and volatility are correctly modeled, thus allowing for a long-run equilibrium, the fit of the regressions sensibly increases if compared to the results of previous  … [truncated]", "attempt1_tokens": {"prompt": 390, "candidates": 175, "total": 565}, "attempt1_parsed": {"summary": "This study investigates the cointegration relationship between credit spreads, leverage, and equity volatility for US companies. It proposes a structural model treating equity as a compound option on firm assets, enabling estimation of unobservable market values and volatility. The model improves credit spread prediction accuracy compared to existing structural models and highlights the importance of accounting for long-run equilibrium dynamics via an error correction mechanism. The findings suggest firm-specific characteristics are primary drivers of credit spread variation.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The study uses a structural model and cointegration analysis, not machine learning. It focuses on corporate credit spreads, not sovereign bonds. It explicitly addresses prediction of credit spreads."}, "mode": "threadpool", "paper_id": "fe9ab6866fa2b491", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Creating investment scheme with state space modeling\nAbstract: This paper proposes a unified approach to creating investment strategies with various desirable properties for investors. Particularly, we provide a new interpretation and the resulting formulations for state space models to attain our investment objectives, which are possibly specified as generating additional returns over benchmark stock indexes or achieving target risk-adjusted returns.Our state space models with particle filtering algorithm are employed to develop expert systems for investment strategies in highly complex financial markets. More concretely, in our state space framework, we apply a system model to representing portfolio weight processes with various constraints, as well as the standard underlying state variables such as volatility processes, Further, we formulate an observation model to stand for target value processes with non-linear functions of observed and latent variables.Numerical experiments demonstrate the effectiveness of our methodology through creating excess returns over S&P 500 and generating investment portfolios with fine risk-return profiles. (C) 2017 Elsevier Ltd. All rights r … [truncated]", "attempt1_tokens": {"prompt": 344, "candidates": 173, "total": 517}, "attempt1_parsed": {"summary": "This paper presents a unified approach using state space models and particle filtering to create investment strategies. The models aim to generate excess returns over benchmarks or achieve target risk-adjusted returns by representing portfolio weights and underlying variables like volatility. Numerical experiments show the methodology's effectiveness in outperforming the S&P 500 and creating portfolios with good risk-return profiles.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The paper uses state space models and particle filtering, which are advanced techniques beyond simple linear models, and applies them to investment strategies, indicating an empirical ML application. It also focuses on generating excess returns and achieving risk-adjusted returns, implying forecasting or prediction."}, "mode": "threadpool", "paper_id": "c924d5a2b4699539", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: De-noising option prices with the wavelet method\nAbstract: Financial time series are known to carry noise. Hence, techniques to de-noise such data deserve great attention. Wavelet analysis is widely used in science and engineering to de-noise data. In this paper we show, through the use of Monte Carlo simulations, the power of the wavelet method in the de-noising of option price data. We also find that the estimation of risk-neutral density functions and out-of-sample price forecasting is significantly improved after noise is removed using the wavelet method. (C) 2012 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 277, "candidates": 108, "total": 385}, "attempt1_parsed": {"summary": "This paper demonstrates the effectiveness of the wavelet method for de-noising option price data, showing improvements in risk-neutral density estimation and out-of-sample price forecasting through Monte Carlo simulations.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The wavelet method can be considered a form of machine learning, and the paper focuses on forecasting improvements."}, "mode": "threadpool", "paper_id": "e1aea4fb54dfddef", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Daily Stock Returns Characteristics and Forecastability\nAbstract: While stock prices and economic activity are interrelated in a nation, they are not coincident with each other. Stock prices are a leading economic indicator of the United States of America's (U.S.A. 's) economy. An economic variable that influences stock market prices is interest rates through an inverse relationship. The changes in stock prices (or stock returns) are generally caused by the demand for stocks. This paper reports on a study that investigates the underlying spectral and time-frequency characteristics of daily Standard and Poor's (S&P) 500, Dow Jones Industrial Average (DJIA), and National Association of Securities Dealers Automated Quotations (NASDAQ) composite stock returns, and changes in interest rate (namely, inverted 3-month Treasury bill). The study thereafter compared these findings with those obtained in a previous study by Joseph et al, which focused on monthly stock returns and interest rate data. Subsequent to studying stock returns and changes in interest rate that showed relatively similar spectral and frequency-time characteristics, this study investigated the forecastability of s … [truncated]", "attempt1_tokens": {"prompt": 468, "candidates": 169, "total": 637}, "attempt1_parsed": {"summary": "This study analyzes the spectral and time-frequency characteristics of daily stock returns (S&P 500, DJIA, NASDAQ) and changes in interest rates (3-month Treasury bill). It compares these findings with a previous study on monthly data. The research then investigates the forecastability of stock returns using interest rate changes over 5 and 30-day horizons, employing neural network models and finding favorable forecasts.", "criteria_hints": {"I1": true, "I2": true, "I3": true}, "notes": "The study uses neural network models, which go beyond simple linear models, and forecasts stock returns using interest rate data, which is related to government bonds and the yield curve."}, "mode": "threadpool", "paper_id": "2bd7bf35284a9fc0", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Day trading profit maximization with multi-task learning and technical analysis\nAbstract: Stock price movements are claimed to be chaotic and unpredictable, and mainstream theories of finance refute the possibility of realizing risk-free profit through predictive modelling. Despite this, a large body of technical analysis work maintains that price movements can be predicted solely from historical market data, i.e., markets are not completely efficient. In this paper we seek to test this claim empirically by developing a novel stochastic trading algorithm in the form of a linear model with a profit maximization objective. Using this method we show improvements over the competitive buy-and-hold baseline over a decade of stock market data for several companies. We further extend the approach to allow for non-stationarity in time, and using multi-task learning to modulate between individual companies and the overall market. Both approaches further improve the predictive profit. Overall this work shows that market movements do exhibit predictable patterns as captured through technical analysis.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not … [truncated]", "attempt1_tokens": {"prompt": 335, "candidates": 166, "total": 501}, "attempt1_parsed": {"summary": "This paper proposes a novel stochastic trading algorithm using multi-task learning and technical analysis to maximize profits in day trading. The algorithm, framed as a linear model with a profit maximization objective, is tested against historical stock market data and shows improvements over the buy-and-hold strategy. Extensions to handle non-stationarity and incorporate market-wide data further enhance predictive profit, suggesting that predictable patterns exist in market movements.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The article uses multi-task learning and extends a linear model, which could be considered beyond simple linear models depending on interpretation. It clearly focuses on prediction and empirical application."}, "mode": "threadpool", "paper_id": "cc76b502f40b1012", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Deep Learning and Machine Learning Insights Into the Global Economic Drivers of the Bitcoin Price\nAbstract: This study examines the connection between Bitcoin and global factors, including the VIX, the oil price, the US dollar index, the gold price, and interest rates estimated using the Federal funds rate and treasury securities rate, for forecasting analysis. Deep learning methodologies, including LSTM, GRU, CNN, and TFT, with machine learning algorithms such as XGBoost, LightGBM, and SVR, were employed to identify the optimal prediction model for the Bitcoin price. The findings indicate that the TFT model is the most successful predictive approach, with the gold price identified as the most relevant component in determining the Bitcoin price. After the gold indicator, the US dollar index was a substantial factor in the explanation of the Bitcoin price. The TFT model also included regulatory decisions and global events. It was estimated that the Bitcoin price was significantly influenced by the COVID-19 pandemic. After that, global climate events and China mining ban strongly affected the Bitcoin price. These findings indicate that regulatory decisions and global events de … [truncated]", "attempt1_tokens": {"prompt": 418, "candidates": 194, "total": 612}, "attempt1_parsed": {"summary": "This study investigates the relationship between Bitcoin prices and global economic factors (VIX, oil price, US dollar index, gold price, interest rates) using deep learning (LSTM, GRU, CNN, TFT) and machine learning (XGBoost, LightGBM, SVR) models. The TFT model proved most effective, identifying gold price and US dollar index as key drivers. The study also highlights the impact of regulatory decisions, global events (COVID-19, climate events, China mining ban), and macroeconomic factors on Bitcoin prices, with VAR analysis confirming a long-term negative influence from gold and oil prices.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study focuses on Bitcoin price prediction, not government/sovereign bonds or the yield curve."}, "mode": "threadpool", "paper_id": "bfb98861daf4485c", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Debt Intolerance: Threshold Level and Composition*\nAbstract: Fiscal vulnerabilities depend on both the level and composition of government debt. This study examines the role of debt thresholds and debt composition in driving the nonlinear behaviour of long-term interest rates through a novel approach, a panel smooth transition regression with a general logistic model. The main findings are threefold. First, the impact of the expected public debt level on interest rates rises exponentially when the share of foreign private holdings exceeds approximately 20% of government debt denominated in local currency. Second, if the public debt level exceeds a certain level, an increase in foreign private holding of government debt could raise interest rates, offsetting the downward pressure from higher market liquidity. Third, out-of-sample forecasts of this novel non-linear model are more accurate than those of previous methods.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting o … [truncated]", "attempt1_tokens": {"prompt": 310, "candidates": 171, "total": 481}, "attempt1_parsed": {"summary": "This study investigates how the level and composition of government debt influence long-term interest rates using a novel panel smooth transition regression model. It finds that foreign holdings of local currency debt above 20% amplify the impact of debt levels on interest rates, and that increased foreign holdings can raise interest rates beyond a certain debt threshold. The model also demonstrates superior out-of-sample forecasting accuracy compared to previous methods.", "criteria_hints": {"I1": true, "I2": true, "I3": true}, "notes": "The study uses a 'panel smooth transition regression with a general logistic model', which is an empirical application beyond simple linear models. It directly addresses government debt and interest rates, and explicitly mentions forecasting accuracy."}, "mode": "threadpool", "paper_id": "7ca6c07a43b0a174", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Debt-stabilizing properties of GDP-linked securities: A macro-finance perspective\nAbstract: We study the debt -stabilizing properties of indexing debt to GDP using a consumption -based macro -finance model. To this end, we derive quasi -analytical pricing formulas for any type of bond/equity by exploiting the discretization of the state -space, making large-scale simulations tractable. We find that GDP -linked security prices would embed time -varying risk premiums of about 40 basis points. For a fixed budget surplus, issuing GDP -linked securities does not imply more beneficial debt -to -GDP ratios in the long -run, while the debt -stabilizing budget surplus is more predictable at the expense of being higher. Our findings call into question the view that such securities tame debt.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 305, "candidates": 152, "total": 457}, "attempt1_parsed": {"summary": "This paper investigates the debt-stabilizing effects of GDP-linked securities using a macro-finance model. It derives pricing formulas and conducts simulations, finding that while these securities embed risk premiums, they do not necessarily lead to more beneficial debt-to-GDP ratios or tame debt effectively. The budget surplus required for debt stabilization becomes higher but more predictable.", "criteria_hints": {"I1": false, "I2": true, "I3": true}, "notes": "The study uses a macro-finance model and simulations, not empirical machine learning. It focuses on government debt and discusses debt stabilization, which implies prediction/forecasting."}, "mode": "threadpool", "paper_id": "d9568c102a3985ba", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Density estimation for nonlinear parametric models with conditional heteroscedasticity\nAbstract: This article studies density and parameter estimation problems for nonlinear parametric models with conditional heteroscedasticity. We propose a simple density estimate that is particularly useful for studying the stationary density of nonlinear time series models. Under a general dependence structure, we establish the root it consistency of the proposed density estimate. For parameter estimation, a Bahadur type representation is obtained for the conditional maximum likelihood estimate. The parameter estimate is shown to be asymptotically efficient in the sense that its limiting variance attains the Cramer-Rao lower bound. The performance of our density estimate is studied by simulations. (C) 2009 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 291, "candidates": 146, "total": 437}, "attempt1_parsed": {"summary": "This article proposes a density estimation method for nonlinear parametric models with conditional heteroscedasticity, useful for analyzing stationary densities of nonlinear time series. It establishes root-n consistency for the density estimate and obtains a Bahadur-type representation for the conditional maximum likelihood estimate, proving its asymptotic efficiency. Simulation studies are included.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The article discusses nonlinear models and density estimation, which can be considered beyond simple linear models. It also addresses parameter estimation and asymptotic efficiency, implying a predictive or forecasting aspect."}, "mode": "threadpool", "paper_id": "96b3bcf64aff81eb", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Discrete-time implementation of continuous-time filters with application to regime-switching dynamics estimation\nAbstract: This paper details the implementation in discrete time of filters for a mean-reverting model formulated under a continuous-time framework, whereby a hidden Markov chain governs the model's parameters. Parameter estimates are determined via adaptive filters designed to extract hidden information from an observable time series. An application involving the dynamic behaviour of spot interest rates is considered. More specifically, we present an empirical study aimed at capturing accurately, on the basis of some benchmarks and statistical validation, the evolution of three country-specific rates in the European zone. Our analysis reveals some similar yield-rate and risk characteristics as well as independent market behaviours of the three EU sovereign states. (C) 2019 Elsevier Ltd. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting  … [truncated]", "attempt1_tokens": {"prompt": 309, "candidates": 166, "total": 475}, "attempt1_parsed": {"summary": "This paper presents a discrete-time implementation of continuous-time filters for a mean-reverting model governed by a hidden Markov chain. It uses adaptive filters to estimate parameters from observable time series and applies this to model the dynamic behavior of spot interest rates in three European countries, revealing similarities and independent behaviors in their markets.", "criteria_hints": {"I1": true, "I2": true, "I3": false}, "notes": "The paper uses adaptive filters and hidden Markov chains, which can be considered machine learning techniques. It applies these to interest rates, which relates to sovereign bonds and the yield curve. While it estimates parameters and captures evolution, it does not explicitly focus on forecasting or prediction."}, "mode": "threadpool", "paper_id": "d6ed6a2f0e967ff5", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Demand for Information, Uncertainty, and the Response of US Treasury Securities to News\nAbstract: We use clickstream data to show that investors' demand for information about macroeconomic factors affecting the path of future interest rates is a measure of their uncertainty about this path. In particular, an increase in information demand ahead of influential economic announcements affecting investors' beliefs about future interest rates predicts a stronger reaction of U.S. Treasury note yields to these announcements, as it should if information demand positively covaries with uncertainty. This relationship does not vanish after using standard measures of uncertainty as predictors, suggesting that clickstream data contain unique information about investors' uncertainty.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 273, "candidates": 181, "total": 454}, "attempt1_parsed": {"summary": "This study utilizes clickstream data to demonstrate that increased investor demand for information regarding macroeconomic factors influencing future interest rates serves as a proxy for uncertainty about those rates. Higher information demand preceding significant economic announcements correlates with a more pronounced reaction in U.S. Treasury note yields, consistent with information demand reflecting uncertainty. This finding persists even when controlling for conventional uncertainty measures, indicating that clickstream data offers distinct insights into investor uncertainty.", "criteria_hints": {"I1": false, "I2": true, "I3": true}, "notes": "The study uses clickstream data, which could be considered a form of data analysis, but it does not explicitly mention machine learning or models beyond linear ones. It focuses on predicting the reaction of yields based on information demand, fitting the prediction criteria."}, "mode": "threadpool", "paper_id": "0f6fe1c92923c038", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Departures from Rational Expectations and Asset Pricing Anomalies\nAbstract: We investigate the potential of the consumption CAPM with pessimism, doubt, and the availability heuristic in the agent's beliefs to resolve the equity premium and risk-free rate puzzles. Using the nonlinear GMM estimation techniques, we find that doubt and the availability heuristic play an important role in explaining the cross-section of asset returns. However, when taken alone, these deviations from rational expectations cannot resolve the equity premium and risk-free rate puzzles. This result is robust to the assumption that the expected value of an uncertain prospect is nonlinear in the subjective outcome probabilities.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 271, "candidates": 192, "total": 463}, "attempt1_parsed": {"summary": "This study explores how deviations from rational expectations, specifically pessimism, doubt, and the availability heuristic, can explain asset pricing anomalies like the equity premium and risk-free rate puzzles within the consumption CAPM framework. While doubt and the availability heuristic are found to be significant in explaining asset returns, they do not fully resolve these puzzles on their own. The findings remain consistent even when considering nonlinear subjective probabilities.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The abstract mentions \"nonlinear GMM estimation techniques\" which is a statistical method, not a machine learning application. The focus is on asset pricing anomalies, not specifically government bonds or yield curves. While asset pricing involves forecasting, the abstract does not explicitly state that the article's primary focus is on forecasting or prediction methods."}, "mode": "threadpool", "paper_id": "eb29f049a6d970f8", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Diffusion copulas: Identification and estimation\nAbstract: We propose a new semiparametric approach for modelling nonlinear univariate diffusions, where the observed process is a nonparametric transformation of an underlying parametric diffusion (UPD). This modelling strategy yields a general class of semiparametric Markov diffusion models with parametric dynamic copulas and nonparametric marginal distributions. We provide primitive conditions for the identification of the UPD parameters together with the unknown transformations from discrete samples. Likelihood-based estimators of both parametric and nonparametric components are developed and we analyse their asymptotic properties. Kernel-based drift and diffusion estimators are also proposed and shown to be normally distributed in large samples. A simulation study investigates the finite sample performance of our estimators in the context of modelling US short-term interest rates. We also present a simple application of the proposed method for modelling the CBOE volatility index data. (C) 2020 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theo … [truncated]", "attempt1_tokens": {"prompt": 333, "candidates": 238, "total": 571}, "attempt1_parsed": {"summary": "This paper introduces a novel semiparametric method for modeling nonlinear univariate diffusions, where the observed process is a nonparametric transformation of an underlying parametric diffusion. This approach results in a class of semiparametric Markov diffusion models with parametric dynamic copulas and nonparametric marginal distributions. The authors establish conditions for identifying UPD parameters and unknown transformations from discrete samples, develop likelihood-based estimators for both parametric and nonparametric components, and analyze their asymptotic properties. Kernel-based drift and diffusion estimators are also proposed and shown to have normal distributions in large samples. The study includes a simulation to assess the finite sample performance of these estimators in modeling US short-term interest rates and an application to CBOE volatility index data.", "criteria_hints": {"I1": true, "I2": false, "I3": false}, "notes": "The paper uses a semiparametric approach which can be considered a machine learning application. It models interest rates and volatility index data, which are not government bonds or the yield curve. The focus is on modeling and estimation, not forecasting."}, "mode": "threadpool", "paper_id": "19c5432dca3228fd", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Digital twin-based cyber-physical manufacturing systems, extended reality metaverse enterprise and production management algorithms, and Internet of Things financial and labor market technologies in generative artificial intelligence economics\nAbstract: Research background: Generative artificial intelligence (AI) and machine learning algorithms support industrial Internet of Things (IoT)-based big data and enterprise asset management in multiphysics simulation environments by industrial big data processing, modeling, and monitoring, enabling business organizational and managerial practices. Machine learning-based decision support and edge generative AI sensing systems can reduce persistent labor shortages and job vacancies and power productivity growth and labor market dynamics, shaping career pathways and facilitating occupational transitions by skill gap identification and laborintensive manufacturing job automation by path planning and spatial cognition algorithms, furthering theoretical implications for management sciences. Generative AI fintech, machine learning algorithms, and behavioral analytics can assist multi-layered payment and transaction processing screening wi … [truncated]", "attempt1_tokens": {"prompt": 768, "candidates": 180, "total": 948}, "attempt1_parsed": {"summary": "This article explores the integration of generative AI, machine learning, digital twins, extended reality, and IoT within manufacturing and financial systems. It discusses how these technologies can optimize production, manage assets, and improve financial operations, including fraud detection and algorithmic trading. The paper also addresses the potential impacts on the labor market, such as job displacement and skill gaps, and outlines the systematic review methods used to analyze relevant literature.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The article discusses machine learning applications in various domains, including financial forecasting and production optimization, which aligns with I1 and I3. However, it does not focus on government/sovereign bonds or the yield curve, making I2 false."}, "mode": "threadpool", "paper_id": "e24f1b7a54c54554", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Do leading indicators forecast US recessions? A nonlinear re-evaluation using historical data\nAbstract: This paper analyses to what extent a selection of leading indicators is able to forecast U.S. recessions, by means of both dynamic probit models and Support Vector Machine (SVM) models, using monthly data from January 1871 to June 2016. The results suggest that the probit models predict U.S. recession periods more accurately than SVM models up to six months ahead, while the SVM models are more accurate over longer horizons. Furthermore, SVM models appear to distinguish between recessions and tranquil periods better than probit models do. Finally, the most accurate forecasting models are those that include oil, stock returns and the term spread as leading indicators.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 302, "candidates": 193, "total": 495}, "attempt1_parsed": {"summary": "This paper evaluates the forecasting ability of leading indicators for US recessions using dynamic probit and Support Vector Machine (SVM) models from 1871 to 2016. Probit models are more accurate for short-term forecasts (up to six months), while SVM models perform better for longer horizons. SVM models also better distinguish recessions from non-recessionary periods. The study identifies oil, stock returns, and the term spread as the most effective leading indicators.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses SVM models, which are non-linear, and compares them to probit models. It focuses on forecasting recessions, not specifically government bonds or the yield curve, although the term spread is mentioned as an indicator."}, "mode": "threadpool", "paper_id": "e8d85a38742f329b", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Do Foreign Yield Curves Predict US Recessions and GDP Growth?\nAbstract: Foreign term spreads constructed from bond yields of non-U.S. G-7 constituents predict future U.S. recessions and foreign term spreads are stronger predictors of U.S. recessions occurring within the next year than U.S. term spreads. U.S. and foreign term spreads are both informative of the U.S. economy but over different horizons and for different components of economic activity. Smaller U.S. term spreads lead to smaller foreign term spreads and U.S. Dollar appreciation. Smaller foreign term spreads do not lead to significant U.S. Dollar depreciation but do lead to persistent declines in U.S. exports and foreign direct investment (FDI) flows into the United States. These findings are consistent with the proposition that foreign term spreads embed growth spillovers from the U.S. and the resulting Dollar strength and slowdown abroad spill back to the United States.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is abo … [truncated]", "attempt1_tokens": {"prompt": 343, "candidates": 232, "total": 575}, "attempt1_parsed": {"summary": "This study investigates whether foreign yield curves can predict U.S. recessions and GDP growth. It finds that foreign term spreads are stronger predictors of upcoming U.S. recessions than domestic ones. Both U.S. and foreign term spreads offer insights into the U.S. economy, but at different time horizons and for different economic activities. Changes in U.S. term spreads influence foreign term spreads and the U.S. dollar, while foreign term spreads impact U.S. exports and FDI. These results support the idea that foreign term spreads reflect U.S. growth spillovers, which then feedback into the U.S. economy.", "criteria_hints": {"I1": false, "I2": true, "I3": true}, "notes": "The abstract does not mention specific machine learning models beyond simple linear ones, hence I1 is false. The study focuses on bond yields and the yield curve, making I2 true. The core of the study is prediction, thus I3 is true."}, "mode": "threadpool", "paper_id": "10aa5400853f1e0c", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Dynamic semiparametric factor models in risk neutral density estimation\nAbstract: Dynamic semiparametric factor models (DSFM) simultaneously smooth in space and are parametric in time, approximating complex dynamic structures by time invariant basis functions and low dimensional time series. In contrast to traditional dimension reduction techniques, DSFM allows the access of the dynamics embedded in high dimensional data through the lower dimensional time series. In this paper, we study the time behavior of risk assessments from investors facing random financial payoffs. We use DSFM to estimate risk neutral densities from a dataset of option prices on the German stock index DAX. The dynamics and term structure of risk neutral densities are investigated by Vector Autoregressive (VAR) methods applied on the estimated lower dimensional time series.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 296, "candidates": 194, "total": 490}, "attempt1_parsed": {"summary": "This paper introduces Dynamic Semiparametric Factor Models (DSFM) for estimating risk-neutral densities from option prices. DSFM models complex dynamics using time-invariant basis functions and low-dimensional time series, allowing insights into high-dimensional data. The study applies DSFM to DAX index option prices to analyze the dynamics and term structure of risk-neutral densities using Vector Autoregressive (VAR) methods on the estimated time series.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The paper uses DSFM, which is a form of dimension reduction and modeling that can be considered beyond simple linear models. It focuses on risk-neutral densities derived from option prices, which is related to forecasting investor risk assessments. It does not directly concern government bonds or the yield curve."}, "mode": "threadpool", "paper_id": "cfa12aa6640fe808", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Distressed debt prices and recovery rate estimation\nAbstract: This paper has two purposes. First, it uses distressed debt prices to estimate recovery rates at default. In this regard, estimates are obtained for three recovery rate models: recovery of face value, recovery of Treasury, and recovery of market value. We show that identifying the economic default date, as distinct from the recorded default date, is crucial to obtaining unbiased estimates. The economic default date is defined to be the first date when the market prices the firm's debt as if it has defaulted. An implication is that the standard industry practice of using 30-day post default prices to compute recovery rate yields biased estimates. Second, we construct and estimate a distressed debt pricing model. We use this model to implicitly estimate the parameters of the embedded recovery rate process and to price distressed debt. Our distressed debt pricing model fits market prices well, with an average pricing error of less than one basis point.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about … [truncated]", "attempt1_tokens": {"prompt": 336, "candidates": 189, "total": 525}, "attempt1_parsed": {"summary": "This paper estimates recovery rates at default using distressed debt prices and proposes three models for recovery rates: recovery of face value, Treasury, and market value. It highlights the importance of identifying the economic default date over the recorded one for unbiased estimates and suggests that standard industry practices using post-default prices are biased. The study also develops and estimates a distressed debt pricing model, which implicitly estimates recovery rate parameters and prices distressed debt, showing a high degree of accuracy.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The paper focuses on estimating recovery rates and pricing distressed debt, which involves prediction of future recovery values. It does not appear to use machine learning beyond simple linear models, nor does it focus on government/sovereign bonds or the yield curve."}, "mode": "threadpool", "paper_id": "8d2944af2307c048", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Dynamics of Money Market Interest Rates in Ghana: Time-Frequency Analysis of Volatility Spillovers\nAbstract: As the second longest practicing inflation targeting economy in Africa, it is of interest to investigate the degree to which policy interest rate influences other money market rates so as to gauge the overall effectiveness of monetary policy transmission in Ghana. This study evaluates the degree of connectedness among money market rates and also determines the most dominant money market rate(s) in Ghana. The basic finding is that the monetary policy rate has a low-to-moderate influence on volatility dynamics of other money market rates in Ghana across historical time-interval and time-frequency domains. This is a reflection of a generally weak capability of policy interest rate to drive other market rates in Ghana. Both monetary policy rate and Treasury bill rate are net transmitters of shocks, while interbank, lending and saving rates are net receivers of shocks in the money market. However, the Treasury bill emerges as the largest shock transmitter in the money market, across all forecast horizons and analytical domains. The lending rate is the largest shock recipie … [truncated]", "attempt1_tokens": {"prompt": 422, "candidates": 195, "total": 617}, "attempt1_parsed": {"summary": "This study analyzes the influence of Ghana's policy interest rate on other money market rates using time-frequency analysis of volatility spillovers. It finds a low-to-moderate influence, indicating weak monetary policy transmission. The Treasury bill rate is the largest shock transmitter, while the lending rate is the largest recipient, suggesting fiscal dominance. The study emphasizes the need for policy reforms to strengthen transmission.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The study uses time-frequency analysis and volatility spillovers, which are advanced econometric techniques but not explicitly machine learning as defined by the criteria. It focuses on money market rates rather than government/sovereign bonds or the yield curve. While it analyzes dynamics and spillovers, the primary focus is not on forecasting or prediction."}, "mode": "threadpool", "paper_id": "116d6c5d4db5f1b9", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Dynamic spillovers and linkages between gold, crude oil, S&P 500, and other economic and financial variables. Evidence from the USA\nAbstract: This paper focuses on the price determinants of gold, and on the challenges associated with gold's safe haven property. Specifically, it analyses the interlinkages and the return spillover effect among gold, crude oil, S&P 500, dollar exchange rate, Consumer Price Index (CPI), economic policy uncertainty and Treasury bills, by employing a Vector Autoregression (VAR) and the spillover index of Diebold and Yilmaz (2012), Diebold and Yilmaz (2014). Monthly realized return series, covering the period from 2nd of January 1986 to 31st of December 2019 are used to examine the short-run linkages, and the return spillovers rolling-window estimates in analyzing the transmission mechanism in a time-varying fashion, respectively. Our findings identify gold as a strong dollar hedge, while crude oil and Treasury bills appear to drive inflation; they also indicate strong spillover effects between exchange rate and gold returns. In general, co-movement dynamics display state-dependent characteristics. Both total and directional spillovers increase sig … [truncated]", "attempt1_tokens": {"prompt": 530, "candidates": 217, "total": 747}, "attempt1_parsed": {"summary": "This paper investigates the dynamic spillovers and return transmission mechanisms among gold, crude oil, S&P 500, dollar exchange rate, CPI, economic policy uncertainty, and Treasury bills using VAR and spillover index models. It finds gold acts as a dollar hedge, crude oil and Treasury bills influence inflation, and significant spillovers exist between exchange rates and gold. Spillovers intensify during financial crises, with economic policy uncertainty, stock returns, and crude oil being key transmitters, and Treasury bills and CPI being key recipients. Gold and exchange rates exhibit dual roles as transmitters and receivers.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The study uses VAR and spillover index models, which are not considered machine learning applications beyond simple linear models. It does not focus on government/sovereign bonds or the yield curve, nor does it primarily focus on forecasting or prediction."}, "mode": "threadpool", "paper_id": "ce15cb2f2a6fe0d0", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: ESTIMATING AN UPPER BOUND ON THE PRATT RISK-AVERSION COEFFICIENT WHEN THE UTILITY FUNCTION IS UNKNOWN\nAbstract: None\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 175, "candidates": 101, "total": 276}, "attempt1_parsed": {"summary": "This paper focuses on estimating an upper bound for the Pratt risk-aversion coefficient when the utility function is unknown. The abstract does not provide further details on the methodology or specific applications.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "Abstract is missing, so no specific information is available to assess the criteria."}, "mode": "threadpool", "paper_id": "ae8ad37dda607103", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: ECONOMIC INJURY LEVEL, ACTION THRESHOLD, AND A YIELD-LOSS MODEL FOR THE PEA APHID, ACYRTHOSIPHON-PISUM (HOMOPTERA, APHIDIDAE), ON GREEN PEAS, PISUM-SATIVUM\nAbstract: Economic injury level, action threshold, and population development studies with the pea aphid (PA), Acyrthosiphon pisum (Harris), were conducted during 1983-85 [Washington, USA]. Pea aphid densities, simulating those in commercial pea fields, were established using insecticides to manipulate infestation levels. Three experiments, incorporating 12 treatments and six replications, were analyzed. A generalized, nonlinear equation relating pea yield to accumulated aphid feeding days (AFD) is described. The model approximates two phases of a sigmoid infestation-yield curve. An upper maximum plateau and a region of rapidly decreasing yield are approximate. Beyond 1,800 aFD, a lower minimum yield plateau is hypothesized. Economic injury levels calculated for the 3 years'' experiments using the generalized model were 22.2, 18.2, and 12.2 AFD, respectively. Action threshold estimates were determined from linear regression estimates of yield versus aphids per plant at bloom. Action thresholds were 3.6, 0.3, and 0.3 aphid … [truncated]", "attempt1_tokens": {"prompt": 509, "candidates": 174, "total": 683}, "attempt1_parsed": {"summary": "This study establishes economic injury levels and action thresholds for pea aphids on green peas by developing a yield-loss model based on accumulated aphid feeding days (AFD). The model accounts for different phases of infestation-yield response, and calculated economic injury levels varied across the study years. Action thresholds were also determined, and high aphid densities were found to affect pea quality components like tenderometer readings and sieve size, but not protein content.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The study uses a generalized, nonlinear equation to model yield loss, which could be considered a form of predictive modeling, but not machine learning in the modern sense. It does not involve government bonds."}, "mode": "threadpool", "paper_id": "d116e668a6034e16", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Efficient importance sampling maximum likelihood estimation of stochastic differential equations\nAbstract: Maximum likelihood estimation (MLE) of stochastic differential equations (SDEs) is difficult because in general the transition density function of these processes is not known in closed form, and has to be approximated somehow. An approximation based on efficient importance sampling (EIS) is detailed. Monte Carlo experiments, based on widely used diffusion processes, evaluate its performance against an alternative importance sampling (IS) strategy, showing that EIS is at least equivalent, if not superior, while allowing a greater flexibility needed when examining more complicated models. (C) 2010 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 278, "candidates": 151, "total": 429}, "attempt1_parsed": {"summary": "This paper proposes an efficient importance sampling (EIS) method for maximum likelihood estimation (MLE) of stochastic differential equations (SDEs) when the transition density function is unknown. The EIS method is evaluated through Monte Carlo experiments using diffusion processes and compared to an alternative importance sampling (IS) strategy, demonstrating its effectiveness and flexibility for complex models.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The paper focuses on statistical estimation methods for SDEs, which can be applied to forecasting, but does not involve machine learning applications beyond basic statistical modeling or government bonds."}, "mode": "threadpool", "paper_id": "b9ed21f0c6e8f1fe", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Equilibrium forward curves for commodities\nAbstract: We develop an equilibrium model of the term structure of forward prices for storable commodities. As a consequence of a nonnegativity constraint on inventory, the spot commodity has an embedded timing option that is absent in forward contracts. This option's value changes over time due to both endogenous inventory and exogenous transitory shocks to supply and demand. Our model makes predictions about Volatilities of forward prices at different horizons and shows how conditional violations of the Samuelson effect occur. We extend the model to incorporate a permanent second factor and calibrate the model to crude oil futures data.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 266, "candidates": 122, "total": 388}, "attempt1_parsed": {"summary": "This paper presents an equilibrium model for commodity forward prices, considering inventory non-negativity and its impact on timing options. It analyzes volatility and the Samuelson effect, extends the model with a second factor, and calibrates it to crude oil futures data.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The paper discusses forecasting and prediction related to commodity prices, but does not involve machine learning applications or government bonds."}, "mode": "threadpool", "paper_id": "02d7fb5a3d6611e6", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: ESG in the headlines: Media-driven reputational risk and stock performance\nAbstract: This study examines the impact of environmental, social, and governance (ESG) reputational risks on stock performance. We use a unique dataset of media-driven ESG reputational risk indicators, covering 4963 Chinese firms from 2009 to 2023. On average, a one-standard-deviation increase in ESG reputational risks is associated with a 4.5 % decrease in simple stock returns, a 14.5 % reduction in excess stock returns relative to the market index, and a 12.2% decline in excess stock returns compared to peer firms of similar size. These negative effects contradict the traditional risk-return relationship predicted by risk premium theory. Further analysis identifies reduced investor confidence and tighter financing constraints as key mechanisms through which ESG reputational risks negatively affect stock returns. Heterogeneity analyses indicate that the negative impact is more pronounced for firms in non-pollution-intensive industries, those facing financing difficulties, and those exposed to environment-related reputational risks.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learnin … [truncated]", "attempt1_tokens": {"prompt": 367, "candidates": 169, "total": 536}, "attempt1_parsed": {"summary": "This study investigates how media-driven ESG reputational risks affect stock performance in Chinese firms. It finds that increased ESG risks lead to significant decreases in stock returns, contradicting traditional risk-return theories. The study identifies reduced investor confidence and financing constraints as key mechanisms and notes that the impact is stronger for certain types of firms.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The study uses a unique dataset and statistical analysis but does not mention machine learning applications beyond simple linear models. It focuses on ESG risks and stock performance, not government bonds or yield curves. While it analyzes effects, the primary focus is not forecasting or prediction in a machine learning context."}, "mode": "threadpool", "paper_id": "03cc69643da21feb", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Equity market valuation, systematic risk and monetary policy\nAbstract: This study examines the relationship between equity market valuation and risk indicators that portend economic downswings. The indicators are implied options volatility, Treasury-Eurodollar (TED) spread and exchange rate. While implied volatility captures market risk in that it reflects the fear factor embedded in the price of an option, TED spread reflects the default risk premium that is priced into a key short-term credit instrument. Equity markets often show a tendency to reflect the incidence of these risk factors. And because they provide valuable information about the health of the economy, many have argued that equity market valuation be taken into account in the formulation of monetary policy. Results of this study not only show a statistically significant inverse relationship between the stock market and these risk factors, but also evidence of a cointegration. In a variance decomposition of the series, we find that equity valuation is a major contributor to the forecast error variances of each of the risk indicators, a finding that lends tacit support to the argument that risk indicators associ … [truncated]", "attempt1_tokens": {"prompt": 366, "candidates": 156, "total": 522}, "attempt1_parsed": {"summary": "This study investigates the link between equity market valuation and risk indicators (implied options volatility, TED spread, exchange rate) that signal economic downturns. It finds a significant inverse relationship and cointegration between stock market and these risk factors. The findings suggest that equity market valuation significantly contributes to the forecast error variances of risk indicators, supporting its consideration in monetary policy.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The study discusses forecasting and prediction in the context of economic downturns and monetary policy, but does not appear to use machine learning or focus on government bonds/yield curves."}, "mode": "threadpool", "paper_id": "f3a9a0a79144a932", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Estimating Interest Rate Curves by Support Vector Regression\nAbstract: A model that seeks to estimate an interest rate curve should have two desirable capabilities in addition to the usual characteristics required from any function-estimation model: it should incorporate the bid-ask spreads of the securities from which the curve is extracted and restrict the curve shape. The goal of this article is to estimate interest rate curves by using Support Vector Regression (SVR), a method derived from the Statistical Learning Theory developed by Vapnik (1995). The motivation is that SVR features these extra capabilities at a low estimation cost. The SVR is specified by a loss function, a kernel function and a smoothing parameter. SVR models the daily U.S. dollar interest rate swap curves, from 1997 to 2001. As expected from a priori and sensibility analyses, the SVR equipped with the kernel generating a spline with an infinite number of nodes was the best performing SVR. Comparing this SVR with other models, it achieved the best cross-validation interpolation performance in controlling the bias-variance trade-off and generating the lowest error considering the desired accuracy fixed … [truncated]", "attempt1_tokens": {"prompt": 386, "candidates": 157, "total": 543}, "attempt1_parsed": {"summary": "This article estimates interest rate curves using Support Vector Regression (SVR), a method that incorporates bid-ask spreads and restricts curve shape. The SVR model, particularly with a spline-generating kernel, demonstrated superior performance in cross-validation, controlling bias-variance trade-off and minimizing error compared to other models.", "criteria_hints": {"I1": true, "I2": false, "I3": false}, "notes": "The article uses SVR for interest rate curve estimation, which is an empirical ML application beyond linear models. However, it focuses on swap curves, not government/sovereign bonds, and is about estimation rather than forecasting."}, "mode": "threadpool", "paper_id": "c621221de8365a99", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Empirical evaluation of the market price of risk using the CIR model\nAbstract: We describe a simple but effective method for the estimation of the market price of risk. The basic idea is to compare the results obtained by following two different approaches in the application of the Cox-Ingersoll-Ross (CIR) model. In the first case, we apply the non-linear least squares method to cross sectional data (i.e., all rates of a single day). In the second case, we consider the short rate obtained by means of the first procedure as a proxy or the real market short rate. Starting from this new proxy, we evaluate the parameters of the CIR model by means of martingale estimation techniques. The estimate of the market price of risk is provided by comparing results obtained with these two techniques, since this approach makes possible to isolate the market price of risk and evaluate, under the Local Expectations Hypothesis, the risk premium given by the market for different maturities. As a test case, we apply the method to data of the European Fixed Income Market. (c) 2006 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application … [truncated]", "attempt1_tokens": {"prompt": 373, "candidates": 174, "total": 547}, "attempt1_parsed": {"summary": "This paper proposes a method to estimate the market price of risk using the Cox-Ingersoll-Ross (CIR) model by comparing two estimation approaches: non-linear least squares on cross-sectional data and martingale estimation techniques using a proxy for the short rate. The method is tested on the European Fixed Income Market.", "criteria_hints": {"I1": false, "I2": true, "I3": false}, "notes": "The study uses the CIR model, which is a theoretical model, and applies statistical estimation techniques. It does not appear to involve machine learning algorithms beyond standard statistical methods. While it deals with fixed income markets and yield curves, the primary focus is on estimating the market price of risk rather than forecasting or prediction."}, "mode": "threadpool", "paper_id": "03ea7ad65ff3f52e", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Estimating dynamic equilibrium models using mixed frequency macro and financial data\nAbstract: We provide a framework for inference in dynamic equilibrium models including financial market data at daily frequency, along with macro series at standard lower frequency. Our formulation of the macro finance model in continuous time conveniently accounts for the difference in observation frequency. We suggest the use of martingale estimating functions (MEF) to infer the structural parameters of the model directly through a nonlinear scheme. This method is compared to regression-based methods and the generalized method of moments (GMM). We illustrate our approaches by estimating various versions of the AK-Vasicek model with mean-reverting interest rates. We provide asymptotic theory and Monte Carlo evidence on the small sample behavior of the estimators and report empirical estimates using 30 years of US macro and financial data. (C) 2016 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield  … [truncated]", "attempt1_tokens": {"prompt": 325, "candidates": 151, "total": 476}, "attempt1_parsed": {"summary": "This paper proposes a framework for estimating dynamic equilibrium models using mixed-frequency macro and financial data. It utilizes continuous-time modeling and martingale estimating functions (MEF) for parameter inference, comparing this approach to regression-based methods and GMM. The authors illustrate their methods with versions of the AK-Vasicek model and provide empirical estimates using US data.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The abstract does not mention machine learning applications, government bonds, or forecasting. The focus is on econometric modeling and estimation techniques for macro-financial data."}, "mode": "threadpool", "paper_id": "8407ab83849bf0fa", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Estimating and testing non-affine option pricing models with a large unbalanced panel of options\nAbstract: In this paper, we considerjoint estimation of objective and risk-neutral parameters for stochastic volatility option pricing models using both stock and option prices. A common strategy simplifies the task by limiting the analysis to just one option per date. We first discuss its drawbacks on the basis of model interpretation, estimation results and pricing exercises. We then turn the attention to a more flexible approach, that successfully exploits the wealth of information contained in large heterogeneous panels of options, and we apply it to actual S&P 500 index and index call options data. Our approach breaks the stochastic singularity between contemporaneous option prices by assuming that every observation is affected by measurement error, essentially recasting the problem as a non-linear filtering one. The resulting likelihood function is evaluated using a Monte Carlo Importance Sampling (MC-IS) strategy, combined with a Particle Filter algorithm. The results provide useful intuitions on the directions that should be followed to extend the model, in particular by  … [truncated]", "attempt1_tokens": {"prompt": 366, "candidates": 199, "total": 565}, "attempt1_parsed": {"summary": "This paper proposes a method for jointly estimating parameters for stochastic volatility option pricing models using both stock and option prices. It addresses the limitations of simpler approaches that use only one option per date and instead utilizes large, heterogeneous panels of options. The method treats option price observations as having measurement errors, reframing the problem as a non-linear filtering task. A Particle Filter and Monte Carlo Importance Sampling are used to evaluate the likelihood function. The results offer insights for extending the model, such as incorporating jumps or regime switching in volatility.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The paper uses advanced statistical and computational methods (non-linear filtering, MC-IS, Particle Filter) for option pricing, which can be considered beyond simple linear models and an empirical application. It also involves forecasting option prices."}, "mode": "threadpool", "paper_id": "85e46487d751331e", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Estimation of Parameters in Mean-Reverting Stochastic Systems\nAbstract: Stochastic differential equation (SDE) is a very important mathematical tool to describe complex systems in which noise plays an important role. SDE models have been widely used to study the dynamic properties of various nonlinear systems in biology, engineering, finance, and economics, as well as physical sciences. Since a SDE can generate unlimited numbers of trajectories, it is difficult to estimatemodel parameters based on experimental observationswhichmay represent only one trajectory of the stochastic model. Although substantial research efforts have been made to develop effective methods, it is still a challenge to infer unknown parameters in SDE models from observations that may have large variations. Using an interest rate model as a test problem, in this work we use the Bayesian inference and Markov Chain Monte Carlo method to estimate unknown parameters in SDE models.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"Th … [truncated]", "attempt1_tokens": {"prompt": 316, "candidates": 156, "total": 472}, "attempt1_parsed": {"summary": "This paper proposes using Bayesian inference and Markov Chain Monte Carlo methods to estimate parameters in mean-reverting stochastic systems, particularly when dealing with limited observational data from a single trajectory. The authors test their approach on an interest rate model.", "criteria_hints": {"I1": true, "I2": true, "I3": true}, "notes": "The abstract mentions an 'interest rate model' which falls under I2. The methods used (Bayesian inference, MCMC) are empirical and go beyond simple linear models, fitting I1. The goal of parameter estimation in SDEs is inherently related to forecasting and prediction, thus fitting I3."}, "mode": "threadpool", "paper_id": "e3fc14003bf395e3", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: European spreads at the interest rate lower bound\nAbstract: This paper analyzes the effect of the interest rate lower bound on long-term sovereign bond spreads in the euro area. We specify a joint shadow rate term structure model for the risk-free, the German, and the Italian sovereign yield curves. In our model, the behavior of long-term spreads becomes strongly nonlinear in the underlying factors when interest rates are close to the lower bound, which occurs in the data since the beginning of 2012. We fit the model via Quasi-Maximum Likelihood and show three consequences of the nonlinear behavior of sovereign spreads: (i) they are asymmetrically distributed, (ii) they are affected by (possibly exogenous) changes in the lower bound, and (iii) they become less informative about sovereign risk than when interest rates are far from the lower bound. Shadow spreads, however, still provide reliable information. (C) 2020 The Author(s). Published by Elsevier B.V.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I … [truncated]", "attempt1_tokens": {"prompt": 349, "candidates": 192, "total": 541}, "attempt1_parsed": {"summary": "This paper examines how the interest rate lower bound impacts long-term sovereign bond spreads in the euro area using a joint shadow rate term structure model for risk-free, German, and Italian sovereign yield curves. The model reveals nonlinear behavior of spreads near the lower bound, leading to asymmetric distributions, sensitivity to lower bound changes, and reduced informativeness about sovereign risk. Shadow spreads, however, remain reliable.", "criteria_hints": {"I1": true, "I2": true, "I3": false}, "notes": "The paper uses a \"joint shadow rate term structure model\" which implies a sophisticated statistical/econometric approach beyond simple linear models, fitting the spirit of I1. While it discusses the informativeness of spreads, it doesn't explicitly focus on forecasting or prediction as its primary goal, hence I3 is false."}, "mode": "threadpool", "paper_id": "12936628b2b341f7", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Estimating the Equity Premium\nAbstract: Existing empirical research investigating the size of the equity premium has largely consisted of a series of innovations around a common theme: producing a better estimate of the equity premium by using better data or a better estimation technique. The equity premium estimate that emerges from most of this work matches one moment of the data alone: the mean difference between an estimate of the return to holding equity and a risk-free rate. We instead match multiple moments of U.S. market data, exploiting the joint distribution of the dividend yield, return volatility, and realized excess returns, and find that the equity premium lies within 50 basis points of 3.5%, a range much narrower than was achieved in previous studies. Additionally, statistical tests based on the joint distribution of these moments reveal that only those models of the conditional equity premium that embed time variation, breaks, and/or trends are supported by the data. In order to develop the joint distribution of the dividend yield, return volatility, and excess returns, we need a model of price and return fundamentals. We document that even recently developed … [truncated]", "attempt1_tokens": {"prompt": 413, "candidates": 220, "total": 633}, "attempt1_parsed": {"summary": "This study estimates the equity premium by matching multiple moments of U.S. market data, including the dividend yield, return volatility, and realized excess returns. The authors find a narrower range for the equity premium (within 50 basis points of 3.5%) than previous studies and suggest that models supporting time variation, breaks, and/or trends are supported by the data. They also explore various models, including those requiring numerical solution methods and simulated method of moments estimation, due to limitations of existing analytically tractable models.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The study uses advanced statistical methods (simulated method of moments) but does not appear to involve machine learning as typically defined in empirical applications beyond simple linear models. It focuses on the equity premium, not government bonds or the yield curve. While it discusses models of the conditional equity premium, the primary focus is estimation rather than forecasting."}, "mode": "threadpool", "paper_id": "49367fe67a852c41", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Experiments on the application of IOHMMs to model financial returns series\nAbstract: Input-output hidden Markov models (IOHMMs) are conditional hidden Markov models in which the emission (and possibly the transition) probabilities can be conditioned on an input sequence. For example, these conditional distributions can be linear, logistic, or nonlinear (using for example multilayer neural networks), We compare the generalization performance of several models which are special cases of input-output hidden Markov models on financial time-series prediction tasks: an unconditional Gaussian, a conditional linear Gaussian, a mixture of Gaussians, a mixture of conditional linear Gaussians, a hidden Markov model, and various IOHMMs. The experiments compare these models on predicting the conditional density of returns of market and sector indices. Note that the unconditional Gaussian estimates the first moment with the historical average. The results show that, although for the first moment the historical average gives the best results, for the higher moments, the IOHMMs yielded significantly better performance, as estimated by the out-of-sample likelihood.\n\nCriteria: {\"I1\": \"The art … [truncated]", "attempt1_tokens": {"prompt": 359, "candidates": 153, "total": 512}, "attempt1_parsed": {"summary": "This study compares the performance of various input-output hidden Markov models (IOHMMs) and their special cases for predicting financial time-series returns. While simple models like the historical average performed best for the first moment, IOHMMs significantly outperformed them for higher moments, as measured by out-of-sample likelihood.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study focuses on financial returns prediction using IOHMMs, which are more complex than simple linear models. It does not specifically address government/sovereign bonds or the yield curve."}, "mode": "threadpool", "paper_id": "e2d7c330b34a3f2e", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Evolving Fuzzy-GARCH Approach for Financial Volatility Modeling and Forecasting\nAbstract: Volatility modeling and forecasting play a key role in asset allocation, risk management, derivatives pricing and policy making. The purpose of this paper is to develop an evolving fuzzy-GARCH modeling approach for stock market asset returns forecasting. The method addresses GARCH volatility modeling within the framwork of evolving fuzzy systems. This hybrid methodology aims to account for time-varying volatility, from GARCH approach, as well as volatility clustering and nonlinear time series identification, from evolving fuzzy systems, which use time-varying data streams to continuously and simultaneously adapt the structure and functionality of fuzzy models. The motivation is to improve model performance as new data is input through gradual model construction, inducing model adaptation and refinement without catastrophic forgetting while keeping current model useful. An empirical application includes the forecasting of S&P 500 and Ibovespa indexes by the evolving fuzzy-GARCH against traditional GARCH-family models and a fuzzy GJR-GARCH methodology. The results indicate the high potent … [truncated]", "attempt1_tokens": {"prompt": 391, "candidates": 201, "total": 592}, "attempt1_parsed": {"summary": "This paper proposes an evolving fuzzy-GARCH model for stock market volatility forecasting, combining GARCH's time-varying volatility with evolving fuzzy systems' ability to adapt to nonlinear time series and data streams. The model aims to improve forecasting by continuously adapting without forgetting past information. Empirical results on S&P 500 and Ibovespa indexes suggest the evolving fuzzy-GARCH model outperforms traditional GARCH models and is comparable to fuzzy GJR-GARCH.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses an evolving fuzzy-GARCH model, which is a machine learning approach, and applies it to forecast stock market volatility, thus meeting criteria I1 and I3. It does not focus on government/sovereign bonds or the yield curve, so I2 is false."}, "mode": "threadpool", "paper_id": "ac8f1b6ca53d639a", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Estimation and Forecasting of Sovereign Credit Rating Migration Based on Regime Switching Markov Chain\nAbstract: Our research aims to develop the regime switching Markov chain (RSMC), a discrete time Markov chain whose underlying regime is depending on a hidden Markov model, which express the dynamics of sovereign credit rating migration. Estimated based on a version of the Expectation-Maximization algorithm, the regime in RSMC indicates either economic expansion or contraction. Then, we apply RSMC to the monthly time series of the sovereign credit rating of 41 nations from January 1994 to December 2018. At first, we confirm that the estimation of RSMC is superior to a homogeneous Markov chain. It implies that the credit rating dynamics are subject to the underlying economic condition. Secondly, we observe that the second tier and non-investment credit ratings in economic contractions are likely to be downgraded. We also detect the continental clustering of economic contractions for the Asian currency and European sovereign debt crises. Lastly, we discover that the forecasting performance of RSMC is superior to that of the benchmark, especially for the second tier and non-in … [truncated]", "attempt1_tokens": {"prompt": 390, "candidates": 192, "total": 582}, "attempt1_parsed": {"summary": "This study introduces a regime-switching Markov chain (RSMC) model to analyze sovereign credit rating migration, considering underlying economic conditions (expansion/contraction). The RSMC model, estimated using an Expectation-Maximization algorithm, demonstrates superior performance over homogeneous Markov chains. Findings indicate that lower credit ratings are more prone to downgrades during economic contractions, with evidence of continental clustering of these contractions. The RSMC model also shows improved forecasting accuracy, particularly for lower credit ratings, suggesting its utility in managing sovereign credit risk.", "criteria_hints": {"I1": true, "I2": true, "I3": true}, "notes": "The study uses a regime-switching Markov chain, which is an empirical application of machine learning beyond simple linear models. It directly addresses sovereign credit ratings and their migration, and explicitly focuses on forecasting performance."}, "mode": "threadpool", "paper_id": "cb24c90ea906f755", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Exploiting Visual Features in Financial Time Series Prediction\nAbstract: The possibility to enhance prediction accuracy for foreign exchange rates was investigated in two ways: first applying an outside the box approach to modeling price graphs by exploiting their visual properties, and secondly employing the most efficient methods to detect patterns to classify the direction of movement. The approach that exploits the visual properties of price graphs which make use of density regions along with high and low values describing the shape; hence, the authors propose the name 'Finance Vision.' The data used in the predictive model consists of 1-hour past price values of 4 different currency pairs, between 2003 and 2016. Prediction performances of state-of-the-art methods; Extreme Gradient Boosting, Artificial Neural Network and Support Vector Machines are compared over the same data with the same sets of features. Results show that density based visual features contribute considerably to prediction performance.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about g … [truncated]", "attempt1_tokens": {"prompt": 330, "candidates": 158, "total": 488}, "attempt1_parsed": {"summary": "This study investigates enhancing foreign exchange rate prediction accuracy by leveraging visual properties of price graphs ('Finance Vision') and pattern detection for movement direction classification. It compares Extreme Gradient Boosting, Artificial Neural Network, and Support Vector Machines using 1-hour past price values of 4 currency pairs (2003-2016). Results indicate that density-based visual features significantly improve prediction performance.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses machine learning models (XGBoost, ANN, SVM) which go beyond simple linear models and applies them to financial time series prediction."}, "mode": "threadpool", "paper_id": "c053694d929092d7", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: FORECASTING DEMAND FOR MONEY UNDER CHANGING TERM STRUCTURE OF INTEREST-RATES - APPLICATION OF RIDGE REGRESSION\nAbstract: None\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 174, "candidates": 110, "total": 284}, "attempt1_parsed": {"summary": "This article applies Ridge Regression to forecast demand for money, considering changes in the term structure of interest rates.", "criteria_hints": {"I1": true, "I2": true, "I3": true}, "notes": "The abstract is missing, but the title suggests the application of Ridge Regression (a machine learning technique beyond simple linear models) to forecast money demand, which is related to the term structure of interest rates."}, "mode": "threadpool", "paper_id": "8edd8d9e068332ee", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: FORECASTING SERIES CONTAINING OFFSETTING BREAKS: OLD SCHOOL AND NEW SCHOOL METHODS OF FORECASTING TRANSNATIONAL TERRORISM\nAbstract: Transnational terrorism data are difficult to forecast because they contain an unknown number of structural breaks of unknown functional form. The rise of religious fundamentalism, the demise of the Soviet Union, and the rise of al Qaeda have changed the nature of transnational terrorism. 'Old School' forecasting methods simply smooth or difference the data. 'New School' methods use estimated break dates to control for regime shifts when forecasting. We compare the various forecasting methods using a Monte Carlo study with data containing different types of breaks. The study's results are used to forecast various types of transnational terrorist incidents.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 290, "candidates": 128, "total": 418}, "attempt1_parsed": {"summary": "This paper compares 'old school' (smoothing/differencing) and 'new school' (estimated break dates) forecasting methods for transnational terrorism data, which are characterized by structural breaks. A Monte Carlo study evaluates these methods, and the results are applied to forecast terrorist incidents.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The paper discusses forecasting methods but does not appear to use machine learning beyond simple models. It is not about government bonds."}, "mode": "threadpool", "paper_id": "99d45debc2b13a50", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: FINITE-SAMPLE PROPERTIES OF THE GENERALIZED-METHOD OF MOMENTS IN TESTS OF CONDITIONAL ASSET PRICING-MODELS\nAbstract: We develop evidence on the finite sample properties of the Generalized Method of Moments (GMM) in an asset pricing context. The models imply nonlinear, cross-equation restrictions on predictive regressions for security returns. We find that a two-stage GMM approach produces goodness-of-fit statistics that reject the restrictions too often. An iterated GMM approach has superior finite sample properties. The coefficient estimates are approximately unbiased in simpler models, but their asymptotic standard errors are understated. Simple adjustments for the standard errors are partially successful in correcting the bias. In more complex models the coefficients and their standard errors can be highly unreliable. The power of the tests to reject a single-premium model is higher against a two-premium, fixed-beta alternative than against a conditional Capital Asset Pricing Model with time-varying betas.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about  … [truncated]", "attempt1_tokens": {"prompt": 336, "candidates": 182, "total": 518}, "attempt1_parsed": {"summary": "This paper investigates the finite sample properties of the Generalized Method of Moments (GMM) when applied to conditional asset pricing models. It finds that a two-stage GMM approach tends to reject restrictions too frequently, while an iterated GMM approach shows better finite sample performance. Coefficient estimates can be biased in simpler models, and their standard errors are often understated, though adjustments can help. In more complex models, both coefficients and standard errors can be unreliable. The study also compares the power of tests against different asset pricing model alternatives.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The study focuses on statistical methods (GMM) applied to asset pricing models, which involves prediction, but does not use machine learning techniques."}, "mode": "threadpool", "paper_id": "f62c257dbca918f1", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: External shocks, cross-border flows and macroeconomic risks in emerging market economies\nAbstract: We study the relationship between cross-border flows and risks to macroeconomic stability for a sample of ten major emerging market economies (EMEs) from 2000 to 2017 in the presence of external shocks. We examine this relationship with a focus on two key channels of cross-border flows, namely external debt securities (EDS) and cross-border loans (CBLs). Our analysis focuses on the transition in cross-border flows post-global financial crisis 2008 (GFC) termed as the second phase of global liquidity (Shin in Keynote address at Federal Reserve Bank of San Francisco Asia economic policy conference, 2013). Panel vector autoregression estimations show that volatility in global risk perception affects cross-border flows to EMEs more as compared to the effect of the US monetary policy stance. Post-GFC, EDS flows rise with shocks in global risk perception, while CBL flows register a decline. CBL flows are also associated with larger risks post-GFC compared to the pre-GFC period, which is in contrast to the result for EDS flows. Second, a panel threshold estimation confirms a nonlinear … [truncated]", "attempt1_tokens": {"prompt": 474, "candidates": 228, "total": 702}, "attempt1_parsed": {"summary": "This study investigates the link between cross-border flows (external debt securities and cross-border loans) and macroeconomic stability risks in emerging market economies (EMEs) from 2000-2017, considering external shocks. The analysis reveals that global risk perception volatility impacts EME cross-border flows more than US monetary policy. Post-2008 financial crisis, external debt securities flows increased with global risk perception shocks, while cross-border loans declined and became riskier. Panel threshold estimations confirm a nonlinear relationship between these flows and macroeconomic risks, influenced by global uncertainty and US GDP growth.", "criteria_hints": {"I1": true, "I2": false, "I3": false}, "notes": "The study uses panel vector autoregression and panel threshold estimation, which can be considered empirical applications beyond simple linear models. It does not focus on government/sovereign bonds or the yield curve. While it analyzes risks and drivers, its primary focus is not forecasting or prediction."}, "mode": "threadpool", "paper_id": "6e8a4c15b74abc41", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Fiscal policy and asset markets: A semiparametric analysis\nAbstract: Using a flexible semiparametric varying coefficient model specification, this paper examines the role of fiscal policy on the US asset markets (stocks, Corporate and treasury bonds). We consider two possible roles of fiscal deficits (or surpluses): as a separate direct information variable and as a (indirect) conditioning information variable indicating binding constraints oil monetary policy actions. The results show that the impact of monetary policy on the stock market varies, depending oil fiscal expansion or contraction. The impact of fiscal policy on corporate and treasury bond yields follow similar patterns as in the equity market. The results are consistent with the notion of strong interdependence between monetary and fiscal policies. (C) 2008 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 304, "candidates": 174, "total": 478}, "attempt1_parsed": {"summary": "This paper uses a semiparametric varying coefficient model to analyze the impact of fiscal policy on US asset markets (stocks, corporate bonds, and treasury bonds). It investigates fiscal deficits as both a direct information variable and an indicator of monetary policy constraints. The findings suggest that fiscal policy influences the effect of monetary policy on the stock market and that fiscal policy impacts corporate and treasury bond yields similarly to equity markets, indicating a strong interdependence between monetary and fiscal policies.", "criteria_hints": {"I1": true, "I2": true, "I3": false}, "notes": "The study uses a semiparametric varying coefficient model, which is more flexible than simple linear models. It directly analyzes government (treasury) bonds and their yields."}, "mode": "threadpool", "paper_id": "30116f36dc2b4789", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Factor Investing Based on Musharakah Principle\nAbstract: Shariah stock investing has become a widely discussed topic in financial industry as part of today's investment strategy. The strategy primarily applies market capitalization allocations. However, some researchers have argued that market capitalization weighting is inherently flawed and have advocated replacing market capitalization allocations with factor allocations. In this paper, we discuss the rationale for factor investing based on Musharakah principle. The essential elements or factors of Musharakah principle such as business sector, management capability, profitability growth and capital efficiency are embedded in the Shariah-compliant stock. We then transform these factors into indexation for better analysis and performance measurement. Investment universe for this research covers Malaysian stocks for the period of January 2009 to December 2013. We found out that these factor indexes have historically earned excess returns over market capitalization weighted indexes and experienced higher Sharpe Ratios.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes … [truncated]", "attempt1_tokens": {"prompt": 327, "candidates": 180, "total": 507}, "attempt1_parsed": {"summary": "This paper proposes a factor investing strategy based on the Musharakah principle for Shariah-compliant stocks. It identifies key factors like business sector, management capability, profitability growth, and capital efficiency, transforms them into indexation, and empirically tests their performance against market capitalization-weighted indexes using Malaysian stocks from 2009-2013. The findings suggest that factor indexes historically outperformed market capitalization indexes and had higher Sharpe Ratios.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The study uses factor analysis and indexation, not machine learning. It focuses on Shariah-compliant stocks, not government bonds. While it discusses historical performance, it does not explicitly focus on forecasting or prediction."}, "mode": "threadpool", "paper_id": "05792d551d45f496", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Forecasting Yield Curves with Survey Information\nAbstract: In recent years, affine term structure models have provided alternatives to the expectations hypothesis and have become very popular in the finance literature. In particular, the widely accepted dynamic Nelson-Siegel model employs ingenious measures of the level, slope, and curvature of the yield curve that captured the attention of Francis and Hua. They supplement the dynamic Nelson-Siegel model with the Federal Reserve's Survey of Professional Forecasters data. Because these data utilize information from dozens of professional forecasters who study numerous macroeconomic variables, the author's wanted to see if this information-rich supplementary data could be used to improve the interest rate forecasting models for out-of-sample forecasts for Treasury bond maturities ranging from three months to 10 years that extend from three months to one year into the future.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecast … [truncated]", "attempt1_tokens": {"prompt": 311, "candidates": 161, "total": 472}, "attempt1_parsed": {"summary": "This study enhances the dynamic Nelson-Siegel model by incorporating the Federal Reserve's Survey of Professional Forecasters data to improve out-of-sample interest rate forecasts for Treasury bonds. The goal is to leverage the information-rich survey data to predict bond yields for maturities ranging from three months to 10 years, with forecasts extending one year into the future.", "criteria_hints": {"I1": false, "I2": true, "I3": true}, "notes": "The abstract does not explicitly mention machine learning techniques beyond the dynamic Nelson-Siegel model, which is a statistical model rather than a machine learning application. Therefore, I1 is marked as false."}, "mode": "threadpool", "paper_id": "8fb8a050541580b5", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Factor Models, Machine Learning, and Asset Pricing\nAbstract: We survey recent methodological contributions in asset pricing using factor models and machine learning. We organize these results based on their primary objectives: estimating expected returns, factors, risk exposures, risk premia, and the stochastic discount factor as well as model comparison and alpha testing. We also discuss a variety of asymptotic schemes for inference. Our survey is a guide for financial economists interested in harnessing modern tools with rigor, robustness, and power to make new asset pricing discoveries, and it highlights directions for future research and methodological advances.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 259, "candidates": 185, "total": 444}, "attempt1_parsed": {"summary": "This article surveys recent methodological advancements in asset pricing that utilize factor models and machine learning. It categorizes these advancements by their main goals, including estimating expected returns, factors, risk exposures, risk premia, and the stochastic discount factor, as well as model comparison and alpha testing. The survey also covers various asymptotic schemes for inference and aims to guide financial economists in applying modern, rigorous, and powerful tools for asset pricing research, while also pointing towards future research directions.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The abstract mentions \"asset pricing discoveries\" and \"forecasting or prediction\" is a common goal in asset pricing. The article explicitly mentions \"machine learning\" and \"factor models\" which are beyond simple linear models."}, "mode": "threadpool", "paper_id": "1783e8e530cbe052", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Forecasting Stock Market Crashes via Machine Learning\nAbstract: This paper uses a comprehensive set of predictor variables from the five largest Eurozone countries to compare the performance of simple univariate and machine learning-based multivariate models in forecasting stock market crashes. In terms of statistical predictive performance, a support vector machine-based crash prediction model outperforms a random classifier and is superior to the average univariate benchmark as well as a multivariate logistic regression model. Incorporating nonlinear and interactive effects is both imperative and foundation for the outperformance of support vector machines. Their ability to forecast stock market crashes outof-sample translates into substantial value-added to active investors. From a policy perspective, the use of machine learning-based crash prediction models can help activate macroprudential tools in time.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or predict … [truncated]", "attempt1_tokens": {"prompt": 296, "candidates": 139, "total": 435}, "attempt1_parsed": {"summary": "This paper compares univariate and machine learning models (specifically Support Vector Machines) for forecasting stock market crashes in the five largest Eurozone countries. The SVM model demonstrated superior predictive performance compared to simpler models and a random classifier, highlighting the importance of capturing nonlinear and interactive effects. The findings suggest significant value for investors and potential policy applications for macroprudential tools.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study focuses on stock market crashes, not government/sovereign bonds or the yield curve."}, "mode": "threadpool", "paper_id": "5afe5ce80e830918", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Financial asset returns, direction-of-change forecasting, and volatility dynamics\nAbstract: We consider three sets of phenomena that feature prominently in the financial economics literature: (1) conditional mean dependence (or lack thereof) in asset returns, (2) dependence (and hence forecastability) in asset return signs, and (3) dependence (and hence forecastability) in asset return volatilities. We show that they are very much interrelated and explore the relationships in detail. Among other things, we show that (1) volatility dependence produces sign dependence, so long as expected returns are nonzero, so that one should expect sign dependence, given the overwhelming evidence of volatility dependence; (2) it is statistically possible to have sign dependence without conditional mean dependence; (3) sign dependence is not likely to be found via analysis of sign autocorrelations, runs tests, or traditional market timing tests because of the special nonlinear nature of sign dependence, so that traditional market timing tests are best viewed as tests for sign dependence arising from variation in expected returns rather than from variation in volatility or higher moments; (4) … [truncated]", "attempt1_tokens": {"prompt": 450, "candidates": 218, "total": 668}, "attempt1_parsed": {"summary": "This paper investigates the interrelationships between conditional mean dependence in asset returns, dependence in asset return signs (direction-of-change forecasting), and dependence in asset return volatilities. It demonstrates that volatility dependence can lead to sign dependence, especially when expected returns are non-zero. The study also highlights that sign dependence might not be easily detected using traditional methods like sign autocorrelations or market timing tests due to its nonlinear nature, and it is more likely to be found at intermediate return horizons. The link between volatility and sign dependence persists even in non-Gaussian environments.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The abstract mentions forecasting and prediction related to asset returns, signs, and volatilities, fulfilling I3. However, it does not explicitly mention empirical machine learning applications beyond simple linear models (I1) or focus on government/sovereign bonds and the yield curve (I2)."}, "mode": "threadpool", "paper_id": "a7bce1b51fda5973", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Financial conditions and nonlinearities in the European Central Bank (ECB) reaction function: In-sample and out-of-sample assessment\nAbstract: Our purpose is to investigate how the European Central Bank (ECB) sets interest rates in the context of both linear and nonlinear policy reaction functions. This work contributes to the current debate on central banks having additional objectives over and above control of inflation and output. Three findings emerge. First, the ECB takes financial conditions into account when setting interest rates. Second, amongst Taylor rule models, linear and nonlinear models are empirically indistinguishable within sample, and model specifications with real-time data provide the best description of in-sample ECB interest rate setting behaviour. Third, the 2007-2009 financial crisis witnessed a shift from inflation targeting to output stabilization, and a shift from an asymmetric policy response to financial conditions at high inflation rates to a more symmetric response regardless of the state of inflation. Finally, guidance is provided as regards models for forecasting interest rates in the Eurozone area. Without imposing an a priori choice of the … [truncated]", "attempt1_tokens": {"prompt": 414, "candidates": 197, "total": 611}, "attempt1_parsed": {"summary": "This study examines the European Central Bank's (ECB) interest rate setting behavior using both linear and nonlinear policy reaction functions. It finds that the ECB considers financial conditions, that linear and nonlinear models are empirically similar within sample, and that model specifications with real-time data best describe in-sample behavior. The 2007-2009 financial crisis marked a shift from inflation targeting to output stabilization and a change in the response to financial conditions. For out-of-sample forecasting, semiparametric models and autoregressive processes outperform linear and nonlinear Taylor rule models.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses nonlinear models and semiparametric models for forecasting, which goes beyond simple linear models. It also explicitly discusses forecasting interest rates."}, "mode": "threadpool", "paper_id": "76f6fcb6934e9db1", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Forecasting and trading credit default swap indices using a deep learning model integrating Merton and LSTMs\nAbstract: Using macroeconomic and financial conditions to forecast credit default swap (CDS) spreads is a challenging task. In this paper, we propose the Merton-LSTM model, a modified LSTM model formed by integrating with the Merton determinants model, to forecast the CDS indices. We provide the rigorous math behind the Merton-LSTM model, which demonstrates that by leveraging the nonlinear learning ability of LSTM with increased model capacity, the Merton-LSTM model is expected to learn the inherent association between the Merton determinants and CDS spreads. Further, the Merton-LSTM model is compared with the machine learning models LSTM, gated recurrent unit (GRU), multilayer perceptron network (MLP), support vector machine (SVM) and a typical sto-chastic series model in forecasting the two most liquid five-year CDS indices, North America High Yield index (CDX.NA.HY) and North America Investment Grade index (CDX.NA.IG) through the root mean squared error (RMSE) and the Diebold-Mariano test. The comparison results show that the RMSEs of the Merton-LSTM model are the  … [truncated]", "attempt1_tokens": {"prompt": 512, "candidates": 155, "total": 667}, "attempt1_parsed": {"summary": "This paper introduces the Merton-LSTM model, a hybrid deep learning approach combining the Merton determinants model with LSTMs, to forecast Credit Default Swap (CDS) indices. The model demonstrates superior forecasting accuracy compared to traditional machine learning and time series models, particularly for longer prediction horizons. Furthermore, a simulated trading strategy based on the Merton-LSTM model shows promising economic significance with high Sharpe ratios and low maximum losses.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study focuses on Credit Default Swap (CDS) indices, not government/sovereign bonds or the yield curve."}, "mode": "threadpool", "paper_id": "fd8a58d650cc3e18", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Forecasting carbon emissions future prices using the machine learning methods\nAbstract: Due to the uncertainty surrounding the coupling and decoupling of natural gas, oil, and energy commodity futures prices, the current study seeks to investigate the interactions between energy commodity futures, oil price futures, and carbon emission futures from a forecasting perspective with implications for environmental sustainability. We employed daily data on natural gas futures prices, crude oil futures prices, carbon futures prices, and Dow Jones energy commodity futures prices from January 2018 to October 2021. For empirical analysis, we applied machine learning tools including traditional multiple linear regression (MLR), artificial neural network (ANN), support vector regression (SVR), and long short-term memory (LSTM). The machine learning analysis provides two key findings. First, the nonlinear frameworks outperform linear models in developing the relationships between future oil prices (crude oil and heating oil) and carbon emission futures prices. Second, the machine learning findings establish that when oil prices and natural gas prices display extreme movement, carbon emis … [truncated]", "attempt1_tokens": {"prompt": 404, "candidates": 157, "total": 561}, "attempt1_parsed": {"summary": "This study forecasts carbon emission futures prices by examining the interactions with natural gas, oil, and energy commodity futures prices using machine learning methods (MLR, ANN, SVR, LSTM). The findings indicate that nonlinear models outperform linear ones, and carbon emission futures prices react nonlinearly to extreme movements in oil and natural gas prices. The research has implications for environmental sustainability, climate policies, and sustainable development goals (SDG 7 and SDG 12).", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses machine learning methods beyond simple linear models and focuses on forecasting."}, "mode": "threadpool", "paper_id": "9169d6d7b0d2309e", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Forecasting day-ahead expected shortfall on the EUR/USD exchange rate: The (I)relevance of implied volatility\nAbstract: The existing literature provides mixed results on the usefulness of implied volatility for managing risky assets, while evidence for expected shortfall predictions is almost nonexistent. Given its forward-looking nature, implied volatility might be more valuable than backward-looking measures of realized price fluctuations. Conversely, the volatility risk premium embedded in implied volatility leads to overestimating the observed price variation. This paper explores the benefits of augmenting econometric models used in forecasting the expected shortfall, a risk measured endorsed in the Basel III Accord, with information on implied volatility obtained from EUR/USD option contracts. The day-ahead forecasts are obtained from several classes of econometric models: historical simulation, EGARCH, quantile regression-based HAR, joint VaR and ES model, and combination forecasts. We verify whether the resulting expected shortfall forecasts are well-specified and test the models' accuracy. Our results provide evidence that the information provided by forward-looking  … [truncated]", "attempt1_tokens": {"prompt": 422, "candidates": 193, "total": 615}, "attempt1_parsed": {"summary": "This paper investigates the usefulness of implied volatility for forecasting day-ahead expected shortfall on the EUR/USD exchange rate. It compares various econometric models, including historical simulation, EGARCH, quantile regression-based HAR, and a joint VaR and ES model, augmented with implied volatility information. The findings suggest that implied volatility, despite its potential drawbacks, provides valuable forward-looking information that improves expected shortfall forecasts, especially during periods of high market uncertainty.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses econometric models that go beyond simple linear models, including EGARCH and quantile regression-based HAR, which can be considered as empirical machine learning applications in a broader sense. The focus is on exchange rates, not government/sovereign bonds or the yield curve."}, "mode": "threadpool", "paper_id": "43890536593eb51e", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Forecasting exchange rate: A bibliometric and content analysis\nAbstract: The study aims to present a systematic overview of the research in the field of exchange rate projection models through bibliometric techniques and content analysis. First, 775 articles published in journals within the scope of the international Web of Science database from 1966 to May 2021 were analyzed through bibliometric techniques. Second, a selected sample of 69 articles was analyzed through a detail content analysis to identify hot topics and new avenues of interest in the field. The research findings suggest that the scientific production on the subject is in wide development. New approaches have been incorporated, such as neural networks, requiring a broad perspective by the researcher in the evaluation of the empirical results.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 298, "candidates": 125, "total": 423}, "attempt1_parsed": {"summary": "This study provides a bibliometric and content analysis of research on exchange rate forecasting models, examining 775 articles from 1966-2021. It identifies emerging trends like neural networks and emphasizes the need for a broad perspective in evaluating empirical results.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The abstract mentions neural networks, which are machine learning applications, and the study focuses on forecasting exchange rates."}, "mode": "threadpool", "paper_id": "b6b619d5ca55a0f5", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Forecasting equity risk premium: The role of investor concern on oil price volatility\nAbstract: We explore the impact of investor concern to oil price volatility on Chinese equity risk premium by constructing an investor oil price volatility concern index (IOPVC) using Baidu Index. Our analysis demonstrates that IOPVC is a strong predictor of equity risk premium, with a negative correlation to future returns. Robustness checks, including the use of different data sources, various prediction windows, and differing levels of risk aversion, confirm that the relationship between IOPVC and equity risk premium is reliable. Furthermore, our analysis reveals that IOPVC is a leading indicator of forthcoming economic scenarios and exerts a significant influence on investor risk aversion.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 284, "candidates": 147, "total": 431}, "attempt1_parsed": {"summary": "This study investigates the relationship between investor concern about oil price volatility and the Chinese equity risk premium. The authors construct an index (IOPVC) using Baidu Index data and find it to be a significant negative predictor of future equity returns. The findings are robust across various checks and suggest that IOPVC also acts as a leading indicator for economic conditions and influences investor risk aversion.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The study uses an index construction and correlation analysis, not explicitly described as a machine learning application beyond simple linear models."}, "mode": "threadpool", "paper_id": "bd6374063056c122", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: From Man vs. Machine to Man plus Machine: The art and AI of stock analyses\nAbstract: An AI analyst trained to digest corporate disclosures, industry trends, and macroeconomic indicators surpasses most analysts in stock return predictions. Nevertheless, humans win Man vs. Machinewhen institutional knowledge is crucial, e.g., involving intangible assets and financial distress. AI wins when information is transparent but voluminous. Humans provide significant incremental value in Man + Machine, which also substantially reduces extreme errors. Analysts catch up with machines after alternative databecome available if their employers build AI capabilities. Documented synergies between humans and machines inform how humans can leverage their advantage for better adaptation to the growing AI prowess.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 284, "candidates": 171, "total": 455}, "attempt1_parsed": {"summary": "This study explores the synergy between human analysts and AI in stock market analysis. While AI excels at processing large volumes of transparent data for stock return predictions, human analysts remain crucial for understanding intangible assets and financial distress. The combination of human and machine intelligence (Man + Machine) significantly improves predictions and reduces extreme errors, with humans catching up to AI when alternative data becomes available and AI capabilities are integrated.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study discusses AI applications in stock analysis and prediction, which aligns with I1 and I3. However, it does not focus on government/sovereign bonds or the yield curve, making I2 false."}, "mode": "threadpool", "paper_id": "8413a03397041cf2", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: GDP Forecasting: Machine Learning, Linear or Autoregression?\nAbstract: This paper compares the predictive power of different models to forecast the real U.S. GDP. Using quarterly data from 1976 to 2020, we find that the machine learning K-Nearest Neighbour (KNN) model captures the self-predictive ability of the U.S. GDP and performs better than traditional time series analysis. We explore the inclusion of predictors such as the yield curve, its latent factors, and a set of macroeconomic variables in order to increase the level of forecasting accuracy. The predictions result to be improved only when considering long forecast horizons. The use of machine learning algorithm provides additional guidance for data-driven decision making.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 294, "candidates": 148, "total": 442}, "attempt1_parsed": {"summary": "This paper compares the predictive power of machine learning (KNN) and traditional time series models for forecasting real U.S. GDP using quarterly data from 1976-2020. The KNN model outperformed traditional methods, especially for long forecast horizons, and the inclusion of predictors like the yield curve and macroeconomic variables improved accuracy. The study suggests machine learning offers valuable guidance for data-driven decision-making.", "criteria_hints": {"I1": true, "I2": true, "I3": true}, "notes": "The study explicitly mentions the yield curve as a predictor, fulfilling I2."}, "mode": "threadpool", "paper_id": "9c9a62d1d4915359", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Forecasting interest rates: a comparative assessment of some second-generation nonlinear models\nAbstract: Modeling and forecasting of interest rates has traditionally proceeded in the framework of linear stationary methods such as ARMA and VAR, but only with moderate success. We examine here three methods, which account for several specific features of the real world asset prices such as nonstationarity and nonlinearity. Our three candidate methods are based, respectively, on a combined wavelet artificial neural network (WANN) analysis, a mixed spectrum (MS) analysis and nonlinear ARMA models with Fourier coefficients (FNLARMA). These models are applied to weekly data on interest rates in India and their forecasting performance is evaluated vis--vis three GARCH models [GARCH (1,1), GARCH-M (1,1) and EGARCH (1,1)] as well as the random walk model. Both the WANN and MS methods show marked improvement over other benchmark models, and may thus hold out several potentials for real world modeling and forecasting of financial data.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The a … [truncated]", "attempt1_tokens": {"prompt": 350, "candidates": 129, "total": 479}, "attempt1_parsed": {"summary": "This study compares the forecasting performance of three nonlinear models (wavelet artificial neural network, mixed spectrum, and nonlinear ARMA with Fourier coefficients) against GARCH models and a random walk model for Indian interest rates. The wavelet artificial neural network and mixed spectrum methods demonstrated significant improvements over the benchmarks.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study focuses on interest rates in general, not specifically government/sovereign bonds or the yield curve."}, "mode": "threadpool", "paper_id": "4f0fd761748ec99f", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Funding shortages, expectations, and forward rate risk premium\nAbstract: This paper estimates term risk premium and expected future spot rates embedded in Treasury forward rates to study the impact of short-term funding shortages on these quantities. Our approach is consistent with dynamic equilibrium models and avoids the arbitrage-free dynamic inconsistency problems exhibited by traditional methods. We find that short-term funding shortages in money markets affect both expectations of spot rates and forward rate risk premium for all maturity forward rates. The leverage ratio of intermediaries (primary dealers) significantly affects term risk premium but not expectations of future spot rates. Yield curve inversion has no impact on the forward rate curve's evolution.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 276, "candidates": 155, "total": 431}, "attempt1_parsed": {"summary": "This paper estimates the term risk premium and expected future spot rates from Treasury forward rates, investigating the influence of short-term funding shortages. The methodology aligns with dynamic equilibrium models, circumventing arbitrage-free dynamic inconsistency issues. Findings indicate that funding shortages impact both expectations and risk premiums across all maturities. The leverage ratio of primary dealers influences term risk premium but not future spot rate expectations. Yield curve inversions do not affect the forward rate curve's progression.", "criteria_hints": {"I1": false, "I2": true, "I3": true}, "notes": "The abstract does not mention machine learning applications or models beyond simple linear ones."}, "mode": "threadpool", "paper_id": "b6874ce0d132be04", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Forecasting foreign exchange rates with an intrinsically nonlinear dynamic speed of adjustment model\nAbstract: Forecasting foreign exchange rates is an important but difficult process; therefore, it is important to use a superior forecasting model. The paper takes up this criterion and proposes to describe and forecast foreign exchange rates by developing an intrinsically nonlinear model with variable and dynamic speeds of adjustment. It is found that the speed of adjusting the random (or expected) to the equilibrium rate is very slow, implying that fiscal policy (statistically insignificat) and monetary policy (statistically significant) may be ineffective to induce changes in the adjustment speed. We also find that the nonlinear dynamic model improves forecasting performance, implying that nonlinearities in the sense of functional forms are exploitable for improved point forecasting of foreign exchange rates.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or predi … [truncated]", "attempt1_tokens": {"prompt": 299, "candidates": 150, "total": 449}, "attempt1_parsed": {"summary": "This paper proposes a nonlinear dynamic model with variable adjustment speeds to forecast foreign exchange rates. The model suggests that the adjustment to equilibrium is slow, and fiscal policy is insignificant while monetary policy is significant in influencing adjustment speed. The nonlinear model demonstrates improved forecasting performance, indicating that exploitable nonlinearities exist for better point forecasting.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The model is described as \"intrinsically nonlinear\" and \"nonlinear dynamic\", and it \"improves forecasting performance\", suggesting it goes beyond simple linear models and is an empirical application."}, "mode": "threadpool", "paper_id": "a8145ad99ef09055", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Front-Page News: The Effect of News Positioning on Financial Markets\nAbstract: This paper estimates the effect of news positioning on the speed of price discovery, using exogenous variation in prominent (front-page) positioning of news articles on the Bloomberg terminal. Front-page articles see 240% higher trading volume and 176% larger absolute excess returns during the first 10 minutes after publication than equally important non-front-page articles. Overall, the information in front-page articles is fully incorporated into prices within an hour of publication. The response to non-front-page information of similar importance eventually converges but takes more than two days to be fully reflected in prices.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 285, "candidates": 174, "total": 459}, "attempt1_parsed": {"summary": "This study investigates how the placement of news (front-page vs. non-front-page) on the Bloomberg terminal impacts the speed of price discovery in financial markets. Front-page news leads to significantly higher trading volume and immediate price changes, with information fully incorporated into prices within an hour. Non-front-page news of similar importance takes over two days to be fully reflected.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The study uses empirical methods but does not appear to involve machine learning beyond simple models. It focuses on general financial markets, not specifically government bonds or yield curves, and while it discusses price discovery, it is not primarily about forecasting or prediction."}, "mode": "threadpool", "paper_id": "5c50a6dc234cc7e9", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Good volatility, bad volatility, and the cross section of cryptocurrency returns\nAbstract: This paper examines the predictability of realized volatility measures (RVM), especially the realized signed jumps (RSJ), on future volatility and returns. We confirm the existence of volatility persistence and future volatility is more strongly related to the volatility of past positive returns than to that of negative returns in the cryptocurrency market. RSJ-sorted cryptocurrency portfolios yield statistically and economically significant differences in the subsequent portfolio returns. After controlling for cryptocurrency market characteristics and existing risk factors, the differences remain significant. The investor attention explains the predictability of realized jump risk in future cryptocurrency returns.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 271, "candidates": 132, "total": 403}, "attempt1_parsed": {"summary": "This paper investigates the predictive power of realized volatility measures, particularly realized signed jumps (RSJ), on future cryptocurrency returns and volatility. It finds that future volatility is more influenced by past positive returns than negative ones, and RSJ-sorted portfolios show significant return differences. Investor attention is identified as a factor explaining this predictability.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The paper uses empirical methods to analyze predictability, which implies machine learning applications beyond simple linear models."}, "mode": "threadpool", "paper_id": "703481e0f3994ee0", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Generating currency trading rules from the term structure of forward foreign exchange premia\nAbstract: The quality of an exchange rate forecasting model has typically been judged relative to a random-walk in terms of out-of-sample forecast errors. The difficulty of outperforming this benchmark is well documented, although Clarida and Taylor have demonstrated how the random walk can be beaten in this metric by exploiting information embedded within the term structure of forward exchange rate premia. But this achievement does not guarantee success within an investment context. We therefore assess whether the Clarida-Taylor framework can be used to generate significant trading profits in combination with an acceptable degree of risk in a realistic investment portfolio context. (C) 2013 Published by Elsevier Ltd.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 293, "candidates": 144, "total": 437}, "attempt1_parsed": {"summary": "This paper evaluates whether the Clarida-Taylor framework, which uses information from the term structure of forward exchange rate premia to forecast exchange rates, can generate significant trading profits in a realistic investment portfolio context, going beyond simply outperforming a random-walk benchmark.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The paper discusses forecasting exchange rates, which aligns with I3. However, it does not appear to involve machine learning beyond simple linear models (I1) or focus on government bonds/yield curves (I2)."}, "mode": "threadpool", "paper_id": "7ff2a7cb36ec3f22", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Habit formation and macroeconomic models of the term structure of interest rates\nAbstract: This paper introduces a new class of nonaffine models of the term structure of interest rates that is supported by an economy with habit formation. Distinguishing features of the model are that the interest rate dynamics are nonlinear, interest rates depend on lagged monetary and consumption shocks, and the price of risk is not a constant multiple of interest rate volatility. We find that habit persistence can help reproduce the nonlinearity of the spot rate process, the documented deviations from the expectations hypothesis, the persistence of the conditional volatility of interest rates, and the lead-lag relationship between interest rates and monetary aggregates.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 275, "candidates": 161, "total": 436}, "attempt1_parsed": {"summary": "This paper presents a new class of non-affine models for the term structure of interest rates, incorporating habit formation in the economy. Key features include nonlinear interest rate dynamics, dependence on lagged monetary and consumption shocks, and a price of risk not directly proportional to interest rate volatility. The model successfully explains the nonlinearity of the spot rate process, deviations from the expectations hypothesis, persistent conditional volatility, and the lead-lag relationship between interest rates and monetary aggregates.", "criteria_hints": {"I1": false, "I2": true, "I3": false}, "notes": "The paper discusses macroeconomic models of the term structure of interest rates and does not appear to use machine learning applications."}, "mode": "threadpool", "paper_id": "82eadd829f8e6a8b", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Generalized Forecast Error Variance Decomposition for Linear and Nonlinear Multivariate Models\nAbstract: We propose a new generalized forecast error variance decomposition with the attractive property that the proportions of the impact accounted for by innovations in each variable sum to unity. Our decomposition is based on the generalized impulse response function, and it can easily be obtained by simulation. The new decomposition is illustrated in an empirical application to US output growth and interest rate spread data.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 229, "candidates": 190, "total": 419}, "attempt1_parsed": {"summary": "This paper introduces a novel generalized forecast error variance decomposition for multivariate models, applicable to both linear and nonlinear cases. The decomposition ensures that the proportions of impact from each variable's innovations sum to one. It is derived from the generalized impulse response function and can be computed via simulation. The method is demonstrated using US output growth and interest rate spread data.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The paper mentions nonlinear models and provides an empirical application, satisfying I1. It uses output growth and interest rate spread data, which is related to the yield curve, but the primary focus is not on bonds themselves, hence I2 is false. The core of the paper is about forecast error variance decomposition, which is a forecasting technique, satisfying I3."}, "mode": "threadpool", "paper_id": "abed275e49b577b6", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Government Bonds of the CIS Countries: Integration Dynamics of Debt Markets in the Context of External Instability\nAbstract: The paper examines development specifics of government bond markets in the CIS countries. The sample includes Russia, Kazakhstan, Uzbekistan and Azerbaijan, since only these countries, among the CIS members, possess enough government bonds included in the global debt market. The relevance of the study is due to the increasing financial uncertainty, which attracts attention to relatively reliable means of public debt; the need to understand the functioning of debt markets against the background of anti -Russian sanctions and the increasing influence of the State. The aim of the work is to empirically verify the connectivity, integration and predictability of the government bond markets of Russia, Kazakhstan, Uzbekistan and Azerbaijan. Empirical data include daily refinancing rates of national central banks, indices of total government bond yields, G -spreads of international bonds of the countries in relation to the conditionally risk -free US bond yield curve for 2019-2023. The effects of market development features are divided into local, regional and … [truncated]", "attempt1_tokens": {"prompt": 561, "candidates": 242, "total": 803}, "attempt1_parsed": {"summary": "This study analyzes the government bond markets of Russia, Kazakhstan, Uzbekistan, and Azerbaijan, focusing on their integration and predictability amidst external instability. Using daily data from 2019-2023, including refinancing rates, bond yield indices, and G-spreads, the research employs correlation, factor, and regression analyses to assess market connectivity. Findings indicate a violation of integration within the CIS region, with Russia and Kazakhstan exhibiting relatively developed but poorly predictable markets. Kazakhstan's monetary policy shows greater connectivity than Russia's. Azerbaijan's bond yields are influenced by Kazakhstan's market, though Azerbaijan's market is less developed. Uzbekistan's market is the least integrated and developed. The study highlights the impact of events like COVID-19 and anti-Russian sanctions on these markets.", "criteria_hints": {"I1": false, "I2": true, "I3": false}, "notes": "The study uses regression analysis, which is a statistical method, not a machine learning application. While it discusses predictability, it does not appear to employ forecasting models."}, "mode": "threadpool", "paper_id": "f274dfa7538c8887", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Have trend-following signals in commodity futures markets become less reliable in recent years?\nAbstract: Various trend-following trading rules have been shown to be valuable for predicting market directions and thus the formulation of investment strategies. However, recent equity market research has provided striking evidence that the predictive power of such rules appears to diminish over time due to increased investor attention and lowered arbitrage barriers. Given that trend-following rules are also very successful and have been widely used in futures markets, we analyze whether a similar effect can be observed for commodity futures contracts. Using a trend regression approach based on time-varying success ratios, we detect significantly higher predictive accuracy for cross-sectional than for time-series strategies. In addition, with the exception of a few commodities, we find no significant trending behavior in trading rule reliability. These results, which are robust in a variety of settings, indicate strong momentum stability in futures markets and justify the application of this class of trading rules in commodity futures investing.\n\nCriteria: {\"I1\": \"The article inc … [truncated]", "attempt1_tokens": {"prompt": 340, "candidates": 162, "total": 502}, "attempt1_parsed": {"summary": "This study investigates whether trend-following signals in commodity futures markets have become less reliable recently, similar to observations in equity markets. The authors analyze trend-following rules using a time-varying success ratio approach. They find that cross-sectional strategies are more accurate than time-series strategies, but overall, there is no significant decline in the reliability of trading rules for most commodities, suggesting stable momentum in futures markets.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The study uses a trend regression approach with time-varying success ratios, which is an empirical method but not explicitly a machine learning application beyond simple linear models."}, "mode": "threadpool", "paper_id": "1ffdcdc331372c74", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: How Are Interbank and Sovereign Debt Markets Linked? Evidence from 14 OECD Countries, the Euro Area and Russia\nAbstract: The paper explores causal linkages between interbank and sovereign bond markets in 14 OECD countries, the Euro area and Russia during the 2008-2009 crisis and post-crisis period. The analysis has been carried out for individual countries and in a multivariate framework. It enables to identify systemically important countries in both markets. The USA, Switzerland, Australia, South Korea and Russia are of particular significance in the interbank lending market. Switzerland, the UK, Poland, Australia and Canada play a pivotal role in the public debt market. The analysis under the multivariate framework reveals substantial heterogeneity in the network structure of both markets. Only 12% of causal relationships coincide, which may fuel financial contagion. Volatility spillovers underlie the causal linkages. They are estimated by means of dynamic volatility indices based on rolling correlation matrices and help identify the transformation of the international banking turmoil into the sovereign debt crisis.\n\nCriteria: {\"I1\": \"The article includes an empirical Mac … [truncated]", "attempt1_tokens": {"prompt": 357, "candidates": 185, "total": 542}, "attempt1_parsed": {"summary": "This paper investigates the causal links between interbank and sovereign bond markets in 14 OECD countries, the Euro area, and Russia, focusing on the 2008-2009 crisis and post-crisis periods. It identifies systemically important countries in both markets and reveals significant heterogeneity in network structures, with only 12% of causal relationships coinciding. Volatility spillovers are identified as the basis for these causal linkages, explaining the transmission of international banking turmoil to sovereign debt crises.", "criteria_hints": {"I1": true, "I2": true, "I3": false}, "notes": "The paper uses \"dynamic volatility indices based on rolling correlation matrices\" which suggests a machine learning application beyond simple linear models. It directly addresses sovereign debt markets and their linkages."}, "mode": "threadpool", "paper_id": "7263c110b7556c6d", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Heterogeneous beliefs with information processing capacity constraints and asset pricing in a monetary economy\nAbstract: This paper proposes a monetary model to explore the influences of heterogeneous beliefs and information processing capacity constraints on the dynamics of asset prices. The capacity constraints not only influence the estimations of the capacity constrained investor, but also generate persistent disagreements among the investors. The model implies that reducing the levels of capacity constraints can alleviate the influences of heterogeneous beliefs and helps to stabilize financial markets. The model also reveals that introducing heterogeneous beliefs about both real and nominal sectors not only leads the stock with low monetary policy exposure to have significantly higher average return than the stock with high monetary policy exposure, but also can explain the mixed results about the relationship between the volatility and the risk premium of the aggregate stock market.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign  … [truncated]", "attempt1_tokens": {"prompt": 298, "candidates": 175, "total": 473}, "attempt1_parsed": {"summary": "This paper develops a monetary model to investigate how heterogeneous beliefs and constraints on information processing capacity affect asset prices. It suggests that these constraints create lasting disagreements among investors and that easing them can stabilize markets. The model also shows that incorporating heterogeneous beliefs about both real and nominal sectors can explain differences in stock returns based on monetary policy exposure and reconcile mixed findings on the relationship between stock market volatility and risk premium.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The paper is theoretical and does not involve empirical applications or machine learning. It focuses on asset pricing in general, not specifically government bonds or the yield curve. While it discusses market dynamics, it does not explicitly focus on forecasting or prediction."}, "mode": "threadpool", "paper_id": "24c473804012a63d", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Improving Quality of Long-Term Bond Price Prediction Using Artificial Neural Networks\nAbstract: Purpose: The aim of this paper is to propose nonlinear autoregressive neural network which can improve quality of bond price forecasting.Methodology/Approach: Due to the complex nature of market information that influence bonds, artificial intelligence could be accurate, robust and fast choice of bond price prediction method.Findings: Our results have reached a coefficient of determination higher than 95% in the training, validation and testing sets. Moreover, we proposed the nonlinear autoregressive network with external inputs using 50 year interest-rate swaps denominated in EUR and volatility index VIX as two external variables.Research Limitation/Implication: Our sample of daily prices between 4th January 2016 and 13th January 2021 (totally 1,270 trading days) suggest that both Levenberg-Marquardt and Scaled conjugate gradient learning algorithms achieved excellent results.Originality/Value of paper: Despite the fact that both learning algorithms achieved satisfying outcomes, implementation of an independent variable into the autoregressive neural network environment had no si … [truncated]", "attempt1_tokens": {"prompt": 387, "candidates": 141, "total": 528}, "attempt1_parsed": {"summary": "This paper proposes a nonlinear autoregressive neural network to improve long-term bond price forecasting, achieving a coefficient of determination over 95%. The study uses daily bond prices from 2016-2021 and explores the impact of external variables like interest-rate swaps and the VIX index, finding no significant improvement from their inclusion.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study focuses on bond price prediction but does not specify if they are government/sovereign bonds."}, "mode": "threadpool", "paper_id": "8a92241a88c5f917", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: How do zero-coupon inflation swaps predict inflation rates in the euro area? Evidence of efficiency and accuracy on 1-year contracts\nAbstract: This paper examines the risk-neutral efficient market hypothesis for inflation swap markets in the euro area from 2005.10 to 2014.07. Overall, we conclude that 1-year zero-coupon inflation swap rates are unbiased predictors of inflation rates. Further, there is no empirical evidence of an inflation risk premium and the assumption of rationality seems to hold. Definitely, these inferences encourage the reading of inflation expectations embedded in short-term inflation swaps. Additionally, we compare the predictive ability of inflation swaps with other measures of inflation expectations. The in-sample results show that, in contrast with surveys, market-based measures are able to accurately forecast inflation rates. In turn, based on an out-of-sample analysis, a straightforward econometric model dominates other sources. Therefore, a combined analysis that uses different sources contributes to a more robust view of future inflation rates.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory).  … [truncated]", "attempt1_tokens": {"prompt": 356, "candidates": 176, "total": 532}, "attempt1_parsed": {"summary": "This paper investigates the efficiency and accuracy of 1-year zero-coupon inflation swaps in predicting euro area inflation rates from 2005 to 2014. It finds that these swap rates are unbiased predictors, with no evidence of an inflation risk premium, suggesting rationality in the market. The study also compares inflation swaps with other expectation measures, concluding that market-based measures, particularly through a simple econometric model, offer accurate inflation forecasts.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The study focuses on inflation swaps, not government bonds or yield curves. While it uses econometric models for forecasting, it does not appear to employ machine learning techniques beyond simple linear models."}, "mode": "threadpool", "paper_id": "585f44403016e47b", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Hawkes-diffusion process and the conditional probability of defaults in the Eurozone\nAbstract: This study examines market information embedded in the European sovereign CDS (credit default swap) market by analyzing the sovereign CDSs of 13 Eurozone countries from January 1, 2008, to February 29, 2012, which includes the recent Eurozone debt crisis period. We design the conditional probability of defaults for the CDS prices based on the Hawkes-diffusion process and obtain the theoretical prices of CDS indexes. To estimate the model parameters, we calibrate the model prices to empirical prices obtained from individual sovereign CDS term structure data. The estimated parameters clearly explain both cross-sectional and time-series data. Our empirical results show that the probability of a huge loss event sharply increased during the Eurozone debt crisis, indicating a contagion effect. Even countries with strong and stable economies, such as Germany and France, suffered from the contagion effect. We also find that the probability of small events is sensitive to the state of the economy, spiking several times due to the global financial crisis and the Greek government debt crisis. … [truncated]", "attempt1_tokens": {"prompt": 388, "candidates": 230, "total": 618}, "attempt1_parsed": {"summary": "This study uses a Hawkes-diffusion process to analyze sovereign credit default swap (CDS) prices for 13 Eurozone countries from 2008 to 2012, a period encompassing the Eurozone debt crisis. The model estimates the conditional probability of defaults and theoretical CDS index prices, calibrated against empirical data. Findings indicate a contagion effect during the crisis, impacting even strong economies, and that the probability of smaller events is sensitive to economic conditions.", "criteria_hints": {"I1": true, "I2": true, "I3": true}, "notes": "The study applies a Hawkes-diffusion process, which is a type of stochastic process, to model CDS prices and estimate default probabilities. This can be considered an empirical application beyond simple linear models. The focus is on sovereign CDSs and their relation to default probabilities, fitting criteria I2. The core of the study is estimating the conditional probability of defaults and analyzing how it changes over time and across countries, which directly relates to forecasting/prediction."}, "mode": "threadpool", "paper_id": "0cdd1e39bd843201", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Impact of Wind Electricity Forecasts on Bidding Strategies\nAbstract: The change in the generation mix from conventional electricity sources to renewables has important implications for bidding behaviour and may have an impact on prices. The main goal of this work is to discover the role played by expected wind production, together with other relevant factors, in explaining the day-ahead market price through a data panel model. The Spanish market, given the huge increase in wind generation observed in the last decade, has been chosen for this study as a paradigmatic example. The results obtained suggest that wind power forecasts are a new key determinant for supply market participants when bidding in the day-ahead market. We also provide a conservative quantification of the effect of such trading strategies on marginal prices at an hourly level for a specific year in the sample. The consequence has been an increase in marginal price to levels higher than what could be expected in a context with notable wind penetration. Therefore, the findings of this work are of interest to practitioners and regulators and support the existence of a wind risk premium embedded in electricity  … [truncated]", "attempt1_tokens": {"prompt": 366, "candidates": 144, "total": 510}, "attempt1_parsed": {"summary": "This study investigates the impact of wind electricity forecasts on bidding strategies in the Spanish day-ahead electricity market. Using a data panel model, it finds that wind power forecasts are a significant factor for market participants' bidding behavior, leading to higher marginal prices due to a 'wind risk premium' that compensates for production uncertainty.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The study uses a data panel model, which is not explicitly a machine learning application and may not go beyond simple linear models. It is about forecasting/prediction."}, "mode": "threadpool", "paper_id": "b1f77f14c23e3395", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: How informative are variance risk premium and implied volatility for Value-at-Risk prediction? International evidence\nAbstract: The aim of this paper is to examine the information embedded in the implied volatility index and the variance risk premium in terms of quantifying market risk for developed and emerging stock markets. The backtesting results indicate that incorporating the relative variance risk premium into the GARCH model, greatly enhances the forecasts of one-day-ahead Value-at-Risk (VaR) for a long trading position in developed markets, while the standard GARCH is the most relevant specification in capturing risk in emerging markets. Results are found to be robust against distressed financial markets and alternative measures of the variance risk premium. Moreover, the empirical evidence shows that the superior performance of these models cannot completely reduce the scope of implied volatility as a risk management tool. Including implied volatility into the GARCH model incurs substantial savings in terms of efficient regulatory capital provisions. (C) 2019 Board of Trustees of the University of Illinois. Published by Elsevier Inc. All rights reserved.\n\nCriteria: … [truncated]", "attempt1_tokens": {"prompt": 356, "candidates": 214, "total": 570}, "attempt1_parsed": {"summary": "This paper investigates the predictive power of implied volatility and variance risk premium for Value-at-Risk (VaR) in developed and emerging stock markets. It finds that the variance risk premium improves VaR forecasts in developed markets when incorporated into a GARCH model, while the standard GARCH model is sufficient for emerging markets. Implied volatility also proves valuable for risk management and efficient regulatory capital provisions.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses GARCH models, which are statistical forecasting models, and evaluates their performance in predicting VaR. While not explicitly 'machine learning' in the modern sense, GARCH models are empirical and go beyond simple linear regression, fitting the spirit of I1. The focus is on stock markets, not government bonds, hence I2 is false. The core of the paper is about forecasting VaR, making I3 true."}, "mode": "threadpool", "paper_id": "4ad7c5f96fa212c7", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Intelligent Optimization Based Multi-Factor Deep Learning Stock Selection Model and Quantitative Trading Strategy\nAbstract: With the rapid development of financial research theory and artificial intelligence technology, quantitative investment has gradually entered people's attention. Compared with traditional investment, the advantage of quantitative investment lies in quantification and refinement. In quantitative investment technology, quantitative stock selection is the foundation. Without good stock selection ability, the effect of quantitative investment will be greatly reduced. Therefore, this paper builds an effective multi-factor stock selection model based on intelligent optimization algorithms and deep learning and proposes corresponding trading strategies based on this. First of all, this paper selects 26 effective factors of financial indicators, technical indicators and public opinion to construct the factor database. Secondly, a Gated Recurrent Unit (GRU) neural network based on the Cuckoo Search (CS) optimization algorithm is used to build a stock selection model. Finally, a quantitative investment strategy is designed, and the proposed multi-factor deep lear … [truncated]", "attempt1_tokens": {"prompt": 439, "candidates": 125, "total": 564}, "attempt1_parsed": {"summary": "This paper proposes a multi-factor stock selection model using a GRU neural network optimized by the Cuckoo Search algorithm. The model incorporates 26 factors from financial, technical, and public opinion data. A quantitative trading strategy based on this model demonstrated strong performance in backtesting, outperforming benchmark models.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study focuses on stock selection and quantitative trading strategies, not government bonds."}, "mode": "threadpool", "paper_id": "6b96cc3b210ad231", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Indian equity options: Smile, risk premiums, and efficiency\nAbstract: We study the pricing of equity options in India which is one of the world's largest options markets. Our findings are supportive of market efficiency: A parsimonious smile-adjusted Black model fits option prices well, and the implied volatility (IV) has incremental predictive power for future volatility. However, the risk premium embedded in IV for Single Stock Options appears to be higher than in other markets. The study suggests that even a very liquid market with substantial participation of global institutional investors can have structural features that lead to systematic departures from the behavior of a fully rational market while being microefficient.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 278, "candidates": 152, "total": 430}, "attempt1_parsed": {"summary": "This study examines Indian equity options, finding evidence of market efficiency with a Black model fitting prices well and implied volatility predicting future volatility. However, it notes a potentially higher risk premium in Indian Single Stock Options compared to other markets, suggesting that even liquid markets can exhibit systematic deviations from rational behavior.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The study mentions implied volatility having predictive power for future volatility, which aligns with criterion I3. However, it does not appear to use machine learning beyond simple models, nor does it focus on government bonds or the yield curve."}, "mode": "threadpool", "paper_id": "049416f83ae200ea", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Institutional investor attention and stock market volatility and liquidity: international evidence\nAbstract: In this paper, we examine the influence of the daily institutional investor attention to particular stocks on stock volatility and liquidity. The institutional investor attention is measured from the number of times that users of Bloomberg terminal, who are mostly institutional investors, search for or read articles on a specific stock. Relying on a large international dataset of approximately a million daily observations over the period 2011-2020 from nine countries (Canada, France, Germany, Japan, Russia, South Korea, Switzerland, the UK, and the US), we find that this recent measure of institutional investor attention has a strong positive effect on stock volatility and liquidity. Confirmed by a battery of robustness tests, our findings suggest that this continuous barometer of attention by institutional investors can be used by financial practitioners to predict future stock volatility and liquidity.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is abou … [truncated]", "attempt1_tokens": {"prompt": 325, "candidates": 138, "total": 463}, "attempt1_parsed": {"summary": "This study investigates how institutional investor attention, measured by Bloomberg terminal searches, impacts stock market volatility and liquidity across nine countries from 2011-2020. The findings indicate a significant positive relationship, suggesting this attention metric can predict future stock volatility and liquidity.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The study uses a novel measure of investor attention but does not appear to employ machine learning techniques beyond simple linear models. It focuses on stock market prediction rather than government bonds."}, "mode": "threadpool", "paper_id": "07ccec1141e89737", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Inflation Prediction Method Based on Deep Learning\nAbstract: Forward-looking forecasting of the inflation rate could help the central bank and other government departments to better use monetary policy to stabilize prices and prevent the impact of inflation on market entities, especially for low- and middle-income groups. It can also help financial institutions and investors better make investment decisions. In this sense, the forecast of inflation rate is of great significance. The existing literature mainly uses linear models such as autoregressive (AR) and vector autoregressive (VAR) models to predict the inflation rate. The nonlinear relationship between variables and the mining of historical data information are relatively lacking. Therefore, the prediction strategies and accuracy of the existing literature need to be improved. The predictive model designed in deep learning can fully mine the nonlinear relationship between variables and process complex long-term time series dynamic information, thereby making up for the deficiencies of existing research. Therefore, this paper employs the recurrent neural networks with gated recurrent unit (GRU-RNN) model to train and an … [truncated]", "attempt1_tokens": {"prompt": 410, "candidates": 155, "total": 565}, "attempt1_parsed": {"summary": "This paper proposes a deep learning model, specifically a Gated Recurrent Unit Recurrent Neural Network (GRU-RNN), for predicting China's inflation rate using Consumer Price Index (CPI) data. The authors argue that deep learning models can better capture nonlinear relationships and long-term dynamics compared to traditional linear models like AR and VAR, leading to improved prediction accuracy. Experimental results demonstrate that the GRU-RNN model outperforms traditional methods.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study focuses on inflation prediction, not government/sovereign bonds or the yield curve."}, "mode": "threadpool", "paper_id": "65827861f45c0f78", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Inference on co-integration parameters in heteroskedastic vector autoregressions\nAbstract: We consider estimation and hypothesis testing on the coefficients of the co-integrating relations and the adjustment coefficients in vector autoregressions driven by shocks which display both conditional and unconditional heteroskedasticity of a quite general and unknown form. We show that the conventional results in Johansen (1996) for the maximum likelihood estimators and associated likelihood ratio tests derived under homoskedasticity do not in general hold under heteroskedasticity. As a result, standard confidence intervals and hypothesis tests on these coefficients are potentially unreliable. Solutions based on Wald tests (using a sandwich estimator of the variance matrix) and on the use of the wild bootstrap are discussed. These do not require the practitioner to specify a parametric model for volatility. We establish the conditions under which these methods are asymptotically valid. A Monte Carlo simulation study demonstrates that significant improvements in finite sample size can be obtained by the bootstrap over the corresponding asymptotic tests in both heteroskedastic and ho … [truncated]", "attempt1_tokens": {"prompt": 411, "candidates": 191, "total": 602}, "attempt1_parsed": {"summary": "This paper investigates the estimation and hypothesis testing of co-integration parameters in vector autoregressions with heteroskedasticity. It demonstrates that standard methods assuming homoskedasticity are unreliable under heteroskedasticity and proposes solutions using Wald tests and wild bootstrap, which do not require specifying a parametric volatility model. A simulation study shows bootstrap improvements, and an application to the US term structure of interest rates illustrates differences in inference.", "criteria_hints": {"I1": false, "I2": true, "I3": true}, "notes": "The paper discusses estimation and hypothesis testing, which can be related to forecasting. It also explicitly mentions an application to the term structure of interest rates, which relates to sovereign bonds and the yield curve. However, it does not appear to involve machine learning applications beyond simple linear models."}, "mode": "threadpool", "paper_id": "f02729ef603cf0e9", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Integrated prediction of green bond return under the dual risks of climate change and energy crisis\nAbstract: Prediction of bond return is a classic problem in financial area, providing an important basis for portfolio construction and risk management. The sustainable investment attribute of green bonds has been favored by investors, so that green bonds have become an important component for major asset allocation. However, due to the specific investment focus of green bonds, investors' return expectations are influenced not only by traditional corporate bond factors, but also by related factors such as climate change and energy transition. Against the backdrop of increasingly severe climate risks and the global energy crisis, this paper analyses the volatility characteristics of China's green bonds at multiple time scales, and introduces exogenous variables such as returns of the alternative financial assets, climate risks and returns of energy markets for prediction. Based on the LSTM model, the volatility of green bond yield at different time scales is separately predicted using optimal exogenous variable before integration. It is found that the new integrated prediction  … [truncated]", "attempt1_tokens": {"prompt": 452, "candidates": 138, "total": 590}, "attempt1_parsed": {"summary": "This paper develops an integrated prediction model for green bond returns, incorporating climate change and energy crisis risks. Using an LSTM model, it analyzes volatility at multiple time scales and integrates exogenous variables like alternative asset returns, climate risks, and energy market returns. The model shows improved forecasting performance compared to traditional methods, with climate and energy variables significantly impacting low-frequency and trend components.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study focuses on green bonds, not government/sovereign bonds."}, "mode": "threadpool", "paper_id": "c98db5fb581ab79d", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Inter-Factor Determinants of Return Reversal Effect with Dynamic Bayesian Network Analysis: Empirical Evidence from Pakistan\nAbstract: Bayesian Networks are multivariate probabilistic factor graphs that are used to assess underlying factor relationships. From January 2005 to December 2018, the study examines how Dynamic Bayesian Networks can be utilized to estimate portfolio risk and return as well as determine inter-factor relationships among reversal profit-generating components in Pakistan's emerging market (PSX). The goal of this article is to uncover the factors that cause reversal profits in the Pakistani stock market. In visual form, Bayesian networks can generate causal and inferential probabilistic relationships. Investors might update their stock return values in the network simultaneously with fresh market information, resulting in a dynamic shift in portfolio risk distribution across the networks. The findings show that investments in low net profit margin, low investment, and high volatility-based designed portfolios yield the biggest dynamical reversal profits. The main triggering aspects related to generation reversal profits in the Pakistan market, in the lon … [truncated]", "attempt1_tokens": {"prompt": 405, "candidates": 190, "total": 595}, "attempt1_parsed": {"summary": "This study uses Dynamic Bayesian Networks to analyze inter-factor relationships and estimate portfolio risk and return in Pakistan's stock market from 2005 to 2018. It identifies factors contributing to reversal profits, suggesting that portfolios with low net profit margin, low investment, and high volatility yield the largest reversal profits. Key long-term triggering factors include net profit margin, market risk premium, investment, size, and volatility. The study recommends investing in small companies with low P/E ratios, small EPS, and minimal volatility.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses Dynamic Bayesian Networks, which is a machine learning application beyond simple linear models. It also focuses on forecasting reversal profits and identifying factors that trigger them."}, "mode": "threadpool", "paper_id": "46aa043b290272aa", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Interest rate prediction: a neuro-hybrid approach with data preprocessing\nAbstract: The following research implements a differential evolution-based fuzzy-type clustering method with a fuzzy inference neural network after input preprocessing with regression analysis in order to predict future interest rates, particularly 3-month T-bill rates. The empirical results of the proposed model is compared against nonparametric models, such as locally weighted regression and least squares support vector machines, along with two linear benchmark models, the autoregressive model and the random walk model. The root mean square error is reported for comparison.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 257, "candidates": 139, "total": 396}, "attempt1_parsed": {"summary": "This study proposes a neuro-hybrid model using differential evolution-based fuzzy-type clustering and a fuzzy inference neural network, preceded by regression analysis, to forecast 3-month T-bill interest rates. The model's performance is evaluated against nonparametric and linear benchmark models using root mean square error.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "While the study predicts interest rates, it specifically focuses on T-bill rates, not government/sovereign bonds or the yield curve in general."}, "mode": "threadpool", "paper_id": "ae189095c8b70524", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Interest rate models on Lie groups\nAbstract: This paper examines an alternative approach to interest rate modeling, in which the nonlinear and random behavior of interest rates is captured by a stochastic differential equation evolving on a curved state space. We consider as candidate state spaces the matrix Lie groups; these offer not only a rich geometric structure, butunlike general Riemannian manifoldsalso allow for diffusion processes to be constructed easily without invoking the machinery of stochastic calculus on manifolds. After formulating bilinear stochastic differential equations on general matrix Lie groups, we then consider interest rate models in which the short rate is defined as linear or quadratic functions of the state. Stochastic volatility is also augmented to these models in a way that respects the Riemannian manifold structure of symmetric positive-definite matrices. Methods for numerical integration, parameter identification, pricing, and other practical issues are addressed through examples.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is  … [truncated]", "attempt1_tokens": {"prompt": 311, "candidates": 170, "total": 481}, "attempt1_parsed": {"summary": "This paper proposes an alternative approach to interest rate modeling using stochastic differential equations on matrix Lie groups, which offer a rich geometric structure. The models capture nonlinear and random interest rate behavior, with the short rate defined as linear or quadratic functions of the state. Stochastic volatility is incorporated while respecting the manifold structure of symmetric positive-definite matrices. Practical aspects like numerical integration, parameter identification, and pricing are discussed with examples.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The paper focuses on theoretical modeling of interest rates using Lie groups and does not appear to include empirical machine learning applications, specific focus on government bonds/yield curve, or explicit forecasting/prediction tasks."}, "mode": "threadpool", "paper_id": "471849c4c2e8e8d5", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Investors' and Central Bank's Uncertainty Embedded in Index Options\nAbstract: Shocks to equity options' implied volatility are followed by persistently lower short-term rates. Shocks to puts' over calls' out-of-the-money implied volatilities (P/C) are followed by persistently higher rates. Stock and Treasury bond implied volatilities, which measure market and policy uncertainty, are countercyclical, while P/C, which measures downside risk, is procyclical. An equilibrium model in which investors and the central bank learn about composite regimes of economic and policy variables explains these dynamics, linking them to a learning-based, forward-looking Taylor rule. Survey data support our model's predictions on the effect of uncertainty on the level and fluctuations of implied volatilities.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 304, "candidates": 166, "total": 470}, "attempt1_parsed": {"summary": "This paper investigates how investor and central bank uncertainty, as reflected in equity options' implied volatility, influences short-term interest rates. It finds that shocks to implied volatility lead to lower rates, while shocks to the ratio of put to call implied volatilities (P/C) lead to higher rates. The study proposes an equilibrium model incorporating learning about economic and policy variables to explain these dynamics, supported by survey data on uncertainty's impact on implied volatilities.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The paper discusses forecasting of interest rates based on implied volatilities, but does not use machine learning or focus on government bonds."}, "mode": "threadpool", "paper_id": "3e4b27043ebffa04", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Investor Sentiment and Bond Risk Premia: Evidence from China\nAbstract: This article shows the statistical signi?cance of a set of variables related to market sentiment and uses them to predict the risk premium embedded in China's sovereign bonds. We construct a composite index of market-wide investor sentiment as a linear combination of proxies for a degree of market participation and risk appetite of investors. Further, we show that these sentiment-related factors can be summarized in a single-return forecasting factor, similar in a spirit of Cochrane and Piazzesi (2005). Our empirical results show that this sentiment factor has predictive power beyond that contained in the yield curve and macroeconomic variables, and this predictability is robust for out-of-sample testing. In addition, the predictive power of the sentiment factor shows relevance during the 2008 global financial crisis, indicating that the forecasting ability of investor sentiment is mainly derived by a sentiment-induced flight-to-quality.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about go … [truncated]", "attempt1_tokens": {"prompt": 341, "candidates": 167, "total": 508}, "attempt1_parsed": {"summary": "This study constructs a composite index of investor sentiment in China's sovereign bond market, combining proxies for market participation and risk appetite. This sentiment index is shown to predict bond risk premia, outperforming traditional yield curve and macroeconomic variables, particularly during the 2008 financial crisis, suggesting a flight-to-quality effect.", "criteria_hints": {"I1": false, "I2": true, "I3": true}, "notes": "The abstract mentions a \"forecasting factor\" and predictive power, indicating I3 is true. The focus on sovereign bonds makes I2 true. However, the abstract does not explicitly mention machine learning or models beyond simple linear ones, making I1 false."}, "mode": "threadpool", "paper_id": "85c477dfff28cf7e", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Is the term structure nonlinear? A semiparametric investigation\nAbstract: A semiparametric error correction model (ECM) is estimated using US term structure data. We use 5 and 10 year interest rates to predict short-term (1 month to 12 month) interest rates. It is found that the semiparametric ECM model predicts better than the popular linear ECM. These results provide further evidence of nonlinearity in the term structure.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 239, "candidates": 154, "total": 393}, "attempt1_parsed": {"summary": "This study estimates a semiparametric error correction model (ECM) using US term structure data, predicting short-term interest rates with 5 and 10-year rates. The semiparametric ECM outperforms the linear ECM, suggesting nonlinearity in the term structure.", "criteria_hints": {"I1": true, "I2": true, "I3": true}, "notes": "The study explicitly compares a semiparametric model to a linear model, indicating it goes beyond simple linear approaches. The data used is US term structure data, which relates to government bonds and the yield curve. The abstract mentions prediction of interest rates."}, "mode": "threadpool", "paper_id": "ea00aca5b04457b7", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Is There an On-the-Run Premium in TIPS?\nAbstract: In the U.S. Treasury market, the most recently issued, or so-called on-the-run, security typically trades at a price above those of more seasoned but otherwise comparable securities. This difference is known as the on-the-run premium. In this paper, yield spreads between pairs of Treasury Inflation-Protected Securities (TIPS) with both matching and nearly-matching maturities but of separate vintages are analyzed. Adjusting for differences in conventional liquidity premiums, values of embedded deflation options, and coupon rates, the results show a small, insignificant premium on recently issued TIPS, which leads us to conclude that there is no on-the-run premium in the TIPS market.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 300, "candidates": 135, "total": 435}, "attempt1_parsed": {"summary": "This paper analyzes yield spreads between Treasury Inflation-Protected Securities (TIPS) of different vintages. After adjusting for liquidity premiums, deflation options, and coupon rates, the study finds a small, insignificant premium on recently issued TIPS, concluding that there is no on-the-run premium in the TIPS market.", "criteria_hints": {"I1": false, "I2": true, "I3": false}, "notes": "The study analyzes yield spreads and does not appear to use machine learning beyond simple models, nor does it focus on forecasting."}, "mode": "threadpool", "paper_id": "1a7da81a013d9178", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Investor attention and Google Search Volume Index: Evidence from an emerging market using quantile regression analysis\nAbstract: This study investigates whether the investor attention measured by the Google Search Volume Index (GSVI) is effective in forecasting stock returns. The evolving literature on investor attention suggests that higher GSVI can predict higher returns for the first one or two weeks, but with a subsequent price reversal. We use a more recent dataset that covers S&P BSE 500 companies listed on the Indian stock exchange for 2012-2017 and employ the quantile regression approach because it alleviates the statistical problems arising from biased distribution data. The results suggest that a higher GSVI predicts positive and significant returns in the subsequent first and second weeks. Higher quantiles of GSVI experience higher excess returns. The panel cointegration test results support the findings regarding the cointegration of the GSVI and stock returns. Our empirical evidence shows that our model is robust when using a trading strategy based on the Fama-French four-factor model. Thus, the model with GSVI acts as a better predictor of both the direction an … [truncated]", "attempt1_tokens": {"prompt": 384, "candidates": 221, "total": 605}, "attempt1_parsed": {"summary": "This study examines the predictive power of Google Search Volume Index (GSVI) for stock returns in the Indian emerging market (S&P BSE 500, 2012-2017). Using quantile regression, it finds that higher GSVI predicts positive and significant returns in the first two weeks, with higher quantiles of GSVI yielding higher excess returns. Panel cointegration tests support the GSVI-stock return relationship, and a trading strategy based on the Fama-French four-factor model confirms the model's robustness. The GSVI model is shown to be a better predictor of excess returns than models without it.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses quantile regression, which is a more advanced statistical method than simple linear models, and applies it to an empirical dataset for forecasting stock returns. It does not involve government bonds or sovereign debt."}, "mode": "threadpool", "paper_id": "26003fee30aeb702", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Is the short rate drift actually nonlinear?\nAbstract: Ait-Sahalia (1996) and Stanton (1997) use nonparametric estimators applied to short-term interest rate data to conclude that the drift function contains important nonlinearities. We study the finite-sample properties of their estimators by applying them to simulated sample paths of a square-root diffusion. Although the drift function is linear, both estimators suggest nonlinearities of the type and magnitude reported in Ait-Sahalia (1996) and Stanton (1997). Combined with the results of a weighted least squares estimator, this evidence implies that nonlinearity of the short rate drift is not a robust stylized fact.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 286, "candidates": 169, "total": 455}, "attempt1_parsed": {"summary": "This paper investigates the robustness of nonlinearities in the short rate drift, as suggested by previous nonparametric estimators. By applying these estimators to simulated data from a linear drift model, the authors demonstrate that the estimators can falsely detect nonlinearities. This suggests that the nonlinearity of the short rate drift may not be a reliable characteristic.", "criteria_hints": {"I1": true, "I2": false, "I3": false}, "notes": "The paper uses nonparametric estimators which go beyond simple linear models, hence I1 is true. However, it focuses on short-term interest rates and not specifically government bonds or the yield curve, and it examines the properties of estimators rather than forecasting, making I2 and I3 false."}, "mode": "threadpool", "paper_id": "465719be2fd4d237", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: LAND OF ADDICTS? AN EMPIRICAL INVESTIGATION OF HABIT-BASED ASSET PRICING MODELS\nAbstract: This paper Studies the ability of a general class of habit-based asset pricing models to match the conditional moment restrictions implied by asset pricing theory. We treat the functional form of the habit as unknown, and estimate it along with the rest of the model's finite dimensional parameters. Using quarterly data on consumption growth, assets returns and instruments, our empirical results indicate that the estimated habit function is nonlinear, that habit formation is better described as internal rather than external, and the estimated time-preference parameter and the power utility parameter are sensible. In addition, the estimated habit function generates a positive stochastic discount factor (SDF) proxy and performs well in explaining cross-sectional stock return data. We find that an internal habit SDF proxy can explain a cross-section of size and book-market sorted portfolio equity returns better than (i) the Fama and French (1993) three-factor model, (ii) the Lettau and LUdvigson (2001b) scaled consumption CAPM model, (iii) an external habit SDF proxy, (iv) the classic CAPM, … [truncated]", "attempt1_tokens": {"prompt": 421, "candidates": 147, "total": 568}, "attempt1_parsed": {"summary": "This empirical study investigates habit-based asset pricing models by estimating an unknown habit function using quarterly data. The findings suggest a nonlinear, internal habit formation, with sensible parameter estimates. The model effectively explains cross-sectional stock returns, outperforming several benchmark models including the Fama-French three-factor model and scaled consumption CAPM.", "criteria_hints": {"I1": true, "I2": false, "I3": false}, "notes": "The study uses empirical data and compares its model to other established models, suggesting it goes beyond simple linear models. It does not focus on government bonds or forecasting."}, "mode": "threadpool", "paper_id": "e63ce5fd8a54f209", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Jump and variance risk premia in the S&P 500\nAbstract: We analyze the risk premia embedded in the S&P 500 spot index and option markets. We use a long time series of spot prices and a large panel of option prices to jointly estimate the diffusive stock risk premium, the price jump risk premium, the diffusive variance risk premium and the variance jump risk premium. The risk premia are statistically and economically significant and move over time. Investigating the economic drivers of the risk premia, we are able to explain up to 63% of these variations. (C) 2016 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 286, "candidates": 165, "total": 451}, "attempt1_parsed": {"summary": "This study analyzes risk premia in the S&P 500 index and its options market. It jointly estimates diffusive and jump risk premia for both stock and variance, finding them to be significant and time-varying. The research also identifies economic drivers that explain a substantial portion of these variations.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The abstract does not mention machine learning applications beyond simple linear models, nor does it focus on government bonds or the yield curve. While it discusses risk premia which can be related to forecasting, the primary focus is on estimation and explanation of existing premia rather than explicit forecasting."}, "mode": "threadpool", "paper_id": "c35e1a92cfdc2c3d", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Learning and forecasts about option returns through the volatility risk premium\nAbstract: We use learning in an equilibrium model to explain the puzzling predictive power of the volatility risk premium (VRP) for option returns. In the model, a representative agent follows a rational Bayesian learning process in an economy under incomplete information with the objective of pricing options. We show that learning induces dynamic differences between probability measures P and Q, which produces predictability patterns from the VRP for option returns. The forecasting features of the VRP for option returns, obtained through our model, exhibit the same behaviour as those observed in an empirical analysis with S&P 500 index options. (C) 2017 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 293, "candidates": 156, "total": 449}, "attempt1_parsed": {"summary": "This paper develops an equilibrium model where rational Bayesian learning explains the predictive power of the volatility risk premium (VRP) for option returns. The model demonstrates how learning creates dynamic differences between probability measures P and Q, leading to VRP-based predictability in option returns, consistent with empirical findings from S&P 500 index options.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The study uses an equilibrium model with learning to explain option return predictability, which is then empirically validated. While it involves prediction, it does not appear to use machine learning applications beyond simple linear models."}, "mode": "threadpool", "paper_id": "d29dd67ecedbed99", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Kernel-Based Aggregating Learning System for Online Portfolio Optimization\nAbstract: Recently, various machine learning techniques have been applied to solve online portfolio optimization (OLPO) problems. These approaches typically explore aggressive strategies to gain excess returns due to the existence of irrational phenomena in financial markets. However, existing aggressive OLPO strategies rarely consider the downside risk and lack effective trend representation, which leads to poor prediction performance and large investment losses in certain market environments. Besides, prediction with a single model is often unstable and sensitive to the noises and outliers, and the subsequent selection of optimal parameters also become obstacles to accurate estimation. To overcome these drawbacks, this paper proposes a novel kernel-based aggregating learning (KAL) system for OLPO. It includes a two-step price prediction scheme to improve the accuracy and robustness of the estimation. Specifically, a component price estimator is built by exploiting additional indicator information and the nonstationary nature of financial time series, and then an aggregating learning method is presen … [truncated]", "attempt1_tokens": {"prompt": 445, "candidates": 180, "total": 625}, "attempt1_parsed": {"summary": "This paper introduces a novel kernel-based aggregating learning (KAL) system for online portfolio optimization (OLPO) to address limitations of existing aggressive strategies, such as neglecting downside risk and poor trend representation. The KAL system features a two-step price prediction scheme using component estimators and an aggregating learning method, enhanced by a kernel-based increasing factor for wealth maximization. An online learning algorithm is designed for efficiency. Experiments show KAL outperforms state-of-the-art systems in cumulative wealth and risk-adjusted metrics, and can handle transaction costs.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The article focuses on portfolio optimization and price prediction using machine learning, not specifically on government bonds or the yield curve."}, "mode": "threadpool", "paper_id": "84a95360c6c72c18", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Likelihood-based scoring rules for comparing density forecasts in tails\nAbstract: We propose new scoring rules based on conditional and censored likelihood for assessing the predictive accuracy of competing density forecasts over a specific region of interest, such as the left tail in financial risk management. These scoring rules can be interpreted in terms of Kullback-Leibler divergence between weighted versions of the density forecast and the true density. Existing scoring rules based on weighted likelihood favor density forecasts with more probability mass in the given region, rendering predictive accuracy tests biased toward such densities. Using our novel likelihood-based scoring rules avoids this problem. (C) 2011 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 283, "candidates": 122, "total": 405}, "attempt1_parsed": {"summary": "This paper introduces new scoring rules for comparing density forecasts, focusing on specific regions of interest like financial risk management tails. These rules are based on conditional and censored likelihood and address biases in existing methods that favor forecasts with more mass in the region of interest.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The paper discusses forecasting and prediction methods but does not appear to involve machine learning applications or government bonds/yield curves."}, "mode": "threadpool", "paper_id": "6603d19b5d0f44e3", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Likelihood inference for dynamic linear models with Markov switching parameters: on the efficiency of the Kim filter\nAbstract: The Kim filter (KF) approximation is widely used for the likelihood calculation of dynamic linear models with Markov regime-switching parameters. However, despite its popularity, its approximation error has not yet been examined rigorously. Therefore, this study investigates the reliability of the KF approximation for maximum likelihood (ML) and Bayesian estimations. To measure the approximation error, we compare the outcomes of the KF method with those of the auxiliary particle filter (APF). The APF is a numerical method that requires a longer computing time, but its numerical error can be sufficiently minimized by increasing simulation size. According to our extensive simulation and empirical studies, the likelihood values obtained from the KF approximation are practically identical to those of the APF. Furthermore, we show that the KF method is reliable, particularly when regimes are persistent and sample size is small. From the Bayesian perspective, we show that the KF method improves the efficiency of posterior simulation. This study contributes … [truncated]", "attempt1_tokens": {"prompt": 373, "candidates": 223, "total": 596}, "attempt1_parsed": {"summary": "This study evaluates the accuracy of the Kim filter (KF) approximation for dynamic linear models with Markov switching parameters, comparing it to the auxiliary particle filter (APF). The findings indicate that the KF approximation is highly reliable, producing likelihood values virtually identical to the APF, especially when regimes are persistent and sample sizes are small. The research also demonstrates that the KF method enhances the efficiency of posterior simulations in Bayesian estimations, thus justifying its use in both maximum likelihood and Bayesian contexts.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The study focuses on the efficiency and reliability of a specific filtering method (Kim filter) for dynamic linear models with Markov switching parameters. While it mentions empirical studies, the core contribution is methodological rather than a specific ML application. It does not discuss government bonds or yield curves. However, the context of dynamic linear models and parameter estimation strongly implies a forecasting or prediction objective, hence I3 is true."}, "mode": "threadpool", "paper_id": "0bd417484ae81bb2", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Likelihood-based specification analysis of continuous-time models of the short-term interest rate\nAbstract: An extensive collection of continuous-time models of the short-term interest rate is evaluated over data sets that have appeared previously in the literature. The analysis, which uses the simulated maximum likelihood procedure proposed by Durham and Gallant (2002), provides new insights regarding several previously unresolved questions. For single factor models, I find that the volatility, not the drift, is the critical component in model specification. Allowing for additional flexibility beyond a constant term in the drift provides negligible benefit. While constant drift would appear to imply that the short rate is nonstationary, in fact, stationarity is volatility-induced. The simple constant elasticity of volatility model fits weekly observations of the three-month Treasury bill rate remarkably well but is easily rejected when compared with more flexible volatility specifications over daily data. The methodology of Durham and Gallant can also be used to estimate stochastic volatility models. While adding the latent volatility component provides a large improvement  … [truncated]", "attempt1_tokens": {"prompt": 386, "candidates": 145, "total": 531}, "attempt1_parsed": {"summary": "This paper evaluates continuous-time models of short-term interest rates using simulated maximum likelihood. It finds that volatility is more critical than drift for model specification and that a simple constant elasticity of volatility model fits weekly Treasury bill rate data well, but is rejected by daily data. Stochastic volatility models improve likelihood but not bond-pricing performance.", "criteria_hints": {"I1": false, "I2": true, "I3": false}, "notes": "The paper uses statistical modeling and simulation, not machine learning. While it analyzes bond data, the focus is on model specification and evaluation rather than forecasting."}, "mode": "threadpool", "paper_id": "dbc0a172a842c869", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: MARKET MANIPULATION: A SURVEY\nAbstract: Despite the significant attention that market manipulation has received in recent years many aspects of it are poorly understood. This article identifies from the theoretical and empirical literature what we do and do not know about market manipulation, and suggests directions for future research. We know that manipulation is possible and that it occurs in a wide variety of markets and circumstances. In contrast, we know little about how often manipulation occurs, its effects and how it responds to regulation. Suggested approaches for future research on these issues include: (1) collecting more comprehensive data sets of manipulation cases; (2) using detection controlled estimation methods to overcome sample selection and partial observability problems and (3) conducting controlled experiments. This article also constructs a novel and broad taxonomy of the different types of market manipulation and discusses approaches to defining manipulation.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds … [truncated]", "attempt1_tokens": {"prompt": 312, "candidates": 158, "total": 470}, "attempt1_parsed": {"summary": "This survey paper reviews the literature on market manipulation, identifying what is known and unknown about its occurrence, effects, and regulatory responses. It proposes future research directions, including data collection, advanced estimation methods, and controlled experiments, and offers a taxonomy of manipulation types.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The article discusses empirical aspects and future research methods but does not appear to apply machine learning itself. It is a broad survey of market manipulation, not specific to sovereign bonds or yield curves, and while it touches on prediction in the context of future research, its primary focus is not forecasting."}, "mode": "threadpool", "paper_id": "08bba97ee6005b87", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Macroeconomic attention, economic policy uncertainty, and stock volatility predictability\nAbstract: This study adopts the newly constructed macroeconomic attention indices (MAI) and category-specific economic policy uncertainty (EPU) indices to predict stock volatility. Principal component analysis (PCA), scaled PCA (sPCA), and partial least squares (PLS) are used to extract the principal components from indicators. The results show that the combination of MAI and EPU indices can obtain additional information for predicting stock market volatility. In addition, the comprehensive index containing all indicator information (F-t(All)) has the strongest short-term forecasting ability, whereas the MAI show the most substantial forecasting ability in long-term forecasting.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 285, "candidates": 181, "total": 466}, "attempt1_parsed": {"summary": "This study uses macroeconomic attention indices (MAI) and economic policy uncertainty (EPU) indices to predict stock volatility. It employs Principal Component Analysis (PCA), scaled PCA (sPCA), and Partial Least Squares (PLS) for component extraction. The findings indicate that combining MAI and EPU improves stock market volatility prediction. A comprehensive index (F-t(All)) shows strong short-term forecasting ability, while MAI demonstrates superior long-term forecasting power.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses PCA, sPCA, and PLS, which are machine learning techniques beyond simple linear models. The focus is on stock volatility prediction, not government bonds or the yield curve."}, "mode": "threadpool", "paper_id": "f8d976b8b744af6c", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Market Returns and a Tale of Two Types of Attention\nAbstract: We provide novel evidence that aggregate investor attention to stocks predicts marketwide returns, but with a striking difference across investor clienteles. Daily aggregate retail attention (ARA) negatively predicts one-week-ahead market returns, is associated with aggregate retail order imbalance and flows to equity mutual funds, and exhibits a stronger predictability during periods of high marketwide uncertainty, poor liquidity, or more costly short selling. In contrast, aggregate institutional attention (AIA), when observed before major news announcements, positively predict future marketwide returns. In cross-sectional analysis, we show that the predictability is stronger for ARA among illiquid stocks and for AIA among high-beta stocks. The predictability results are robust out-of-sample and correspond to meaningful expected utility gains even for diversified investors. The findings are consistent with the idea that attention-driven retail buying can generate an aggregate price pressure on the stock market, whereas institutional attention precedes the resolution of marketwide uncertainty and the accrual of ri … [truncated]", "attempt1_tokens": {"prompt": 354, "candidates": 170, "total": 524}, "attempt1_parsed": {"summary": "This study investigates how aggregate investor attention, categorized into retail (ARA) and institutional (AIA), influences market returns. ARA negatively predicts near-term returns and is linked to retail trading behavior, especially during uncertain or illiquid market conditions. AIA, particularly around news events, positively predicts returns. The findings suggest retail attention can cause price pressure, while institutional attention anticipates market uncertainty resolution and risk premium accrual.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses empirical methods to analyze market returns and investor attention, going beyond simple linear models by examining different investor types and market conditions. It clearly focuses on prediction and forecasting of market returns."}, "mode": "threadpool", "paper_id": "cda1c91e62a7ddb3", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Linear Gaussian affine term structure models with unobservable factors: Calibration and yield forecasting\nAbstract: This paper provides a significant numerical evidence for out-of-sample forecasting ability of linear Gaussian interest rate models with unobservable underlying factors. We calibrate one, two and three factor linear Gaussian models using the Kalman filter oil two different bond yield data sets and compare their out-of-sample forecasting performance. One-step ahead as well as four-step ahead out-of-sample forecasts are analyzed based on the weekly data. When evaluating the one-step ahead forecasts, it is shown that a one factor model may be adequate when only the short-dated or only the long-dated yields are considered, but two and three factor models performs significantly better when the entire yield spectrum is considered. Furthermore, the results demonstrate that the predictive ability of multi-factor models remains intact far ahead out-of-sample, with accurate predictions available up to one year after the last calibration for one data set and up to three months after the last calibration for the second, more volatile data set. The experimental data denotes  … [truncated]", "attempt1_tokens": {"prompt": 558, "candidates": 196, "total": 754}, "attempt1_parsed": {"summary": "This paper investigates the out-of-sample forecasting performance of linear Gaussian interest rate models with unobservable factors. It calibrates one, two, and three-factor models using the Kalman filter on two bond yield datasets and compares their forecasting accuracy for one-step and four-step ahead predictions. The study finds that multi-factor models generally outperform one-factor models, especially when considering the entire yield spectrum. The paper also proposes a non-linear filter based on linear programming to improve term structure matching, which enhances out-of-sample fit with minimal computational cost.", "criteria_hints": {"I1": true, "I2": true, "I3": true}, "notes": "The paper uses linear Gaussian models but also proposes a non-linear filter, suggesting it goes beyond simple linear models. The focus is on bond yield data and forecasting."}, "mode": "threadpool", "paper_id": "1bbee78dc006f8fe", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Long memory affine term structure models\nAbstract: We develop a Gaussian discrete time essentially affine term structure model with long, memory state variables. This feature reconciles the strong persistence observed in nominal yields and inflation with the theoretical implications of affine models, especially for long maturities. We characterize in closed form the dynamic and cross-sectional implications of long memory for our model. We explain how long memory can naturally arise within the term structure of interest rates, providing a theoretical underpinning for our model. Despite the infinite-dimensional structure that long memory implies, we show how to cast the model in state space and estimate it by maximum likelihood. An empirical application of our model is presented. (C) 2015 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 296, "candidates": 210, "total": 506}, "attempt1_parsed": {"summary": "This paper presents a Gaussian discrete time essentially affine term structure model incorporating long memory state variables. This model addresses the strong persistence in yields and inflation, aligning with affine model theory for long maturities. It provides closed-form solutions for the dynamic and cross-sectional impacts of long memory, explaining its natural emergence in interest rates. The model is cast in state space for maximum likelihood estimation, despite its infinite-dimensional nature, and includes an empirical application.", "criteria_hints": {"I1": true, "I2": true, "I3": true}, "notes": "The abstract mentions an \"empirical application\" and \"estimate it by maximum likelihood\", suggesting it goes beyond pure theory. The topic is explicitly about the \"term structure of interest rates\" and \"yields\", fitting I2. The model's focus on \"dynamic implications\" and \"forecasting\" (implied by term structure modeling) fits I3."}, "mode": "threadpool", "paper_id": "b86b59095cc0f715", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Maximum likelihood estimation of partially observed diffusion models\nAbstract: This paper develops a maximum likelihood (ML) method to estimate partially observed diffusion models based on data sampled at discrete times. The method combines two techniques recently proposed in the literature in two separate steps. In the first step, the closed form approach of Ait-Sahalia (2008) is used to obtain a highly accurate approximation to the joint transition probability density of the latent and the observed states. In the second step, the efficient importance sampling technique of Richard and Zhang (2007) is used to integrate out the latent states, thereby yielding the likelihood function. Using both simulated and real data, we show that the proposed ML method works better than alternative methods. The new method does not require the underlying diffusion to have an affine structure and does not involve infill simulations. Therefore, the method has a wide range of applicability and its computational cost is moderate. (C) 2014 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple l … [truncated]", "attempt1_tokens": {"prompt": 349, "candidates": 175, "total": 524}, "attempt1_parsed": {"summary": "This paper proposes a maximum likelihood estimation method for partially observed diffusion models using discrete-time sampled data. It combines Ait-Sahalia's (2008) closed-form approximation for transition densities with Richard and Zhang's (2007) importance sampling technique to integrate out latent states and obtain the likelihood function. The method is shown to outperform alternatives on simulated and real data, offering broad applicability due to its independence from affine structures and infill simulations, with moderate computational cost.", "criteria_hints": {"I1": true, "I2": false, "I3": false}, "notes": "The paper uses simulation and real data, indicating an empirical application. The diffusion models and estimation techniques described go beyond simple linear models."}, "mode": "threadpool", "paper_id": "5080e4a5f3be7be5", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Maximum Likelihood Estimation in Markov Regime-Switching Models With Covariate-Dependent Transition Probabilities\nAbstract: This paper considers maximum likelihood (ML) estimation in a large class of models with hidden Markov regimes. We investigate consistency of the ML estimator and local asymptotic normality for the models under general conditions, which allow for autoregressive dynamics in the observable process, Markov regime sequences with covariate-dependent transition matrices, and possible model misspecification. A Monte Carlo study examines the finite-sample properties of the ML estimator in correctly specified and misspecified models. An empirical application is also discussed.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 264, "candidates": 159, "total": 423}, "attempt1_parsed": {"summary": "This paper focuses on maximum likelihood estimation for Markov regime-switching models, particularly when transition probabilities depend on covariates. It examines the theoretical properties of the estimator, including consistency and local asymptotic normality, under various conditions such as autoregressive dynamics and potential model misspecification. The study is supported by a Monte Carlo simulation to assess finite-sample performance and includes an empirical application.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The empirical application mentioned in the abstract suggests it goes beyond pure theory. The abstract mentions 'forecasting or prediction' implicitly through the context of regime-switching models and their application."}, "mode": "threadpool", "paper_id": "53a26b3792f39f00", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Measuring ESG risk premia with contingent claims\nAbstract: We propose a contingent claims approach for estimating ESG risk premia from market information and market participants' decisions. To this end, we infer the asset value dynamics via the structural model of Merton [1974, On the Pricing of Corporate Debt: The Risk Structure of Interest Rates. Journal of Finance 29: 449-470.] for a large panel of S&P 500 firms using an estimation algorithm that utilizes the information embedded in stock market prices, CDS spreads, and default probabilities. We find a statistically significant relationship between the ESG score and the volatility and drift terms of the asset value process, suggesting that ESG factors are structurally connected to the value of the firm. We establish a mapping between ESG scores and the cost of equity and debt as implied by firm's contingent claims, and derive estimates of the ESG risk premium across different ESG and leverage profiles. In addition, we break down the ESG risk premia by industry, and demonstrate how practitioners can adjust the weighed average cost of capital of ESG laggard firms for valuation and decision making purposes.\n\nCriteria: {\"I1\": … [truncated]", "attempt1_tokens": {"prompt": 377, "candidates": 180, "total": 557}, "attempt1_parsed": {"summary": "This paper proposes a contingent claims approach to estimate ESG risk premia using market data and participant decisions. It applies Merton's structural model to S&P 500 firms, incorporating stock prices, CDS spreads, and default probabilities. The study finds a link between ESG scores and asset value dynamics, implying ESG factors influence firm value. It derives ESG risk premia and shows how to adjust the cost of capital for ESG laggard firms.", "criteria_hints": {"I1": true, "I2": false, "I3": false}, "notes": "The paper uses an estimation algorithm and structural models which can be considered empirical applications beyond simple linear models. It does not focus on government bonds or the yield curve, nor is its primary focus forecasting or prediction."}, "mode": "threadpool", "paper_id": "97f84ada8039939e", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Man versus Machine Learning Revisited\nAbstract: Binsbergen, Han, and Lopez-Lira (2023) predict analysts' forecast errors using a random forest model. A strategy that trades against this model's predictions earns a monthly alpha of 1.54% ($ t $-value = 5.84). This estimate represents a large improvement over studies using classical statistical methods. We attribute the difference to a look-ahead bias. Removing the bias erases the alpha. Linear models yield as accurate forecasts and superior trading profits. Neither alternative machine learning models nor combinations thereof resurrect the predictability. We discuss the state of research into the term structure of analysts' forecasts and its causal relationship with returns.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 290, "candidates": 209, "total": 499}, "attempt1_parsed": {"summary": "This study re-examines the use of machine learning (ML) in predicting analysts' forecast errors, finding that a random forest model initially suggested a profitable trading strategy. However, upon closer inspection, this profitability was attributed to a look-ahead bias. After correcting for this bias, the alpha disappeared. The study concludes that linear models are equally accurate for forecasting and yield superior trading profits, and that other ML models do not revive the predictability. The research also touches upon the term structure of analysts' forecasts and its link to returns.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses a random forest model, which is a machine learning application beyond simple linear models. It focuses on analyst forecast errors and their relationship with returns, which implies forecasting. However, the abstract does not mention government/sovereign bonds or the yield curve."}, "mode": "threadpool", "paper_id": "44be03948484da4f", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Measuring Risk Premiums Using Financial Reports and Actuarial Disclosures\nAbstract: Insurance companies increasingly augment their financial reports by releasing actuarial measures the so-called embedded value to supply information about the value of their life insurance activities. Both accounting and actuarial measures differ with respect to the timeliness of profit realisation and its reliability, and their performance in yielding information may differ. This paper asks if and how embedded values help in assessing risk premiums. We estimate multifactor market models in the spirit of Fama and French, and find that actuarial disclosures are superior to financial accounting in estimating these risk premiums. They further add information to financial reports as an estimator for growth opportunities.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 280, "candidates": 142, "total": 422}, "attempt1_parsed": {"summary": "This paper investigates whether actuarial disclosures, specifically embedded values, improve the estimation of risk premiums compared to traditional financial reports for insurance companies. Using multifactor market models, the study finds that actuarial disclosures provide superior information for estimating risk premiums and offer additional insights into growth opportunities.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The study uses multifactor market models, which are statistical models but not typically considered machine learning in the context of the criteria. The study does focus on prediction/forecasting of risk premiums."}, "mode": "threadpool", "paper_id": "bcd2cc2bd87fa5d9", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Merchant Commodity Storage and Term-Structure Model Error\nAbstract: Merchant operations involves valuing and hedging the cash flows of commodity- and energy-conversion assets as real options based on stochastic models that inevitably embed model error. In this paper we quantify how empirically calibrated model errors concerning the futures term structure affect the valuation and hedging of natural gas storage. We find that even small model errors-on the order of 1%-2% of the empirical futures price variance-can have a disproportionate impact on storage valuation and hedging. In particular, theoretically equivalent hedging strategies have very different sensitivities to model error, with one natural strategy exhibiting potentially catastrophic performance in the presence of small model errors. We propose effective approaches to mitigate the negative effect of futures term-structure model error on hedging, also taking into account futures contract illiquidity, and provide theoretical justification for some of these approaches. Beyond commodity storage, our analysis has relevance for other real and financial options that depend on futures term-structure dynamics, as well as for … [truncated]", "attempt1_tokens": {"prompt": 363, "candidates": 170, "total": 533}, "attempt1_parsed": {"summary": "This paper quantifies the impact of empirically calibrated model errors in futures term structure on the valuation and hedging of natural gas storage. It finds that even small errors can significantly affect storage valuation and hedging, leading to potentially catastrophic performance for certain hedging strategies. The study proposes mitigation approaches and discusses broader relevance for other options and investment policies.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The paper discusses model error and hedging strategies, which relates to prediction and forecasting of futures prices and their impact on storage valuation. However, it does not explicitly mention machine learning applications beyond simple linear models, nor does it focus on government/sovereign bonds or the yield curve."}, "mode": "threadpool", "paper_id": "465b71f382fbc310", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Measuring the natural rate of interest\nAbstract: The natural rate of interest-the real interest rate consistent with output equaling its natural rate and stable inflation-plays a central role in macroeconomic theory and monetary policy. Estimation of the natural rate of interest, however, has received little attention. We apply the Kalman filter to estimate jointly time-varying natural rates of interest and output and trend growth. We find a close link between the natural rate of interest and the trend growth rate, as predicted by theory. Estimates of the natural rate of interest, however, are very imprecise and subject to considerable real-time measurement error.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 268, "candidates": 145, "total": 413}, "attempt1_parsed": {"summary": "This paper estimates the natural rate of interest, defined as the real interest rate consistent with output at its natural rate and stable inflation, using the Kalman filter to jointly estimate time-varying natural rates of interest and output and trend growth. The study finds a link between the natural rate of interest and trend growth, but notes that estimates are imprecise and subject to measurement error.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The Kalman filter is an advanced statistical technique often used in conjunction with machine learning for time-series analysis and forecasting."}, "mode": "threadpool", "paper_id": "4d23cc600c835687", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Measuring the Level and Uncertainty of Trend Inflation\nAbstract: Firmly anchored inflation expectations are widely viewed as playing a central role for the conduct of monetary policy. This paper presents estimates of trend inflation, based on information contained in monthly data on realized inflation, survey expectations, and the term structure of interest rates. In order to assess whether inflation expectations are anchored, a timevarying volatility of trend shocks is estimated as well. While there is some commonality in inflation- and survey-based estimates of trend inflation, yield-based trend estimates embed a highly persistent component orthogonal to trend inflation. Trimmed-mean inflation rates and survey forecasts are most indicative of trend inflation.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 278, "candidates": 197, "total": 475}, "attempt1_parsed": {"summary": "This paper estimates trend inflation using monthly data on realized inflation, survey expectations, and the term structure of interest rates. It also estimates time-varying volatility of trend shocks to assess inflation expectations anchoring. Trimmed-mean inflation rates and survey forecasts are found to be most indicative of trend inflation, while yield-based estimates include a persistent component orthogonal to trend inflation.", "criteria_hints": {"I1": false, "I2": true, "I3": true}, "notes": "The paper uses statistical methods to estimate trend inflation and its volatility, which could be considered a form of machine learning, but it does not explicitly state the use of machine learning algorithms beyond standard statistical modeling. It focuses on the term structure of interest rates, which relates to sovereign bonds and the yield curve. The estimation of trend inflation and its volatility is inherently a forecasting/prediction task."}, "mode": "threadpool", "paper_id": "19c588a05711d2d3", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Metals: resources or financial assets? A multivariate cross-sectional analysis\nAbstract: Metals are very important resources for industrial production, but recently they have attracted more and more attention from investors. While certainly industrial producers, consumers, and financial investors do have some influence on metal price development, the role of relevant price factors is not yet quite clear. Therefore, in this paper, we examine the explanatory power of various fundamental factors and characteristics known from financial markets, specifically on the expected returns in a unique data sample of 30 metals. We apply-to our knowledge for the first time in this context-the widely accepted method of characteristic-sorted portfolios, extended by the very recent method of two-way portfolio sorts as an alternative to classical multivariate regressions. This mostly nonparametric approach, combined with portfolio aggregation, provides very robust results. Our major finding is that the financial characteristics value and momentum have a very high predictive power for monthly returns of metal portfolios. Metal-specific fundamental factors like stocks, secondary production, app … [truncated]", "attempt1_tokens": {"prompt": 468, "candidates": 183, "total": 651}, "attempt1_parsed": {"summary": "This paper investigates whether metals are primarily industrial resources or financial assets by analyzing the predictive power of financial and fundamental factors on metal returns. Using characteristic-sorted portfolios and two-way sorts, the study finds that financial characteristics like value and momentum significantly predict metal returns, suggesting metals function more as financial assets. While metal-specific factors have some influence, their predictive power is weaker. The findings contribute to understanding metal price dynamics.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses portfolio sorting methods which are advanced statistical techniques beyond simple linear models, fitting criteria I1. The focus is on predicting metal returns, fitting criteria I3. The subject is metals, not government bonds, thus not fitting criteria I2."}, "mode": "threadpool", "paper_id": "e16d12b73176b407", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Misaligned expectations and bond term premium measures☆\nAbstract: This paper shows that inflation expectations and those embedded in short-term interest rate expectations as reported in the Survey of Professional Forecasters show evidence of misaligned expectations. This misalignment seems to have been substantial in recent times, featuring a low correlation between inflation and the policy rate. This empirical evidence motivates an alternative explanation, based on uncertainty rather than risk, of the bond term premium measures found in the literature. This paper estimates an expectational term premium driven by misaligned short-term interest rate expectations from a behavioral DSGE model that introduces model uncertainty by assuming adaptive learning with discretionary beliefs. The estimated 10-year expectational term premium shares important features with the corresponding term premium measures obtained using no-arbitrage affine term structure models. Thus, the expectational term premium is sizable, highly persistent, mildly countercyclical, and highly correlated with those term premium measures in the most recent period studied. In short, a potential misalignment of shor … [truncated]", "attempt1_tokens": {"prompt": 364, "candidates": 209, "total": 573}, "attempt1_parsed": {"summary": "This paper investigates misaligned inflation and short-term interest rate expectations, particularly in recent times, suggesting this misalignment impacts bond term premium measures. It proposes an alternative explanation based on uncertainty rather than risk, estimating an expectational term premium using a behavioral DSGE model with adaptive learning and discretionary beliefs. The estimated term premium exhibits characteristics similar to those derived from no-arbitrage affine term structure models, indicating its significance, persistence, and countercyclicality, and its correlation with recent term premium measures. The misalignment of short-term interest expectations with inflation expectations is presented as a key factor in explaining recent bond premium behavior.", "criteria_hints": {"I1": false, "I2": true, "I3": true}, "notes": "The paper uses a DSGE model with adaptive learning and discretionary beliefs, which is a form of behavioral modeling but not typically considered a Machine Learning application in the context of empirical ML."}, "mode": "threadpool", "paper_id": "43f035f3fb505552", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Measuring sovereign contagion in Europe\nAbstract: This paper analyzes sovereign risk shift-contagion, i.e. positive and significant changes in the propagation mechanisms, using bond yield spreads for the major eurozone countries. By emphasizing the use of two econometric approaches based on quantile regressions (standard quantile regression and Bayesian quantile regression with heteroskedasticity) we find that the propagation of shocks in euro's bond yield spreads shows almost no presence of shift-contagion in the sample periods considered (2003-2006, Nov. 2008-Nov. 2011, Dec. 2011-Apr. 2013). Shock transmission is no different on days with big spread changes and small changes. This is the case even though a significant number of the countries in our sample have been extremely affected by their sovereign debt and fiscal situations. The risk spillover among these countries is not affected by the size or sign of the shock, implying that so far contagion has remained subdued. However, the US crisis does generate a change in the intensity of the propagation of shocks in the eurozone between the 2003-2006 pre-crisis period and the Nov. 2008-Nov. 2011 post-Lehman one, but the coef … [truncated]", "attempt1_tokens": {"prompt": 546, "candidates": 197, "total": 743}, "attempt1_parsed": {"summary": "This paper investigates sovereign contagion in Europe using bond yield spreads for major eurozone countries. Employing quantile regression techniques, it finds limited evidence of significant shift-contagion, meaning the propagation mechanisms of shocks did not substantially change during the analyzed periods. While the US crisis influenced shock propagation intensity between pre- and post-Lehman periods, the effect was a decrease in coefficient values, not an increase. The study attributes observed increases in correlation to larger shocks and data heteroskedasticity rather than heightened shock intensity. The authors highlight the novelty of their Bayesian quantile regression approach, which is adept at handling nonlinear and unstable transmission mechanisms.", "criteria_hints": {"I1": true, "I2": true, "I3": false}, "notes": "The paper uses econometric approaches like quantile regressions, which can be considered beyond simple linear models and are empirical applications."}, "mode": "threadpool", "paper_id": "3bf14016cea7e254", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Model of corporate bond spread based on improved neural network\nAbstract: The reasons for credit spreads can be divided into enterprise-specific non-systematic risks and widespread macroeconomic systemic risks. The previous traditional research mainly focused on the perspective of unsystematic risk. However, at present, more and more scholars are beginning to focus on systemic risks. Based on the neural network algorithm, this paper constructs an improved neural network-based corporate bond spread model to explore the impact of macro systemic risks on credit spreads. Based on the multi-factor no-arbitrage model, the linear relationship between the credit spread and the risk premium of each factor is obtained. At the same time, based on previous research results and observations of the current market reality, this paper identifies five important macroeconomic factors: actual economic output factors, inflation factors, stock market volatility factors, stock market return factors and inter-bank funding factors. The research results show that the model constructed in this paper has excellent performance.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning appli … [truncated]", "attempt1_tokens": {"prompt": 340, "candidates": 135, "total": 475}, "attempt1_parsed": {"summary": "This paper proposes an improved neural network model to analyze corporate bond spreads, focusing on the impact of macroeconomic systemic risks. It incorporates five key macroeconomic factors and demonstrates the model's strong performance.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses a neural network, which is a machine learning application beyond simple linear models. It focuses on corporate bonds, not government bonds. The model is designed to explore the impact of factors on spreads, implying a predictive or forecasting element."}, "mode": "threadpool", "paper_id": "6e6b601cc5aa10eb", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Model risk for European-style stock index options\nAbstract: In empirical modeling, there have been two strands for pricing in the options literature, namely the parametric and nonparametric models. Often, the support for the nonparametric methods is based on a benchmark such as the Black-Scholes (BS) model with constant volatility. In this paper, we study the stochastic volatility (SV) and stochastic volatility random jump (SVJ) models as parametric benchmarks against feedforward neural network (FNN) models, a class of neural network models. Our choice for FNN models is due to their well-studied universal approximation properties of an unknown function and its partial derivatives. Since the partial derivatives of an option pricing formula are risk pricing tools, an accurate estimation of the unknown option pricing function is essential for pricing and hedging. Our findings indicate that FNN models offer themselves as robust option pricing tools, over their sophisticated parametric counterparts in predictive settings. There are two routes to explain the superiority of FNN models over the parametric models in forecast settings. These are normormality of return distributions an … [truncated]", "attempt1_tokens": {"prompt": 363, "candidates": 145, "total": 508}, "attempt1_parsed": {"summary": "This paper compares the performance of feedforward neural network (FNN) models against parametric models (stochastic volatility and stochastic volatility random jump) for pricing European-style stock index options. The study finds that FNN models are more robust and superior in predictive settings, attributing this to the non-normality of return distributions and adaptive learning capabilities.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses FNN models, which are a type of machine learning, and compares their predictive performance against parametric models for option pricing."}, "mode": "threadpool", "paper_id": "f296d1ee3108d658", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Modelling and management of mortality risk: a review\nAbstract: In the first part of the paper, we consider the wide range of extrapolative stochastic mortality models that have been proposed over the last 15-20 years. A number of models that we consider are framed in discrete time and place emphasis on the statistical aspects of modelling and forecasting. We discuss how these models can be evaluated, compared and contrasted. We also discuss a discrete-time market model that facilitates valuation of mortality-linked contracts with embedded options. We then review several approaches to modelling mortality in continuous time. These models tend to be simpler in nature, but make it possible to examine the potential for dynamic hedging of mortality risk. Finally, we review a range of financial instruments (traded and over-the-counter) that could be used to hedge mortality risk. Some of these, such as mortality swaps, already exist, while others anticipate future developments in the market.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds … [truncated]", "attempt1_tokens": {"prompt": 334, "candidates": 141, "total": 475}, "attempt1_parsed": {"summary": "This paper reviews stochastic mortality models, focusing on both discrete-time and continuous-time approaches. It discusses model evaluation, forecasting, and the valuation of mortality-linked contracts. The review also covers hedging strategies for mortality risk using existing and potential financial instruments.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The abstract mentions forecasting, which aligns with I3. However, it does not explicitly mention empirical machine learning applications beyond simple linear models (I1) or government/sovereign bonds (I2)."}, "mode": "threadpool", "paper_id": "febabd5bf9c00b9b", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Modeling volatility changes in the 10-year Treasury\nAbstract: This paper examines the daily volatility of changes in the 10-year Treasury note utilizing the iterated cumulative sums of squares algorithm [C. Inclan, G. Tiao, Use of cumulative sums of squares for retrospective detection of changes of variance, J. Am. Stat. Assoc. 89 (1994) 913-923]. The ICSS algorithm can detect regime shifts in the volatility of the interest rate changes. A general model allows for endogenously determined changes in variance while the more restrictive model forces the variance to follow the same process throughout the sample period. A comparison of the out-of-sample volatility forecasting performance of two competing models is made using asymmetric error measures. The asymmetric error statistics penalize models for under- or over-predicting volatility. The results shed light on the importance of ignoring volatility regime shifts when performing out-of-sample forecasts. The findings are important to financial market participants who require accurate forecasts of future volatility in order to implement and evaluate asset performance. (c) 2006 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": … [truncated]", "attempt1_tokens": {"prompt": 385, "candidates": 185, "total": 570}, "attempt1_parsed": {"summary": "This paper uses the iterated cumulative sums of squares (ICSS) algorithm to model and detect regime shifts in the daily volatility of 10-year Treasury note changes. It compares the out-of-sample forecasting performance of models that account for volatility regime shifts versus those that do not, using asymmetric error measures. The findings highlight the importance of considering volatility regime shifts for accurate out-of-sample volatility forecasts, which is crucial for financial market participants.", "criteria_hints": {"I1": false, "I2": true, "I3": true}, "notes": "The paper uses a statistical algorithm (ICSS) for change point detection, which is not typically considered a machine learning application in the context of predictive modeling beyond linear models. It focuses on statistical modeling and forecasting rather than machine learning algorithms."}, "mode": "threadpool", "paper_id": "2cde5aab7ff222dc", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Modeling spot rate using a realized stochastic volatility model with level effect and dynamic drift\nAbstract: This paper proposes a class of realized stochastic volatility model based on both various realized volatility measures and spot rate. It applies the realized stochastic volatility model (Takahashi, Omori, & Watanabe, 2009, and Koopman & Scharth, 2013) to the spot rate model with dynamic drift and level effect setups (RSVL). A jointly approximated maximum likelihood procedure is used to estimate this model. The simulation results show that the RSVL model can be consistently estimated and noise-and-jump-robust realized volatility measures improve the accuracy of the estimation. This study empirically investigates the Chinese interbank repo market with RSVL model, which manifested the advantage of taking the level effect and nonlinear drift into consideration. The noise-and-jump-robust realized volatility measures (e.g. subsample realized volatility and threshold pre-average realized volatility) decrease the volatility fitting error. The nonparametric testing suggests that the RSVL model with noise-and-jump-robust realized volatility measures has more power on forecasti … [truncated]", "attempt1_tokens": {"prompt": 402, "candidates": 213, "total": 615}, "attempt1_parsed": {"summary": "This paper introduces a realized stochastic volatility model (RSVL) for spot rates, incorporating dynamic drift and level effects. The model utilizes realized volatility measures and is estimated using a jointly approximated maximum likelihood procedure. Simulation results indicate that noise-and-jump-robust realized volatility measures enhance estimation accuracy. Empirical analysis on the Chinese interbank repo market demonstrates the RSVL model's effectiveness in capturing level effects and nonlinear drifts. The study also shows that the RSVL model with robust realized volatility measures improves volatility fitting and forecasting of excess kurtosis, fat tails, and higher-order autocorrelation dynamics.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study applies a statistical model to financial data, which can be considered an empirical application beyond pure theory. While it deals with spot rates and volatility, it does not specifically focus on government/sovereign bonds or the yield curve."}, "mode": "threadpool", "paper_id": "4a7f853347d7646b", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Modelling nonlinearities in equity returns: the mean impact curve analysis\nAbstract: A time-varying model of equity returns consisting of a volatility factor with time-varying loading, is specified to investigate the dynamical effects of shocks on expected returns. The proposed specification yields a nonlinear relationship between the conditional mean and the news, referred to as the mean impact curve (MIC). Applying this framework to the AORD, Hang Seng and Straits Times equity indexes yields estimated MICs with qualitatively similar nonlinear characteristics for each equity market. An important implication of the empirical results is that the relationship between the conditional mean and the news is found to be dependent upon the size of the shock, a result which is consistent with equity markets displaying mean aversion over short horizons and mean reversion over long horizons.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 298, "candidates": 225, "total": 523}, "attempt1_parsed": {"summary": "This paper proposes a time-varying model for equity returns that incorporates a volatility factor with time-varying loading to study the dynamic effects of shocks on expected returns. The model results in a nonlinear relationship between the conditional mean and news, termed the mean impact curve (MIC). Empirical analysis of the AORD, Hang Seng, and Straits Times equity indexes reveals similar nonlinear MIC characteristics across these markets. The findings suggest that the relationship between the conditional mean and news depends on shock size, aligning with equity markets exhibiting mean aversion in the short term and mean reversion in the long term.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses a nonlinear model (MIC) and applies it empirically, fitting criteria I1. It focuses on equity returns, not government bonds, making I2 false. The analysis of shock effects on expected returns and the discussion of mean aversion/reversion imply a forecasting or prediction aspect, fitting I3."}, "mode": "threadpool", "paper_id": "b1e9b249051157d6", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Moments, shocks and spillovers in Markov-switching VAR models\nAbstract: To investigate how economies, financial markets or institutions can deal with stress, we often analyze the effects of shocks conditional on being in a recession or a bear market. MSVAR models are perfectly suited for such analyses because they combine gradual movements with sudden regime switches. In this paper, we develop a comprehensive methodology to conduct these analyses. We derive first and second moments conditional only on the regime distribution and propose impulse response functions for both moments. By formulating the MSVAR as an extended linear non-Gaussian VAR, all results are available in closed-form. We illustrate our methods with an application to stock and bond return predictability. We show how forecasts of means, volatilities and (auto-)correlations depend on the regimes. The effect of shocks becomes highly nonlinear, and they propagate via different channels. During bear markets, shocks have stronger effects on means and volatilities and die out more slowly. & COPY; 2023 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativeco … [truncated]", "attempt1_tokens": {"prompt": 385, "candidates": 177, "total": 562}, "attempt1_parsed": {"summary": "This paper develops a methodology for analyzing the effects of shocks in Markov-switching VAR (MSVAR) models, which are suitable for capturing both gradual movements and sudden regime switches. The authors derive conditional moments and impulse response functions in closed-form by formulating the MSVAR as an extended linear non-Gaussian VAR. An application to stock and bond returns demonstrates how forecasts of means, volatilities, and autocorrelations are regime-dependent, with shocks having nonlinear and slower-dying effects during bear markets.", "criteria_hints": {"I1": true, "I2": true, "I3": true}, "notes": "The paper uses MSVAR models, which are more complex than simple linear models, and applies them to stock and bond returns for forecasting purposes."}, "mode": "threadpool", "paper_id": "8a16f3a80a9ddf11", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Modelling of Chinese corporate bond default - A machine learning approach\nAbstract: We apply machine learning techniques to construct a series of models of corporate bond defaults. By combining Chinese accounting information and corporate bond data from January 2012 to December 2019, we construct an out-of-sample forecast that significantly improves the identification rate of corporate bond defaults, with an area under the receiver operating characteristics curve greater than 90 percent. Our models are robust to different machine learning models, including stacking, boosting, and bagging ensembling models. Our models consider cross-sectional heterogeneity, such as different ownership structures, accessibility to external finance, industry heterogeneity, different sample periods, and government policy impact.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 288, "candidates": 139, "total": 427}, "attempt1_parsed": {"summary": "This study uses machine learning to model Chinese corporate bond defaults, combining accounting and bond data from 2012-2019. The models, including ensembling techniques like stacking, boosting, and bagging, significantly improve default identification with an AUC over 90%. They account for factors like ownership, finance accessibility, industry, time periods, and government policy.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study focuses on corporate bonds, not government/sovereign bonds."}, "mode": "threadpool", "paper_id": "49b43eda4b92602b", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Moment Risk Premia and Stock Return Predictability\nAbstract: We study the predictive power of option-implied moment risk premia embedded in the conventional variance risk premium. We find that although the second-moment risk premium predicts market returns in short horizons with positive coefficients, the third-moment (fourth-moment) risk premium predicts market returns in medium horizons with negative (positive) coefficients. Combining the higher-moment risk premia with the second-moment risk premium improves the stock return predictability over multiple horizons, both in sample and out of sample. The finding is economically significant in an asset-allocation exercise and survives a series of robustness checks.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 275, "candidates": 179, "total": 454}, "attempt1_parsed": {"summary": "This study investigates the predictive power of option-implied moment risk premia for stock returns. It finds that second-moment risk premia predict returns at short horizons, while third- and fourth-moment risk premia predict at medium horizons. Combining these higher-moment risk premia enhances stock return predictability across multiple horizons, with significant economic implications and robustness.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The abstract mentions \"improves the stock return predictability\" and \"predicts market returns\", indicating a focus on prediction (I3). However, it does not mention machine learning applications beyond simple linear models (I1) or focus on government/sovereign bonds (I2)."}, "mode": "threadpool", "paper_id": "2c7bb54560d01c08", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Monetary reforms and inflation expectations in Japan: Evidence from inflation-indexed bonds\nAbstract: We assess the impact of news concerning recent Japanese monetary reforms on long-term inflation expectations using an arbitrage-free term structure model of nominal and real yields. Our model accounts for the value of deflation protection embedded in Japanese inflation-indexed bonds issued since 2013, which is sizable and time-varying. Our results suggest that Japanese long-term inflation expectations have remained pos-itive despite extensive spells of deflation, leaving inflation risk premia mostly negative during this period. Moreover, adjusting for deflation protection demonstrates that market responses to policy changes were not as inflationary as they appear under standard modeling procedures. Consequently, the reforms were less disappointingthan is widely perceived.(c) 2021 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or pr … [truncated]", "attempt1_tokens": {"prompt": 311, "candidates": 205, "total": 516}, "attempt1_parsed": {"summary": "This study analyzes the effect of Japanese monetary reforms on long-term inflation expectations using an arbitrage-free term structure model applied to nominal and real yields of inflation-indexed bonds issued since 2013. The model incorporates the value of deflation protection, which is significant and fluctuates over time. Findings indicate that Japanese long-term inflation expectations stayed positive even during deflationary periods, with inflation risk premia generally negative. After accounting for deflation protection, the market's reaction to policy changes was less inflationary than suggested by conventional models, implying the reforms were less disappointing than commonly believed.", "criteria_hints": {"I1": false, "I2": true, "I3": false}, "notes": "The study uses a term structure model, not machine learning. While it analyzes market responses to policy changes, the primary focus is not on forecasting future inflation but on assessing past expectations and policy impacts."}, "mode": "threadpool", "paper_id": "3f8ab1bb9836d2d3", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Mortgage prepayment with an uncertain holding period\nAbstract: A discrete-time-option pricing model is developed to value a mortgage and its embedded prepayment option when the effective life of the mortgage is a random variable with a probability distribution of known parameters. The model can be applied when the borrower's ex ante expectation of his tenure follows any probability distribution bounded to the left at zero. The Gamma distribution is used to illustrate the model.The pricing model is further applied to determine the conditions under which financially motivated prepayment is optimal. The results show that the certainty model understates the Interest Rate Differential needed to justify prepayment (IRD) for short Expected Holding Period (EHP) borrowers and overstates the IRD for long EHP borrowers. When the EHP is relatively long, the certainty model provides relatively good estimates of IRD during the beginning years of the mortgage life. Under most other conditions, the estimates of the certainty holding period model are biased.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear model … [truncated]", "attempt1_tokens": {"prompt": 334, "candidates": 158, "total": 492}, "attempt1_parsed": {"summary": "This paper develops a discrete-time-option pricing model for mortgages with embedded prepayment options, considering an uncertain borrower holding period. It applies the Gamma distribution to illustrate the model and analyzes conditions for financially motivated prepayment. The model reveals biases in the certainty model's Interest Rate Differential (IRD) estimates, particularly for borrowers with short or long Expected Holding Periods (EHP).", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The article discusses a model for mortgage prepayment and its implications for Interest Rate Differential (IRD) calculations, which can be seen as a form of prediction or forecasting under uncertainty."}, "mode": "threadpool", "paper_id": "79f5b96bb6a44999", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Mutual Fund Liquidity Transformation and Reverse Flight to Liquidity\nAbstract: We identify fixed-income mutual funds as an important contributor to the unusually high selling pressure in liquid asset markets during the COVID-19 crisis. We show that mutual funds experienced pronounced investor outflows amplified by their liquidity transformation. In meeting redemptions, funds followed a pecking order by first selling their liquid assets, including Treasuries and high-quality corporate bonds, which generated the most concentrated selling pressure in these markets. Overall, the estimated price impact of mutual funds was sizable at a third of the increase in Treasury yields and a quarter of the increase in corporate bond yields during the COVID-19 crisis.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 282, "candidates": 163, "total": 445}, "attempt1_parsed": {"summary": "This study investigates the role of fixed-income mutual funds in exacerbating selling pressure in liquid asset markets during the COVID-19 crisis. It highlights how investor outflows, amplified by the funds' liquidity transformation, led to the sale of liquid assets like Treasuries and corporate bonds, significantly impacting yields. The paper quantifies the price impact of these sales on Treasury and corporate bond yields.", "criteria_hints": {"I1": false, "I2": true, "I3": false}, "notes": "The study focuses on the impact of mutual fund behavior on bond markets rather than forecasting. While it quantifies price impacts, it does not appear to involve machine learning applications beyond simple models."}, "mode": "threadpool", "paper_id": "f7635ac88ef7da8e", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Mortgage convexity\nAbstract: Most home mortgages in the United States are fixed-rate loans with an embedded prepayment option. When long-term rates decline, the effective duration of mortgage-backed securities (MBS) falls due to heightened refinancing expectations. I show that these changes in MBS duration function as large-scale shocks to the quantity of interest rate risk that must be borne by professional bond investors. I develop a simple model in which the risk tolerance of bond investors is limited in the short run, so these fluctuations in MBS duration generate significant variation in bond risk premia. Specifically, bond risk premia are high when aggregate MBS duration is high. The model offers an explanation for why long-term rates could appear to be excessively sensitive to movements in short rates and explains how changes in MBS duration act as a positive-feedback mechanism that amplifies interest rate volatility. I find strong support for these predictions in the time series of US government bond returns. (C) 2014 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond  … [truncated]", "attempt1_tokens": {"prompt": 352, "candidates": 141, "total": 493}, "attempt1_parsed": {"summary": "This paper examines how mortgage-backed securities (MBS) duration fluctuations, driven by prepayment options and interest rate changes, impact bond risk premia. It proposes a model where limited investor risk tolerance amplifies interest rate volatility when MBS duration is high, finding empirical support in US government bond returns.", "criteria_hints": {"I1": false, "I2": true, "I3": false}, "notes": "The study uses empirical data but does not appear to employ machine learning techniques beyond simple models. It focuses on government bond returns as a proxy for broader bond market behavior."}, "mode": "threadpool", "paper_id": "e148b89b6640d34c", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Negative option values are possible: The impact of treasury bond futures on the cash US Treasury market\nAbstract: This paper uses a unique financial instrument in the U.S. Treasury market to study the price behavior of the put option embedded in the November 2009-14 callable U.S. Treasury bond. We find that, beginning in August 1993, the estimated option value was persistently negative on nearly every day for the ensuing eight months. We show that the anomalous pricing behavior arose because the underlying callable bond became the cheapest to deliver issue against U.S. Treasury bond futures contracts. Hence, this paper provides direct evidence that derivative assets can significantly distort pricing in the primary asset market.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 290, "candidates": 160, "total": 450}, "attempt1_parsed": {"summary": "This paper investigates the price behavior of a put option embedded in a callable U.S. Treasury bond by examining the impact of U.S. Treasury bond futures. The study found that the option's value was persistently negative for an extended period, attributed to the callable bond becoming the cheapest to deliver issue against futures contracts. This suggests that derivative assets can significantly influence pricing in the primary asset market.", "criteria_hints": {"I1": false, "I2": true, "I3": false}, "notes": "The paper focuses on option pricing and market distortion rather than forecasting. While it uses financial data, it does not appear to employ machine learning beyond simple models."}, "mode": "threadpool", "paper_id": "d1cbe8bbd0caa6f9", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Network Centrality and Managerial Market-Timing Ability\nAbstract: We document that long-run excess returns following announcements of share buyback authorizations and insider purchases are a U-shaped function of firm centrality in the input-output trade-flow network. These results conform to a model of investors endowed with a large but finite capacity for analyzing firms. Additional links weaken insiders' informational advantage in peripheral firms (simple firms whose cash flows depend on few economic links), provided investors' capacity is large enough, but eventually amplify that advantage in central firms (firms with many links) as a result of investors' limited capacity. These findings shed light on the sources of managerial market-timing ability.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 283, "candidates": 181, "total": 464}, "attempt1_parsed": {"summary": "This study investigates the relationship between a firm's centrality in the input-output trade-flow network and managerial market-timing ability, specifically concerning share buyback authorizations and insider purchases. The findings suggest that long-run excess returns exhibit a U-shaped pattern based on firm centrality, explained by a model where investors have limited capacity for analyzing firms. Increased network links can weaken or amplify insiders' informational advantage depending on firm centrality and investor capacity.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The study uses a model to explain empirical results, but the core methodology described does not appear to be a machine learning application beyond simple linear models. The study is about forecasting returns based on network structure and insider actions."}, "mode": "threadpool", "paper_id": "eddff9ceb55591e5", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: News Media and Attention Spillover across Energy Markets: A Powerful Predictor of Crude Oil Futures Prices\nAbstract: We develop two news-based investor attention measures from the news trends function of the Bloomberg terminal and investigate their predictive power for returns on crude oil futures contracts with various maturities. Our main results after controlling for relevant macroeconomic variables show that the Oil-based Institutional Attention Index is useful in predicting oil futures returns, especially during price downturn periods, while the forecasting accuracy is further improved when the Commodity Market Institutional Attention Index is used. This forecasting accuracy decreases, however, with the maturity of oil futures contracts. Moreover, we find some evidence of Granger-causality and regime-dependent interactions between investor attention measures and oil futures returns. Finally, variable selection algorithms matter before making predictions since they create the best forecasting results in many cases considered. These findings are important for informed traders and policymakers to better understand the price dynamics of the oil markets.\n\nCriteria: {\"I1\": \"T … [truncated]", "attempt1_tokens": {"prompt": 333, "candidates": 136, "total": 469}, "attempt1_parsed": {"summary": "This study develops two news-based investor attention measures to predict crude oil futures returns. The Oil-based Institutional Attention Index and the Commodity Market Institutional Attention Index show predictive power, particularly during price downturns and for shorter-term futures contracts. The research also highlights the importance of variable selection algorithms for forecasting accuracy and suggests implications for traders and policymakers.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses \"variable selection algorithms\" which implies machine learning applications beyond simple linear models."}, "mode": "threadpool", "paper_id": "4717510223fcce48", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: New frontiers for arch models\nAbstract: In the 20 years following the publication of the ARCH model, there has been a vast quantity of research uncovering the properties of competing volatility models. Wide-ranging applications to financial data have discovered important stylized facts and illustrated both the strengths and weaknesses of the models. There are now many surveys of this literature. This paper looks forward to identify promising areas of new research. The paper lists five new frontiers. It briefly discusses three-high-frequency volatility models, large-scale multivariate ARCH models, and derivatives pricing models. Two further frontiers are examined in more detail-application of ARCH models to the broad class of non-negative processes, and use of Least Squares Monte Carlo to examine non-linear properties of any model that can be simulated. Using this methodology, the paper analyses more general types of ARCH models, stochastic volatility models, long-memory models and breaking volatility models. The volatility of volatility is defined, estimated and compared with option-implied volatilities. Copyright (C) 2002 John Wiley Sons, Ltd.\n\nCriteria: {\"I1\": \"The article … [truncated]", "attempt1_tokens": {"prompt": 362, "candidates": 219, "total": 581}, "attempt1_parsed": {"summary": "This paper reviews the ARCH model literature over the past 20 years, highlighting its applications to financial data and identifying five promising areas for future research. It discusses high-frequency volatility models, large-scale multivariate ARCH models, and derivatives pricing models. Two areas are explored in depth: applying ARCH models to non-negative processes and using Least Squares Monte Carlo for non-linear properties of simulated models. The paper also defines, estimates, and compares the volatility of volatility with option-implied volatilities.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The paper discusses \"new frontiers\" and \"promising areas of new research\" which implies forecasting. It also mentions \"application of ARCH models to the broad class of non-negative processes\" and \"use of Least Squares Monte Carlo to examine non-linear properties of any model that can be simulated\" which suggests empirical applications beyond simple linear models."}, "mode": "threadpool", "paper_id": "2576c128066ee3ce", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Nominal US Treasuries Embed Liquidity Premiums, Too\nAbstract: A novel arbitrage-free model of nominal U.S. Treasuries that decomposes yields into frictionless expected rates, frictionless term premiums, and liquidity premiums produces four key results from Jan. 1987 to Aug. 2023. First, liquidity loadings are larger than for the slope and higher-order principal components. Second, the countercyclicality of required nominal Treasury returns owes to liquidity, if anything, not frictionless term premiums. Third, Federal Reserve large-scale asset purchases generally work through expected rates and frictionless term premiums, not liquidity premiums. Fourth, given similar estimates using TIPS, inflation expectations are less moored around the Federal Reserve's price objectives than other models say.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 300, "candidates": 167, "total": 467}, "attempt1_parsed": {"summary": "This paper introduces a new arbitrage-free model for nominal U.S. Treasuries, separating yields into frictionless expected rates, frictionless term premiums, and liquidity premiums. The model reveals that liquidity premiums are significant, countercyclical returns are driven by liquidity rather than frictionless term premiums, Federal Reserve asset purchases primarily affect expected rates and frictionless term premiums, and inflation expectations are less anchored than previously thought. The analysis spans from January 1987 to August 2023.", "criteria_hints": {"I1": false, "I2": true, "I3": false}, "notes": "The abstract does not mention machine learning or forecasting. It focuses on modeling and empirical analysis of Treasury yields."}, "mode": "threadpool", "paper_id": "636a11ccd9e2eab8", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Nonlinear Kalman Filtering in Affine Term Structure Models\nAbstract: The extended Kalman filter, which linearizes the relationship between security prices and state variables, is widely used in fixed-income applications. We investigate whether the unscented Kalman filter should be used to capture nonlinearities and compare the performance of the Kalman filter with that of the particle filter. We analyze the cross section of swap rates, which are mildly nonlinear in the states, and cap prices, which are highly nonlinear. When caps are used to filter the states, the unscented Kalman filter significantly outperforms its extended counterpart. The unscented Kalman filter also performs well when compared with the much more computationally intensive particle filter. These findings suggest that the unscented Kalman filter may be a good approach for a variety of problems in fixed-income pricing.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 304, "candidates": 144, "total": 448}, "attempt1_parsed": {"summary": "This paper compares the performance of the extended Kalman filter, unscented Kalman filter, and particle filter in affine term structure models. It finds that the unscented Kalman filter outperforms the extended Kalman filter, especially for highly nonlinear instruments like cap prices, and performs well compared to the particle filter. The authors suggest the unscented Kalman filter as a suitable approach for fixed-income pricing problems.", "criteria_hints": {"I1": true, "I2": true, "I3": false}, "notes": "I3 is false because the paper focuses on filtering and pricing, not explicitly forecasting or prediction."}, "mode": "threadpool", "paper_id": "3304d99c523318c7", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Non-linearities in the relationship of agricultural futures prices\nAbstract: The movement of food prices remains a controversial issue owing to the intense rise in volatility that has been observed in recent years. Agricultural futures markets have experienced a similar pattern and simplistic linear models seem to be no longer reliable when analysing their functions. Against this background, this study contributes to the literature by adopting a non-linear smooth transition approach to examine the relationship between prices for first and second nearby futures contracts of seven agricultural commodities. Our main objective is to distinguish between contango and backwardation regimes when analysing the relationship between the futures spread and changes in the first nearby futures price. Our findings reveal that a linear framework neglects important dynamics, as futures prices adjust only under specific circumstances, and that the predictive power of the futures spread is much stronger during backwardation regimes.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is a … [truncated]", "attempt1_tokens": {"prompt": 311, "candidates": 166, "total": 477}, "attempt1_parsed": {"summary": "This study uses a non-linear smooth transition approach to analyze the relationship between first and second nearby futures contracts for seven agricultural commodities. It aims to differentiate between contango and backwardation regimes and finds that linear models are insufficient, with futures prices adjusting only under specific circumstances and the futures spread being a stronger predictor during backwardation regimes.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses a non-linear approach beyond simple linear models, which aligns with I1. It focuses on agricultural futures prices, not government bonds, making I2 false. The study discusses the predictive power of the futures spread, aligning with I3."}, "mode": "threadpool", "paper_id": "6d1f179bacc01371", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Nonparametric model validations for hidden Markov models with applications in financial econometrics\nAbstract: We address the nonparametric model validation problem for hidden Markov models with partially observable variables and hidden states. We achieve this goal by constructing a nonparametric simultaneous confidence envelope for transition density function of the observable variables and checking whether the parametric density estimate is contained within such an envelope. Our specification test procedure is motivated by a functional connection between the transition density of the observable variables and the Markov transition kernel of the hidden states. Our approach is applicable for continuous-time diffusion models, stochastic volatility models, nonlinear time series models, and models with market microstructure noise. (C) 2011 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 287, "candidates": 161, "total": 448}, "attempt1_parsed": {"summary": "This paper proposes a nonparametric model validation method for hidden Markov models with partially observable variables and hidden states. It constructs a confidence envelope for the transition density function of observable variables and checks if the parametric density estimate falls within it. This approach is applicable to various models including diffusion models, stochastic volatility models, and nonlinear time series models, with potential applications in financial econometrics.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The paper discusses applications in financial econometrics and mentions models like stochastic volatility and nonlinear time series, which often involve more complex than simple linear models. The validation method implies a predictive or forecasting aspect."}, "mode": "threadpool", "paper_id": "bf41f43cc6cb3759", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: OPTIMAL ASSET ALLOCATION WITH MULTIVARIATE BAYESIAN DYNAMIC LINEAR MODELS\nAbstract: We introduce a fast, closed-form, simulation-free method to model and forecast multiple asset returns and employ it to investigate the optimal ensemble of features to include when jointly predicting monthly stock and bond excess returns. Our approach builds on the Bayesian dynamic linear models of West and Harrison (Bayesian Forecasting and Dynamic Models (1997) Springer), and it can objectively determine, through a fully automated procedure, both the optimal set of regressors to include in the predictive system and the degree to which the model coefficients, volatilities and covariances should vary over time. When applied to a portfolio of five stock and bond returns, we find that our method leads to large forecast gains, both in statistical and economic terms. In particular, we find that relative to a standard no-predictability benchmark, the optimal combination of predictors, stochastic volatility and time-varying covariances increases the annualized certainty equivalent returns of a leverage-constrained power utility investor by more than 500 basis points.\n\nCriteria: {\"I1\": \"The article i … [truncated]", "attempt1_tokens": {"prompt": 366, "candidates": 148, "total": 514}, "attempt1_parsed": {"summary": "This paper presents a novel, efficient method for modeling and forecasting multiple asset returns using Bayesian dynamic linear models. It automates the selection of optimal regressors, stochastic volatilities, and time-varying covariances. Applied to stock and bond returns, the method significantly improves forecast accuracy and investor returns compared to a benchmark.", "criteria_hints": {"I1": true, "I2": true, "I3": true}, "notes": "The abstract mentions 'stock and bond returns', which implies government/sovereign bonds are included. The method described is an empirical application beyond simple linear models and focuses on forecasting."}, "mode": "threadpool", "paper_id": "1475fced39bb3621", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Optimal Time Varying Parameters in Yield Curve Modeling and Forecasting: A Simulation Study on BRICS Countries\nAbstract: The term structure of interest rates is a fundamental decision-making tool for various economic activities. Despite the huge number of contributions in the field, the development of a reliable framework for both fitting and forecasting under various market conditions (either stable or very volatile) still remains a topical issue. Motivated by this problem, this study introduces a methodology relying on optimal time-varying parameters for three and five factor models in the Nelson-Siegel class that can be employed for an effective in-sample fitting and out-of-sample forecasting of the term structure. In detail, for the in-sample fitting we discussed a two-step estimation procedure leading to optimal models parameters and evaluated the performances of this approach in terms of flexibility and fitting accuracy gains. For what it concerns the forecasting, we suggest an approach overcoming the well-known issue between the stability of factor models' parameters and the optimal dynamic decay terms. To such aim, we use either autoregressive or machine learning tec … [truncated]", "attempt1_tokens": {"prompt": 474, "candidates": 152, "total": 626}, "attempt1_parsed": {"summary": "This study proposes a methodology using optimal time-varying parameters for Nelson-Siegel class models to improve yield curve fitting and forecasting. It employs a two-step estimation for in-sample fitting and utilizes autoregressive or machine learning techniques for out-of-sample forecasting, tested on BRICS countries' bond market data. The results show that time-varying parameters enhance fitting power and prediction accuracy.", "criteria_hints": {"I1": true, "I2": true, "I3": true}, "notes": "The abstract explicitly mentions machine learning techniques for forecasting and discusses yield curve dynamics and predictions for BRICS countries' bond markets."}, "mode": "threadpool", "paper_id": "429dc98f1a74b78f", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Nonlinear adjustments in deviations from the law of one price for wholesale hog prices\nAbstract: This article applies the Band-Threshold Autoregression (Band-TAR) model to investigate whether the law of one price (LOOP) holds in Taiwanese wholesale hog markets during the period from May 1987 through December 2003. We find evidence of a nonlinear mean reversion in deviations from the LOOP for relative hog prices. Our empirical study confirms the presence of thresholds and provides strong evidence in support of the view that the regional hog markets have been tightly integrated in Taiwan and that the wholesale hog market in Taiwan is an efficient market economy. Furthermore, the estimated half-lives from the nonlinear generalized impulse response analysis are as short as four months.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 295, "candidates": 167, "total": 462}, "attempt1_parsed": {"summary": "This study uses a Band-Threshold Autoregression (Band-TAR) model to examine the law of one price (LOOP) in Taiwanese wholesale hog markets. The findings indicate nonlinear mean reversion in deviations from the LOOP for relative hog prices, suggesting tight integration and efficiency in these markets. The estimated half-lives for adjustments are as short as four months.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses a nonlinear model (Band-TAR) which goes beyond simple linear models, fitting I1. The study investigates market efficiency and price adjustments, which can be interpreted as a form of forecasting or prediction, fitting I3."}, "mode": "threadpool", "paper_id": "44636fa0fc84cad0", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: On the specification of the drift and diffusion functions for continuous-time models of the spot interest rate\nAbstract: This paper explores the specification of drift and diffusion functions for continuous-time short-term interest rate models. Various forms for the drift and diffusion of 7-day Eurodollar rates are proposed and then estimated by discrete maximum-likelihood. The results suggest that a nonparametric specification of drift and volatility in terms of orthogonal polynomial expansions is effective in eliminating problems of parameter identification encountered previously. Some evidence is found to support the claim that the drift of the short term interest rate is nonlinear.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 263, "candidates": 183, "total": 446}, "attempt1_parsed": {"summary": "This paper investigates the specification of drift and diffusion functions for continuous-time models of short-term interest rates. It proposes and estimates various forms for the drift and diffusion of 7-day Eurodollar rates using discrete maximum-likelihood. The findings indicate that a nonparametric specification using orthogonal polynomial expansions effectively addresses parameter identification issues and suggests a nonlinear drift for short-term interest rates.", "criteria_hints": {"I1": true, "I2": false, "I3": false}, "notes": "The paper uses discrete maximum-likelihood estimation and orthogonal polynomial expansions, which can be considered empirical applications beyond simple linear models. However, it does not directly focus on government bonds or yield curves, nor does it explicitly state forecasting as its primary goal, although the models could potentially be used for prediction."}, "mode": "threadpool", "paper_id": "8c483dd1af8aab26", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Optimal Filtering of Jump Diffusions: Extracting Latent States from Asset Prices\nAbstract: This paper provides an optimal filtering methodology in discretely observed continuous-time jump-diffusion models. Although the filtering problem has received little attention, it is useful for estimating latent states, forecasting volatility and returns, computing model diagnostics such as likelihood ratios, and parameter estimation. Our approach combines time-discretization schemes with Monte Carlo methods. It is quite general, applying in nonlinear and multivariate jump-diffusion models and models with nonanalytic observation equations. We provide a detailed analysis of the filter's performance, and analyze four applications: disentangling jumps from stochastic volatility, forecasting volatility, comparing models via likelihood ratios, and filtering using option prices and returns. (JEL C11, C13, C15, C51, C52, G11, G12, G17)\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting o … [truncated]", "attempt1_tokens": {"prompt": 329, "candidates": 176, "total": 505}, "attempt1_parsed": {"summary": "This paper presents an optimal filtering method for discretely observed continuous-time jump-diffusion models, useful for estimating latent states, forecasting, and parameter estimation. The approach combines time-discretization and Monte Carlo methods, applicable to nonlinear, multivariate, and nonanalytic models. Applications include disentangling jumps from stochastic volatility, volatility forecasting, model comparison, and filtering using option prices and returns.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The paper uses Monte Carlo methods and discusses applications like forecasting volatility and comparing models, which aligns with I1 and I3. It does not appear to focus on government/sovereign bonds or the yield curve, hence I2 is false."}, "mode": "threadpool", "paper_id": "c9bb2437e26f8d84", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: On the time-varying relation between monetary policy uncertainty and bond risk premia\nAbstract: This paper examines the time-varying relationship between monetary policy uncertainty (MPU) and bond excess returns. To do so, we introduce a nonparametric time-varying coefficient predictive regression model for bond returns, and employ a kernel-based two-step method to estimate the time-varying coefficients. Next, we apply the methodologies to analyze the dynamic forecasting relationship between zero-coupon bond returns and MPU from 1985 to 2022. We find that MPU significantly and positively predicts bond returns in over 75% of the sample period, with the strongest effect observed in 2005. Thus, the expectations hypothesis is only transiently valid. After controlling for the shape of the yield curve, MPU still retains its ability to predict bond returns in 50% to 80% of the sample period. Our conclusions are robust to the so-called embedded endogeneity. Additionally, we find that bond excess returns are less responsive to MPU during periods of high economic activities and are more responsive during periods of low economic activities.\n\nCriteria: {\"I1\": \"The article includes an em … [truncated]", "attempt1_tokens": {"prompt": 380, "candidates": 167, "total": 547}, "attempt1_parsed": {"summary": "This paper investigates the dynamic relationship between monetary policy uncertainty (MPU) and bond excess returns using a nonparametric time-varying coefficient model. The study finds that MPU significantly predicts bond returns for a substantial portion of the 1985-2022 period, with varying effects depending on economic activity levels. The findings suggest the expectations hypothesis is only temporarily valid.", "criteria_hints": {"I1": true, "I2": true, "I3": true}, "notes": "The paper uses a nonparametric time-varying coefficient model, which goes beyond simple linear models. It directly analyzes bond returns and the yield curve. The core of the paper is about predicting bond returns based on MPU."}, "mode": "threadpool", "paper_id": "e4db19deb8251edb", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Option Pricing Under a Stochastic Interest Rate and Volatility Model with Hidden Markovian Regime-Switching\nAbstract: In this paper we discuss an option pricing problem in a hidden Markovian regime-switching model with a stochastic interest rate and volatility. Regime switches are attributed to structural changes in an hidden economic environment and are described by a continuous-time, finite-state, unobservable Markov chain. The model is then applied to the valuation of a standard European option. By means of the standard separation principle, filtering and option valuation problems are separated. Robust filters for the hidden states of the economy and their robust filtered estimates of unknown parameters from the expectation maximization algorithm are presented based on standard techniques in filtering theory. Then an explicit expression of a conditional characteristic function relevant to option pricing is presented and the valuation of the option is discussed using the inverse Fourier transformation approach. Using the limiting behavior of the conditional characteristic function, an efficient implementation of the transform inversion integral is considered. Numerical exp … [truncated]", "attempt1_tokens": {"prompt": 359, "candidates": 171, "total": 530}, "attempt1_parsed": {"summary": "This paper presents an option pricing model incorporating hidden Markovian regime-switching, stochastic interest rates, and volatility. It separates filtering and option valuation, develops robust filters using the expectation maximization algorithm, and derives an explicit expression for the conditional characteristic function. The valuation is performed using inverse Fourier transformation, with numerical experiments demonstrating the model's flexibility and the impact of regime-switching.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The paper uses advanced statistical techniques (hidden Markov models, expectation maximization, Fourier transforms) which can be considered as empirical machine learning applications beyond simple linear models. It also explicitly discusses option pricing and uses numerical experiments, indicating a predictive aspect."}, "mode": "threadpool", "paper_id": "4c6402a19fd533ec", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Option-implied correlation between iTraxx Europe Financials and Non-Financials Indexes: A measure of spillover effect in European debt crisis\nAbstract: This paper proposes an analytic method to estimate the option-implied correlation embedded in options on the iTraxx Europe CDS indexes. The option-implied correlation is suggested as a measure of the spill-over effect of default risk between the financial and corporate sectors in Europe. In particular, the correlation between the iTraxx Financials and Non-Financials sub-indexes is estimated from options on the iTraxx Main Index, which is considered as a basket option with the two sub-indexes being its underlyings. The abrupt changes of the realized correlation anticipated information of the corresponding option prices. The sovereign default risk, funding liquidity risk, level of risk aversion, and equity market performance are identified to be significant determinants of the option-implied correlation, implying interdependence amongst various markets during the European debt crisis. (C) 2013 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory).  … [truncated]", "attempt1_tokens": {"prompt": 359, "candidates": 191, "total": 550}, "attempt1_parsed": {"summary": "This paper introduces a method to calculate option-implied correlation between iTraxx Europe Financials and Non-Financials indexes, using options on the iTraxx Main Index. This implied correlation serves as a metric for default risk spillover between European financial and corporate sectors. The study identifies sovereign default risk, funding liquidity risk, risk aversion, and equity market performance as key drivers of this correlation, highlighting market interdependencies during the European debt crisis.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The paper focuses on an analytic method and correlation estimation, not machine learning applications. It discusses debt crisis and market interdependencies but not specifically government bonds or yield curves. While it analyzes determinants of correlation, it's not framed as a forecasting or prediction study."}, "mode": "threadpool", "paper_id": "624b406f55b3ee95", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Option-implied skewness: Insights from ITM-options\nAbstract: While the standard to calculate model-free option-implied skewness (MFIS) relies on out-of-the-money (OTM) options, we examine the empirical and economic implications of using in-the-money (ITM) options. We find that the positive short-term return predictability of OTM-based MFIS significantly reverses if ITM-options are used instead. While this reversal is inconsistent with an explanation based on skewness preferences, MFIS apparently reflects information that is not timely incorporated in stock prices due to market frictions. Based on these insights, we introduce Delta MFIS as a new measure of additional option-embedded information that significantly predicts subsequent returns beyond a large range of other option-based return predictors. (C) 2021 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 329, "candidates": 207, "total": 536}, "attempt1_parsed": {"summary": "This study investigates the implications of using in-the-money (ITM) options versus out-of-the-money (OTM) options for calculating model-free option-implied skewness (MFIS). The authors find that using ITM options reverses the short-term return predictability observed with OTM-based MFIS. This suggests that MFIS captures information not immediately reflected in stock prices due to market frictions. They propose a new measure, Delta MFIS, which demonstrates significant predictive power for subsequent returns, outperforming other option-based predictors.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses option-implied skewness, which is a form of empirical analysis beyond simple linear models, and introduces a new predictive measure (Delta MFIS). It does not concern government bonds or the yield curve."}, "mode": "threadpool", "paper_id": "4f8ea2176a7d1090", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Pairs selection and outranking: An application to the S&P 100 index\nAbstract: Pairs trading is a popular quantitative speculation strategy. This article proposes a general and flexible framework for pairs selection. The method uses multiple return forecasts based on bivariate information sets and multi-criteria decision techniques. Our approach can be seen as a sort of forecast combination but the output of the method is a ranking. It helps to detect potentially under- and overvalued stocks. A first application with S&P 100 index stocks provides promising results in terms of excess return and directional forecasting. (C) 2008 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 281, "candidates": 153, "total": 434}, "attempt1_parsed": {"summary": "This article proposes a general framework for pairs selection in pairs trading, utilizing multiple return forecasts from bivariate information sets and multi-criteria decision techniques. The method generates a ranking to identify potentially under- and overvalued stocks, and an application to the S&P 100 index shows promising results for excess return and directional forecasting.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The article uses multi-criteria decision techniques which can be considered beyond simple linear models, and it includes an application to the S&P 100 index, indicating an empirical application."}, "mode": "threadpool", "paper_id": "9a9d7a03be6464dd", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Oracle Properties, Bias Correction, and Bootstrap Inference for Adaptive Lasso for Time Series M-Estimators\nAbstract: We derive new theoretical results on the properties of the adaptive least absolute shrinkage and selection operator (adaptive lasso) for possibly nonlinear time series models. In particular, we investigate the question of how to conduct inference on the parameters given an adaptive lasso model. Central to this study is the test of the hypothesis that a given adaptive lasso parameter equals zero, which therefore tests for a false positive. To this end, we introduce a recentered bootstrap procedure and show, theoretically and empirically through extensive Monte Carlo simulations, that the adaptive lasso can combine efficient parameter estimation, variable selection, and inference in one step. Moreover, we analytically derive a bias correction factor that is able to significantly improve the empirical coverage of the test on the active variables. Finally, we apply the adaptive lasso and the recentered bootstrap procedure to investigate the relation between the short rate dynamics and the economy, thereby providing a statistical foundation (from a model choice pe … [truncated]", "attempt1_tokens": {"prompt": 359, "candidates": 136, "total": 495}, "attempt1_parsed": {"summary": "This paper presents theoretical results for the adaptive lasso in time series models, focusing on inference for parameters and hypothesis testing. It introduces a recentered bootstrap procedure and a bias correction factor to improve parameter estimation, variable selection, and inference. The method is applied to analyze the relationship between short-term interest rates and the economy, providing a statistical basis for the Taylor rule.", "criteria_hints": {"I1": true, "I2": false, "I3": false}, "notes": "The application to the Taylor rule is more about model justification than forecasting."}, "mode": "threadpool", "paper_id": "b3a10d9772e08e18", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Physical vs. Transition climate risks: Asymmetric effects on stock return predictability\nAbstract: This paper examines the predictive role of two dominant climate risk categories-physical and transition risks-in forecasting U.S. equity market risk premiums. The results reveal a pronounced asymmetry: physical climate risk significantly and negatively predicts stock returns both in-sample and out-of-sample, whereas transition climate risk demonstrates insignificant forecasting ability. This superior performance of physical risk delivers greater economic gains to investors and remains robust even after controlling for widely used economic predictors. However, its predictability is state-dependent, weakening during economic disruptions and strengthening following the COP21 Agreement. Further analysis shows that the cash flow and sentiment channels potentially drive the strong predictability of physical risk. Overall, our findings underscore the importance of incorporating physical climate risk into equity return forecasting models, offering actionable insights for financial decision-making processes.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning applicati … [truncated]", "attempt1_tokens": {"prompt": 321, "candidates": 135, "total": 456}, "attempt1_parsed": {"summary": "This study investigates the predictive power of physical and transition climate risks on U.S. equity market risk premiums. It finds that physical climate risk significantly predicts stock returns, while transition climate risk does not. The predictability of physical risk is state-dependent and potentially driven by cash flow and sentiment channels, offering insights for financial decision-making.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The paper uses out-of-sample forecasting which implies machine learning applications beyond simple linear models."}, "mode": "threadpool", "paper_id": "dde88ce8c7f12863", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: PARAMETRIC SPECIFICATION TEST FOR NONLINEAR AUTOREGRESSIVE MODELS\nAbstract: The paper considers testing parametric assumptions on the conditional mean and variance functions for nonlinear autoregressive models. To this end, we compare the kernel density estimate of the marginal density of the process with a convolution-type density estimate. It is shown that, interestingly, the latter estimate has a parametric (root n) rate of convergence, thus substantially improving the classical kernel density estimates whose rates of convergence are much inferior. Our results are confirmed by a simulation study for threshold autoregressive processes and autoregressive conditional heteroskedastic processes.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 267, "candidates": 154, "total": 421}, "attempt1_parsed": {"summary": "This paper proposes a parametric specification test for nonlinear autoregressive models by comparing kernel density estimates of the marginal density with a convolution-type density estimate. The convolution-type estimate achieves a parametric rate of convergence, outperforming classical kernel estimates. The method is validated through simulations on threshold autoregressive and autoregressive conditional heteroskedastic processes.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The paper discusses nonlinear autoregressive models and uses simulation studies, which can be considered empirical applications beyond simple linear models. The focus on density estimation for these models implies a forecasting or prediction context."}, "mode": "threadpool", "paper_id": "f2eb271685e466d0", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: PROPERTY AND NUMERICAL SIMULATION OF THE AIT-SAHALIA-RHO MODEL WITH NONLINEAR GROWTH CONDITIONS\nAbstract: The Ait-Sahalia-Rho model is an important tool to study a number of financial problems, including the term structure of interest rate. However, since the functions of this model do not satisfy the linear growth condition, we cannot study the properties for the solution of this model by using the traditional techniques. In this paper we overcome the mathematical difficulties due to the nonlinear growth condition by using numerical simulation. Thus we first discuss analytical properties of the model and the convergence property of numerical solutions in probability for the Ait-Sahalia-Rho model. Finally, an example for option pricing is given to illustrate that the numerical solution is an effective method to estimate the expected payoffs.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 306, "candidates": 172, "total": 478}, "attempt1_parsed": {"summary": "This paper analyzes the Ait-Sahalia-Rho model, commonly used for interest rate term structure, overcoming mathematical challenges posed by its nonlinear growth conditions through numerical simulation. It discusses analytical and convergence properties of the model's solutions and provides an example of option pricing to demonstrate the effectiveness of numerical estimation for expected payoffs.", "criteria_hints": {"I1": false, "I2": true, "I3": true}, "notes": "The paper discusses the Ait-Sahalia-Rho model which is used for term structure of interest rates, thus relating to I2. It also mentions option pricing and estimating expected payoffs, which relates to I3. However, it does not appear to use machine learning applications, making I1 false."}, "mode": "threadpool", "paper_id": "93fd092bafa0fe9d", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Option-implied preferences adjustments, density forecasts, and the equity risk premium\nAbstract: The main objective of this paper is to analyse the value of information contained in prices of options on the IBEX 35 index at the Spanish Stock Exchange Market. The forward looking information is extracted using implied risk-neutral density functions estimated by a mixture of two-lognormals and several alternative risk adjustments. Our results show that, between October 1996 and March 2000, we can reject the hypothesis that the risk-neutral densities provide accurate predictions of the distributions of future realisations of the IBEX 35 index at 4- and 8-week horizons. When forecasting through risk-adjusted densities the performance of this period is statistically improved and we no longer reject that hypothesis. We show that risk adjustments based on a power specification for the stochastic discount factor-which is the approach used so far in the literature that derives the objective density function from option prices- generates an excessive volatility of risk premia. We use alternative risk adjustments and find that the forecasting performance of the distribution improves sli … [truncated]", "attempt1_tokens": {"prompt": 445, "candidates": 158, "total": 603}, "attempt1_parsed": {"summary": "This paper analyzes the information in IBEX 35 index options to extract implied risk-neutral densities. It finds that these densities do not accurately predict future index distributions but that risk adjustments improve forecasting performance. The study also critiques existing risk adjustment methods and explores time-varying risk aversion, concluding that implied risk premiums are higher than those compensating for realized volatility.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The paper focuses on option pricing and risk-neutral densities, not machine learning applications beyond simple models. It deals with equity index options, not government bonds. It explicitly discusses forecasting and prediction."}, "mode": "threadpool", "paper_id": "a6849cec0ad130d5", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: PHANGS-JWST First Results: Dust-embedded Star Clusters in NGC 7496 Selected via 3.3 μm PAH Emission\nAbstract: The earliest stages of star formation occur enshrouded in dust and are not observable in the optical. Here we leverage the extraordinary new high-resolution infrared imaging from JWST to begin the study of dust-embedded star clusters in nearby galaxies throughout the Local Volume. We present a technique for identifying dust-embedded clusters in NGC 7496 (18.7 Mpc), the first galaxy to be observed by the PHANGS-JWST Cycle 1 Treasury Survey. We select sources that have strong 3.3 mu m PAH emission based on a F300M - F335M color excess and identify 67 candidate embedded clusters. Only eight of these are found in the PHANGS-HST optically selected cluster catalog, and all are young (six have SED fit ages of similar to 1 Myr). We find that this sample of embedded cluster candidates may significantly increase the census of young clusters in NGC 7496 from the PHANGS-HST catalog; the number of clusters younger than similar to 2 Myr could be increased by a factor of 2. Candidates are preferentially located in dust lanes and are coincident with the peaks in the PHANGS-ALMA CO ( … [truncated]", "attempt1_tokens": {"prompt": 556, "candidates": 214, "total": 770}, "attempt1_parsed": {"summary": "This study uses JWST infrared imaging to identify dust-embedded star clusters in NGC 7496 by analyzing 3.3 μm PAH emission. The technique identified 67 candidate clusters, significantly increasing the census of young clusters compared to previous optical catalogs. These candidates are often found in dust lanes and CO emission peaks. The study also examines cluster properties like concentration, luminosity, SEDs, and stellar masses, laying the groundwork for future analysis of embedded clusters in star formation and evolution.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The study uses a novel technique to identify star clusters, which could be considered a form of pattern recognition, but it does not explicitly state the use of machine learning algorithms beyond simple linear models. The primary focus is on observational astronomy and astrophysical analysis, not on developing or applying machine learning for prediction in a financial or economic context."}, "mode": "threadpool", "paper_id": "9e28d710498e479b", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Predictability and Financial Sufficiency of Health Insurance in Colombia: An Actuarial Analysis With a Bayesian Approach\nAbstract: Every year, the Colombian government provides a prospective premium, known as the capitation payment unit (CPU), for each affiliated person (according to sex, region, and age) to each health insurance company, in order to manage the corresponding risk in health. This article studies the prediction capacity for the health expenditure for the more than 20 million affiliates to the contributory regime of health, as well as the CPU's financial sufficiency, using an actuarial approach. Using the pure risk premium method and generalized linear models, both classic and Bayesian, the CPU is estimated; these results are compared to actual expenditure by an index of forecasting ability. It is concluded that the use of historical information about expenditure on health, as well as the Bayesian inference, among the other methodological innovations developed, provides an advantage for obtaining more accurate prospective values. These technical recommendations seek to support an improvement in the public budget allocation of more than 6 billion dollars per yea … [truncated]", "attempt1_tokens": {"prompt": 364, "candidates": 151, "total": 515}, "attempt1_parsed": {"summary": "This actuarial analysis investigates the predictive power and financial sufficiency of Colombia's capitation payment unit (CPU) for health insurance. Using both classic and Bayesian generalized linear models, the study estimates the CPU and compares it to actual health expenditures. The findings suggest that incorporating historical expenditure data and employing Bayesian inference leads to more accurate prospective values, potentially improving public budget allocation for the Colombian health system.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses generalized linear models, which can be considered a form of machine learning, and explicitly mentions prediction and forecasting."}, "mode": "threadpool", "paper_id": "a316a8dab4798b82", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Positive forward rates in the maximum smoothness framework\nAbstract: In this paper we present a nonlinear dynamic programming algorithm for the computation of forward rates within the maximum smoothness framework. The algorithm implements the forward rate positivity constraint for a one-parametric family of smoothness measures and it handles price spreads in the constraining data set. We investigate the outcome of the algorithm using the Swedish Bond market showing examples where the absence of the positive constraint leads to negative interest rates. Furthermore we investigate the predictive accuracy of the algorithm as we move along the family of smoothness measures. Among other things we observe that the inclusion of spreads not only improves the smoothness of forward curves but also significantly reduces the predictive error.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 279, "candidates": 186, "total": 465}, "attempt1_parsed": {"summary": "This paper introduces a nonlinear dynamic programming algorithm to compute forward rates within the maximum smoothness framework, incorporating a positivity constraint for forward rates and handling price spreads. The algorithm is tested on the Swedish Bond market, demonstrating that the absence of the positivity constraint can lead to negative interest rates. The study also examines the predictive accuracy across different smoothness measures, finding that including spreads improves curve smoothness and reduces predictive error.", "criteria_hints": {"I1": true, "I2": true, "I3": true}, "notes": "The application of a nonlinear dynamic programming algorithm to a real-world market (Swedish Bond market) and the investigation of predictive accuracy suggest an empirical application beyond pure theory and simple linear models. The focus on forward rates, yield curve, and predictive accuracy directly aligns with I2 and I3."}, "mode": "threadpool", "paper_id": "2614c15f385e0a14", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Parameter Estimations of Heston Model Based on Consistent Extended Kalman Filter\nAbstract: Heston model is widely applied to financial institutions, while there still exist difficulties in estimating the parameters and volatilities of this model. In this paper, the pseudo Maximum Likelihood Estimation and consistent extended Kalman filter (PMLE-CEKF) are implemented synchronously to estimate the Heston model. For parameter estimations, PMLE for the state equation and the measurement equation of the Heston model are conducted independently. For volatility estimations, the consistent extended Kalman filter (CEKF) algorithm is introduced to ensure the volatility to be well evaluated. Additionally, the estimation results of the Heston model are compared between PMLE-CEKF and PMLE-EKF algorithm. The numerical simulations illustrate that PMLE-CEKF algorithm works more efficiently than PMLE-EKF algorithm. Application of the PMLE-CEKF to S&P 500 shows the utility of the proposed algorithm. (C) 2017, IFAC (International Federation of Automatic Control) Hosting by Elsevier Ltd. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not  … [truncated]", "attempt1_tokens": {"prompt": 367, "candidates": 217, "total": 584}, "attempt1_parsed": {"summary": "This paper implements a synchronous approach using Pseudo Maximum Likelihood Estimation (PMLE) and a Consistent Extended Kalman Filter (CEKF) to estimate parameters and volatilities of the Heston model, a model commonly used in financial institutions. The study compares the PMLE-CEKF algorithm with the PMLE-EKF algorithm, demonstrating the former's superior efficiency through numerical simulations and an application to the S&P 500 index.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The paper applies the Heston model, which is a stochastic volatility model, and uses estimation techniques like PMLE and CEKF. While not explicitly 'Machine Learning' in the common sense, these are advanced statistical/econometric methods for parameter estimation and filtering, which can be considered empirical applications beyond simple linear models. The application to S&P 500 suggests forecasting or prediction relevance."}, "mode": "threadpool", "paper_id": "f6f95c3a073ff11d", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Predicting Wheat Futures Prices in India\nAbstract: Futures markets perform their economic roles of price discovery and hedging only when they are efficient. One of the important features of efficient market is that one cannot make abnormal profits from the futures markets by trading in it. This paper addresses the question of whether Indian wheat futures prices can be forecast. This would add to our knowledge whether wheat futures market is efficient, and would enable brokers, traders and speculators to develop profitable trading strategy. We employ the economic variable model to predict the wheat futures prices, and employ out of sample point forecasts. We also evaluate the robustness of our results by employing several alternative specifications, viz. ARMA process and artificial neural network technique. We then test the statistical significance of point forecast using the Diebold and Mariano test. We consider random walk orecast as the bench mark. In order to predict the evolution of wheat futures prices, we use traders' expectations about the futures prices, a number of economic variables and futures prices (lagged) of wheat. The study finds that the futures price of whe … [truncated]", "attempt1_tokens": {"prompt": 369, "candidates": 120, "total": 489}, "attempt1_parsed": {"summary": "This paper investigates the efficiency of the Indian wheat futures market by attempting to forecast wheat futures prices using an economic variable model, ARMA process, and artificial neural networks. The study concludes that wheat futures prices cannot be forecast, indicating an efficient market.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses ARMA and artificial neural networks, which go beyond simple linear models. It is about forecasting."}, "mode": "threadpool", "paper_id": "a924be2f5f701f16", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Predictability and habit persistence\nAbstract: This paper highlights the role of persistence in explaining predictability of excess returns. To this end, we develop a CCAPM model with habit formation when the growth rate of endowments follows a first order Gaussian autoregression. We provide a closed form solution of the price-dividend ratio and determine conditions that guarantee the existence of a bounded equilibrium. The habit stock model is found to possess internal propagation mechanisms that increase persistence. It outperforms the time separable and a 'Catching up with the Joneses' version of the model in terms of predictability therefore highlighting the role of persistence in explaining the puzzle. (c) 2005 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 288, "candidates": 171, "total": 459}, "attempt1_parsed": {"summary": "This paper develops a CCAPM model with habit formation to explain the predictability of excess returns, emphasizing the role of persistence. The model, which incorporates a first-order Gaussian autoregression for endowment growth, yields a closed-form solution for the price-dividend ratio and establishes conditions for a bounded equilibrium. The habit stock model demonstrates internal propagation mechanisms that enhance persistence and outperforms simpler models in predictability, thus underscoring the significance of persistence in addressing this financial puzzle.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The paper focuses on theoretical modeling rather than empirical machine learning applications. It does not discuss government/sovereign bonds or the yield curve."}, "mode": "threadpool", "paper_id": "c1bf075f530f6e7a", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Predicting Stock Jumps and Crashes Using Options\nAbstract: This paper investigates the informativeness of option-implied volatility and Greeks in forecasting extreme stock returns. Using a large data set of U.S. stocks and options from 1996 to 2022 and employing Light Gradient-Boosting Machine as a machine learning algorithm, we show that option characteristics, particularly implied volatility and delta, are strong predictors of extreme returns. The long-short portfolio utilizing option variables significantly outperforms a benchmark using only stock characteristics, suggesting that options provide information beyond what can be inferred from stock characteristics. Put options are revealed to be more informative than call options, and crashes are easier to predict than jumps.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 287, "candidates": 162, "total": 449}, "attempt1_parsed": {"summary": "This study explores the use of option-implied volatility and Greeks to predict extreme stock returns (jumps and crashes). Utilizing a Light Gradient-Boosting Machine on a large dataset from 1996-2022, the research demonstrates that option characteristics, especially implied volatility and delta, are effective predictors. The findings indicate that options offer valuable information beyond stock characteristics, with put options being more predictive than call options, and crashes being more predictable than jumps.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses a machine learning algorithm (Light Gradient-Boosting Machine) and focuses on prediction."}, "mode": "threadpool", "paper_id": "d40c4143dfb51f9e", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Predicting Equity Premium: A New Momentum Indicator Selection Strategy With Machine Learning\nAbstract: We propose a new momentum-determined indicator-switching (N-MDIS) strategy, harnessing the power of machine learning to enhance the accuracy of equity premium prediction. Specifically, we re-examine the regime-dependent feature of univariate predictive regression relative to the benchmark. Furthermore, we investigate the prediction mechanism of the momentum-determined indicator-switching (MDIS) strategy and validate the significance of market regime information for the MDIS. Our findings demonstrate an overwhelmingly superior ex-post forecasting performance compared with the MDIS. More notably, our empirical results substantiate that machine learning greatly aids in momentum indicator selection. The results show that the N-MDIS with machine learning generates more accurate ex-ante equity premium forecasts than both MDIS strategy and N-MDIS strategy with logistic regression, yielding statistically and economically significant results. Moreover, our new approach exhibits robust forecasting performance across a series of robustness tests.\n\nCriteria: {\"I1\": \"The article include … [truncated]", "attempt1_tokens": {"prompt": 341, "candidates": 179, "total": 520}, "attempt1_parsed": {"summary": "This study introduces a novel momentum-determined indicator-switching (N-MDIS) strategy that utilizes machine learning to improve equity premium prediction. The strategy re-examines regime-dependent features of predictive regressions and validates the importance of market regime information for the MDIS. The N-MDIS with machine learning demonstrates superior ex-post forecasting performance compared to existing methods, including the MDIS strategy and N-MDIS with logistic regression, providing accurate ex-ante equity premium forecasts with statistically and economically significant results. The approach also shows robust forecasting performance.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study focuses on equity premium, not government/sovereign bonds or the yield curve."}, "mode": "threadpool", "paper_id": "85aad9d7cb4653db", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Predicting the milk yield curve of dairy cows in the subsequent lactation period using deep learning\nAbstract: Existing lactation models predict milk yields based on a fixed amount of observed milk production in early lactation. In contrast, this study proposes a model to predict the entire lactation curve of dairy cows by leveraging historical milk yield information observed in the preceding cycle. More specifically, we present a deep learning framework to encode the model inputs, predict the latent representation of the milk yield sequences and generate the corresponding lactation curves. Results show that the proposed framework outperforms the baseline models and that during the first 26 days of lactation, the model's predictions are more accurate than those of a state-of-the-art lactation model which is able to leverage the observed milk yields. As a result, the framework presented in this study allows farmers to increase their forecast horizon with respect to predicting its herd's total production and hence facilitates optimal herd management. Additionally, the model can be used to compare a cow's actual and expected milk yield over the entire course of the lactation cy … [truncated]", "attempt1_tokens": {"prompt": 409, "candidates": 166, "total": 575}, "attempt1_parsed": {"summary": "This study introduces a deep learning framework to predict the entire lactation curve of dairy cows using historical milk yield data from the previous lactation cycle. The model outperforms baseline methods and shows higher accuracy in early lactation compared to a state-of-the-art model. It aids in optimizing herd management, accelerating disease detection, enhancing animal monitoring, and improving financial forecasting by incorporating health, reproduction, and management events.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses deep learning, which goes beyond simple linear models. While it predicts a 'yield curve' for milk production, it is not related to government/sovereign bonds."}, "mode": "threadpool", "paper_id": "0f8cf13e7b3aab14", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Predicting tanker freight rates using parsimonious variables and a hybrid artificial neural network with an adaptive genetic algorithm\nAbstract: Short-term prediction of tanker freight rates (TFRs) is strategically important to stakeholders in the oil shipping industry. This study develops a hybrid TFR prediction model based on an artificial neural network (ANN) and an adaptive genetic algorithm (AGA). The AGA adaptively searches satisficing network parameters such as input delay size. The ANN iteratively optimizes a prediction network considering parsimonious variables and time-lag effects as predictors. Three parsimonious variables (crude oil price, fleet productivity and bunker price) are selected by a stepwise regression of TFR variables. The article compares the performance of its hybrid model with two traditional approaches (regression and moving average), as well as with the findings of existing ANN studies. The results of our model (root mean squared error (RMSE)= 11.2 WS) are not only significantly superior to the regression approach (RMSE = 21.6 WS) and the moving average approach (RMSE = 17.5 WS), but are even slightly superior to the results of existing ANN studi … [truncated]", "attempt1_tokens": {"prompt": 397, "candidates": 150, "total": 547}, "attempt1_parsed": {"summary": "This study proposes a hybrid model combining an Artificial Neural Network (ANN) with an Adaptive Genetic Algorithm (AGA) to predict tanker freight rates (TFRs). The model utilizes parsimonious variables like crude oil price, fleet productivity, and bunker price, and demonstrates superior performance compared to regression, moving average, and existing ANN studies.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses an ANN and AGA, which are machine learning techniques, and compares them to regression and moving average models. It focuses on prediction of tanker freight rates."}, "mode": "threadpool", "paper_id": "cc4a38ac36cf4d52", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Predicting daily oil prices: Linear and non-linear models\nAbstract: In this paper, we assess the accuracy of linear and nonlinear models in predicting daily crude oil prices. Competing forecasts of crude oil prices are generated from parsimonious linear models which require no parameter estimation, as well as linear and nonlinear models. Two of the linear models that we employ exploit the informational content of oil demand and the increasing correlation between oil and equity prices and are novel to the literature. The nonlinear model that we consider is an artificial neural network. More specifically, we consider a bagged neural network, a neural network trained using the genetic algorithm as well as a neural network with fuzzy logic. We find that some of the linear models outperform the random walk in terms of out-of-sample statistical forecast accuracy. Our findings also suggest that while the buy-and-hold strategy dominates some of the models in terms of dollar payoffs and risk-adjusted returns under a long-only strategy, all the models that we consider generate higher dollar payoffs than the buy-and-hold strategy under the short-only strategy. An investor obtains the l … [truncated]", "attempt1_tokens": {"prompt": 386, "candidates": 191, "total": 577}, "attempt1_parsed": {"summary": "This paper compares the predictive accuracy of linear and nonlinear models for daily crude oil prices. It includes novel linear models incorporating oil demand and oil-equity correlations, and nonlinear models such as bagged neural networks, genetic algorithm-trained neural networks, and fuzzy logic neural networks. Some linear models outperform the random walk, and while buy-and-hold is superior for long-only strategies, all models yield better payoffs than buy-and-hold for short-only strategies. Trading based on the moving average convergence divergence indicator yields the largest profits.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses artificial neural networks and genetic algorithms, which are machine learning techniques, and compares them to linear models. The focus is on predicting oil prices, not government bonds."}, "mode": "threadpool", "paper_id": "fd7272563aec0c21", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Predicting Recessions with Leading Indicators: Model Averaging and Selection over the Business Cycle\nAbstract: Four methods of model selectionequally weighted forecasts, Bayesian model-averaged forecasts, and two models produced by the machine-learning algorithm boostingare applied to the problem of predicting business cycle turning points with a set of common macroeconomic variables. The methods address a fundamental problem faced by forecasters: the most useful model is simple but makes use of all relevant indicators. The results indicate that successful models of recession condition on different economic indicators at different forecast horizons. Predictors that describe real economic activity provide the clearest signal of recession at very short horizons. In contrast, signals from housing and financial markets produce the best forecasts at longer forecast horizons. A real-time forecast experiment explores the predictability of the 2001 and 2007 recessions. Copyright (c) 2015 John Wiley & Sons, Ltd.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about govern … [truncated]", "attempt1_tokens": {"prompt": 331, "candidates": 183, "total": 514}, "attempt1_parsed": {"summary": "This study compares four methods (equal weighting, Bayesian model averaging, and two boosting models) for predicting business cycle turning points using macroeconomic variables. It finds that different indicators are useful at different forecast horizons, with real activity indicators best for short horizons and housing/financial market indicators best for longer horizons. A real-time experiment tests the models' ability to predict the 2001 and 2007 recessions.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses boosting, a machine learning algorithm, and applies it to forecasting business cycle turning points, which aligns with I1 and I3. However, it does not focus on government bonds or the yield curve, making I2 false."}, "mode": "threadpool", "paper_id": "dc45f56e400e0bc4", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Predictive power of investor sentiment for Bitcoin returns: Evidence from COVID-19 pandemic\nAbstract: In this paper, we examine the impact of investor sentiment on Bitcoin returns. Using a large dataset of messages discussed on social media and several financial indicators, we create a sentiment indicator based on computational text analysis and driven by the principal component analysis (PCA) method. We utilize a vector autoregressive analysis and other analytical methods to examine the sentiment index-bitcoin return nexus. Our findings reveal that the sentiment index is a strong predictor of cryptocurrency market returns in the short term. Furthermore, we confirm that during the COVID-19 pandemic, investors' sentiments significantly impacted Bitcoin returns. Our results show that the proposed sentiment index can generate excess returns for investors who utilize it as a return predictor. Our empirical findings suggest important policy implications.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"Th … [truncated]", "attempt1_tokens": {"prompt": 309, "candidates": 153, "total": 462}, "attempt1_parsed": {"summary": "This study investigates the predictive power of investor sentiment on Bitcoin returns, particularly during the COVID-19 pandemic. The authors construct a sentiment indicator using social media data and financial indicators via principal component analysis (PCA) and computational text analysis. Their findings indicate that this sentiment index is a significant short-term predictor of cryptocurrency returns, with a notable impact observed during the pandemic.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses PCA and computational text analysis, which can be considered machine learning applications beyond simple linear models. The focus is on prediction of Bitcoin returns."}, "mode": "threadpool", "paper_id": "7b8a48fc4285407a", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Predicting recessions using trends in the yield spread\nAbstract: The yield spread, measured as the difference between long- and short-term interest rates, is widely regarded as one of the strongest predictors of economic recessions. In this paper, we propose an enhanced recession prediction model that incorporates trends in the value of the yield spread. We expect our model to generate stronger recession signals because a steadily declining value of the yield spread typically indicates growing pessimism associated with the reduced future business activity. We capture trends in the yield spread by considering both the level of the yield spread at a lag of 12 months as well as its value at each of the previous two quarters leading up to the forecast origin, and we evaluate its predictive abilities using both logit and artificial neural network models. Our results indicate that models incorporating information from the time series of the yield spread correctly predict future recession periods much better than models only considering the spread value as of the forecast origin. Furthermore, the results are strongest for our artificial neural network model and logistic regression  … [truncated]", "attempt1_tokens": {"prompt": 377, "candidates": 211, "total": 588}, "attempt1_parsed": {"summary": "This paper proposes an enhanced recession prediction model using trends in the yield spread, which is the difference between long- and short-term interest rates. The model incorporates historical yield spread values (12 months and previous two quarters) to capture trends, hypothesizing that a declining spread indicates pessimism about future business activity. The predictive abilities are evaluated using logit and artificial neural network models. Results show that models incorporating time series information of the yield spread predict recessions better than those only considering the current spread value, with the artificial neural network model and logistic regression with interaction terms performing strongest.", "criteria_hints": {"I1": true, "I2": true, "I3": true}, "notes": "The paper explicitly mentions \"artificial neural network models\" and \"logistic regression model that includes interaction terms\", which go beyond simple linear models. The core topic is the \"yield spread\" and its relation to \"recession prediction\"."}, "mode": "threadpool", "paper_id": "c77ed07cec4b8ecf", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Pricing Deflation Risk with US Treasury Yields\nAbstract: We use an arbitrage-free term structure model with spanned stochastic volatility to determine the value of the deflation protection option embedded in Treasury inflation-protected securities. The model accurately prices the deflation protection option prior to the financial crisis when its value was near zero; at the peak of the crisis in late 2008 when deflationary concerns spiked sharply; and in the post-crisis period. During 2009, the average value of this option at the 5-year maturity was 41 basis points on a par-yield basis. The option value is shown to be closely linked to overall market uncertainty as measured by the VIX, especially during and after the 2008 financial crisis.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 302, "candidates": 177, "total": 479}, "attempt1_parsed": {"summary": "This paper utilizes an arbitrage-free term structure model with spanned stochastic volatility to price the deflation protection option within US Treasury Inflation-Protected Securities. The model's accuracy is demonstrated across different periods, including pre-crisis, the peak of the 2008 financial crisis, and the post-crisis era. The value of the deflation protection option is found to be closely correlated with market uncertainty, particularly the VIX, especially around the 2008 crisis.", "criteria_hints": {"I1": false, "I2": true, "I3": false}, "notes": "The study uses a term structure model, not machine learning. While it prices an option, the primary focus isn't forecasting future yield curve movements or inflation rates."}, "mode": "threadpool", "paper_id": "6414f68f51ea9ec4", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Predictive power of Markovian models: Evidence from US recession forecasting\nAbstract: This paper provides extensions to the application of Markovian models in predicting US recessions. The proposed Markovian models, including the hidden Markov and Markov models, incorporate the temporal autocorrelation of binary recession indicators in a traditional but natural way. Considering interest rates and spreads, stock prices, monetary aggregates, and output as the candidate predictors, we examine the out-of-sample performance of the Markovian models in predicting the recessions 1-12 months ahead, through rolling window experiments as well as experiments based on the fixed full training set. Our study shows that the Markovian models are superior to the probit models in detecting a recession and capturing the recession duration. But sometimes the rolling window method may affect the models' prediction reliability as it could incorporate the economy's unsystematic adjustments and erratic shocks into the forecast. In addition, the interest rate spreads and output are the most efficient predictor variables in explaining business cycles.\n\nCriteria: {\"I1\": \"The article includes an empiri … [truncated]", "attempt1_tokens": {"prompt": 346, "candidates": 199, "total": 545}, "attempt1_parsed": {"summary": "This paper extends the use of Markovian models (hidden Markov and Markov models) for forecasting US recessions, incorporating temporal autocorrelation of recession indicators. It compares their out-of-sample performance against probit models using interest rates, spreads, stock prices, monetary aggregates, and output as predictors. The Markovian models demonstrate superiority in recession detection and duration capture, although rolling window methods can impact reliability. Interest rate spreads and output are identified as the most effective predictors.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses Markovian models, which are a form of machine learning, and compares them to probit models, indicating it goes beyond simple linear models. While it discusses interest rates and spreads, the primary focus is on recession forecasting, not sovereign bonds or the yield curve in isolation."}, "mode": "threadpool", "paper_id": "70093db6f3573ba2", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Pricing real options based on linear loss functions and conditional value at risk\nAbstract: The main purpose of this paper is to expand real option analysis out of the realm of pure financial option pricing techniques. To overcome many of the well-known concerns by adopting the financial option pricing techniques for modeling real options problems such as replicating portfolio concept, geometric Brownian motion as underlying stochastic process, and estimating project volatility, we propose an alternative real option valuation based on the loss function approach. The option value determined by the loss function approach is equivalent to the expected value of perfect information (EVPI) in decision analysis. It basically sets the upper bound of risk premium to pay in retaining the options. In practice, many firms utilize the concept of Value at Risk to manage their portfolio risk. If a firm sets a target VAR, then we may be able to link this VAR in refining the actual risk premium to pay in hedging the risk embedded in the investment. With this practice in mind, we present a logic to figure out an appropriate amount of real option premium to pay for a given level of risk tolera … [truncated]", "attempt1_tokens": {"prompt": 382, "candidates": 153, "total": 535}, "attempt1_parsed": {"summary": "This paper proposes an alternative method for valuing real options by using a loss function approach, which is equivalent to the expected value of perfect information. It aims to overcome limitations of traditional financial option pricing techniques for real options. The approach incorporates Value at Risk (VaR) to determine an appropriate risk premium for hedging investment risks, with a comprehensive example provided.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The paper discusses option pricing and risk management but does not appear to involve machine learning applications, government bonds, or forecasting/prediction in the sense of time-series forecasting."}, "mode": "threadpool", "paper_id": "72f324761d5c5db3", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Quadratic stochastic intensity and prospective mortality tables\nAbstract: We consider a quadratic stochastic intensity model with a Gaussian autoregressive factor, derive explicit formulas for predictive mortality tables and recursive updating formulas are also provided. We also explain how to use appropriately the Kalman filter to estimate the parameters of the model and to approximate the values of the underlying factor. This methodology is applied to French human mortality tables. (C) 2008 Elsevier B.V. Ail rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 240, "candidates": 138, "total": 378}, "attempt1_parsed": {"summary": "This paper presents a quadratic stochastic intensity model with a Gaussian autoregressive factor, offering explicit formulas for predictive mortality tables and recursive updating. It also details the use of the Kalman filter for parameter estimation and factor approximation, with an application to French human mortality data.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The model described, while statistical, involves complex methods beyond simple linear models, fitting the spirit of I1. The application to mortality tables clearly indicates a forecasting/prediction aspect for I3."}, "mode": "threadpool", "paper_id": "1f915ca874b0d1e8", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Real economic activity leading indicators: should we have paid more attention?\nAbstract: The ability to predict business cycle activity is an invaluable skill for governments and policy makers alike, especially before an economy enters a downturn. We analyse causality relationships between key leading economic indicators and economic growth for three countries from 1970 to 2010. We find that while many indicators do not help explain current movements in GDP growth, lags of these indicators do. In addition, the direction of the change and the size of the change in the lagged economic indicators are very important in many cases. This is particularly true for housing indicators.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 272, "candidates": 130, "total": 402}, "attempt1_parsed": {"summary": "This study investigates the predictive power of leading economic indicators for GDP growth in three countries between 1970 and 2010. It finds that while current indicator values are not always predictive, lagged values, particularly changes in housing indicators, are significant predictors of economic growth.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The study uses causality analysis, not machine learning. It focuses on economic indicators, not government bonds or the yield curve."}, "mode": "threadpool", "paper_id": "eae365d7b19c2a45", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: RETRACTED: Wheat Futures Prices Prediction in China: A Hybrid Approach (Retracted Article)\nAbstract: Stocks markets play their financial roles of price shocks and hedging just when they are proficient. The imperative highlights of productive market are that one cannot make extraordinary profit from the stocks markets. This research investigates whether China wheat futures price can be predicted by employing artificial intelligence neural network. This would add to our knowledge whether wheat futures market is resourceful and would enable traders, sellers, and investors to improve cost-effective trading strategy. We utilize the traditional financial model to forecast the wheat futures price and acquire out of sample point estimates. We additionally assess the robustness of our outcomes by applying several alternative forecasting techniques such as artificial intelligence with one hidden layer and autoregressive integrated moving average (ARIMA) model. Furthermore, the statistical significance of our point estimation was further tested through the Mariano and Diebold test. Considering random walk forecast as the bench mark, we used a number of economic indicators, trader's exp … [truncated]", "attempt1_tokens": {"prompt": 435, "candidates": 116, "total": 551}, "attempt1_parsed": {"summary": "This retracted article investigates the predictability of China's wheat futures prices using artificial intelligence (ANN) and ARIMA models. The study found that the ANN model with one hidden layer outperformed other methods, suggesting that wheat futures prices can be predicted and the market is not efficient. The research aimed to inform trading strategies.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The article has been retracted."}, "mode": "threadpool", "paper_id": "d59e983e8c52c31d", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Public debt stabilization: the relevance of policymakers' time horizons\nAbstract: Policymakers are stuck in time. Political short-termism, policy myopia, policy short-sightedness, and similar words have been coined to emphasize the present-centric policy thinking. Politics tends to produce short time horizons, and as a result, policymakers often fail to use present opportunities to mitigate future harms. Focusing on fiscal and monetary strategic interactions, given different separate decision makers, our paper aims to explore the effects of policymakers' time horizons on debt stabilization. To formalize our ideas, we use the novel concept of Nonlinear-model-predictive-control Feedback Nash Equilibrium (NFNE) and find that present-centric policy thinking and decision horizons matters under several dimensions.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 296, "candidates": 156, "total": 452}, "attempt1_parsed": {"summary": "This paper investigates how policymakers' short-term perspectives affect public debt stabilization, using a novel Nonlinear-model-predictive-control Feedback Nash Equilibrium (NFNE) framework. It explores the fiscal and monetary strategic interactions and concludes that present-centric policy thinking significantly impacts debt stabilization.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The paper uses a novel theoretical framework (NFNE) but does not appear to involve an empirical machine learning application. It focuses on debt stabilization rather than government bonds or yield curves specifically, and while it discusses policy implications, its primary focus is not forecasting."}, "mode": "threadpool", "paper_id": "f61f9aaf4b3a4dc3", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Quantitative assessment of common practice procedures in the fair evaluation of embedded options in insurance contracts\nAbstract: This work analyses the common industry practice used to evaluate financial options written on with profit policies issued by European insurance companies. In the last years regulators introduced, with the Solvency II directive, a market consistent valuation framework for determining the fair value of asset and liabilities of insurance funds. A relevant aspect is how to deal with the estimation of sovereign credit and liquidity risk, that are important components in the valuation of the majority of insurance funds, which are usually heavily invested in treasury bonds. The common practice is the adoption of the certainty equivalent approach (CEQ) for the risk neutral evaluation of insurance liabilities, which results in a deterministic risk adjustment of the securities cash flows. In this paper, we propose an arbitrage free stochastic model for interest rate, credit and liquidity risks, that takes into account the dependences between different government bond issuers. We test the impact of the common practice against our proposed model, via Monte Ca … [truncated]", "attempt1_tokens": {"prompt": 439, "candidates": 163, "total": 602}, "attempt1_parsed": {"summary": "This paper analyzes the common industry practice for evaluating financial options in European insurance contracts, specifically the certainty equivalent approach (CEQ) for risk-neutral valuation. It proposes an arbitrage-free stochastic model for interest rate, credit, and liquidity risks, considering inter-issuer dependencies. The study concludes that the CEQ approach is inappropriate for options based on statutory accounting rules, common in European with-profit insurance products, and advocates for more comprehensive models like the one presented for market-consistent evaluations.", "criteria_hints": {"I1": false, "I2": true, "I3": false}, "notes": "The paper discusses stochastic modeling and Monte Carlo simulations, but not machine learning applications."}, "mode": "threadpool", "paper_id": "c2f42ac2b768296a", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Quantitative law describing market dynamics before and after interest-rate change\nAbstract: We study the behavior of U.S. markets both before and after U.S. Federal Open Market Commission meetings and show that the announcement of a U.S. Federal Reserve rate change causes a financial shock, where the dynamics after the announcement is described by an analog of the Omori earthquake law. We quantify the rate n(t) of aftershocks following an interest-rate change at time T and find power-law decay which scales as n(t-T) similar to (t-T)(-Omega), with Omega positive. Surprisingly, we find that the same law describes the rate n'(vertical bar t-T vertical bar) of preshocks before the interest-rate change at time T. This study quantitatively relates the size of the market response to the news which caused the shock and uncovers the presence of quantifiable preshocks. We demonstrate that the news associated with interest-rate change is responsible for causing both the anticipation before the announcement and the surprise after the announcement. We estimate the magnitude of financial news using the relative difference between the U.S. Treasury Bill and the Federal Funds effective rate … [truncated]", "attempt1_tokens": {"prompt": 433, "candidates": 206, "total": 639}, "attempt1_parsed": {"summary": "This study investigates U.S. market dynamics before and after Federal Reserve interest-rate changes, revealing that rate changes act as financial shocks. The authors found that both aftershocks and preshocks follow a power-law decay, similar to the Omori earthquake law. They quantified the magnitude of financial news and observed that bad news has a larger impact than good news, with significant volatility aftershocks indicating market under-reaction.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The study uses quantitative analysis and power-law decay models, but does not appear to involve machine learning applications beyond simple linear models. It focuses on general market dynamics rather than specifically government/sovereign bonds or the yield curve. While it describes market behavior, it is not primarily focused on forecasting or prediction in the sense of predicting future price movements."}, "mode": "threadpool", "paper_id": "457adc903852506e", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Pseudo-True SDFs in Conditional Asset Pricing Models\nAbstract: This article is motivated by the need to bridge some gap between modern asset pric-ing theory and recent developments in econometric methodology. While asset pric-ing theory enhances the use of conditional pricing models, econometric inference of conditional models can be challenging due to misspecification or weak identifica-tion. To tackle the case of misspecification, we utilize the conditional Hansen and Jagannathan (1997) (HJ) distance as studied by Gagliardini and Ronchetti (2016), but we set the focus on interpretation and estimation of the pseudo-true value defined as the argument of the minimum of this distance. While efficient Generalized Method of Moments (GMM) has no meaning for estimation of a pseudo-true value, the HJ-distance not only delivers a meaningful loss function, but also features an additional advantage for the interpretation and estimation of man-aged portfolios whose exact pricing characterizes the pseudo-true pricing kernel (stochastic discount factor (SDF)). For conditionally affine pricing kernels, we can display some managed portfolios which are well-defined independently of the pseu … [truncated]", "attempt1_tokens": {"prompt": 669, "candidates": 219, "total": 888}, "attempt1_parsed": {"summary": "This paper addresses the challenge of estimating conditional asset pricing models, particularly when misspecification or weak identification is present. It focuses on the concept of the pseudo-true value, defined as the minimizer of the conditional Hansen-Jagannathan (HJ) distance. The authors propose a smooth minimum distance (SMD) estimator, which is robust to issues like small density values of conditioning variables and can handle weaker identification contexts. The SMD estimator is shown to have Gaussian asymptotic distribution properties.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The paper discusses econometric methodology for asset pricing models and introduces a new estimator (SMD). While it mentions 'managed portfolios' and 'factors', it does not appear to be an empirical application of machine learning beyond standard econometric techniques. It is not about government bonds or yield curves, nor is it primarily about forecasting, but rather about the estimation of asset pricing models."}, "mode": "threadpool", "paper_id": "2278b01d1d5eb032", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Real estate climate index and aggregate stock returns: Evidence from China\nAbstract: We show that China's real estate climate index (RECI) can be used to forecast the aggregate stock market return. It outperforms popular return predictors both in- and out-of-sample, especially at the monthly horizon. Additionally, RECI's predictive ability is stronger among stocks of small market capitalization and low momentum. For a typical mean-variance investor, RECI's predictive power may provide an additional utility gain of 3.41%. We discuss three potential sources of RECI's predictive ability and present the corresponding evidence, including the cash flow channel, the firm fundamental channel, and the investment substitution channel.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 289, "candidates": 165, "total": 454}, "attempt1_parsed": {"summary": "This study demonstrates that China's Real Estate Climate Index (RECI) can predict aggregate stock market returns, outperforming existing predictors, particularly at the monthly horizon. The predictive power is more pronounced for small-cap and low-momentum stocks. The research explores three potential mechanisms behind this predictability: the cash flow channel, the firm fundamental channel, and the investment substitution channel.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The study focuses on forecasting stock returns using a real estate index, not government bonds or yield curves. While it uses statistical methods for prediction, it does not explicitly mention machine learning applications beyond simple linear models."}, "mode": "threadpool", "paper_id": "7098321857875dd1", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Real-time Bayesian learning and bond return predictability\nAbstract: The paper examines statistical and economic evidence of out-of-sample bond return predictability for a real-time Bayesian investor who learns about parameters, hidden states, and predictive models over time. We find some statistical evidence using information contained in forward rates. However, such statistical predictability can hardly generate any economic value for investors. Furthermore, we find that strong statistical and economic evidence of bond return predictability from fully-revised macroeconomic data vanishes when real-time macroeconomic information is used. We also show that highly levered investments in bonds can improve short-run bond return predictability. (C) 2021 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 288, "candidates": 134, "total": 422}, "attempt1_parsed": {"summary": "This paper investigates bond return predictability for a real-time Bayesian investor. While some statistical predictability is found using forward rates, it offers little economic value. Predictability using fully-revised macroeconomic data disappears with real-time information. Highly leveraged bond investments can enhance short-run predictability.", "criteria_hints": {"I1": true, "I2": true, "I3": true}, "notes": "The paper mentions Bayesian learning and models that learn over time, which can be considered beyond simple linear models. It directly addresses bond return predictability."}, "mode": "threadpool", "paper_id": "9f0348c54278ded0", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Regime Changes and Financial Markets\nAbstract: Regime-switching models can match the tendency of financial markets to often change their behavior abruptly and the phenomenon that the new behavior of financial variables often persists for several periods after such a change. Although the regimes captured by regime-switching models are identified by an econometric procedure, they often correspond to different periods in regulation, policy, and other secular changes. In empirical estimates, the means, volatilities, autocorrelations, and cross-covariances of asset returns often differ across regimes in a manner that allows regime-switching models to capture the stylized behavior of many financial series including fat tails, heteroskedasticity, skewness, and time-varying correlations. In equilibrium models, regimes in fundamental processes, such as consumption or dividend growth, strongly affect the dynamic properties of equilibrium asset prices and can induce non-linear risk-return trade-offs. Regime switches also lead to potentially large consequences for investors' optimal portfolio choice.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not  … [truncated]", "attempt1_tokens": {"prompt": 339, "candidates": 211, "total": 550}, "attempt1_parsed": {"summary": "This paper discusses regime-switching models and their application to financial markets. These models can capture abrupt changes in market behavior and the persistence of new behaviors. The identified regimes often align with changes in regulation, policy, or other secular shifts. Empirical estimates show differences in means, volatilities, autocorrelations, and cross-covariances of asset returns across regimes, which helps capture stylized behaviors like fat tails, heteroskedasticity, skewness, and time-varying correlations. In equilibrium models, regime switches in fundamental processes impact asset prices and investor portfolio choices.", "criteria_hints": {"I1": true, "I2": false, "I3": false}, "notes": "The abstract mentions empirical estimates and capturing stylized behaviors, suggesting an empirical application beyond pure theory. It does not specifically mention government bonds or the yield curve. While it discusses changes in market behavior, it doesn't explicitly focus on forecasting or prediction."}, "mode": "threadpool", "paper_id": "4e8eda2d3ff61821", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Reconciling interest rates evidence with theory: Rejecting unit roots when the HD(1) is a competing alternative\nAbstract: The paper introduces the HD(1), a Markovian process of order one with reversion rates that are faster the farther the process is from equilibrium. The aHD(1) approximation is introduced to allow for an estimation -calibration procedure based on available ARMA routines. Critical values of unit root tests with aHD(1) alternative are tabulated for the signed likelihood -ratio statistic. Revisiting the non-stationarity of interest rates stylized fact, the aHD(1) is found to be preferred to ARMA, SETAR and RCA and the resulting tests to reject the unit root hypothesis for all rates and yields considered.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 297, "candidates": 189, "total": 486}, "attempt1_parsed": {"summary": "This paper proposes the HD(1) model, a Markovian process with faster reversion rates when further from equilibrium, and its approximation aHD(1) for estimation. It introduces critical values for unit root tests against the aHD(1) alternative and applies them to interest rates. The findings suggest the aHD(1) is superior to other models (ARMA, SETAR, RCA) and leads to the rejection of the unit root hypothesis for all examined interest rates and yields.", "criteria_hints": {"I1": false, "I2": true, "I3": false}, "notes": "The paper focuses on statistical modeling and hypothesis testing related to interest rates, rather than empirical machine learning applications. While it deals with interest rates and yields, the primary focus is not forecasting but rather testing for stationarity."}, "mode": "threadpool", "paper_id": "d709c1992299932e", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Renewable integration and energy storage management and conversion in grid systems: A comprehensive review\nAbstract: The dynamic behaviours of battery energy storage systems (BESSs) make their cutting-edge technology for power grid applications. A BESS must have a Battery Management System (BMS) for dependable, efficient, and risk-free operation. With an emphasis on BESSs and the control strategies for their state-of-charge (SoC) balancing, this article thoroughly reviews energy storage systems (ESSs) on a grid scale. It delves into the future of grid-scale BESSs and the function of ESS, focusing on Li-ion battery systems and drawing attention to the essential features and integration hurdles of Li-ion cells. This review examines the many sides, specifically the cost-benefit analysis, operational efficiencies, and financial incentives that push people to use ESSs. To further improve energy storage and utilization, the article delves into managing hybrid storage systems, which combine photovoltaics (PV), batteries, and supercapacitors. Innovative solutions and technological advancements are the main focus of this examination of current trends in power conversion systems (PCS) … [truncated]", "attempt1_tokens": {"prompt": 406, "candidates": 167, "total": 573}, "attempt1_parsed": {"summary": "This review comprehensively examines grid-scale battery energy storage systems (BESSs), focusing on Battery Management Systems (BMS), State-of-Charge (SoC) balancing, and Li-ion battery integration. It discusses cost-benefit analyses, operational efficiencies, financial incentives, and the management of hybrid storage systems (PV, batteries, supercapacitors). The review also covers power conversion systems (PCS) and future developments for grid stability and reliability.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The abstract does not mention machine learning applications, government bonds, or forecasting/prediction. It focuses on energy storage systems and grid integration."}, "mode": "threadpool", "paper_id": "fcef337c3b0fa757", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Satellite-detected gain in built-up area as a leading economic indicator\nAbstract: Leading indicators of future economic activity include measures such as new housing starts, managers purchasing index, money supply, and bond yields. Such macroeconomic and financial indicators hold predictive power in signaling recessionary periods. However, many indicators are constrained by the fact that data are often published with some delay and are subject to constant revision (Bandholz and Funke 2003, Huanget al 2018, Orphanides 2003). In this research, we propose a leading indicator derived from satellite imagery, the expansion of anthropogenic bare ground. Satellite-detected gain in built-up area, a major land cover and land use (LCLU) outcome of anthropogenic bare ground gain (ABGG), provides an inexpensive, consistent, and near-real-time indicator of global and regional macroeconomic change. Our panel data analysis across four major regions of the world from 2001 to 2012 shows that the logarithm of total ABGG, mostly owing to its major LCLU outcome, the expansion of built-up land in either year t, t - 1 or t - 2, significantly correlated with the year t logarithm of gross domestic  … [truncated]", "attempt1_tokens": {"prompt": 607, "candidates": 166, "total": 773}, "attempt1_parsed": {"summary": "This research proposes using satellite-detected built-up area expansion as a near-real-time leading economic indicator. The study analyzes panel data from 2001-2012 across four major regions, finding a significant correlation between the gain in built-up area and the de-trended logarithm of GDP. The authors suggest that satellite data analysis can provide valuable information for economic forecasting.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The abstract mentions \"advances in machine learning\" and \"scalable methods\" in the context of analyzing satellite data for economic forecasting, suggesting an empirical ML application beyond simple linear models."}, "mode": "threadpool", "paper_id": "031c94725788365c", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Risk-neutral valuation of participating life insurance contracts in a stochastic interest rate environment\nAbstract: Over the last years, the valuation of life insurance contracts using concepts from financial mathematics has become a popular research area for actuaries as well as financial economists. In particular, several methods have been proposed of how to model and price participating policies, which are characterized by an annual interest rate guarantee and some bonus distribution rules. However, despite the long terms of life insurance products, most valuation models allowing for sophisticated bonus distribution rules and the inclusion of frequently offered options assume a simple Black-Scholes setup and, more specifically, deterministic or even constant interest rates.We present a framework in which participating life insurance contracts including predominant kinds of guarantees and options can be valuated and analyzed in a stochastic interest rate environment. In particular, the different option elements can be priced and analyzed separately. We use Monte Carlo and discretization methods to derive the respective values.The sensitivity of the contract and guarantee  … [truncated]", "attempt1_tokens": {"prompt": 496, "candidates": 154, "total": 650}, "attempt1_parsed": {"summary": "This paper presents a framework for valuing participating life insurance contracts in a stochastic interest rate environment, using Monte Carlo and discretization methods. It analyzes the sensitivity of contract and guarantee values to various parameters, finding that stochastic interest rates significantly impact embedded options, and that the proportion of stocks in the insurer's portfolio affects contract value. The study contrasts these findings with models assuming constant interest rates.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The abstract does not mention machine learning applications, government bonds, or explicit forecasting/prediction, focusing instead on financial modeling and valuation of insurance contracts."}, "mode": "threadpool", "paper_id": "98742387c3e762c5", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Semiparametric inference in a GARCH-in-mean model\nAbstract: A new semiparametric estimator for an empirical asset pricing model with general nonparametric risk-return tradeoff and GARCH-type underlying volatility is introduced. Based on the profile likelihood approach, it does not rely on any initial parametric estimator of the conditional mean function, and it is under stated conditions consistent, asymptotically normal, and efficient, i.e., it achieves the semiparametric lower bound. A sampling experiment provides finite sample comparisons with the parametric approach and the iterative semiparametric approach with parametric initial estimate of Conrad and Mammen (2008). An application to daily stock market returns suggests that the risk-return relation is indeed nonlinear. (C) 2011 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 312, "candidates": 162, "total": 474}, "attempt1_parsed": {"summary": "This paper introduces a new semiparametric estimator for an asset pricing model that incorporates a nonparametric risk-return tradeoff and GARCH-type volatility. The estimator is shown to be consistent, asymptotically normal, and efficient. Empirical results suggest a nonlinear risk-return relationship in stock market returns.", "criteria_hints": {"I1": true, "I2": false, "I3": false}, "notes": "The paper uses a semiparametric approach which is a form of machine learning. It goes beyond simple linear models by incorporating a nonparametric risk-return tradeoff. The application is to stock market returns, not government bonds or yield curves, and it does not focus on forecasting."}, "mode": "threadpool", "paper_id": "58ae8ef2afebef3f", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Restrictions on Risk Prices in Dynamic Term Structure Models\nAbstract: Restrictions on the risk-pricing in dynamic term structure models (DTSMs) tighten the link between cross-sectional and time-series variation of interest rates, and make absence of arbitrage useful for inference about expectations. This article presents a new econometric framework for estimation of affine Gaussian DTSMs under restrictions on risk prices, which addresses the issues of a large model space and of model uncertainty using a Bayesian approach. A simulation study demonstrates the good performance of the proposed method. Data for U.S.Treasury yields calls for tight restrictions on risk pricing: only level risk is priced, and only changes in the slope affect term premia. Incorporating the restrictions changes the model-implied short-rate expectations and term premia. Interest rate persistence is higher than in a maximally flexible model, hence expectations of future short rates are more variablerestrictions on risk prices help resolve the puzzle of implausibly stable short-rate expectations in this literature. Consistent with survey evidence and conventional macro wisdom, restricted models attribut … [truncated]", "attempt1_tokens": {"prompt": 390, "candidates": 246, "total": 636}, "attempt1_parsed": {"summary": "This article proposes a Bayesian econometric framework for estimating affine Gaussian dynamic term structure models (DTSMs) with restrictions on risk prices. The method addresses challenges related to a large model space and model uncertainty. Empirical analysis of U.S. Treasury yields suggests that only level risk is priced, and term premia are influenced by changes in the slope. These restrictions lead to higher interest rate persistence and more variable short-rate expectations compared to more flexible models, helping to resolve issues of implausibly stable short-rate expectations in existing literature. The restricted models attribute a significant portion of the long-term decline in interest rates to expectations of future nominal short rates, aligning with survey data and conventional macroeconomic understanding.", "criteria_hints": {"I1": false, "I2": true, "I3": true}, "notes": "The article uses a Bayesian approach for estimation, which is a form of statistical modeling but not typically classified as 'Machine Learning' in the context of empirical applications beyond simple linear models. The focus is on term structure models and interest rate expectations, which directly relates to government bonds and forecasting."}, "mode": "threadpool", "paper_id": "bdf7a90ab5ce3943", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: SIMULTANEOUS SPECIFICATION TESTING OF MEAN AND VARIANCE STRUCTURES IN NONLINEAR TIME SERIES REGRESSION\nAbstract: This paper proposes a nonparametric simultaneous test for parametric specification of the conditional mean and variance functions in a time series regression model. The test is based on an empirical likelihood (EL) statistic that measures the goodness of fit between the parametric estimates and the nonparametric kernel estimates of the mean and variance functions. A unique feature of the test is its ability to distribute natural weights automatically between the mean and the variance components of the goodness-of-fit measure. To reduce the dependence of the test on a single pair of smoothing bandwidths, we construct an adaptive test by maximizing a standardized version of the empirical likelihood test statistic over a set of smoothing bandwidths. The test procedure is based on a bootstrap calibration to the distribution of the empirical likelihood test statistic. We demonstrate that the empirical likelihood test is able to distinguish local alternatives that are different from the null hypothesis at an optimal rate.\n\nCriteria: {\"I1\": \"The article includes an empir … [truncated]", "attempt1_tokens": {"prompt": 341, "candidates": 193, "total": 534}, "attempt1_parsed": {"summary": "This paper introduces a nonparametric simultaneous test for parametric specification of conditional mean and variance functions in nonlinear time series regression. It utilizes an empirical likelihood (EL) statistic comparing parametric and nonparametric kernel estimates, with automatic weighting between mean and variance components. An adaptive test is developed by maximizing a standardized EL statistic over smoothing bandwidths, and bootstrap calibration is used for its distribution. The test can distinguish local alternatives from the null hypothesis at an optimal rate.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The paper discusses nonlinear time series regression and proposes a new testing methodology. While not a direct ML application in the sense of prediction models, the nonparametric kernel estimation and adaptive testing approach have ML characteristics. The paper also addresses the ability to distinguish local alternatives, which relates to forecasting/prediction."}, "mode": "threadpool", "paper_id": "32f440cd8991d935", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Short rate nonlinearities and regime switches\nAbstract: Using non-parametric estimation methods, various authors have shown distinct non-linearities in the drift and volatility function of the US short rate, which are inconsistent with standard affine term structure models. We document how a regime-switching model with state-dependent transition probabilities between regimes can replicate the patterns found by the non-parametric studies. To do so, we use data from the UK and Germany in addition to US data and include term spreads in some of our models. We also examine the drift and volatility function of the term spread. (C) 2002 Elsevier Science B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 280, "candidates": 181, "total": 461}, "attempt1_parsed": {"summary": "This paper uses a regime-switching model with state-dependent transition probabilities to replicate non-linearities in the US short rate's drift and volatility, as identified by non-parametric methods. The study extends its analysis to UK and German data, incorporating term spreads and examining their drift and volatility functions.", "criteria_hints": {"I1": true, "I2": true, "I3": false}, "notes": "The abstract mentions non-parametric estimation methods and regime-switching models, which can be considered beyond simple linear models and indicative of machine learning applications. The focus on short rates, term spreads, and yield curve dynamics clearly relates to government bonds and the yield curve. While the study aims to replicate patterns, it does not explicitly state a forecasting or prediction objective."}, "mode": "threadpool", "paper_id": "b426a797302dce5e", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Shrinking return forecasts\nAbstract: We develop a new approach that shrinks a given model forecast to the benchmark model forecast in order to improve forecasting performance. Simulation results show the superior performance of our approach, relative to popular methods such as forecast combination and the robustness to model misspecification. We apply our method to forecasting the returns on the S&P 500 index and find significant predictability when shrinking the principal component (PC) regression forecasts based on statistical and economic evaluation criteria. The forecast improvement from our shrinkage approach can be explained by the ability of its hyperparameters to be better predict real economic changes.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 267, "candidates": 143, "total": 410}, "attempt1_parsed": {"summary": "This paper introduces a novel method for improving forecast accuracy by shrinking a given model's forecast towards a benchmark model's forecast. The approach demonstrates superior performance in simulations compared to existing methods and is applied to forecasting S&P 500 returns, showing significant predictability. The improvement is attributed to hyperparameters better predicting economic changes.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The application to S&P 500 returns and the mention of principal component regression suggest a machine learning application beyond simple linear models."}, "mode": "threadpool", "paper_id": "f566ead501f33867", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Sovereign Default Forecasting in the Era of the COVID-19 Crisis\nAbstract: The COVID-19 crisis has revealed the economic vulnerability of various countries and, thus, has instigated the systematic exploration and forecasting of sovereign default risks. Multivariate statistical and stochastic process-based sovereign default risk forecasting has a 50-year developmental history. This article describes a continuous, non-homogeneous Markov chain method as the basis for a COVID-19-related sovereign default risk forecast model. It demonstrates the estimation of sovereign probabilities of default (PDs) over a five-year horizon period with the developed model reflecting the impact of the COVID-19 crisis. The COVID-19-adopted Markov model estimates PDs for most countries, including those that are advanced with AAA and AA ratings, to suggest that no sovereign nation's economy is secure from the financial impact of the COVID-19 pandemic. The dynamics of the estimated PDs are indicative of contemporary evidence as experienced in the recent financial crisis. The empirical results of this article have policy implications for foreign investors, sovereign lenders, export finance institutions, … [truncated]", "attempt1_tokens": {"prompt": 412, "candidates": 144, "total": 556}, "attempt1_parsed": {"summary": "This article proposes a continuous, non-homogeneous Markov chain model to forecast sovereign default risks, specifically incorporating the impact of the COVID-19 crisis. The model estimates probabilities of default over a five-year horizon, suggesting no nation is immune to the pandemic's financial effects. The findings have implications for various financial stakeholders and can aid in early risk recognition and mitigation.", "criteria_hints": {"I1": true, "I2": true, "I3": true}, "notes": "The Markov chain method is a form of machine learning. The article explicitly discusses sovereign default risk and forecasting."}, "mode": "threadpool", "paper_id": "b10f6a1cfc7ff655", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Specification tests for nonlinear dynamic models\nAbstract: We propose a new adequacy test and a graphical evaluation tool for nonlinear dynamic models. The proposed techniques can be applied in any set-up where parametric conditional distribution of the data is specified and, in particular, to models involving conditional volatility, conditional higher moments, conditional quantiles, asymmetry, Value at Risk models, duration models, diffusion models, etc. Compared to other tests, the new test properly controls the nonlinear dynamic behaviour in conditional distribution and does not rely on smoothing techniques that require a choice of several tuning parameters. The test is based on a new kind of multivariate empirical process of contemporaneous and lagged probability integral transforms. We establish weak convergence of the process under parameter uncertainty and local alternatives. We justify a parametric bootstrap approximation that accounts for parameter estimation effects often ignored in practice. Monte Carlo experiments show that the test has good finite-sample size and power properties. Using the new test and graphical tools, we check the adequacy of various popular h … [truncated]", "attempt1_tokens": {"prompt": 352, "candidates": 169, "total": 521}, "attempt1_parsed": {"summary": "This paper introduces a new adequacy test and graphical tool for nonlinear dynamic models, applicable to various parametric conditional distribution models including those with conditional volatility, quantiles, and Value at Risk. The test controls for nonlinear dynamics without smoothing, uses a multivariate empirical process, and is supported by bootstrap approximation and Monte Carlo simulations. It is applied to heteroscedastic models for stock index data.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The paper discusses nonlinear dynamic models and their adequacy testing, which can be considered a form of prediction or forecasting evaluation. The application to heteroscedastic models for stock index data suggests an empirical application beyond simple linear models."}, "mode": "threadpool", "paper_id": "ee3869d724e9f558", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Stochastic Gradient Descent in Continuous Time\nAbstract: Stochastic gradient descent in continuous time (SGDCT) provides a computationally efficient method for the statistical learning of continuous-time models, which are widely used in science, engineering, and finance. The SGDCT algorithm follows a (noisy) descent direction along a continuous stream of data. SGDCT performs an online parameter update in continuous time with the parameter updates theta(t) satisfying a stochastic differential equation. We prove that lim(t ->infinity) del(g) over bar(theta(t)) = 0, where (g) over bar is is a natural objective function for the estimation of the continuous-time dynamics. The convergence proof leverages ergodicity by using an appropriate Poisson equation to help describe the evolution of the parameters for large times. For certain continuous-time problems, SGDCT has some promising advantages compared to a traditional stochastic gradient descent algorithm. This paper mainly focuses on applications in finance, such as model estimation for stocks, bonds, interest rates, and financial derivatives. SGDCT can also be used for the optimization of high-dimensional continuous time models, … [truncated]", "attempt1_tokens": {"prompt": 404, "candidates": 203, "total": 607}, "attempt1_parsed": {"summary": "This paper introduces Stochastic Gradient Descent in Continuous Time (SGDCT), an efficient method for learning continuous-time models used in various fields like finance. SGDCT updates parameters in continuous time following a stochastic differential equation and is proven to converge. It offers advantages over traditional SGD for certain problems, particularly in finance for estimating models of stocks, bonds, interest rates, and derivatives. An example application combines SGDCT with deep neural networks for pricing high-dimensional American options.", "criteria_hints": {"I1": true, "I2": true, "I3": true}, "notes": "The paper mentions applications in finance including bonds and derivatives, and uses deep neural networks for option pricing, fulfilling I1. It explicitly mentions bonds and the yield curve (implied by interest rates and derivatives), fulfilling I2. The context of model estimation and option pricing implies forecasting/prediction, fulfilling I3."}, "mode": "threadpool", "paper_id": "ff04be79fa6dba3b", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Sovereign default risk premia: Evidence from the default swap market\nAbstract: This study explores the risk premia embedded in sovereign default swaps using a term structure model. The risk premia remunerate investors for unexpected changes in the default intensity. A number of interesting results emerge from the analysis. First, the risk premia contribution to spreads decreases over the sample, 2003-07, and rebounds at the start of the 'credit crunch.' Second, daily risk premia co-move with US macro variables and corporate default risk. Third, global factors explain most of Latin American countries' premia, and local factors best explain European and Asian premia. The importance of global factors grows over time. Finally, conditioning on lagged local and global variables at a weekly frequency, sovereign risk premia are highly predictable. (c) 2012 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 329, "candidates": 193, "total": 522}, "attempt1_parsed": {"summary": "This study analyzes sovereign default swap data from 2003-07 to understand risk premia. It finds that risk premia, which compensate investors for unexpected changes in default intensity, decreased over the sample and then increased during the credit crunch. Daily risk premia correlate with US macro variables and corporate default risk. Global factors significantly influence Latin American countries' premia, while local factors are more important for European and Asian countries, with the influence of global factors increasing over time. The study also demonstrates that sovereign risk premia are predictable using lagged local and global variables at a weekly frequency.", "criteria_hints": {"I1": false, "I2": true, "I3": true}, "notes": "The study uses a term structure model and analyzes predictability, but does not explicitly mention machine learning applications beyond simple linear models."}, "mode": "threadpool", "paper_id": "6d7c5e9a1bb1743e", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: THE VALUE OF MORTGAGE PREPAYMENT AND DEFAULT OPTIONS\nAbstract: We use an implicit alternating direction numerical procedure to estimate the value of a fixed-rate mortgage (FRM) with embedded default and prepayment options. The value of FRMs depends on interest rates, the house value, and mortgage maturity. Our numerical results suggest that the joint option value of prepayment and default is considerably high, even at loan origination. We extend the model to include prepayment penalties in FRM valuation. (C) 2009 Wiley Periodicals, Inc. Jrl Fut Mark 29:840-861, 2009\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 279, "candidates": 157, "total": 436}, "attempt1_parsed": {"summary": "This paper uses a numerical procedure to estimate the value of fixed-rate mortgages with embedded default and prepayment options, considering interest rates, house value, and mortgage maturity. The study finds that the combined option value of prepayment and default is significant even at loan origination and explores the impact of prepayment penalties.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The abstract mentions a numerical procedure, not an empirical machine learning application. The focus is on mortgage valuation, not government bonds or yield curves. While valuation can imply prediction, the abstract does not explicitly state forecasting or prediction as the primary goal."}, "mode": "threadpool", "paper_id": "b1aa1d6583414fbd", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Stock market risk and return: An equilibrium approach\nAbstract: Empirical evidence that expected stock returns are weakly related to volatility at the market level appears to contradict the intuition that risk and return are positively related. We investigate this issue in a general equilibrium exchange economy characterized by a regime-switching consumption process with time-varying transition probabilities between regimes. When estimated using consumption data, the model generates a complex, nonlinear and time-varying relation between expected returns and volatility, duplicating the salient features of the risk/return trade-off in the data. The results emphasize the importance of time-varying investment opportunities and highlight the perils of relying on intuition from static models.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 276, "candidates": 162, "total": 438}, "attempt1_parsed": {"summary": "This paper investigates the relationship between stock market risk and return using a general equilibrium model with a regime-switching consumption process. The model generates a complex, nonlinear, and time-varying relationship between expected returns and volatility, which aligns with empirical observations and challenges static model intuitions.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The abstract does not mention machine learning applications or empirical data analysis. It focuses on theoretical modeling. The topic is stock market risk and return, not government bonds. While the model aims to explain observed relationships, it is not explicitly about forecasting or prediction in the sense of predicting future market movements."}, "mode": "threadpool", "paper_id": "ab47e4e16b2d8b40", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Targeting Long Rates in a Model with Segmented Markets\nAbstract: This paper develops a model of segmented financial markets in which the net worth of financial institutions limits the degree of arbitrage across the term structure. The model is embedded into the canonical Dynamic New Keynesian (DNK) framework. We estimate the model using data on the term premium. Our principal results include the following. First, the estimated segmentation coefficient implies a nontrivial effect of central bank asset purchases on yields and real activity. Second, there are welfare gains to having the central bank respond to the term. premium, e.g., including the term premium in the Taylor Rule. Third, a policy that directly targets the term premium sterilizes the real economy from shocks originating in the financial sector. A term-premium peg can have sigmficant welfare effects. (ILL E12, E23, E31, E43, E52, E58)\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 336, "candidates": 157, "total": 493}, "attempt1_parsed": {"summary": "This paper presents a model of segmented financial markets where institutional net worth restricts arbitrage across the term structure. Embedded within the Dynamic New Keynesian (DNK) framework, the model is estimated using term premium data. Key findings suggest that central bank asset purchases significantly impact yields and economic activity, and that central bank policy should consider the term premium for welfare gains and to insulate the real economy from financial shocks.", "criteria_hints": {"I1": false, "I2": true, "I3": false}, "notes": "The paper uses an estimated model but does not explicitly mention machine learning applications beyond simple linear models. It focuses on policy implications rather than forecasting."}, "mode": "threadpool", "paper_id": "c3de53dd92c70087", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Structural break threshold VARs for predicting us recessions using the spread\nAbstract: This paper proposes a model to predict recessions that accounts for non-linearity and a structural break when the spread between long- and short-term interest rates is the leading indicator. Estimation and model selection procedures allow us to estimate and identify time-varying non-linearity in a VAR. The structural break threshold VAR (SBTVAR) predicts better the timing of recessions than models with constant threshold or with only a break. Using real-time data, the SBTVAR with spread as leading indicator is able to anticipate correctly the timing of the 2001 recession. Copyright (c) 2006 John Wiley & Sons, Ltd.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 294, "candidates": 158, "total": 452}, "attempt1_parsed": {"summary": "This paper introduces a Structural Break Threshold Vector Autoregression (SBTVAR) model to predict US recessions using the interest rate spread as a leading indicator. The model accounts for non-linearity and structural breaks, outperforming simpler models in predicting recession timing. It successfully anticipated the 2001 recession using real-time data.", "criteria_hints": {"I1": true, "I2": true, "I3": true}, "notes": "The model proposed is a non-linear VAR, which goes beyond simple linear models. The interest rate spread is directly related to the yield curve. The primary goal of the model is to predict recessions."}, "mode": "threadpool", "paper_id": "fdcb4bf37f076de6", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Technical trading rules and the size of the risk premium in security returns\nAbstract: Among analysts, technical trading rules are widely used for forecasting security returns. Recent literature provides evidence that these rules may provide positive profits after accounting for transaction costs. This would be contrary to the theory of the efficient market hypothesis which states that security prices cannot be forecasted from their past values or other past variables. This paper uses the daily Dow Jones Industrial Average Index from 1963 to 1988 to examine the linear and nonlinear predictability of stock market returns with simple technical trading rules, by using the nearest neighbors and the feedforward network regressions. Evidence of nonlinear predictability is found in the stock market returns by using the past returns and the buy and sell signals of the moving average rules.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 301, "candidates": 129, "total": 430}, "attempt1_parsed": {"summary": "This paper investigates the predictability of stock market returns using technical trading rules and machine learning techniques like nearest neighbors and feedforward network regressions. The study finds evidence of nonlinear predictability in the Dow Jones Industrial Average Index returns from 1963-1988, contradicting the efficient market hypothesis.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses machine learning techniques beyond simple linear models for empirical analysis and focuses on forecasting stock market returns."}, "mode": "threadpool", "paper_id": "80e3352b0bafafdc", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Stock return predictability in emerging markets: Does the choice of predictors and models matter across countries?\nAbstract: This study aims to examine return predictability in 24 emerging markets disaggregated in different regions. We propose four specifications, including a benchmark model. Then, an augmented model appropriate for each country, including a large set of potential factors, is evaluated. Furthermore, a dynamic multifactor model is investigated for all countries. Finally, we relax the symmetric hypothesis in asset return predictability based on a non-parametric non-linear approach: the projection pursuit regression model. Our study reveals three main findings. First, we reject all previous findings supporting a standard model of asset return predictability that is valuable for all countries, as we show that each country has specific domestic factors (both macroeconomic and financial) useful to predict future returns. Second, our empirical framework shows that asset return predictability might be robustly modelled based on non-linear specification based on the projection pursuit regression model. Our findings' explanatory power of out-of-sample estimations is e … [truncated]", "attempt1_tokens": {"prompt": 371, "candidates": 186, "total": 557}, "attempt1_parsed": {"summary": "This study investigates stock return predictability in 24 emerging markets, exploring the impact of predictor choice and model specification. It evaluates benchmark, augmented, and dynamic multifactor models, and introduces a non-parametric non-linear approach (projection pursuit regression). Key findings indicate that domestic factors are crucial for predicting returns in each country, and that non-linear specifications offer robust and economically relevant out-of-sample predictability. The results are relevant for investors and policymakers.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses projection pursuit regression, which is a non-linear machine learning technique, and explicitly tests for predictability, fulfilling I1 and I3. It focuses on stock returns, not bonds, so I2 is false."}, "mode": "threadpool", "paper_id": "6444a892b0e70ae6", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Strategy and tactics in public debt management\nAbstract: We examine the public debt management problem with respect to the maturity mix of new issues in a mean-variance framework. After identifying the main determinants of the long-run target (strategy), we focus on which interest rate conditions allow for a temporary deviation (tactics). The study is partly motivated by the apparent window of opportunity to issue more heavily at longer maturities given the recent historically low yields. We show that the room for long tactical positions on the long-term bond is actually narrower than predicted by rules of thumb based on Sharpe-like ratios. Once the model is augmented to embed real world features such as no price-taking and transaction costs, the scope for tactical position shrinks further. We discuss the model results and its implications in terms of the principal agent dilemma (government vs. debt manager); the paper also explores the financial stability implications arising from public debt issuance choices. All in all, our findings provide a rationale for the degree of caution often shown by many public debt managers in fulfilling their mandate. (C) 2015 Society for Poli … [truncated]", "attempt1_tokens": {"prompt": 378, "candidates": 205, "total": 583}, "attempt1_parsed": {"summary": "This paper analyzes public debt management, focusing on the maturity mix of new debt issues within a mean-variance framework. It distinguishes between long-run strategy (target maturity mix) and short-term tactics (deviations based on interest rate conditions). The study finds that the opportunity for tactical issuance at longer maturities, despite historically low yields, is narrower than commonly assumed, especially when considering real-world factors like non-price-taking behavior and transaction costs. The findings also touch upon principal-agent issues and financial stability implications, suggesting a rationale for the cautious approach often adopted by debt managers.", "criteria_hints": {"I1": false, "I2": true, "I3": false}, "notes": "The abstract does not mention machine learning or empirical applications beyond theoretical modeling. While it discusses interest rate conditions and potential deviations, it does not explicitly focus on forecasting or prediction as a primary goal."}, "mode": "threadpool", "paper_id": "86f133ebdc5eaa79", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Temperature effects on crop yields in heat index insurance\nAbstract: Heat can cause substantial yield losses in crop production and climate change is increasing the risk of this kind of damage. Weather index insurance can help to reduce the financial losses resulting from heat exposure. This paper introduces crop-specific payout functions based on restricted cubic splines in heat index insurance. The use of restricted cubic splines is a cutting-edge method to reflect empirically estimated temperature effects on crop yields and to estimate temperature-related yield losses. The integration of these temperature effects in payout functions facilitates insurance design and allows hourly temperatures to be used as the underlying index. An empirical analysis is used to assess heat stress effects for a panel of East German winter wheat and winter rapeseed producers, to calibrate insurance contracts accordingly and simulate the resulting risk reducing capacities. We find that the insurance scheme introduced here leads to statistically and economically significant out-of-sample risk reducing capacities for farmers, i.e. risk premiums are reduced by up to approximately 20% at the media … [truncated]", "attempt1_tokens": {"prompt": 392, "candidates": 163, "total": 555}, "attempt1_parsed": {"summary": "This paper introduces crop-specific payout functions for heat index insurance using restricted cubic splines to model temperature effects on crop yields. An empirical analysis on East German winter wheat and rapeseed producers demonstrates the insurance's risk-reducing capacity, potentially lowering risk premiums by up to 20%. The study suggests policy support for data collection to enhance market-based weather index insurance.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The use of restricted cubic splines is a novel method for modeling temperature effects, fitting the 'beyond simple linear models' aspect of I1. The paper forecasts risk reduction capacities, fitting I3."}, "mode": "threadpool", "paper_id": "f0b7502b7b81fe47", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Supply, demand, and risk premiums in electricity markets\nAbstract: We model the impact of supply and demand on risk premiums in electricity futures, using daily data for 2003-2014. The model provides a satisfactory fit and allows for unspanned economic risk not embedded in futures prices. Model-implied spot risk premiums and forward biases are large, negative, highly time-varying, and exhibit plausible seasonal patterns. They differ from existing models, especially in periods of market turmoil, have not decreased in size over time, and help predict future returns. Both demand and supply have an economically significant impact on risk premiums. The risk premium associated with supply is characterized by large positive outliers. (c) 2021 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 305, "candidates": 219, "total": 524}, "attempt1_parsed": {"summary": "This study models the impact of supply and demand on risk premiums in electricity futures using daily data from 2003-2014. The model, which fits the data well, reveals unspanned economic risk and shows that model-implied spot risk premiums and forward biases are large, negative, time-varying, and have seasonal patterns. These premiums differ from existing models, especially during market turmoil, have not diminished over time, and aid in predicting future returns. Both supply and demand significantly influence risk premiums, with supply-related risk premiums exhibiting large positive outliers.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The abstract mentions a model that provides a satisfactory fit and allows for unspanned economic risk, but it does not specify if it's a machine learning application or if it goes beyond simple linear models. It does explicitly state that the model helps predict future returns."}, "mode": "threadpool", "paper_id": "cd2aff9d49f38305", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Testing for two-regime threshold cointegration in vector error-correction models\nAbstract: This paper examines a two-regime vector error-correction model with a single cointegrating vector and a threshold effect in the error-correction term. We propose a relatively simple algorithm to obtain maximum likelihood estimation of the complete threshold cointegration model for the bivariate case. We propose a SupLM test for the presence of a threshold. We derive the null asymptotic distribution, show how to simulate asymptotic critical values, and present a bootstrap approximation. We investigate the performance of the test using Monte Carlo simulation, and find that the test works quite well. Applying our methods to the term structure model of interest rates, we find strong evidence for a threshold effect. (C) 2002 Published by Elsevier Science B.V.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 312, "candidates": 159, "total": 471}, "attempt1_parsed": {"summary": "This paper proposes a method for testing two-regime threshold cointegration in vector error-correction models, including an algorithm for maximum likelihood estimation and a SupLM test for the threshold effect. Monte Carlo simulations show the test performs well, and an application to the term structure of interest rates provides evidence for a threshold effect.", "criteria_hints": {"I1": false, "I2": true, "I3": false}, "notes": "The paper applies the proposed method to the term structure of interest rates, which relates to the yield curve. However, it does not explicitly mention Machine Learning applications beyond simple linear models, nor does it focus on forecasting or prediction."}, "mode": "threadpool", "paper_id": "3d256d8a11b4dd5d", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Term structure of risk under alternative econometric specifications\nAbstract: This paper characterizes the term structure of risk measures such as value at risk (VaR) and expected shortfall under different econometric approaches including multivariate regime switching, GARCH-in-mean models with Student-t errors, two-component GARCH models and a nonparametric bootstrap. We show how to derive the risk measures for each of these models and document large variations in term structures across econometric specifications. An out-of-sample forecasting experiment applied to stock, bond and cash portfolios suggests that the best model is asset- and horizon specific but that the bootstrap and regime switching model are best overall for VaR levels of 5% and 1%, respectively. (c) 2005 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 304, "candidates": 179, "total": 483}, "attempt1_parsed": {"summary": "This paper compares different econometric models (multivariate regime switching, GARCH-in-mean with Student-t errors, two-component GARCH, and nonparametric bootstrap) for estimating risk measures like VaR and expected shortfall. It highlights significant differences in term structures across these models and finds that the best model for out-of-sample forecasting is asset- and horizon-specific, with the bootstrap and regime switching models performing best overall for VaR at 5% and 1% levels, respectively.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The paper uses various econometric models, some of which can be considered advanced beyond simple linear models. It explicitly mentions out-of-sample forecasting."}, "mode": "threadpool", "paper_id": "187c6ade28f04d09", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: The Macroeconomics of Financial Speculation\nAbstract: I review the literature on financial speculation driven by belief disagreements from a macroeconomics perspective. To highlight unifying themes, I develop a stylized macroeconomic model that embeds several mechanisms. With short-selling constraints, speculation can generate overvaluation and speculative bubbles. Leverage can substantially inflate speculative bubbles, and leverage limits depend on perceived downside risks. Shifts in beliefs about downside tail scenarios can explain the emergence and the collapse of leveraged speculative bubbles. Speculative bubbles are related to rational bubbles, but they match better the empirical evidence on the predictability of asset returns. Even without short-selling constraints, speculation induces procyclical asset valuation. When speculation affects the price of aggregate assets, it also influences macroeconomic outcomes such as aggregate consumption, investment, and output. Speculation in the boom years reduces asset prices, aggregate demand, and output in the subsequent recession. Macroprudential policies that restrict speculation in the boom can improve macroeconomic stability … [truncated]", "attempt1_tokens": {"prompt": 339, "candidates": 160, "total": 499}, "attempt1_parsed": {"summary": "This paper reviews the literature on financial speculation driven by belief disagreements from a macroeconomic perspective. It presents a model where short-selling constraints and leverage can lead to speculative bubbles, affecting macroeconomic outcomes like consumption, investment, and output. The paper suggests that macroprudential policies can enhance stability by restricting speculation during boom periods.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The paper discusses prediction of asset returns and macroeconomic outcomes, fitting I3. However, it does not include an empirical ML application (I1) nor is it specifically about government/sovereign bonds or the yield curve (I2)."}, "mode": "threadpool", "paper_id": "0f494f7b776c3aa9", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Testing for UIP-Type Relationships: Nonlinearities, Monetary Announcements and Interest Rate Expectations\nAbstract: This paper tests for UIP-type relationships by estimating first a benchmark linear Cointegrated VAR including the nominal exchange rate and the interest rate differential as well as central bank announcements, and then a Smooth Transition Cointegrated VAR (STCVAR) model incorporating nonlinearities and also taking into account the role of interest rate expectations. The analysis is conducted for five inflation targeting countries (the UK, Canada, Australia, New Zealand and Sweden) and three non-targeters (the US, the Euro-Area and Switzerland) using daily data from January 2000 to December 2020. While we cannot confirm the validity of UIP in its strictest theoretical sense, we find evidence for the existence of an equilibrium relationship between the exchange rate and the interest rate differential. Specifically, the nonlinear framework appears to be more appropriate to capture the adjustment towards the long-run equilibrium, since the estimated speed of adjustment is substantially faster and the short-run dynamic linkages more significant. Further, interest ra … [truncated]", "attempt1_tokens": {"prompt": 462, "candidates": 204, "total": 666}, "attempt1_parsed": {"summary": "This paper investigates the UIP-type relationship between exchange rates and interest rate differentials using linear and nonlinear (STCVAR) models for several inflation targeting and non-targeting countries. It finds that nonlinearities and interest rate expectations are crucial for capturing the equilibrium relationship, with faster adjustments and more significant short-run dynamics in the nonlinear framework. Central bank credibility, particularly in inflation targeting countries, enhances this relationship and the impact of announcements.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses a nonlinear model (STCVAR) which goes beyond simple linear models, fulfilling I1. It focuses on exchange rates and interest rate differentials, not directly on government bonds or the yield curve, thus not fulfilling I2. The paper discusses adjustment speeds and short-run dynamics, implying a predictive element, thus fulfilling I3."}, "mode": "threadpool", "paper_id": "5b36dfbcd718497b", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: The Price of Predictability: Estimating Inconsistency Premiums in Social Interactions\nAbstract: For financial decision-making, people trade off the expected value (return) and the variance (risk) of an option, preferring higher returns to lower ones and lower risks to higher ones. To make decision-makers indifferent between a risky and risk-free option, the expected value of the risky option must exceed the value of the risk-free option by a certain amount-the risk premium. Previous psychological research suggests that similar to risk aversion, people dislike inconsistency in an interaction partner's behavior. In eight experiments (total N = 2,412) we pitted this inconsistency aversion against the expected returns from interacting with an inconsistent partner. We identified the additional expected return of interacting with an inconsistent partner that must be granted to make decision-makers prefer a more profitable, but inconsistent partner to a consistent, but less profitable one. We locate this inconsistency premium at around 31% of the expected value of the risk-free option.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theor … [truncated]", "attempt1_tokens": {"prompt": 354, "candidates": 178, "total": 532}, "attempt1_parsed": {"summary": "This study investigates inconsistency aversion in social interactions, analogous to risk aversion in financial decision-making. Through eight experiments with 2,412 participants, the researchers quantified the 'inconsistency premium' – the additional expected return needed to make individuals indifferent between interacting with a profitable but inconsistent partner and a consistent but less profitable one. The findings suggest this premium is approximately 31% of the expected value of the risk-free option.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The study focuses on behavioral economics and decision-making, not machine learning applications. While it deals with 'predictability' and 'expected value', it does not directly concern government bonds or the yield curve."}, "mode": "threadpool", "paper_id": "f4423359be40fb95", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: The Markov-switching jump diffusion LIBOR market model\nAbstract: In this paper, we introduce an extension to the LIBOR Market Model (LMM) that is suitable to incorporate both sudden market shocks as well as changes in the overall economic climate into the interest rate dynamics. This is achieved by substituting the simple diffusion process of the original LMM by a regime-switching jump diffusion. We demonstrate that the new Markov-switching jump diffusion (MSJD) LMM can be embedded into a generalized regime-switching Heath-Jarrow-Morton model and prove that the considered market is arbitrage-free. We derive pricing formulas for caps, floors and swaptions using Fourier pricing techniques and show how the model can be calibrated to real market data.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 296, "candidates": 166, "total": 462}, "attempt1_parsed": {"summary": "This paper extends the LIBOR Market Model (LMM) by incorporating a regime-switching jump diffusion process to capture market shocks and economic climate changes. The proposed Markov-switching jump diffusion (MSJD) LMM is shown to be embeddable in a generalized Heath-Jarrow-Morton model, ensuring an arbitrage-free market. The authors derive pricing formulas for caps, floors, and swaptions using Fourier techniques and demonstrate model calibration to market data.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The paper focuses on financial modeling and pricing derivatives, not on empirical machine learning applications, government bonds, or forecasting."}, "mode": "threadpool", "paper_id": "6e5ee856b4eecc9e", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Testing predictability and nonlinear dependence in the Indian stock market\nAbstract: This paper suggests a systematic approach to studying predictability and nonlinear dependence in the context of the Indian stock market, one of the most important emerging stock markets in the world. The proposed approach considers nonlinear dependence in returns and envisages appropriate specification of both the conditional first- and second-order moments, so that final conclusions are free from any probable statistical consequences of misspecification. To this end, a number of rigorous tests are applied on the returns, based on four major daily indices of the Indian stock market. It is found that the Indian stock market is predictable, and this observed lack of efficiency is due to serial correlation, nonlinear dependence, day-of-the week effects, parameter instability, conditional heteroskedasticity (GARCH), daily-level seasonality in volatility, the short-term interest rate (in some subperiods of some indices), and some dynamics in the higher-order moments.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear m … [truncated]", "attempt1_tokens": {"prompt": 339, "candidates": 171, "total": 510}, "attempt1_parsed": {"summary": "This paper investigates predictability and nonlinear dependence in the Indian stock market using a systematic approach that accounts for nonlinearities in returns and specifies conditional moments. Applying rigorous tests to daily index returns, the study finds the market to be predictable due to factors like serial correlation, nonlinear dependence, day-of-the-week effects, parameter instability, GARCH, seasonality in volatility, interest rates, and higher-order moment dynamics.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The paper mentions \"nonlinear dependence\" and \"GARCH\", which suggests machine learning applications beyond simple linear models. The core of the paper is about \"predictability\" and \"forecasting\"."}, "mode": "threadpool", "paper_id": "00b59708a2208b21", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: The advantages of CBOE credit VIXs for corporate bond investors in North America: A sectoral analysis\nAbstract: This paper examines the safe-haven role of the recently introduced CBOE credit VIXs for investment-grade and high-yield corporate bonds, at both aggregate and sectoral levels. Using a time-varying quantile-based framework and daily data from June 5, 2014 to December 10, 2023, the safe-haven role of credit VIX is confirmed irrespective of bond sector. The safe-haven property of credit VIX is pronounced for high-yield bonds which embed a high credit risk-premium. This result stands when taking into account interest rate volatility, as measured by the MOVE index. A time-varying analysis shows the persistence of credit VIX as a safe-haven for all bond sectors after the COVID-19 pandemic and during a high US interest-rate regime. Corporate bond investors and traders can use these findings to refine their investment and trading decisions and offset credit risk during both normal and turbulent periods.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about gove … [truncated]", "attempt1_tokens": {"prompt": 365, "candidates": 188, "total": 553}, "attempt1_parsed": {"summary": "This paper investigates the safe-haven properties of CBOE credit VIXs for corporate bonds in North America, finding they act as a safe haven for both investment-grade and high-yield bonds across sectors. The safe-haven effect is stronger for high-yield bonds and persists even when considering interest rate volatility and post-COVID-19 periods. The findings can aid investors in managing credit risk.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The study uses a time-varying quantile-based framework, which is a statistical method rather than a machine learning application. The focus is on corporate bonds, not government/sovereign bonds. While the findings can inform investment decisions, the primary focus is not on forecasting or prediction."}, "mode": "threadpool", "paper_id": "85e5ea3195695843", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: The Pricing of Time-Varying Exchange Rate Risk in the Stock Market: A Nonparametric Approach\nAbstract: This paper reexamines the pricing of exchange rate risk in the U.S. stock market. We first construct stock portfolios based on the Foreign Exchange Income (FEI), a measure of currency exposure of firms, reported in their annual reports. We then develop two-factor and multi-factor nonparametric models that allow time variation in risk exposure and risk premium, and nonlinearity in the return generating process. When we assume that risk exposure can be time-varying but risk premium is constant, the estimated premium for exchange rate risk is significant only for the most positive FEI-ranked portfolio and marginally significant for the most negative FEI-ranked portfolio. When we further assume that both risk exposure and risk premium can be time-varying, results suggest that exchange rate risk is significantly priced for all the FEI-ranked portfolios except the one with little exposure.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bond … [truncated]", "attempt1_tokens": {"prompt": 336, "candidates": 184, "total": 520}, "attempt1_parsed": {"summary": "This paper investigates the pricing of exchange rate risk in the U.S. stock market using a nonparametric approach. It constructs stock portfolios based on Foreign Exchange Income (FEI) and develops time-varying multi-factor models to account for nonlinearities and time variations in risk exposure and premium. The study finds that exchange rate risk is priced, particularly for portfolios with significant currency exposure.", "criteria_hints": {"I1": true, "I2": false, "I3": false}, "notes": "The study uses nonparametric models which go beyond simple linear models, fitting criteria I1. It focuses on stock market and exchange rate risk, not government bonds or yield curves (I2). While it estimates risk premiums, the primary focus is not forecasting or prediction in the typical sense (I3)."}, "mode": "threadpool", "paper_id": "dd8b9653412c131a", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: The Relationship Between Volatility and Sovereign Credit Risk in the Emerging Markets: A Nonlinear ARDL Approach\nAbstract: This study investigates the short- and long-run nexus between the volatility index of VIX and sovereign credit risk represented by CDS spread in emerging markets, namely Turkey, China, Russia, Brazil, and Mexico. The emerging markets are at the center of investors' interest due to high return opportunities. The relationship between volatility and sovereign credit risk has been studied many times via linear models. However, financial series exhibit asymmetric dynamics, as volatility clustering, excess kurtosis, and others. Thus, we use nonlinear autoregressive distributed lags (NARDL) analysis to capture nonlinear relations between the volatility and the sovereign credit risks of these countries by using daily data from 04.01.2010 to 29.11.2019. The bounds test of the NARDL model confirms the cointegration between VIX and CDS spreads of the countries under study. The analysis of estimated NARDL parameters shows that negative shocks of the volatility index have a long-lasting impact on CDS spreads. Chinese CDS spread are more sensitive to VIX index changes … [truncated]", "attempt1_tokens": {"prompt": 457, "candidates": 218, "total": 675}, "attempt1_parsed": {"summary": "This study examines the short- and long-run relationship between the VIX volatility index and sovereign credit risk (CDS spread) in emerging markets (Turkey, China, Russia, Brazil, Mexico) using a nonlinear autoregressive distributed lags (NARDL) approach. The NARDL model captures asymmetric dynamics, revealing that negative VIX shocks have a lasting impact on CDS spreads. The study also highlights country-specific short-run sensitivities and asymmetric effects of volatility changes on CDS spreads.", "criteria_hints": {"I1": true, "I2": true, "I3": false}, "notes": "The study uses NARDL, which is a statistical modeling technique that can capture nonlinear relationships, and it goes beyond simple linear models. The study focuses on sovereign credit risk (CDS spread), which is directly related to sovereign bonds. While the study analyzes the impact of volatility on credit risk, it does not explicitly focus on forecasting or prediction as its primary objective."}, "mode": "threadpool", "paper_id": "6baa570f6471f39a", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: The VIX Premium\nAbstract: Ex ante estimates of the volatility premium embedded in VIX futures, known as the VIX premium, fall or stay flat when ex ante measures of risk rise. This is not an artifact of mismeasurement: (i) ex ante premiums reliably predict ex post returns to VIX futures with a coefficient near one, and (ii) falling ex ante premiums predict increasing ex post market and investment risk, creating profitable trading opportunities. Falling hedging demand helps explain this behavior, as premiums and trader exposures tend to fall together when risk rises. These facts provide a puzzle for theories of why investors hedge volatility. Received January 13, 2017; editorial decision April 26, 2018 by Editor Stijn Van Nieuwerburgh. Authors have furnished an Internet Appendix, which is available on the Oxford University Press Web site next to the link to the final published paper online.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 332, "candidates": 209, "total": 541}, "attempt1_parsed": {"summary": "This paper examines the VIX premium, which represents the ex ante estimate of the volatility premium in VIX futures. It finds that this premium declines when risk measures increase, which is not due to measurement errors. The study shows that falling VIX premiums predict future market and investment risk, offering trading opportunities. Reduced hedging demand is suggested as a reason for this phenomenon, as premiums and trader exposures decrease together during periods of heightened risk. This behavior presents a challenge to existing theories on why investors hedge volatility.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The abstract mentions predicting ex post returns and increasing ex post market and investment risk, which aligns with I3. However, it does not mention any empirical Machine Learning applications beyond simple linear models (I1) or focus on government/sovereign bonds and/or the yield curve (I2)."}, "mode": "threadpool", "paper_id": "2d7d94f5cc68cb41", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: The dynamic relationship between the prices of ADRs and their underlying stocks: evidence from the threshold vector error correction model\nAbstract: This paper sets out to estimate the dynamic relationship that exists between the prices of ADRs and their underlying stocks, in both the short run and the long run, using a number of recent developments of the threshold cointegration framework. The empirical results support the notion of nonlinear mean reversion of the prices of ADRs and their underlying stocks.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 238, "candidates": 110, "total": 348}, "attempt1_parsed": {"summary": "This paper investigates the dynamic relationship between ADR prices and their underlying stocks using a threshold vector error correction model, finding evidence of nonlinear mean reversion.", "criteria_hints": {"I1": true, "I2": false, "I3": false}, "notes": "The use of a 'threshold vector error correction model' suggests a model beyond simple linear regression, potentially fitting the 'beyond simple linear models' aspect of I1."}, "mode": "threadpool", "paper_id": "1254037e95e027a2", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: The determinants of capitalization rates: evidence from the US real estate markets\nAbstract: Purpose Establishing the strength of a novel variable-mortgage debt as a fraction of US gross domestic product (GDP)-on forecasting capitalization rates in both the US office and multifamily sectors. Design/methodology/approach The authors specifies a vector error correction model (VECM) to the data. VECM are used to address the nonstationarity issues of financial variables while maintaining the information embedded in the levels of the data, as opposed to their differences. The cap rate series used are from Green Street Advisors and represent transaction cap rates which avoids the problem of artificial smoothness found in appraisal-based cap rates. Findings Using a VECM specified with the novel variable, unemployment and past cap rates contains enough information to produce more robust forecasts than the traditional variables (return expectations and risk premiums). The method is robust both in and out of sample. Practical implications This has direct implications for governmental policy, offering a path to real estate price stability and growth through mortgage access-functions lar … [truncated]", "attempt1_tokens": {"prompt": 478, "candidates": 175, "total": 653}, "attempt1_parsed": {"summary": "This study investigates the impact of mortgage debt as a fraction of US GDP on forecasting capitalization rates in the US office and multifamily sectors. Using a vector error correction model (VECM), the research finds that this novel variable, along with unemployment and past cap rates, provides more robust forecasts than traditional variables like return expectations and risk premiums. The findings have implications for governmental policy aimed at real estate price stability and offer an alternative to interest rate-based forecasting models.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The study uses a VECM, which is a statistical model, not a machine learning application. It does not focus on government/sovereign bonds or the yield curve."}, "mode": "threadpool", "paper_id": "f82cbd04c25cf4f6", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: The determinants of main stock exchange index changes in emerging countries: evidence from Turkey in COVID-19 pandemic age\nAbstract: With the emergence and spreading of COVID-19 pandemic all over the world, the uncertainty has been increasing for countries. Depending on this condition, especially emerging countries have been affected negatively by foreign portfolio investment outflows from stock exchanges, and main stock exchange indices have been collapsed. The study examines the causes of the main stock exchange index changes in Turkey in the COVID-19 period. In this context, 14 variables (3 global, 6 country-level, 5 market-level) are analyzed by employing random forest and support vector machine algorithms and using daily data between 01.02.2020 and 05.15.2020, which includes the pre-pandemic and the pandemic periods. The findings prove that (i) the most important variables are the retention amount of foreign investors in the equity market, credit default swap spreads, government bonds interest rates, Morgan Stanley Capital International (MSCI) emerging markets index, and volatility index in the pre-pandemic period; (ii) the importance of variables changes as MSCI emergi … [truncated]", "attempt1_tokens": {"prompt": 454, "candidates": 184, "total": 638}, "attempt1_parsed": {"summary": "This study investigates the determinants of changes in Turkey's main stock exchange index during the COVID-19 pandemic using daily data from February to May 2020. It employs Random Forest and Support Vector Machine algorithms to analyze 14 variables, including global, country-level, and market-level factors. The findings highlight the varying importance of factors like foreign investor retention, credit default swap spreads, government bond interest rates, and market indices in pre-pandemic versus pandemic periods, with SVM showing superior accuracy.", "criteria_hints": {"I1": true, "I2": true, "I3": true}, "notes": "The study uses machine learning algorithms (Random Forest, SVM) and analyzes government bond interest rates, fitting criteria I1, I2, and I3."}, "mode": "threadpool", "paper_id": "3a2fe3ca49a5826d", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: The composition of CMBS risk\nAbstract: This paper identifies the put-option, liquidity availability proportion, and shadow liquidity risk premia embedded within commercial mortgage backed securities (CMBS) using reduced form and structural generalization models. These risk values are then interpreted as trading signals which are tested with automated trading strategies that buy undervalued and sell overvalued CMBS from November 2007 through June 2015. All three signals generate substantial positive trading profits in testing for the reduced form model but not for the structural generalization. The risk signals constructed independently of market pricing provide more profitable automated trading insights than those constructed from interactions between modeled risk measures and market spreads. In my tests of the information content of the risk signals with respect to future macroeconomic indicators, I find statistically significant evidence in keeping with recent studies. While I cannot reject CMBS efficiency, this paper's disclosure of new risk measures, the profitability of automated strategies based on those risk measures, and the statistical significance of their forward  … [truncated]", "attempt1_tokens": {"prompt": 381, "candidates": 218, "total": 599}, "attempt1_parsed": {"summary": "This paper develops and tests three risk measures for Commercial Mortgage-Backed Securities (CMBS): put-option, liquidity availability proportion, and shadow liquidity risk premia. These measures are used as trading signals in automated strategies. The reduced form model's signals were profitable, while the structural generalization model's were not. The risk signals, independent of market pricing, outperformed those derived from market interactions. The signals also showed predictive power for future macroeconomic indicators, contributing to the understanding of CMBS risk and the credit spread puzzle.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The paper uses \"automated trading strategies\" which implies an empirical application beyond pure theory. The models used (reduced form and structural generalization) are not explicitly defined as ML, but the application of trading signals and testing suggests empirical analysis. The paper also explicitly mentions \"forward guidance capabilities\" and \"forecasting\" macroeconomic indicators."}, "mode": "threadpool", "paper_id": "e1060b1fe4f87af2", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: The impact of contagion effects of media reports, investors' sentiment and attention on the stock market based on HAR-RV model\nAbstract: In this paper, the Shanghai Securities Composite Index and 18 A-share listed companies are used to justify the impact of contagion effects of media reports, investors' sentiment and attention on stock market. Five indicators are built: The news media optimistic tendency, investors' attention, investors' sentiment, investors' sentiment disagreement and media sentiment disagreement. Furthermore, theoretical models are constructed based on HAR-RV model to analyze the contagion structure between media sentiment and investors' sentiment and its impact on the performance of stock market. Additionally, the reverse silence spiral theory is proposed to analyze the regulatory role of sentiment disagreement in the contagion effects according to the information communication theory. The empirical results demonstrate the following conclusions. (1) The optimism degree of media reports positively affects investors' subjective sentiment and increases their transaction volume. (2) Strengthening investors' attention to corporate-related information is the ma … [truncated]", "attempt1_tokens": {"prompt": 418, "candidates": 174, "total": 592}, "attempt1_parsed": {"summary": "This study investigates the influence of media reports, investor sentiment, and attention on the stock market using the HAR-RV model. It constructs indicators for media optimism, investor attention, sentiment, and disagreement. The findings suggest that media optimism boosts investor sentiment and trading volume, with investor attention acting as a key mediator. Media sentiment indirectly impacts stock returns and volatility via investor sentiment and trading. Sentiment disagreement, both in media and among investors, mitigates these contagion effects.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses the HAR-RV model, which is an empirical model, and discusses forecasting stock market performance. It does not focus on government bonds or yield curves."}, "mode": "threadpool", "paper_id": "19ee415ed7fb651b", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: The impact of serial correlation on testing for structural change in binary choice model: Monte Carlo evidence\nAbstract: This paper examines the finite sample properties of structural change tests with an unknown breakpoint for the probit model in the presence of serial correlation. The combination of structural change and serial correlation renders model estimation challenging, affecting the consistency of coefficient estimates. Although there is vast literature concerning structural change tests for linear time series models, the literature for such tests in the context of binary choice models is somewhat sparse. More importantly, the empirical literature has applied the standard tests of structural change on the discrete choice model, despite the fact that most of these tests were developed specifically for the linear regression model. Subsequently, the theoretical properties of these tests in the context of non-linear models are unknown. This includes the class of discrete choice models, such as probit and logit. The issue becomes even more complicated in the presence of serial correlation, since typical tests for structural change often require the assumption of indepen … [truncated]", "attempt1_tokens": {"prompt": 516, "candidates": 163, "total": 679}, "attempt1_parsed": {"summary": "This paper investigates the performance of structural change tests in probit models with serial correlation, finding that standard tests designed for linear models perform poorly and exhibit significant size distortion in non-linear contexts. Bootstrapping offers some improvement but cannot fully address the issues caused by high serial correlation.", "criteria_hints": {"I1": true, "I2": false, "I3": false}, "notes": "The paper discusses binary choice models (probit) and their non-linear nature, which aligns with 'beyond simple linear models'. It focuses on testing methodologies rather than direct forecasting, hence I3 is false. There is no mention of government bonds or yield curves, making I2 false."}, "mode": "threadpool", "paper_id": "ae0436eaafdd494a", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: The implied volatility smirk of commodity options\nAbstract: This paper studies the implied volatility (IV) smirks in four commodity markets by adopting Zhang and Xiang's methodology. First, we document the term structure and dynamics of IV smirks. Overall, the commodity IV curves are negatively skewed with a positive curvature. Then we analyze the commodity and S&P 500 returns' predictability based on in-sample and out-of-sample tests and find that the information embedded in IV smirks can significantly predict monthly commodity and S&P 500 returns. For example, the risk-neutral fourth cumulant (FC) from the crude oil market outperforms all of the standard predictors in predicting the S&P 500 returns.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 300, "candidates": 155, "total": 455}, "attempt1_parsed": {"summary": "This paper analyzes implied volatility (IV) smirks in four commodity markets, finding negatively skewed IV curves with positive curvature. It demonstrates that information from IV smirks can predict monthly returns in commodity and S&P 500 markets, with the risk-neutral fourth cumulant from crude oil showing superior predictive power for S&P 500 returns.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The paper uses statistical methods (Zhang and Xiang's methodology) and tests for predictability, but does not appear to employ machine learning beyond simple linear models."}, "mode": "threadpool", "paper_id": "7fa232dd8733f7ab", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: The impact of variability and correlation of selected geological parameters on the economic assessment of bituminous coal deposits with use of non parametric bootstrap and copula-based Monte Carlo simulation\nAbstract: This paper presents an assessment of the impact of variability and interdependencies of selected deposit parameters on the net present value (NPV) and internal rate of return (IRR). The subjects of the analyses were three economically viable seams at one of the bituminous coal deposits in Poland. The source of information was the geological model and operational data of the mine X. The simulation was developed based on non-parametric bootstrapping, where the influence of coal quality parameters, seam thickness, spatial density of coal, and waste rock derived from coal partings, floor cutting and dinting, and roof falls, was tested.The interdependencies of geological and mining parameters were replicated in a simulation model using Gaussian and empirical copulas. In the model, the relationship between the amount of total waste rock and operating costs was associated with the use of elaborate mathematical formulas. Economic appraisal was based on an income approa … [truncated]", "attempt1_tokens": {"prompt": 580, "candidates": 212, "total": 792}, "attempt1_parsed": {"summary": "This study assesses how variability and interdependencies of geological parameters (coal quality, seam thickness, density, waste rock) affect the economic viability (NPV, IRR) of bituminous coal deposits. Using non-parametric bootstrapping and copula-based Monte Carlo simulations on data from a Polish mine, the research quantifies the impact of these factors on financial metrics. Results show that correlations can lead to significant differences in NPV and IRR, with empirical copulas showing less variation than Gaussian copulas. The study suggests the additional risk from parameter volatility and correlation is relatively low and that the amount of geological information is of secondary importance for valuation.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses Monte Carlo simulation and copulas, which can be considered advanced statistical modeling techniques akin to machine learning applications in this context. It focuses on forecasting economic outcomes based on geological parameters."}, "mode": "threadpool", "paper_id": "e0d0deaef6ebe9c8", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: The performance of variance ratio unit root tests under nonlinear stationary TAR and STAR processes: Evidence from Monte Carlo simulations and applications\nAbstract: This paper investigates the performance of variance ratio unit root tests under nonlinear stationary three-regime threshold autoregressive (TAR) and smooth transition autoregressive (STAR) processes that are significant for some economic theories and variables. Variance ratio unit root tests are effective tools in empirical analysis because they can theoretically consider broad classes of nonlinear stationary processes under the null or alternative hypothesis. Nevertheless, our Monte Carlo simulations demonstrate that these tests perform poorly (with severe size distortions or low power) under stationary TAR and STAR processes. To verify our Monte Carlo results, we apply these tests to yield spreads such as the TAR and STAR processes.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 294, "candidates": 180, "total": 474}, "attempt1_parsed": {"summary": "This paper evaluates the effectiveness of variance ratio unit root tests when applied to nonlinear stationary TAR and STAR processes, which are relevant in economic contexts. Through Monte Carlo simulations, the study finds that these tests exhibit poor performance, characterized by significant size distortions or low power, when applied to stationary TAR and STAR processes. The findings are further validated by applying the tests to yield spreads, which are modeled as TAR and STAR processes.", "criteria_hints": {"I1": true, "I2": true, "I3": false}, "notes": "The study uses TAR and STAR models, which are nonlinear models. It also applies the tests to yield spreads, which relates to the yield curve. However, the primary focus is on evaluating the performance of unit root tests, not on forecasting."}, "mode": "threadpool", "paper_id": "af84a9d1919ec35b", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: The informational content of the embedded deflation option in TIPS\nAbstract: We estimate the value of the embedded option in U.S. Treasury Inflation-Protected Securities (TIPS). The embedded option value exhibits time variation that is correlated with periods of deflationary expectations. We construct embedded option explanatory variables that are statistically and economically significant for explaining future inflation, even in the presence of traditional inflation variables such as lagged inflation, the gold return, the crude oil return, the VIX return, liquidity, surveys, and the yield spread between nominal Treasuries and TIPS. After conducting robustness tests, we conclude that the TIPS embedded option contains useful information for future inflation. (C) 2016 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 293, "candidates": 139, "total": 432}, "attempt1_parsed": {"summary": "This study estimates the value of the embedded option in U.S. Treasury Inflation-Protected Securities (TIPS), finding that its value varies with deflationary expectations. The authors demonstrate that this option provides significant explanatory power for future inflation, even when controlling for traditional inflation predictors. Robustness tests confirm the informational content of the TIPS embedded option for predicting future inflation.", "criteria_hints": {"I1": false, "I2": true, "I3": true}, "notes": "The study uses statistical models but does not explicitly mention machine learning applications beyond simple linear models."}, "mode": "threadpool", "paper_id": "73a39603932c7788", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: The information content of option-implied volatility for credit default swap valuation\nAbstract: Credit default swaps (CDS) are similar to out-of-the-money put options in that both offer a low cost and effective protection against downside risk. This study investigates whether put option-implied volatility is an important determinant of CDS spreads. Using a large sample of firms with both CDS and options data, we find that individual firms put option-implied volatility dominates historical volatility in explaining the time-series variation in CDS spreads. To understand this result, we show that implied volatility is a more efficient forecast for future realized volatility than historical volatility. More importantly, the volatility risk premium embedded in option prices covaries with the CDS spread. These findings complement existing empirical evidence based on market-level data. (C) 2010 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecast … [truncated]", "attempt1_tokens": {"prompt": 322, "candidates": 155, "total": 477}, "attempt1_parsed": {"summary": "This study examines the relationship between option-implied volatility and Credit Default Swap (CDS) spreads. It finds that implied volatility is a significant predictor of CDS spreads, outperforming historical volatility. The research suggests that the volatility risk premium in option prices is linked to CDS spreads, providing empirical evidence for this connection.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The study uses empirical data but does not appear to employ machine learning techniques beyond simple linear models. It focuses on corporate credit risk, not sovereign bonds. The core of the study is about forecasting the variation in CDS spreads."}, "mode": "threadpool", "paper_id": "2c0b739e8ae2211a", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: The price of power: The valuation of power and weather derivatives\nAbstract: Pricing contingent claims on power presents numerous challenges due to (1) the unique behavior of power prices, and (2) time-dependent variations in prices. We propose and implement a model in which the spot price of power is a function of two state variables: demand (load) and fuel price. in this model, any power derivative price must satisfy a PDE with boundary conditions that reflect capacity limits and the non-linear relation between load and the spot price of power. Moreover, since power is non-storable and demand is not a traded asset, the power derivative price embeds a market price of risk. Using inverse problem techniques and power forward prices from the PJM market, we solve for this market price of risk function. During 1999-2001, the upward bias in the forward price was as large as $50/MWh for some days in July. By 2005, the largest estimated upward bias had fallen to $19/MWh. These large biases are plausibly due to the extreme right skewness of power prices: this induces left skewness in the payoff to short forward positions, and a large risk premium is required to induce traders to sel … [truncated]", "attempt1_tokens": {"prompt": 442, "candidates": 250, "total": 692}, "attempt1_parsed": {"summary": "This paper proposes a model for pricing power derivatives, considering the unique behavior of power prices and time-dependent variations. It incorporates demand (load) and fuel price as state variables, leading to a PDE with boundary conditions reflecting capacity limits and non-linear relationships. The model also accounts for the market price of risk due to power's non-storability and demand not being a traded asset. Using PJM market data, the study estimates this risk premium, finding significant upward biases in forward prices between 1999-2001, which decreased by 2005. These biases are attributed to the right skewness of power prices, suggesting the power market is not fully integrated with broader financial markets.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The paper uses \"inverse problem techniques\" which could be considered a form of machine learning, but it's not explicitly stated as such and the focus is on the financial modeling rather than the ML methodology itself. It is about forecasting/prediction of power prices and derivative values."}, "mode": "threadpool", "paper_id": "edcd8b865beeefe6", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: The risk premia embedded in index options\nAbstract: We study the dynamic relation between market risks and risk premia using time series of index option surfaces. We find that priced left tail risk cannot be spanned by market volatility (and its components) and introduce a new tail factor. This tail factor has no incremental predictive power for future volatility and jump risks, beyond current and past volatility, but is critical in predicting future market equity and variance risk premia. Our findings suggest a wide wedge between the dynamics of market risks and their compensation, which typically displays a far more persistent reaction following market crises. (C) 2015 Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 282, "candidates": 168, "total": 450}, "attempt1_parsed": {"summary": "This study investigates the relationship between market risks and risk premia using index option data. It identifies a priced left tail risk that is not explained by market volatility alone, introducing a new tail factor. This factor is crucial for predicting future market equity and variance risk premia, indicating a persistent discrepancy between market risk dynamics and compensation, especially after market crises.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The study uses index options and focuses on predicting risk premia, which aligns with I3. However, it does not appear to use machine learning beyond simple models (I1) and is not about government bonds or yield curves (I2)."}, "mode": "threadpool", "paper_id": "eac1579c9a56f66f", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: The term structure of Eurozone peripheral bond yields: an asymmetric regime-switching equilibrium correction approach\nAbstract: Several studies have established the predictive power of the yield curve i.e. the difference between long and short-term bond rates and the role of asymmetries in the term structure of bond yields with respect to real economic activity. Using an extensive dataset, comprising 3-month, 1-year, 5-year and 10-year constant maturity Treasury bonds for the Eurozone southern periphery countries - the so-called PIIGS - from January 1999 to April 2019, we investigate the links between bond yields of different maturities for the Eurozone southern peripheral countries and we find they co-evolve in line with the predictions of the Expectations Hypothesis theory. We demonstrate the presence of nonlinearities in the term structure, and utilize a multivariate asymmetric two-regime Markov-switching VAR methodology to model them properly. Moreover, we address the economic reasoning behind the introduction of an equilibrium-correction regime-switching approach, hence providing potentially important insights on the behaviour of the entire yield curve. We reveal that t … [truncated]", "attempt1_tokens": {"prompt": 437, "candidates": 161, "total": 598}, "attempt1_parsed": {"summary": "This study investigates the term structure of Eurozone peripheral bond yields using a multivariate asymmetric two-regime Markov-switching VAR methodology. It finds that bond yields co-evolve according to the Expectations Hypothesis and demonstrates nonlinearities and regime shifts related to the business cycle. The approach provides insights into the behavior of the entire yield curve, particularly in economies where monetary policy responds to output deviations.", "criteria_hints": {"I1": true, "I2": true, "I3": true}, "notes": "The study uses a Markov-switching VAR methodology, which is a form of regime-switching model, and investigates the predictive power of the yield curve, aligning with the criteria."}, "mode": "threadpool", "paper_id": "e3dd71ea367283cb", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: The meaning of structural breaks for risk management: new evidence, mechanisms, and innovative views for the post-COVID-19 era\nAbstract: This paper quantitatively reveals the meaning of structural breaks for risk management by analyzing US and major European banking sector stocks. Applying newly extended Glosten-Jagannathan-Runkle generalized autoregressive conditional heteroscedasticity models, we supply the following new evidence. First, we find that incorporating structural breaks is always effective in estimating banking stock volatilities. Second, we clarify that structural breaks partially explain the tail fatness of banking stock returns. Third, we find that when incorporating structural breaks, the estimated volatilities more accurately capture their downside risk, proving that structural breaks matter for risk management. Fourth, our news impact curve and model parameter analyses also uncover that when incorporating structural breaks, the asymmetry in volatility responses to return shocks is more accurately captured. This proves why the estimated volatilities by incorporating structural breaks better explain downside risk. In addition, we further reveal that the est … [truncated]", "attempt1_tokens": {"prompt": 447, "candidates": 194, "total": 641}, "attempt1_parsed": {"summary": "This paper examines the impact of structural breaks on risk management in the US and European banking sectors. Using an extended GJR-GARCH model, it demonstrates that incorporating structural breaks improves volatility estimation, explains tail fatness in returns, and better captures downside risk and asymmetric volatility responses. The study also highlights increased volatility during major crises (Lehman, European debt, Brexit, COVID-19) and suggests implications for AI-driven risk management post-COVID-19.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The paper uses GJR-GARCH models, which are advanced econometric models, and discusses implications for AI in risk management, suggesting an empirical ML application beyond simple linear models. It also focuses on volatility estimation and risk, which are predictive in nature."}, "mode": "threadpool", "paper_id": "25fb4c8caf97c256", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: The shape of the risk premium: Evidence from a semiparametric generalized autoregressive conditional heteroscedasticity model\nAbstract: We examine the relationship between the risk premium on the Center for Research on Security Prices (CRSP) value-weighted index total return and its conditional variance. We propose a new serniparametric model in which the conditional variance process is parametric and the conditional mean is an arbitrary function of the conditional variance. For monthly CRSP value-weighted excess returns, the relationship between the two moments that we uncover is nonlinear and nonmonotonic.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 259, "candidates": 148, "total": 407}, "attempt1_parsed": {"summary": "This paper investigates the link between the risk premium of the CRSP value-weighted index total return and its conditional variance using a novel semiparametric GARCH model. The findings indicate a nonlinear and nonmonotonic relationship between these two moments for monthly excess returns.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The model proposed is semiparametric, allowing for a flexible, non-linear relationship between the conditional variance and the conditional mean, which goes beyond simple linear models. The study focuses on forecasting the risk premium based on conditional variance."}, "mode": "threadpool", "paper_id": "f275f02d334fb4b6", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: The role of uncertainty and sentiment for intraday volatility connectedness between oil and financial markets\nAbstract: We quantify intraday volatility connectedness between oil and key financial assets and assess how it is related to uncertainty and sentiment measures. For that purpose, we integrate the well-known spillover methodology with a TVP VAR model estimated on a unique, vast dataset of roughly 300 thousand 5 min quotations for most heavily traded financial assets: crude oil, the US dollar, S&P 500 index, gold and US treasury bonds. This distinguishes our investigation from previous studies, which usually employ relatively short samples of daily or weekly data and focus on connectedness between two asset classes. We contribute to the literature across three margins. First, we document that market connectedness at intraday frequency presents a different picture on markets co-movement compared to the estimates obtained using daily data. Second, we show that at 5 min frequency volatility is mostly transmitted from the stock market and absorbed by the bond and dollar markets, with oil and gold markets being occasionally important for volatility transmission. Third, we p … [truncated]", "attempt1_tokens": {"prompt": 436, "candidates": 236, "total": 672}, "attempt1_parsed": {"summary": "This study quantifies intraday volatility connectedness between oil and financial assets (US dollar, S&P 500, gold, US treasury bonds) using a TVP VAR model on 5-minute data. It finds that market connectedness differs from daily estimates, with volatility primarily transmitted from the stock market to bond and dollar markets. Daily connectedness measures are influenced by sentiment and market-specific uncertainty, decreasing during periods of high volatility driven by specific factors. The study emphasizes the value of high-frequency data for understanding market dynamics.", "criteria_hints": {"I1": true, "I2": true, "I3": false}, "notes": "The study uses a TVP VAR model, which is a time-varying parameter vector autoregression, and analyzes 5-minute data, suggesting an empirical application beyond simple linear models. It explicitly mentions US treasury bonds, fulfilling I2. While it discusses market dynamics and influences on connectedness, it does not appear to focus on forecasting or prediction as its primary objective, hence I3 is false."}, "mode": "threadpool", "paper_id": "4b19ed1da48dc4e8", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Transition densities for interest rate and other nonlinear diffusions\nAbstract: This paper applies to interest rate models the theoretical method developed in Ait-Sahalia (1998) to generate accurate closed-form approximations to the transition function of an arbitrary diffusion. While the main focus of this paper is on the maximum-likelihood estimation of interest rate models with otherwise unknown transition functions, applications to the valuation of derivative securities are also briefly discussed.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 233, "candidates": 97, "total": 330}, "attempt1_parsed": {"summary": "This paper adapts a theoretical method to approximate transition functions of diffusions, focusing on interest rate models for maximum-likelihood estimation and derivative valuation.", "criteria_hints": {"I1": false, "I2": true, "I3": false}, "notes": "The paper focuses on theoretical methods and their application to interest rate models, not empirical ML or forecasting."}, "mode": "threadpool", "paper_id": "361f9221ee0d7559", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: The volatility structure of the fixed income market under the HJM framework: A nonlinear filtering approach\nAbstract: The dynamics for interest rate processes within the well-known multi-factor Heath, Jarrow and Morton (HJM) specification are considered. Despite the flexibility of and the notable advances in theoretical research about the HJM model, the number of empirical studies of it is still very sparse. This paucity is principally due to the difficulties in estimating models in this class, which are not only high-dimensional, but also nonlinear and involve latent state variables. The estimation of a fairly broad class of HJM models as a nonlinear filtering problem is undertaken by adopting the local linearization filter, which is known to have some desirable statistical and numerical features, so enabling the estimation of the model via the maximum likelihood method. The estimator is then applied to the US, the UK and the Australian markets. Different two- and three-factor models are found to be the best for each market, with the factors being the level, the slope and the twist effect. The contribution of each factor towards overall variability of the interest rates and … [truncated]", "attempt1_tokens": {"prompt": 401, "candidates": 203, "total": 604}, "attempt1_parsed": {"summary": "This paper estimates a broad class of Heath, Jarrow, and Morton (HJM) interest rate models using a nonlinear filtering approach, specifically the local linearization filter. The method is applied to US, UK, and Australian markets, identifying two- and three-factor models (level, slope, twist) as optimal. The study finds significant cross-market differences in factor contributions to interest rate variability and risk premia.", "criteria_hints": {"I1": true, "I2": true, "I3": false}, "notes": "The paper uses a nonlinear filtering approach for estimation, which can be considered an advanced statistical/machine learning technique beyond simple linear models. It focuses on interest rate dynamics and the yield curve, which relates to government bonds. While it estimates and analyzes market dynamics, the primary focus is not on forecasting future interest rates but on understanding their current structure and drivers."}, "mode": "threadpool", "paper_id": "75536a9b5e21dd96", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: The value of options for time charterparty extension: an artificial neural networks (ANN) approach\nAbstract: The most frequently associated options in the physical shipping market are options to extend the charter period on time charters and additional shipment options on contracts of affreightment. The value of freight options, in practice, is estimated mostly by referring to forward curves. An option on freight has different properties from its financial counterparts, and the straightforward adoption of theoretical models does not produce promising results. In this paper, extension options, which have the property of options on futures, were transformed into regular European options before the application of the Black-Scholes model (BSM). The efficient market hypothesis, which justifies the parity of the performance of a long-term charter to that of repetitive short-term charters, worked as the basis for the transformation. The option values determined by the BSM were compared with actual realized values. Additionally, the artificial neural networks (ANN) was employed to derive the option values. This study is meaningful as the first-time application of both the closed-for … [truncated]", "attempt1_tokens": {"prompt": 412, "candidates": 187, "total": 599}, "attempt1_parsed": {"summary": "This paper explores the valuation of time charterparty extension options in the shipping market using both the Black-Scholes model and Artificial Neural Networks (ANN). It addresses the limitations of traditional financial models for freight options and proposes a method to transform extension options into European options for BSM application. The study highlights the novelty of using both closed-form solutions and ANN for valuing physical freight options and suggests implications for chartering decisions and credit risk assessment.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses ANN, which is an empirical ML application and goes beyond simple linear models. It also forecasts option values, fulfilling I3. However, it does not discuss government/sovereign bonds or yield curves, so I2 is false."}, "mode": "threadpool", "paper_id": "693d8e2872e52e11", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Tools for non-linear time series forecasting in economics - An empirical comparison of regime switching vector autoregressive models and recurrent neural networks\nAbstract: The purpose of this study is to contrast the forecasting performance of two non-linear models, a regime-switching vector autoregressive model (RS-VAR) and a recurrent neural network (RNN), to that of a linear benchmark VAR model. Our specific forecasting experiment is U.K. inflation and we utilize monthly data from 1969 to 2003. The RS-VAR and the RNN perform approximately on par over both monthly and annual forecast horizons. Both non-linear models perform significantly better than the VAR model.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 282, "candidates": 173, "total": 455}, "attempt1_parsed": {"summary": "This study compares the forecasting performance of a regime-switching vector autoregressive model (RS-VAR) and a recurrent neural network (RNN) against a linear benchmark VAR model using UK inflation data from 1969 to 2003. Both non-linear models performed similarly and significantly better than the linear model for both monthly and annual forecast horizons.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study compares non-linear models (RS-VAR and RNN) to a linear model, fulfilling I1. It focuses on forecasting, fulfilling I3. However, it does not concern government bonds or the yield curve, so I2 is false."}, "mode": "threadpool", "paper_id": "dba4d319451eec06", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Twin Picks: Disentangling the Determinants of Risk- Taking in Household Portfolios\nAbstract: This paper investigates risk-taking in the liquid portfolios held by a large panel of Swedish twins. We document that the portfolio share invested in risky assets is an increasing and concave function of financial wealth, leading to different risk sensitivities across investors. Human capital, which we estimate directly from individual labor income, also affects risk-taking positively, while internal habit and expenditure commitments tend to reduce it. Our microfindings lend strong support to decreasing relative risk aversion and habit formation preferences. Furthermore, heterogeneous risk sensitivities across investors help reconcile individual preferences with representative-agent models.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 276, "candidates": 152, "total": 428}, "attempt1_parsed": {"summary": "This study examines risk-taking behavior in household investment portfolios using a large dataset of Swedish twins. It finds that financial wealth, human capital, and habit formation preferences influence risk-taking. The results support theories of decreasing relative risk aversion and habit formation, and suggest that heterogeneous risk sensitivities can bridge individual preferences with representative-agent models.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The study uses empirical data but does not appear to involve machine learning applications beyond simple models. It focuses on household portfolios, not government bonds, and does not seem to be about forecasting."}, "mode": "threadpool", "paper_id": "ed3aad09e82d6a77", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Unemployment insurance and mortgages\nAbstract: We consider mortgages including the possibility of an unemployment insurance. The insurance company pays the cash flows of the credit as soon as the borrower becomes unemployed, for a maximal number of payments fixed in the contract. We develop a probabilistic model for describing the cash flows paid by the insurance company. We jointly take into account unemployment, job search and prepayment phenomena. With such a model it is possible to study the probabilistic properties of the cash flow pattern as a function of the age of the credit. Finally, we discuss the estimation of the parameters of such a model and its use for pricing the insurance contract. (C) 1997 Elsevier Science B.V.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 285, "candidates": 115, "total": 400}, "attempt1_parsed": {"summary": "This paper models mortgages with unemployment insurance, where the insurer covers payments upon borrower unemployment for a fixed period. It incorporates unemployment, job search, and prepayment dynamics to analyze cash flow patterns and price insurance contracts.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The paper discusses a probabilistic model and parameter estimation for pricing insurance contracts related to mortgages, which implies a forecasting/prediction aspect."}, "mode": "threadpool", "paper_id": "398476d4b3717e7e", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: USING NONLINEAR METHODS TO SEARCH FOR RISK PREMIA IN CURRENCY FUTURES\nAbstract: This paper uses currency futures prices to test the joint null hypotheses of rational expectations and absence of a time-varying risk premium in the foreign exchange market. We find no linear predictability in the logarithm of futures price changes, either using its own past or past interest differentials. Also we establish that there is no non-linear predictability in log price changes, conditioning on its own past, or past interest rate differentials. Thus, if a time-varying risk premium exists in currency futures market, it is not related to its own past or past interest rate differentials.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 275, "candidates": 159, "total": 434}, "attempt1_parsed": {"summary": "This paper investigates the presence of risk premia in currency futures markets using both linear and nonlinear methods. The study finds no evidence of predictability in currency futures price changes, whether linear or nonlinear, based on past price movements or past interest rate differentials. Therefore, if a time-varying risk premium exists, it is not explained by these factors.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The paper uses nonlinear methods, which goes beyond simple linear models, fulfilling I1. It is about currency futures, not government bonds, so I2 is false. It tests for predictability, fulfilling I3."}, "mode": "threadpool", "paper_id": "bac2a47404767287", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Uncertainty and Forecasts of US Recessions\nAbstract: We estimate Boosted Regression Trees (BRT) on a sample of monthly data that extends back to 1889 to recover the predictive value of disaggregated news-based uncertainty indexes for U.S recessions. We control for widely-studied standard predictors and use out-of-sample metrics to assess forecast performance. We find that war-related uncertainty is among the top five predictors of recessions at three different forecast horizons (3, 6, and 12 months). The predictive value of war-related uncertainty has fallen in the second half of the 20th century. Uncertainty regarding the state of securities markets has gained in relative importance. The probability of a recession is a nonlinear function of war-related and securities-markets uncertainty. Receiver-operating-characteristic curves show that uncertainty improves out-of-sample forecast performance at the longer forecast horizons. A dynamic version of the BRT approach sheds light on the importance of various lags of government-related uncertainty for recession forecasting at the long forecast horizon.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning applicati … [truncated]", "attempt1_tokens": {"prompt": 363, "candidates": 175, "total": 538}, "attempt1_parsed": {"summary": "This study uses Boosted Regression Trees (BRT) on monthly US data from 1889 to assess the predictive power of news-based uncertainty indexes for recessions. It finds that war-related uncertainty is a significant predictor, though its importance has waned. Uncertainty in securities markets has become more relevant. The study also notes the nonlinear relationship between these uncertainties and recession probability, and that uncertainty improves out-of-sample recession forecasts at longer horizons.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses Boosted Regression Trees, which is a machine learning technique beyond simple linear models. It focuses on recession forecasting, not government bonds or the yield curve."}, "mode": "threadpool", "paper_id": "0a0c2a619cb3d88f", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Uncovering nonlinear dependencies in the Treasury-funds rate spread: Quantile-based explanation\nAbstract: This study examines the structural dynamics of the spread between the 10-year Treasury yield and the federal funds rate, a key indicator of U.S. financial conditions. Cross-quantilogram analysis reveals a nonlinear dependency across adjacent periods, with stronger connectedness observed in the tail distribution than in the middle. Additionally, the functional quantile autoregression model confirms the spread's nonlinear and asymmetric nature from a distributional perspective. Specifically, higher quantiles of the previous spread exert a stronger influence on the current spread, indicating a positive persistence mechanism. Conversely, lower quantiles of the previous spread negatively affect the higher quantiles of the current spread. These findings suggest that bullish market conditions tend to sustain themselves, whereas bearish conditions hinder upward momentum, underscoring the need for quantile-specific policy interventions.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": … [truncated]", "attempt1_tokens": {"prompt": 326, "candidates": 182, "total": 508}, "attempt1_parsed": {"summary": "This study uses cross-quantilogram and functional quantile autoregression to analyze the nonlinear and asymmetric relationship between the 10-year Treasury yield and the federal funds rate spread. It finds stronger dependencies in the tails of the distribution and suggests that market conditions tend to persist, with higher quantiles influencing current spreads more strongly. The authors advocate for quantile-specific policy interventions.", "criteria_hints": {"I1": true, "I2": true, "I3": false}, "notes": "The study uses advanced statistical methods that go beyond linear models, fitting I1. It directly analyzes the Treasury yield and federal funds rate spread, fitting I2. While it discusses persistence, the primary focus is on explaining existing dynamics rather than forecasting future values, thus I3 is false."}, "mode": "threadpool", "paper_id": "1a56df038c2e8350", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Understanding Two Remarkable Findings about Stock Yields and Growth\nAbstract: Two regularities regarding stock prices and expected inflation have received less attention than they deserve. First, earnings and dividend yields move with long-term expected inflation and risk-free rates. Second, analysts' forecasts of nominal growth, not real growth, vary little with expected inflation. These patterns are remarkable because financial economists predict exactly the opposite. One explanation for these contrary findings is that stock prices are too high (low) when inflation is low (high), because investors confuse nominal and real growth rates. The authors assert that investors are unlikely to be so systematically naive about expected inflation and argue that the contrary evidence is, in fact, consistent with a rational market. The key insight offered by the authors is that reported earnings include inflationary holding gains, which causes higher earnings yields when inflation is high (the first regularity) and, in turn, explains why forecasts of nominal growth need not vary with inflation (the second regularity).\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learnin … [truncated]", "attempt1_tokens": {"prompt": 342, "candidates": 197, "total": 539}, "attempt1_parsed": {"summary": "This paper discusses two underappreciated regularities concerning stock prices and expected inflation: earnings and dividend yields correlate with long-term expected inflation and risk-free rates, and analysts' forecasts of nominal growth, not real growth, show little variation with expected inflation. The authors propose that these patterns, contrary to financial economists' predictions, are consistent with a rational market. They explain that reported earnings include inflationary holding gains, which lead to higher earnings yields during high inflation periods, thus explaining the second regularity.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The paper discusses forecasting of nominal growth, which aligns with I3. However, it does not involve empirical machine learning applications beyond simple linear models (I1) and focuses on stock yields rather than government/sovereign bonds (I2)."}, "mode": "threadpool", "paper_id": "39f83bbe481f5679", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Unit-root tests and asymmetric adjustment with an example using the term structure of interest rates\nAbstract: This article develops critical values to test the null hypothesis of a unit root against the alternative of stationarity with asymmetric adjustment. Specific attention is paid to threshold and momentum threshold autoregressive processes. The standard Dickey-Fuller tests emerge as a special case. Within a reasonable range of adjustment parameters, the power of the new tests is shown to be greater than that of the corresponding Dickey-Fuller test. The use of the tests is illustrated using the term structure of interest rates. It is shown that the movements toward the long-run equilibrium relationship are best estimated as an asymmetric process.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 284, "candidates": 117, "total": 401}, "attempt1_parsed": {"summary": "This paper introduces new unit-root tests that account for asymmetric adjustment, outperforming standard Dickey-Fuller tests in power. The methodology is applied to the term structure of interest rates, revealing asymmetric adjustments towards long-run equilibrium.", "criteria_hints": {"I1": false, "I2": true, "I3": false}, "notes": "The paper uses an econometric technique (unit-root tests) applied to financial data, not machine learning."}, "mode": "threadpool", "paper_id": "2648f39614791bbd", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Unstable volatility: the break-preserving local linear estimator\nAbstract: The objective of this paper is to introduce the break-preserving local linear (BPLL) estimator for the estimation of unstable volatility functions for independent and asymptotically independent processes. Breaks in the structure of the conditional mean and/or the volatility functions are common in Finance. Nonparametric estimators are well suited for these events due to the flexibility of their functional form and their good asymptotic properties. However, the local polynomial kernel estimators are not consistent at points where the volatility function has a break. The estimator presented in this paper generalises the classical local linear (LL). The BPLL estimator maintains the desirable properties of the LL estimator with regard to the bias and the boundary estimation while it estimates the breaks consistently. An extensive Monte Carlo study is shown as well as detailed proofs of the estimator asymptotic behaviour.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereig … [truncated]", "attempt1_tokens": {"prompt": 316, "candidates": 150, "total": 466}, "attempt1_parsed": {"summary": "This paper introduces the break-preserving local linear (BPLL) estimator for unstable volatility functions in independent and asymptotically independent processes, addressing common breaks in financial conditional mean and volatility functions. The BPLL estimator generalizes the classical local linear estimator, maintaining desirable properties while consistently estimating breaks, unlike standard local polynomial kernel estimators.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The paper focuses on theoretical estimation methods for volatility, not empirical applications of machine learning. While it discusses forecasting volatility, it doesn't appear to be specifically about government bonds or the yield curve."}, "mode": "threadpool", "paper_id": "164d3e1462675b58", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Using the yield curve to forecast economic growth\nAbstract: This paper finds the yield curve to have a well-performing ability to forecast the real gross domestic product growth in the USA, compared to professional forecasters and time series models. Past studies have different arguments concerning growth lags, structural breaks, and ultimately the ability of the yield curve to forecast economic growth. This paper finds such results to be dependent on the estimation and forecasting techniques employed. By allowing various interest rates to act as explanatory variables and various window sizes for the out-of-sample forecasts, significant forecasts from many window sizes can be found. These seemingly good forecasts may face issues, including persistent forecasting errors. However, by using statistical learning algorithms, such issues can be cured to some extent. The overall result suggests, by scientifically deciding the window sizes, interest rate data, and learning algorithms, many outperforming forecasts be produced for all lags from one quarter to 3 years, although some may be worse than the others due to the irreducible noise of the data.\n\nCriteria: {\"I1\": \"The article in … [truncated]", "attempt1_tokens": {"prompt": 350, "candidates": 160, "total": 510}, "attempt1_parsed": {"summary": "This paper investigates the predictive power of the yield curve for US real GDP growth, comparing its performance against professional forecasters and time series models. It highlights that forecast accuracy depends on estimation and forecasting techniques, including the choice of interest rates and window sizes. The study demonstrates that statistical learning algorithms can mitigate issues like persistent forecasting errors, enabling the production of outperforming forecasts across various lags by optimizing window sizes, interest rate data, and learning algorithms.", "criteria_hints": {"I1": true, "I2": true, "I3": true}, "notes": "The paper explicitly mentions using statistical learning algorithms, which goes beyond simple linear models and constitutes an empirical machine learning application."}, "mode": "threadpool", "paper_id": "4e973d99311508de", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Using Markov-switching models with Markov chain Monte Carlo inference methods in agricultural commodities trading\nAbstract: In this work, the use of Markov-switching GARCH (MS-GARCH) models is tested in an active trading algorithm for corn and soybean future markets. By assuming that a given investor lives in a two-regime world (with low- and high-volatility time periods), a trading algorithm was simulated (from January 2000 to March 2019), which helped the investor to forecast the probability of being in the high-volatility regime att + 1. Once this probability was known, the investor could decide to invest either in commodities, during low-volatility periods or in the 3-month US Treasury bills, during high-volatility periods. Our results suggest that the Gaussian MS-GARCH model is the most appropriate to generate alpha or extra returns (from a passive investment strategy) in the corn market and thet-Student MS-GARCH is the best one for soybean trading.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\" … [truncated]", "attempt1_tokens": {"prompt": 356, "candidates": 232, "total": 588}, "attempt1_parsed": {"summary": "This study applies Markov-switching GARCH (MS-GARCH) models with Markov chain Monte Carlo inference to an active trading algorithm for corn and soybean futures. The algorithm simulates a two-regime world (low and high volatility) to forecast the probability of being in a high-volatility regime. Based on this forecast, an investor decides between commodities (low volatility) or US Treasury bills (high volatility). The Gaussian MS-GARCH model proved best for corn, while the t-Student MS-GARCH was optimal for soybean trading, suggesting potential for generating alpha.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses MS-GARCH models, which are more complex than simple linear models, and applies them to a trading algorithm, indicating an empirical machine learning application. While it mentions US Treasury bills as an alternative investment, the primary focus is on agricultural commodities trading and forecasting volatility regimes, not on government bonds or the yield curve themselves."}, "mode": "threadpool", "paper_id": "09d07317676ce634", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Using Neural Networks to Forecast Volatility for an Asset Allocation Strategy Based on the Target Volatility\nAbstract: The objective of this study is to use artificial neural networks for volatility forecasting to enhance the ability of an asset allocation strategy based on the target volatility. The target volatility level is achieved by dynamically allocating between a risky asset and a risk-free cash position. However, a challenge to data-driven approaches is the limited availability of data since periods of high volatility, such as during financial crises, are relatively rare. To resolve this issue, we apply a stability-oriented approach to compare data for the current period to a past set of data for a period of low volatility, providing a much more abundant source of data for comparison. In order to explore the impact of the proposed model, the results of this approach will be compared to different volatility forecast methodologies, such as the volatility index, the historical volatility, the exponentially weighted moving average (EWMA), and the generalized autoregressive conditional heteroskedasticity (GARCH) model. Trading measures are used to evaluate the performanc … [truncated]", "attempt1_tokens": {"prompt": 437, "candidates": 168, "total": 605}, "attempt1_parsed": {"summary": "This study uses neural networks to forecast volatility for an asset allocation strategy based on target volatility. It addresses data limitations by comparing current data to past low-volatility periods and compares its model to other forecasting methods (VIX, historical volatility, EWMA, GARCH) using trading measures for evaluation. The empirical study uses KOSPI 200 and CD rates from 2006-2016.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses neural networks, which are beyond simple linear models, for volatility forecasting. It does not focus on government/sovereign bonds or the yield curve."}, "mode": "threadpool", "paper_id": "061c3ab3803363ee", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: VALUATION OF EMBEDDED OPTIONS IN NON-MARKETABLE CALLABLE BONDS: A NEW NUMERICAL APPROACH\nAbstract: The issue of how to price options embedded in callable bonds has attracted a lot of interest over the years. The usual bond valuation methods rely on yield curves, risk premium, and other parameters to estimate interest rates used in discounted cash flow calculations. The option to retire the bond is, however, neglected in the standard pricing models, causing a systematic overvaluation of callable bonds. In the event of a decline in interest rates, investors are exposed to the risk of a lower return on investment than indicated by the yield to maturity. We propose a novel approach to valuing the risk that the issuer will use the right to buy back the bond at a specific call price. While prior models are focused on valuing marketable callable bonds, we deliver a unique approach to valuing bonds with an embedded European option (or a multiple option) that are traded solely through private transactions. These can typically be characterized by the lack of historical records on transaction prices. The modular character of calculation we propose allows us to take into account additio … [truncated]", "attempt1_tokens": {"prompt": 397, "candidates": 142, "total": 539}, "attempt1_parsed": {"summary": "This paper proposes a new numerical approach to value embedded options in non-marketable callable bonds, addressing the systematic overvaluation issue in standard pricing models that neglect the issuer's right to retire the bond. The method accounts for factors like issuer behavior and interest rate development, offering a unique approach for bonds traded privately without historical price records.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The abstract does not mention machine learning, government bonds, or forecasting. It focuses on a novel numerical valuation method for callable bonds."}, "mode": "threadpool", "paper_id": "b369ae93e8a8645c", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Variable Selection and Oversampling in the Use of Smooth Support Vector Machines for Predicting the Default Risk of Companies\nAbstract: In the era of Basel II a powerful tool for bankruptcy prognosis is vital for banks. The tool must be precise but also easily adaptable to the bank's objectives regarding the relation of false acceptances (Type I error) and false rejections (Type II error). We explore the suitability of smooth support vector machines (SSVM), and investigate how important factors such as the selection of appropriate accounting ratios (predictors), length of training period and structure of the training sample influence the precision of prediction. Moreover, we show that oversampling can be employed to control the trade-off between error types, and we compare SSVM with both logistic and discriminant analysis. Finally, we illustrate graphically how different models can be used jointly to support the decision-making process of loan officers. Copyright (C) 2008 John Wiley & Sons, Ltd.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sov … [truncated]", "attempt1_tokens": {"prompt": 338, "candidates": 165, "total": 503}, "attempt1_parsed": {"summary": "This study investigates the use of smooth support vector machines (SSVM) for predicting company default risk, comparing it with logistic and discriminant analysis. It explores the impact of variable selection, training period length, and sample structure on prediction accuracy. The research also demonstrates how oversampling can be used to manage the trade-off between Type I and Type II errors, offering a tool for loan officers to support decision-making.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses SSVM, which is a machine learning technique, and compares it with other models. It is focused on prediction of default risk, not government bonds."}, "mode": "threadpool", "paper_id": "2a226e9e781cf124", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Using proxies for the short rate: When are three months like an instant?\nAbstract: The dynamics of the unobservable short rate are frequently estimated directly using a proxy. We examine the biases resulting From this practice (the proxy problem). Analytic results show that the proxy problem is not economically significant for single-factor affine models. In the two-factor affine model of Longstaff and Schwartz (1992), the proxy problem is only economically significant for pricing discount bonds with maturities of more than five years. We also describe two different numerical procedures for assessing the magnitude of the proxy problem in a general interest rate model. When applied to a nonlinear single-factor model, they suggest that the proxy problem can be economically significant.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 291, "candidates": 184, "total": 475}, "attempt1_parsed": {"summary": "This paper investigates the biases that arise when using proxies for the unobservable short rate in estimating interest rate dynamics. It finds that the 'proxy problem' is not economically significant for single-factor affine models but can be significant for two-factor affine models when pricing long-maturity discount bonds. The study also proposes numerical methods to assess the proxy problem's magnitude in general interest rate models, suggesting it can be significant in nonlinear single-factor models.", "criteria_hints": {"I1": false, "I2": true, "I3": false}, "notes": "The paper discusses interest rate models and their application to bond pricing, which relates to the yield curve. However, it does not appear to involve machine learning applications beyond simple linear models, nor does it focus on forecasting or prediction."}, "mode": "threadpool", "paper_id": "c77bf0ed0e86b54c", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Valuing the Treasury's Capital Assistance Program\nAbstract: T he Capital Assistance Program (CAP) was created by the U. S. government in February 2009 to provide backup capital to large financial institutions unable to raise sufficient capital from private investors. Under the terms of the CAP, a participating bank receives contingent capital by issuing preferred shares to the Treasury combined with embedded options for both parties: The bank gets the option to redeem the shares or convert them to common equity, with conversion mandatory after seven years; the Treasury earns dividends on the preferred shares and gets warrants on the bank's common equity. We develop a contingent claims framework in which to estimate market values of these CAP securities. The interaction between the competing options held by the buyer and issuer of these securities creates a game between the two parties, and our approach captures this strategic element of the joint valuation problem and clarifies the incentives it creates. We apply our method to the 18 publicly held bank holding companies that participated in the Supervisory Capital Assessment Program (the stress test) launched together with t … [truncated]", "attempt1_tokens": {"prompt": 502, "candidates": 198, "total": 700}, "attempt1_parsed": {"summary": "This paper values the U.S. Treasury's Capital Assistance Program (CAP) using a contingent claims framework that accounts for the strategic interaction between the bank and the Treasury regarding embedded options. The authors estimate that CAP securities provided a net value of approximately 30% of the invested capital to participating banks, with significant variation across institutions. The valuation aligns with abnormal stock returns observed around the CAP announcement, suggesting market recognition of the program's value.", "criteria_hints": {"I1": false, "I2": false, "I3": false}, "notes": "The paper uses a contingent claims framework and game theory for valuation, not machine learning. It focuses on capital assistance to financial institutions, not government bonds or the yield curve. While it estimates value, it's not a direct forecast or prediction in the typical sense of time-series forecasting."}, "mode": "threadpool", "paper_id": "5a945e5c94a9bfc5", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Volatility Expectations and Returns\nAbstract: We provide evidence that agents have slow-moving beliefs about stock market volatility that lead to initial underreaction to volatility shocks followed by delayed overreaction. These dynamics are mirrored in the VIX and variance risk premiums, which reflect investor expectations about volatility, and are also supported in both surveys and firm-level option prices. We embed these expectations into an asset pricing model and find that the model can account for a number of stylized facts about market returns and return volatility that are difficult to reconcile, including a weak or even negative risk-return trade-off.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 264, "candidates": 200, "total": 464}, "attempt1_parsed": {"summary": "This study investigates how slow-moving beliefs about stock market volatility influence investor reactions to volatility shocks, leading to initial underreaction and subsequent overreaction. These patterns are observed in market indicators like the VIX and variance risk premiums, and are further supported by survey data and option prices. The authors incorporate these findings into an asset pricing model, which helps explain several established market return and volatility characteristics, including a weak or negative risk-return trade-off.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The abstract mentions an asset pricing model but does not specify the use of machine learning beyond simple linear models. The focus is on market returns and volatility, not government bonds. The study does discuss how expectations are mirrored in market indicators and how the model accounts for stylized facts, suggesting a predictive or forecasting element."}, "mode": "threadpool", "paper_id": "8ac774f2ab9a9ab4", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Volatility measures and Value-at-Risk\nAbstract: We evaluate and compare the abilities of the implied volatility and historical volatility models to provide accurate Value-at-Risk forecasts. Our empirical tests on the S&P 500, Dow Jones Industrial Average and Nasdaq 100 indices over long time series of more than 20 years of daily data indicate that an implied volatility based Value-at-Risk cannot beat, and tends to be outperformed by, a simple GJR-GARCH based Value-at-Risk. This finding is robust to the use of the likelihood ratio, the dynamic quantile test or a statistical loss function for evaluating the Value-at-Risk performance.The poor performance of the option based Value-at-Risk is due to the volatility risk premium embedded in implied volatilities. We apply both non-parametric and parametric adjustments to correct for the negative price of the volatility risk. However, although this adjustment is effective in reducing the bias, it still does not allow the implied volatility to outperform the historical volatility models.These results are in contrast to the volatility forecasting literature, which favors implied volatilities over the historical volatility model. We sho … [truncated]", "attempt1_tokens": {"prompt": 459, "candidates": 194, "total": 653}, "attempt1_parsed": {"summary": "This study compares implied volatility and historical volatility models for Value-at-Risk (VaR) forecasting using S&P 500, Dow Jones Industrial Average, and Nasdaq 100 indices. Results show that GJR-GARCH based VaR outperforms implied volatility based VaR, even after adjustments for volatility risk premium. The authors suggest that while implied volatility is good for volatility forecasting, it's less effective for VaR prediction due to the non-linear dynamics of the volatility risk premium.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The study uses GJR-GARCH, which is a time-series model, not typically considered a machine learning application in the context of the criteria. The focus is on financial indices, not government bonds."}, "mode": "threadpool", "paper_id": "2212fd81e8d3b6aa", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Volatility in equity markets and monetary policy rate uncertainty\nAbstract: Asset pricing models assume the risk-free rate to be a key factor for equity prices. Hence, there should be a strong link between monetary policy rate uncertainty and equity return volatility, both in theory and data. This paper uses regression-based projections for realized variance to examine the relationship between short horizon forecasts of equity variance and proxies for monetary policy rate uncertainty. By assessing various projection models for UK, US and euro area equity indices, we show that the proxies for monetary policy rate uncertainty have a significant and positive predictive power for the equity return variance. Adding monetary policy rate uncertainty variables can significantly improve forecasting models for equity variance and volatility at weekly, monthly and even quarterly horizons. The findings imply that market views of short-term interest rate developments may indeed be embedded in equity prices and their variations. (C) 2017 The Bank of England. Published by Elsevier B.V. All rights reserved.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (n … [truncated]", "attempt1_tokens": {"prompt": 341, "candidates": 150, "total": 491}, "attempt1_parsed": {"summary": "This paper investigates the relationship between monetary policy rate uncertainty and equity return volatility using regression-based projections for realized variance. The study finds that proxies for monetary policy rate uncertainty have significant predictive power for equity return variance across UK, US, and euro area equity indices, improving forecasting models at various horizons. The results suggest that market expectations of short-term interest rate movements are reflected in equity prices and their fluctuations.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The study uses regression-based projections, which are not considered machine learning applications beyond simple linear models."}, "mode": "threadpool", "paper_id": "339ab8be530e9be4", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Volatility forecasts embedded in the prices of crude-oil options\nAbstract: This paper evaluates the ability of alternative option-implied volatility measures to forecast crude-oil return volatility. We find that a corridor implied volatility measure that aggregates information from a narrow range of option contracts consistently outperforms forecasts obtained by the popular Black-Scholes and model-free volatility expectations, as well as those generated by a realized volatility model. This measure ranks favorably in regression-based tests, delivers the lowest forecast errors under different loss functions, and generates economically significant gains in volatility timing exercises. Our results also show that the Chicago Board Options Exchange's oil-VIX index performs poorly, as it routinely produces the least accurate forecasts.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 288, "candidates": 186, "total": 474}, "attempt1_parsed": {"summary": "This paper assesses how well different option-implied volatility measures can predict crude-oil return volatility. A novel corridor implied volatility measure, which uses data from a specific range of option contracts, proves superior to common methods like Black-Scholes, model-free expectations, and realized volatility models. This measure also performs well in regression tests, shows lower forecast errors across various loss functions, and offers practical benefits in volatility timing. The study also highlights the poor performance of the Chicago Board Options Exchange's oil-VIX index in forecasting.", "criteria_hints": {"I1": false, "I2": false, "I3": true}, "notes": "The study uses regression-based tests and volatility timing exercises, which are empirical applications, but does not explicitly mention machine learning beyond simple linear models."}, "mode": "threadpool", "paper_id": "f0282a2efcf8a584", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: What the current yield curve says, and what the future prices of energy do\nAbstract: Y Policymakers have always looked at the difference between the yields on long- and short-term Treasury securities as an indication of where the economy is heading. In this study, we extend the literature by examining the yield curve's ability to predict the short-term prices of crude oil and other energy products. Using linear and non-linear (parametric quantile) causality tests on daily data from 1986 to February 2020, our findings confirm that changes in the yield spread not only correlate with, but also drive the returns on crude oil, heating oil and natural gas in the short run. This short-run relationship is relatively absent from 1986 to 2003. However, since 2004, the relationship has remained quite strong, confirming that these products have been financialized. Market participants and policy makers may find our findings useful in understanding the nature of relationship between the shape of the term structure and future innovations in energy prices.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models … [truncated]", "attempt1_tokens": {"prompt": 365, "candidates": 160, "total": 525}, "attempt1_parsed": {"summary": "This study investigates the predictive power of the yield curve for short-term energy prices (crude oil, heating oil, natural gas). Using causality tests on daily data from 1986-2020, the research finds that yield spread changes drive energy returns, particularly since 2004, suggesting financialization of these products. The findings are relevant for market participants and policymakers.", "criteria_hints": {"I1": false, "I2": true, "I3": true}, "notes": "The study uses linear and non-linear parametric quantile causality tests, which are statistical methods rather than machine learning applications. Therefore, I1 is false."}, "mode": "threadpool", "paper_id": "d534573971c63b10", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: What Does the Individual Option Volatility Smirk Tell Us About Future Equity Returns?\nAbstract: The shape of the volatility smirk has significant cross-sectional predictive power for future equity returns. Stocks exhibiting the steepest smirks in their traded options underperform stocks with the least pronounced volatility smirks in their options by 10.9% per year on a risk-adjusted basis. This predictability persists for at least 6 months, and firms with the steepest volatility smirks are those experiencing the worst earnings shocks in the following quarter. The results are consistent with the notion that informed traders with negative news prefer to trade out-of-the-money put options, and that the equity market is slow in incorporating the information embedded in volatility smirks.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/sovereign bonds and/or the yield curve\", \"I3\": \"The article is about forecasting or prediction\"}", "attempt1_tokens": {"prompt": 299, "candidates": 184, "total": 483}, "attempt1_parsed": {"summary": "The study finds that the shape of the volatility smirk in stock options predicts future equity returns. Stocks with steeper smirks underperform those with flatter smirks by 10.9% annually on a risk-adjusted basis. This predictability lasts for at least 6 months and is linked to firms experiencing negative earnings shocks, suggesting informed traders use out-of-the-money put options and that markets are slow to incorporate this information.", "criteria_hints": {"I1": true, "I2": false, "I3": true}, "notes": "The study uses the volatility smirk as a predictor, which can be considered a form of feature engineering for predictive modeling. While not explicitly stating 'machine learning', the predictive power and cross-sectional analysis suggest methods beyond simple linear models."}, "mode": "threadpool", "paper_id": "5e5d1d9ac9087b48", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: ZONE-TARGETING MONETARY POLICY PREFERENCES AND FINANCIAL MARKET CONDITIONS: A FLEXIBLE NON-LINEAR POLICY REACTION FUNCTION OF THE SARB MONETARY POLICY\nAbstract: We estimate a flexible model of the monetary policy reaction function of the South African Reserve Bank based on a representation of the policymaker's preferences that capture asymmetries and zone-targeting behaviours. We augment the analysis to allow for responses to financial market conditions over and above inflation and output stabilisation to address the current debate on the importance of financial asset prices in monetary policy decision making. The empirical results show that the monetary authorities' response to inflation is zone symmetric. Secondly, the monetary authorities' response to output is asymmetric with increased reaction during business cycle downturns relative to upturns. Thirdly, the monetary authorities pay close attention to the financial conditions index by placing an equal weight on financial market booms and recessions.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about gover … [truncated]", "attempt1_tokens": {"prompt": 321, "candidates": 166, "total": 487}, "attempt1_parsed": {"summary": "This study estimates a flexible, non-linear monetary policy reaction function for the South African Reserve Bank, incorporating zone-targeting preferences and responses to financial market conditions. The findings indicate symmetric responses to inflation, asymmetric responses to output (stronger during downturns), and significant attention to financial conditions, with equal weight given to booms and recessions.", "criteria_hints": {"I1": true, "I2": false, "I3": false}, "notes": "The study uses a flexible model and analyzes policy reaction functions, which can be considered an empirical application beyond simple linear models. However, it does not focus on government/sovereign bonds or yield curves, nor does it explicitly focus on forecasting."}, "mode": "threadpool", "paper_id": "40d5957f898788bd", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Yield Curve Estimation Based on Government Security Prices in the Croatian Financial Market\nAbstract: This article investigates the estimation of the yield curve based on government security prices using the Nelson-Siegel model in the Croatian financial market. The yield curve was estimated for samples of government securities with and without currency clauses. Since the Croatian financial market is less developed characterized by limited trading activity in government bonds, Treasury bills were also included in the analysis. To examine the difference in the estimation of yield curve parameters between a less developed and a developed market, the U.S. sample was considered. The yield curve was estimated for the full US sample and for artificially created U.S. samples corresponding to the Croatian samples of government bonds with and without currency clauses. Despite the less developed Croatian financial market, it is possible to estimate the yield curve and derive meaningful economic interpretations from the estimates.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article … [truncated]", "attempt1_tokens": {"prompt": 318, "candidates": 159, "total": 477}, "attempt1_parsed": {"summary": "This study estimates the yield curve in the Croatian financial market using the Nelson-Siegel model with government securities, including those with and without currency clauses, and Treasury bills due to limited trading activity. A comparison with U.S. market data was made to understand differences between developed and less developed markets. The findings suggest that yield curve estimation and economic interpretation are feasible even in less developed markets.", "criteria_hints": {"I1": false, "I2": true, "I3": false}, "notes": "The study uses the Nelson-Siegel model, which is a statistical model, not a machine learning application. It does not appear to involve forecasting or prediction."}, "mode": "threadpool", "paper_id": "4c58711a707af646", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: Yield Spread and Economic Policy Uncertainty: Evidence from Japan\nAbstract: In this paper, we adopt the nonlinear autoregressive distributed lags (NARDL) model extended by Shin et al. (2014) to investigate the relationship between the treasury yield spread and economic policy uncertainty (EPU) in Japan. This model helps us to explore the short- and long-run asymmetric reactions of explained variables through positive and negative partial sum decompositions of changes in the explanatory variable(s). In our research, the testing of the NARDL specification reveals the existence of a significant long-run asymmetric equilibrium between the yield spread and EPU in Japan. On the other hand, we find a significant positive nexus between the treasury yield spread and EPU reduction in the long run. We speculate that because of low inflation, a poor economic outlook and the low interest rate environment since 1990, financial agents are markedly sensitive to negative shocks resulting from EPU. This means that when facing a good economy, bond agents are quick to sell, especially with higher-risk long-term interest rate bonds. Meanwhile, because the Bank of Japan announced the Stock Purcha … [truncated]", "attempt1_tokens": {"prompt": 475, "candidates": 188, "total": 663}, "attempt1_parsed": {"summary": "This study uses the NARDL model to analyze the relationship between the treasury yield spread and economic policy uncertainty (EPU) in Japan, finding a significant long-run asymmetric equilibrium and a positive nexus between the yield spread and EPU reduction. The authors suggest financial agents are sensitive to negative EPU shocks due to Japan's economic conditions and low interest rates, influencing bond market behavior. They also consider the impact of unconventional monetary policy and the stock market on these relationships.", "criteria_hints": {"I1": true, "I2": true, "I3": false}, "notes": "The NARDL model is a type of econometric model that can capture nonlinearities and asymmetries, which goes beyond simple linear models. While the study analyzes relationships and influences, it does not explicitly focus on forecasting or prediction."}, "mode": "threadpool", "paper_id": "416e57a3c9d256bd", "model": "gemini-2.5-flash-lite"}
{"prompt_preview": "Title: What Drives Short Rate Dynamics? A Functional Gradient Descent Approach\nAbstract: Functional gradient descent (FGD), a recent technique coming from computational statistics, is applied to the estimation of the conditional moments of the short rate process with the goal of finding the main drivers of the drift and volatility dynamics. FGD can improve the accuracy of some reasonable starting estimates obtained using classical short rate models introduced in the literature. It exploits the predictive information of an enlarged set of variables, including yields at other maturities, time, and macroeconomic indicators. Fitting this methodology to the time series of monthly US 3-month Treasury bill rates, we find that the drift dynamics react mostly in a nonlinear way to changes in macroeconomic variables, whereas volatility dynamics are subjected to time-dependent regime-switches. Finally we show the superior performance of the final predictions obtained by applying FGD in a forecasting exercise.\n\nCriteria: {\"I1\": \"The article includes an empirical Machine Learning application (not pure theory). It goes beyond simple linear models.\", \"I2\": \"The article is about government/soverei … [truncated]", "attempt1_tokens": {"prompt": 318, "candidates": 228, "total": 546}, "attempt1_parsed": {"summary": "This paper applies Functional Gradient Descent (FGD), a statistical technique, to estimate the conditional moments of the short rate process, aiming to identify key drivers of drift and volatility. FGD enhances initial estimates from classical models by incorporating a wider range of variables, including other maturities' yields, time, and macroeconomic indicators. Applied to US 3-month Treasury bill rates, the study finds nonlinear responses of drift to macroeconomic changes and time-dependent regime switches in volatility. The method demonstrates superior forecasting performance.", "criteria_hints": {"I1": true, "I2": true, "I3": true}, "notes": "The abstract explicitly mentions \"Functional gradient descent (FGD), a recent technique coming from computational statistics\" and its application to \"estimation of the conditional moments of the short rate process\" and \"forecasting exercise\", indicating an empirical ML application beyond simple models and a focus on prediction. The data used is \"US 3-month Treasury bill rates\", which falls under government bonds and the yield curve."}, "mode": "threadpool", "paper_id": "ec360be29d8293a3", "model": "gemini-2.5-flash-lite"}
